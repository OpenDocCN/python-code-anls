# `.\graphrag\graphrag\query\question_gen\local_gen.py`

```py
# 引入必要的日志记录模块
import logging
# 引入时间模块
import time
# 引入类型提示工具
from typing import Any

# 引入自定义模块 tiktoken
import tiktoken

# 从 graphrag.query.context_builder.builders 中引入 LocalContextBuilder 类
from graphrag.query.context_builder.builders import LocalContextBuilder
# 从 graphrag.query.context_builder.conversation_history 中引入 ConversationHistory 类
from graphrag.query.context_builder.conversation_history import (
    ConversationHistory,
)
# 从 graphrag.query.llm.base 中引入 BaseLLM 和 BaseLLMCallback 类
from graphrag.query.llm.base import BaseLLM, BaseLLMCallback
# 从 graphrag.query.llm.text_utils 中引入 num_tokens 函数
from graphrag.query.llm.text_utils import num_tokens
# 从 graphrag.query.question_gen.base 中引入 BaseQuestionGen 和 QuestionResult 类
from graphrag.query.question_gen.base import BaseQuestionGen, QuestionResult
# 从 graphrag.query.question_gen.system_prompt 中引入 QUESTION_SYSTEM_PROMPT 常量
from graphrag.query.question_gen.system_prompt import QUESTION_SYSTEM_PROMPT

# 设置日志记录器，使用当前模块的名称
log = logging.getLogger(__name__)

# 定义 LocalQuestionGen 类，继承自 BaseQuestionGen 类
class LocalQuestionGen(BaseQuestionGen):
    """Search orchestration for global search mode."""

    # 初始化方法，接受多个参数，包括 llm、context_builder 等
    def __init__(
        self,
        llm: BaseLLM,
        context_builder: LocalContextBuilder,
        token_encoder: tiktoken.Encoding | None = None,
        system_prompt: str = QUESTION_SYSTEM_PROMPT,
        callbacks: list[BaseLLMCallback] | None = None,
        llm_params: dict[str, Any] | None = None,
        context_builder_params: dict[str, Any] | None = None,
    ):
        # 调用父类的初始化方法，传入部分参数
        super().__init__(
            llm=llm,
            context_builder=context_builder,
            token_encoder=token_encoder,
            llm_params=llm_params,
            context_builder_params=context_builder_params,
        )
        # 设置系统提示信息
        self.system_prompt = system_prompt
        # 设置回调函数列表
        self.callbacks = callbacks

    # 异步方法，生成问题的主要逻辑
    async def agenerate(
        self,
        question_history: list[str],
        context_data: str | None,
        question_count: int,
        **kwargs,
    ) -> QuestionResult:
        """
        Generate a question based on the question history and context data.

        If context data is not provided, it will be generated by the local context builder
        """
        start_time = time.time()  # 记录函数开始执行的时间戳

        if len(question_history) == 0:
            question_text = ""  # 如果问题历史为空，则问题文本为空字符串
            conversation_history = None  # 对话历史设置为None
        else:
            # 构建当前查询和对话历史
            question_text = question_history[-1]  # 获取最后一个问题文本
            history = [
                {"role": "user", "content": query} for query in question_history[:-1]
            ]  # 构建用户查询历史列表
            conversation_history = ConversationHistory.from_list(history)  # 创建对话历史对象

        if context_data is None:
            # 如果未提供上下文数据，则基于问题历史生成上下文数据
            context_data, context_records = self.context_builder.build_context(
                query=question_text,
                conversation_history=conversation_history,
                **kwargs,
                **self.context_builder_params,
            )  # 调用上下文生成器的方法来生成上下文数据
        else:
            context_records = {"context_data": context_data}  # 设置上下文记录为给定的上下文数据

        log.info("GENERATE QUESTION: %s. LAST QUESTION: %s", start_time, question_text)
        system_prompt = ""
        try:
            system_prompt = self.system_prompt.format(
                context_data=context_data, question_count=question_count
            )  # 根据上下文数据和问题计数格式化系统提示文本
            question_messages = [
                {"role": "system", "content": system_prompt},  # 系统角色的消息
                {"role": "user", "content": question_text},  # 用户角色的消息
            ]

            response = await self.llm.agenerate(
                messages=question_messages,
                streaming=True,
                callbacks=self.callbacks,
                **self.llm_params,
            )  # 调用语言模型生成响应

            return QuestionResult(
                response=response.split("\n"),  # 将响应按行拆分为列表
                context_data={
                    "question_context": question_text,
                    **context_records,
                },  # 返回生成的问题结果和上下文数据
                completion_time=time.time() - start_time,  # 计算函数执行时间
                llm_calls=1,  # 记录语言模型调用次数
                prompt_tokens=num_tokens(system_prompt, self.token_encoder),  # 计算系统提示的标记数
            )

        except Exception:
            log.exception("Exception in generating question")  # 记录异常日志
            return QuestionResult(
                response=[],  # 返回空响应列表
                context_data=context_records,  # 返回上下文数据
                completion_time=time.time() - start_time,  # 计算函数执行时间
                llm_calls=1,  # 记录语言模型调用次数
                prompt_tokens=num_tokens(system_prompt, self.token_encoder),  # 计算系统提示的标记数
            )

    def generate(
        self,
        question_history: list[str],  # 问题历史列表参数
        context_data: str | None,  # 上下文数据字符串或None
        question_count: int,  # 问题计数整数
        **kwargs,  # 其他关键字参数
    ) -> QuestionResult:
        """
        Generate a question based on the question history and context data.

        If context data is not provided, it will be generated by the local context builder
        """
        start_time = time.time()  # 记录函数开始时间

        if len(question_history) == 0:
            question_text = ""  # 如果问题历史为空，则问题文本为空字符串
            conversation_history = None  # 对话历史设为None
        else:
            # 构建当前查询和对话历史
            question_text = question_history[-1]
            history = [
                {"role": "user", "content": query} for query in question_history[:-1]
            ]
            conversation_history = ConversationHistory.from_list(history)

        if context_data is None:
            # 根据问题历史生成上下文数据
            context_data, context_records = self.context_builder.build_context(
                query=question_text,
                conversation_history=conversation_history,
                **kwargs,
                **self.context_builder_params,
            )  # type: ignore
        else:
            context_records = {"context_data": context_data}  # 直接使用提供的上下文数据
        log.info(
            "GENERATE QUESTION: %s. QUESTION HISTORY: %s", start_time, question_text
        )  # 记录生成问题的信息，包括开始时间和问题文本
        system_prompt = ""
        try:
            system_prompt = self.system_prompt.format(
                context_data=context_data, question_count=question_count
            )  # 根据上下文数据和问题计数格式化系统提示
            question_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question_text},
            ]  # 创建包含系统提示和用户问题文本的消息列表

            response = self.llm.generate(
                messages=question_messages,
                streaming=True,
                callbacks=self.callbacks,
                **self.llm_params,
            )  # 使用语言模型生成响应

            return QuestionResult(
                response=response.split("\n"),  # 将响应按行拆分为列表
                context_data={
                    "question_context": question_text,
                    **context_records,
                },  # 返回生成问题的上下文数据和记录
                completion_time=time.time() - start_time,  # 计算完成生成问题所花费的时间
                llm_calls=1,  # 记录调用语言模型的次数
                prompt_tokens=num_tokens(system_prompt, self.token_encoder),  # 统计系统提示的token数
            )

        except Exception:
            log.exception("Exception in generating questions")  # 记录生成问题时的异常信息
            return QuestionResult(
                response=[],  # 返回空响应列表
                context_data=context_records,  # 返回上下文记录
                completion_time=time.time() - start_time,  # 计算完成的时间
                llm_calls=1,  # 记录调用语言模型的次数
                prompt_tokens=num_tokens(system_prompt, self.token_encoder),  # 统计系统提示的token数
            )
```