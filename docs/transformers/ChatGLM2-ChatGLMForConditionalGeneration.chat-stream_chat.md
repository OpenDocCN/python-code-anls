<!--yml
category: æœªåˆ†ç±»
date: 2023-10-10 23:17:51
-->

# ChatGLM2 æºç åˆ†æï¼š`ChatGLMForConditionalGeneration.chat, .stream_chat`_ç»ä¸åŸåˆ›çš„é£é¾™çš„åšå®¢-CSDNåšå®¢

> æ¥æºï¼š[https://blog.csdn.net/wizardforcel/article/details/132841359](https://blog.csdn.net/wizardforcel/article/details/132841359)

### `.chat`

è°ƒç”¨åˆ†æï¼š

```
In [1]: q = 'ä½ å¥½'

In [2]: r, his = model.chat(tokenizer, q)

In [3]: r
Out[3]: 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'

In [4]: his
Out[4]: [('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚')]

In [5]: q = 'ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ'

In [6]: r, his = model.chat(tokenizer, q, his)

In [7]: r
Out[7]: 'æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘å¯ä»¥ï¼š\n\n1\.  å›ç­”é—®é¢˜ï¼šåƒäººç±»ä¸€æ ·å›ç­”æ‚¨çš„é—®é¢˜ï¼Œæˆ–è€…æä¾› ç›¸å…³ä¿¡æ¯ã€‚\n\n2\.  æä¾›å»ºè®®ï¼šæ ¹æ®æ‚¨çš„é—®é¢˜æä¾›ä¸€äº›å»ºè®®ï¼Œæˆ–è€…æä¾›ä¸€äº›å‚è€ƒä¿¡æ¯ã€‚\n\n3\.  è¿›è¡Œç¿»è¯‘ï¼šå°†ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ï¼Œæˆ–è€…å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ã€‚\n\n4\.  ç”Ÿæˆæ–‡æœ¬ï¼šæ ¹æ®æ‚¨çš„é—®é¢˜ç”Ÿæˆä¸€äº›æ–‡æœ¬ï¼Œæ¯”å¦‚æ–‡ç« ã€æ•…äº‹ã€æ–°é—»æŠ¥é“ç­‰ã€‚\n\n5\.  è‡ªåŠ¨æ–‡æœ¬æ‘˜è¦ï¼šè‡ªåŠ¨æ¦‚æ‹¬æ–‡æœ¬çš„å†…å®¹ï¼Œå¹¶ç”Ÿæˆæ‘˜è¦ã€‚\n\n6\.  æƒ…æ„Ÿåˆ†æï¼šåˆ¤æ–­æ–‡æœ¬ä¸­æƒ…æ„Ÿçš„ç¨‹åº¦ï¼Œå¹¶è¿”å›ç›¸åº”çš„æƒ…æ„Ÿä¿¡æ¯ã€‚\n\n7\.  æ™ºèƒ½å¯¹è¯ï¼šè¿›è¡Œæ™ºèƒ½å¯¹è¯ï¼Œä¸äººç±»äº¤æµå¹¶å®Œæˆä»»åŠ¡ã€‚\n\nè¯·æ³¨æ„ï¼Œæˆ‘æ˜¯ä¸€ä¸ªæœºå™¨ï¼Œæˆ‘çš„å›ç­”å¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼Œä¹Ÿå¯èƒ½ä¼šæœ‰æ‰€è¯¯å¯¼ã€‚'

In [8]: his
Out[8]:
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'),
 ('ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ',
  'æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘å¯ä»¥ï¼š\n\n1\.  å›ç­”é—®é¢˜ï¼šåƒäººç±»ä¸€æ ·å›ç­”æ‚¨çš„é—®é¢˜ï¼Œæˆ–è€…æä¾›ç›¸å…³ä¿¡æ¯ ã€‚\n\n2\.  æä¾›å»ºè®®ï¼šæ ¹æ®æ‚¨çš„é—®é¢˜æä¾›ä¸€äº›å»ºè®®ï¼Œæˆ–è€…æä¾›ä¸€äº›å‚è€ƒä¿¡æ¯ã€‚\n\n3\.  è¿›è¡Œç¿»è¯‘ï¼šå°†ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ï¼Œæˆ–è€…å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ã€‚\n\n4\.  ç”Ÿæˆæ–‡æœ¬ï¼šæ ¹æ®æ‚¨çš„é—®é¢˜ç”Ÿæˆä¸€äº›æ–‡æœ¬ï¼Œæ¯”å¦‚æ–‡ç« ã€æ•…äº‹ã€æ–°é—»æŠ¥é“ç­‰ã€‚\n\n5\.  è‡ªåŠ¨æ–‡æœ¬æ‘˜è¦ï¼šè‡ªåŠ¨æ¦‚æ‹¬æ–‡æœ¬çš„å†…å®¹ï¼Œå¹¶ç”Ÿæˆæ‘˜è¦ã€‚\n\n6\.  æƒ…æ„Ÿåˆ†æï¼šåˆ¤æ–­æ–‡æœ¬ä¸­æƒ…æ„Ÿçš„ç¨‹åº¦ï¼Œå¹¶è¿”å›ç›¸åº”çš„æƒ…æ„Ÿä¿¡æ¯ã€‚\n\n7\.  æ™ºèƒ½å¯¹è¯ï¼šè¿›è¡Œæ™ºèƒ½å¯¹è¯ï¼Œä¸äººç±»äº¤æµå¹¶å®Œæˆä»»åŠ¡ã€‚\n\nè¯·æ³¨æ„ï¼Œæˆ‘æ˜¯ä¸€ä¸ªæœºå™¨ï¼Œæˆ‘çš„å›ç­”å¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼Œä¹Ÿå¯èƒ½ä¼šæœ‰æ‰€è¯¯å¯¼ã€‚')] 
```py

æºç ï¼š

```
 @torch.inference_mode()
    def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, max_length: int = 8192, num_beams=1,
             do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None, **kwargs):
        if history is None:
            history = []
        if logits_processor is None:
            logits_processor = LogitsProcessorList()
        logits_processor.append(InvalidScoreLogitsProcessor())

        gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "do_sample": do_sample, "top_p": top_p,
                      "temperature": temperature, "logits_processor": logits_processor, **kwargs}

        inputs = self.build_inputs(tokenizer, query, history=history)

        outputs = self.generate(**inputs, **gen_kwargs)

        '''
        prompt: 'ä½ å¥½, output: tensor([[64790, 64792,   790, 30951,   517, 30910, 30939, 30996,    13,    13,
         54761, 31211, 39701,    13,    13, 55437, 31211, 36474, 54591,   243,
           162,   148,   142, 31404, 33030, 34797, 42481, 22011, 10461, 30944,
         30943, 30941, 30978, 30949, 31123, 48895, 35214, 54622, 31123, 32616,
         39905, 31901, 31639, 31155,     2]], device='cuda:0')
        tokenizer.decode(output[0]): '[Round 1]\n\né—®ï¼šä½ å¥½\n\nç­”ï¼š ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'
        '''
        outputs = outputs.tolist()[0][len(inputs["input_ids"][0]):]

        response = tokenizer.decode(outputs)

        response = self.process_response(response)

        history = history + [(query, response)]
        return response, history

    def build_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):
        '''
        å°†å†å²é—®ç­”å’Œå½“å‰æé—®ç»„è£…æˆæ•´ä¸ªè¾“å…¥
        In [1]: tokenizer.build_prompt('Q3', [('Q1', 'A1'),('Q2', 'A2')])
        Out[1]: '[Round 1]\n\né—®ï¼šQ1\n\nç­”ï¼šA1\n\n[Round 2]\n\né—®ï¼šQ2\n\nç­”ï¼šA2\n\n[Round 3]\n\né—®ï¼šQ3\n\nç­”ï¼š'
        '''
        prompt = tokenizer.build_prompt(query, history=history)
        '''
        æ•´ä¸ªæé—®ä¼ ç»™åˆ†è¯å™¨å¾—åˆ°å•è¯ID
        In [2]: tokenizer(['ä½ å¥½'], return_tensors="pt")
        Out[2]: {
           'input_ids': tensor([[64790, 64792, 36474, 54591]]), 
           'attention_mask': tensor([[1, 1, 1, 1]]), 
           'position_ids': tensor([[0, 1, 2, 3]])
        }
        '''
        inputs = tokenizer([prompt], return_tensors="pt")
        inputs = inputs.to(self.device)
        return inputs 
```py

### `.stream_chat`

è°ƒç”¨åˆ†æï¼š

```
In [133]: q = 'ä½ å¥½'

In [134]: it = model.stream_chat(tokenizer, q)

In [135]: for r, his in it: print(r); print(his)
ä½ 
[('ä½ å¥½', 'ä½ ')]
ä½ å¥½
[('ä½ å¥½', 'ä½ å¥½')]
ä½ å¥½ğŸ‘‹
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹')]
...
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜')]
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚')]
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚')]

In [136]: q = 'ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ'

In [137]: it = model.stream_chat(tokenizer, q, his)

In [138]: for r, his in it: print(r); print(his)
æˆ‘
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ', 'æˆ‘')]
æˆ‘æ˜¯ä¸€æ¬¾
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ', 'æˆ‘æ˜¯ä¸€æ¬¾')]
æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ', 'æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹')]
...
æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆï¼Œä»¥åŠæä¾›å„ç§æœåŠ¡å’Œå’¨è¯¢ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©äººä»¬æ›´æ–¹ä¾¿ã€é«˜æ•ˆåœ°è·å–ä¿¡æ¯ã€è§£å†³é—®é¢˜å’Œäº¤æµæ²Ÿé€š
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ', 'æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆï¼Œä»¥åŠæä¾›å„ç§æœåŠ¡å’Œå’¨è¯¢ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©äººä»¬æ›´æ–¹ä¾¿ã€é«˜æ•ˆåœ°è·å–ä¿¡æ¯ã€è§£å†³é—®é¢˜å’Œäº¤æµæ²Ÿé€š')]
æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆï¼Œä»¥åŠæä¾›å„ç§æœåŠ¡å’Œå’¨è¯¢ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©äººä»¬æ›´æ–¹ä¾¿ã€é«˜æ•ˆåœ°è·å–ä¿¡æ¯ã€è§£å†³é—®é¢˜å’Œäº¤æµæ²Ÿé€šã€‚
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ', 'æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆï¼Œä»¥åŠæä¾›å„ç§æœåŠ¡å’Œå’¨è¯¢ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©äººä»¬æ›´æ–¹ä¾¿ã€é«˜æ•ˆåœ°è·å–ä¿¡æ¯ã€è§£å†³é—®é¢˜å’Œäº¤æµæ²Ÿé€šã€‚')]
æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆï¼Œä»¥åŠæä¾›å„ç§æœåŠ¡å’Œå’¨è¯¢ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©äººä»¬æ›´æ–¹ä¾¿ã€é«˜æ•ˆåœ°è·å–ä¿¡æ¯ã€è§£å†³é—®é¢˜å’Œäº¤æµæ²Ÿé€šã€‚
[('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('ä½ å¯ä»¥åšä»€ä¹ˆï¼Ÿ', 'æˆ‘æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆï¼Œä»¥åŠæä¾›å„ç§æœåŠ¡å’Œå’¨è¯¢ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©äººä»¬æ›´æ–¹ä¾¿ã€é«˜æ•ˆåœ°è·å–ä¿¡æ¯ã€è§£å†³é—®é¢˜å’Œäº¤æµæ²Ÿé€šã€‚')] 
```py

æºç ï¼š

```
 @torch.inference_mode()
    def stream_chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, past_key_values=None,
                    max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None,
                    return_past_key_values=False, **kwargs):

        if history is None:
            history = []
        if logits_processor is None:
            logits_processor = LogitsProcessorList()
        logits_processor.append(InvalidScoreLogitsProcessor())
        gen_kwargs = {"max_length": max_length, "do_sample": do_sample, "top_p": top_p,
                      "temperature": temperature, "logits_processor": logits_processor, **kwargs}
        if past_key_values is None and not return_past_key_values:

            inputs = self.build_inputs(tokenizer, query, history=history)
        else:

            inputs = self.build_stream_inputs(tokenizer, query, history=history)
        if past_key_values is not None:

            past_length = past_key_values[0][0].shape[0]

            if self.transformer.pre_seq_len is not None:
                past_length -= self.transformer.pre_seq_len

            inputs.position_ids += past_length

            attention_mask = inputs.attention_mask
            attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)
            inputs['attention_mask'] = attention_mask
        for outputs in self.stream_generate(**inputs, past_key_values=past_key_values,
                                            return_past_key_values=return_past_key_values, **gen_kwargs):
            if return_past_key_values:
                outputs, past_key_values = outputs

            outputs = outputs.tolist()[0][len(inputs["input_ids"][0]):]
            '''
            q: 'ä½ å¥½'
            iter1 response: 'ä½ '
            iter2 response: 'ä½ å¥½'
            ...
            iterN response: 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'
            '''
            response = tokenizer.decode(outputs)

            if response and response[-1] != "ï¿½":

                response = self.process_response(response)

                new_history = history + [(query, response)]
                if return_past_key_values:
                    yield response, new_history, past_key_values
                else:
                    yield response, new_history

    def build_stream_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):

        if history:

            prompt = "\n\n[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š".format(len(history) + 1, query)
            '''
            å°† prompt è½¬æˆå•è¯ IDï¼Œå»æ‰å¼€å¤´çš„ ID64790ã€ID64792
            In [147]: tokenizer.encode('\n\nä½ å¥½', add_special_tokens=False)
            Out[147]: [30910, 13, 13, 39701]
            In [149]: tokenizer.encode('\n\nä½ å¥½')
            Out[149]: [64790, 64792, 30910, 13, 13, 39701]
            '''
            input_ids = tokenizer.encode(prompt, add_special_tokens=False)

            input_ids = input_ids[1:]
            '''
            ä¸º input_ids ç”Ÿæˆç›¸åº”çš„ attention_mask å’Œ position_ids
            In [151]: tokenizer.batch_encode_plus(
                [([13,13,39701], None)], 
                return_tensors="pt", 
                add_special_tokens=False
            )
            Out[151]: {
                'input_ids': tensor([[   13,    13, 39701]]), 
                'attention_mask': tensor([[1, 1, 1]]), 
                'position_ids': tensor([[0, 1, 2]])
            }
            '''
            inputs = tokenizer.batch_encode_plus([(input_ids, None)], return_tensors="pt", add_special_tokens=False)
        else:

            prompt = "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š".format(len(history) + 1, query)
            inputs = tokenizer([prompt], return_tensors="pt")
        inputs = inputs.to(self.device)
        return inputs 
```