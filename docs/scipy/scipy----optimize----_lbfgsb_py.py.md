# `D:\src\scipysrc\scipy\scipy\optimize\_lbfgsb_py.py`

```
"""
Functions
---------
.. autosummary::
   :toctree: generated/

    fmin_l_bfgs_b

"""

## License for the Python wrapper
## ==============================

## Copyright (c) 2004 David M. Cooke <cookedm@physics.mcmaster.ca>

## Permission is hereby granted, free of charge, to any person obtaining a
## copy of this software and associated documentation files (the "Software"),
## to deal in the Software without restriction, including without limitation
## the rights to use, copy, modify, merge, publish, distribute, sublicense,
## and/or sell copies of the Software, and to permit persons to whom the
## Software is furnished to do so, subject to the following conditions:

## The above copyright notice and this permission notice shall be included in
## all copies or substantial portions of the Software.

## THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
## IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
## FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
## AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
## LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
## FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
## DEALINGS IN THE SOFTWARE.

## Modifications by Travis Oliphant and Enthought, Inc. for inclusion in SciPy

import numpy as np
from numpy import array, asarray, float64, zeros
from . import _lbfgsb
from ._optimize import (MemoizeJac, OptimizeResult, _call_callback_maybe_halt,
                        _wrap_callback, _check_unknown_options,
                        _prepare_scalar_function)
from ._constraints import old_bound_to_new

from scipy.sparse.linalg import LinearOperator

__all__ = ['fmin_l_bfgs_b', 'LbfgsInvHessProduct']


def fmin_l_bfgs_b(func, x0, fprime=None, args=(),
                  approx_grad=0,
                  bounds=None, m=10, factr=1e7, pgtol=1e-5,
                  epsilon=1e-8,
                  iprint=-1, maxfun=15000, maxiter=15000, disp=None,
                  callback=None, maxls=20):
    """
    Minimize a function func using the L-BFGS-B algorithm.

    Parameters
    ----------
    func : callable f(x,*args)
        Function to minimize.
    x0 : ndarray
        Initial guess.
    fprime : callable fprime(x,*args), optional
        The gradient of `func`. If None, then `func` returns the function
        value and the gradient (``f, g = func(x, *args)``), unless
        `approx_grad` is True in which case `func` returns only ``f``.
    args : sequence, optional
        Arguments to pass to `func` and `fprime`.
    approx_grad : bool, optional
        Whether to approximate the gradient numerically (in which case
        `func` returns only the function value).
    bounds : list, optional
        Bounds for variables (only used if `fprime` is None).
    m : int, optional
        The maximum number of variable metric corrections used to define
        the limited memory matrix. (The limited memory BFGS method does not
        store the full Hessian but uses this many terms in an approximation
        to it.)
    factr : float, optional
        The iteration stops when ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1}
        <= factr * eps`` where ``eps`` is the machine precision, which is
        automatically generated by the code. Typical values for `factr`
        are: 1e12 for low accuracy; 1e7 for moderate accuracy;
        10.0 for extremely high accuracy. See Notes for relationship to `pgtol`.
    pgtol : float, optional
        The iteration will stop when ``max{|proj g_i | i = 1, ..., n}
        <= pgtol`` where ``pg_i`` is the i-th component of the projected
        gradient. See Notes for relationship to `factr`.
    epsilon : float, optional
        Step size used when `approx_grad` is True, for numerical
        approximation to the gradient.
    iprint : int, optional
        Controls the frequency of output. ``iprint < 0`` means no output;
        ``iprint = 0`` means only error messages; ``iprint = 1`` means
        normal output. ``iprint = 2`` means extra output.
    maxfun : int, optional
        Maximum number of function evaluations before termination.
    maxiter : int, optional
        Maximum number of iterations.
    disp : int, optional
        If `disp` is True, the convergence report is printed. If False,
        no output is printed.
    callback : callable, optional
        Called after each iteration, as ``callback(xk)``, where ``xk`` is the
        current parameter vector.
    maxls : int, optional
        Maximum number of line search steps (per iteration). Increase if the
        algorithm is terminated prematurely due to reaching the maximum
        number of line search steps.

"""
    bounds : list, optional
        # 参数的边界条件，可选项
        ``(min, max)`` pairs for each element in ``x``, defining
        # 每个元素在 ``x`` 中的边界 ``(min, max)`` 对，定义了参数的取值范围
        the bounds on that parameter. Use None or +-inf for one of ``min`` or
        # 在该参数的一个方向上使用 None 或 +-inf 表示无限大或无限小
        ``max`` when there is no bound in that direction.

    m : int, optional
        # 变量度量校正的最大次数，可选项
        The maximum number of variable metric corrections
        # 变量度量 BFGS 方法用来定义有限记忆矩阵的最大校正次数

    factr : float, optional
        # 迭代停止条件的因子，可选项
        The iteration stops when
        # 当 ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps`` 时迭代停止，
        # 其中 ``eps`` 是机器精度，由代码自动生成
        Typical values for `factr` are: 1e12 for low accuracy; 1e7 for moderate accuracy; 10.0 for extremely high accuracy.
        # `factr` 的典型值：1e12 表示低精度；1e7 表示中等精度；10.0 表示极高精度

    pgtol : float, optional
        # 迭代停止条件的梯度投影最大值，可选项
        The iteration will stop when
        # 当 ``max{|proj g_i | i = 1, ..., n} <= pgtol`` 时迭代停止，
        # 其中 ``proj g_i`` 是投影梯度的第 i 个分量

    epsilon : float, optional
        # 当 `approx_grad` 为 True 时使用的步长，用于数值计算梯度
        Step size used when `approx_grad` is True, for numerically
        # 在 `approx_grad` 为 True 时使用的步长，用于数值计算梯度

    iprint : int, optional
        # 控制输出频率的参数，可选项
        Controls the frequency of output.
        # 控制输出频率，值的含义：``iprint < 0`` 表示无输出；``iprint = 0`` 表示仅在最后一次迭代输出一行；
        # ``0 < iprint < 99`` 表示每 iprint 次迭代输出 f 和 ``|proj g|``；
        # ``iprint = 99`` 表示输出每次迭代的详细信息，除了 n-vectors；
        # ``iprint = 100`` 表示输出活动集合的变化和最终 x 的详细信息；
        # ``iprint > 100`` 表示输出每次迭代的详细信息，包括 x 和 g。

    disp : int, optional
        # 控制输出级别的参数，可选项
        If zero, then no output. If a positive number, then this over-rides `iprint`.
        # 若为零，则不输出。若为正数，则覆盖 `iprint` 参数的输出级别。

    maxfun : int, optional
        # 最大函数评估次数，可选项
        Maximum number of function evaluations.
        # 函数评估的最大次数，注意由于数值微分可能会超过该限制。

    maxiter : int, optional
        # 最大迭代次数，可选项
        Maximum number of iterations.
        # 迭代的最大次数。

    callback : callable, optional
        # 每次迭代后调用的回调函数，可选项
        Called after each iteration, as ``callback(xk)``, where ``xk`` is the current parameter vector.
        # 每次迭代后调用的回调函数，参数为当前的参数向量 ``xk``。

    maxls : int, optional
        # 最大线搜索步数（每次迭代），默认为 20
        Maximum number of line search steps (per iteration). Default is 20.

    Returns
    -------
    x : array_like
        # 最小值的估计位置，返回值为数组形式
        Estimated position of the minimum.

    f : float
        # 在最小值处的函数值
        Value of `func` at the minimum.
    d : dict
        Information dictionary.

        * d['warnflag'] is

          - 0 if converged,
          - 1 if too many function evaluations or too many iterations,
          - 2 if stopped for another reason, given in d['task']

        * d['grad'] is the gradient at the minimum (should be 0 ish)
        * d['funcalls'] is the number of function calls made.
        * d['nit'] is the number of iterations.


    See also
    --------
    minimize: Interface to minimization algorithms for multivariate
        functions. See the 'L-BFGS-B' `method` in particular. Note that the
        `ftol` option is made available via that interface, while `factr` is
        provided via this interface, where `factr` is the factor multiplying
        the default machine floating-point precision to arrive at `ftol`:
        ``ftol = factr * numpy.finfo(float).eps``.


    Notes
    -----
    License of L-BFGS-B (FORTRAN code):

    The version included here (in fortran code) is 3.0
    (released April 25, 2011). It was written by Ciyou Zhu, Richard Byrd,
    and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following
    condition for use:

    This software is freely available, but we expect that all publications
    describing work using this software, or all commercial products using it,
    quote at least one of the references given below. This software is released
    under the BSD License.


    References
    ----------
    * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound
      Constrained Optimization, (1995), SIAM Journal on Scientific and
      Statistical Computing, 16, 5, pp. 1190-1208.
    * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,
      FORTRAN routines for large scale bound constrained optimization (1997),
      ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.
    * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,
      FORTRAN routines for large scale bound constrained optimization (2011),
      ACM Transactions on Mathematical Software, 38, 1.


    Examples
    --------
    Solve a linear regression problem via `fmin_l_bfgs_b`. To do this, first we
    define an objective function ``f(m, b) = (y - y_model)**2``, where `y`
    describes the observations and `y_model` the prediction of the linear model
    as ``y_model = m*x + b``. The bounds for the parameters, ``m`` and ``b``,
    are arbitrarily chosen as ``(0,5)`` and ``(5,10)`` for this example.

    >>> import numpy as np
    >>> from scipy.optimize import fmin_l_bfgs_b
    >>> X = np.arange(0, 10, 1)
    >>> M = 2
    >>> B = 3
    >>> Y = M * X + B
    >>> def func(parameters, *args):
    ...     x = args[0]
    ...     y = args[1]
    ...     m, b = parameters
    ...     y_model = m*x + b
    ...     error = sum(np.power((y - y_model), 2))
    ...     return error

    >>> initial_values = np.array([0.0, 1.0])
    # 使用 L-BFGS-B 方法进行函数最小化优化，返回优化后的最优参数 x_opt、目标函数值 f_opt 和详细信息 info
    >>> x_opt, f_opt, info = fmin_l_bfgs_b(func, x0=initial_values, args=(X, Y),
    ...                                    approx_grad=True)
    
    # 打印优化后的最优参数 x_opt 和目标函数值 f_opt
    >>> x_opt, f_opt
    array([1.99999999, 3.00000006]), 1.7746231151323805e-14  # 可能会有微小的变化

    # 确认优化后的参数 x_opt 与真实参数 m 和 b 一致。接下来，使用 bounds 参数进行约束优化。
    The optimized parameters in ``x_opt`` agree with the ground truth parameters
    ``m`` and ``b``. Next, let us perform a bound constrained optimization using
    the `bounds` parameter.

    # 定义变量 bounds 作为优化的边界条件
    >>> bounds = [(0, 5), (5, 10)]
    
    # 使用 L-BFGS-B 方法进行约束优化，返回优化后的最优参数 x_opt、目标函数值 f_opt 和详细信息 info
    >>> x_opt, f_op, info = fmin_l_bfgs_b(func, x0=initial_values, args=(X, Y),
    ...                                   approx_grad=True, bounds=bounds)
    
    # 打印优化后的最优参数 x_opt 和目标函数值 f_opt
    >>> x_opt, f_opt
    array([1.65990508, 5.31649385]), 15.721334516453945  # 可能会有微小的变化    
    """
    # 处理 fprime/approx_grad 参数
    if approx_grad:
        # 如果 approx_grad 为 True，则使用 func 作为目标函数，不使用雅可比矩阵
        fun = func
        jac = None
    elif fprime is None:
        # 如果 fprime 为 None，则使用 MemoizeJac 类封装 func，并使用其导数作为雅可比矩阵
        fun = MemoizeJac(func)
        jac = fun.derivative
    else:
        # 否则，使用给定的 fprime 作为雅可比矩阵
        fun = func
        jac = fprime

    # 构建优化选项的字典 opts
    callback = _wrap_callback(callback)  # 将回调函数 callback 进行包装
    opts = {'disp': disp,                # 控制优化过程中是否显示信息
            'iprint': iprint,            # 控制输出级别
            'maxcor': m,                 # L-BFGS 方法中存储最近的几次梯度和步长的对数
            'ftol': factr * np.finfo(float).eps,  # 控制优化过程中的终止精度
            'gtol': pgtol,               # 控制梯度的终止精度
            'eps': epsilon,              # 控制数值梯度计算时的步长
            'maxfun': maxfun,            # 控制最大函数调用次数
            'maxiter': maxiter,          # 控制最大迭代次数
            'callback': callback,        # 设置优化过程中的回调函数
            'maxls': maxls}              # 控制每次迭代中最大的线搜索次数

    # 调用 _minimize_lbfgsb 函数进行 L-BFGS-B 方法的优化
    res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,
                           **opts)

    # 构建返回结果的字典 d
    d = {'grad': res['jac'],             # 最终优化点的梯度
         'task': res['message'],         # 优化结束时的状态信息
         'funcalls': res['nfev'],        # 函数调用次数
         'nit': res['nit'],              # 迭代次数
         'warnflag': res['status']}     # 警告标志位

    f = res['fun']                       # 最优值对应的函数值
    x = res['x']                         # 最优解

    return x, f, d
# 使用 L-BFGS-B 算法最小化一个或多个变量的标量函数。
def _minimize_lbfgsb(fun, x0, args=(), jac=None, bounds=None,
                     disp=None, maxcor=10, ftol=2.2204460492503131e-09,
                     gtol=1e-5, eps=1e-8, maxfun=15000, maxiter=15000,
                     iprint=-1, callback=None, maxls=20,
                     finite_diff_rel_step=None, **unknown_options):
    """
    Minimize a scalar function of one or more variables using the L-BFGS-B
    algorithm.

    Options
    -------
    disp : None or int
        If `disp is None` (the default), then the supplied version of `iprint`
        is used. If `disp is not None`, then it overrides the supplied version
        of `iprint` with the behaviour you outlined.
    maxcor : int
        The maximum number of variable metric corrections used to
        define the limited memory matrix. (The limited memory BFGS
        method does not store the full hessian but uses this many terms
        in an approximation to it.)
    ftol : float
        The iteration stops when ``(f^k -
        f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.
    gtol : float
        The iteration will stop when ``max{|proj g_i | i = 1, ..., n}
        <= gtol`` where ``proj g_i`` is the i-th component of the
        projected gradient.
    eps : float or ndarray
        If `jac is None` the absolute step size used for numerical
        approximation of the jacobian via forward differences.
    maxfun : int
        Maximum number of function evaluations. Note that this function
        may violate the limit because of evaluating gradients by numerical
        differentiation.
    maxiter : int
        Maximum number of iterations.
    iprint : int, optional
        Controls the frequency of output. ``iprint < 0`` means no output;
        ``iprint = 0``    print only one line at the last iteration;
        ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;
        ``iprint = 99``   print details of every iteration except n-vectors;
        ``iprint = 100``  print also the changes of active set and final x;
        ``iprint > 100``  print details of every iteration including x and g.
    maxls : int, optional
        Maximum number of line search steps (per iteration). Default is 20.
    finite_diff_rel_step : None or array_like, optional
        If ``jac in ['2-point', '3-point', 'cs']`` the relative step size to
        use for numerical approximation of the jacobian. The absolute step
        size is computed as ``h = rel_step * sign(x) * max(1, abs(x))``,
        possibly adjusted to fit into the bounds. For ``method='3-point'``
        the sign of `h` is ignored. If None (default) then step is selected
        automatically.

    Notes
    -----
    The option `ftol` is exposed via the `scipy.optimize.minimize` interface,
    but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The
    relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.
    """
    # 检查并处理未知选项
    _check_unknown_options(unknown_options)
    
    # 将 maxcor 赋值给 m
    m = maxcor
    
    # 将 gtol 赋值给 pgtol
    pgtol = gtol
    
    # 计算 factr，将 ftol 除以机器浮点数精度的 eps 常量
    factr = ftol / np.finfo(float).eps

    # 将 x0 转换为一个一维数组
    x0 = asarray(x0).ravel()
    
    # 获取 x0 的长度 n
    n, = x0.shape

    # 如果 bounds 为 None，则不做处理；否则检查 bounds 的长度是否与 x0 相等，并将其转换为新式边界表示
    if bounds is None:
        pass
    elif len(bounds) != n:
        raise ValueError('length of x0 != length of bounds')
    else:
        bounds = np.array(old_bound_to_new(bounds))

        # 检查边界
        if (bounds[0] > bounds[1]).any():
            raise ValueError(
                "LBFGSB - one of the lower bounds is greater than an upper bound."
            )

        # 将初始向量 x0 截断到边界内，以避免 ScalarFunction 和 approx_derivative 引发问题
        x0 = np.clip(x0, bounds[0], bounds[1])

    # 根据 disp 的设置确定 iprint 的值，控制输出详细程度
    if disp is not None:
        if disp == 0:
            iprint = -1
        else:
            iprint = disp

    # 准备标量函数 sf，根据参数设置包括边界信息
    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,
                                  bounds=bounds,
                                  finite_diff_rel_step=finite_diff_rel_step)

    # 获取标量函数和梯度的结合体
    func_and_grad = sf.fun_and_grad

    # 获取 Fortran 标准整数类型
    fortran_int = _lbfgsb.types.intvar.dtype

    # 初始化边界信息的数据结构
    nbd = zeros(n, fortran_int)
    low_bnd = zeros(n, float64)
    upper_bnd = zeros(n, float64)

    # 定义边界映射，将 (l, u) 映射为整数
    bounds_map = {(-np.inf, np.inf): 0,
                  (1, np.inf): 1,
                  (1, 1): 2,
                  (-np.inf, 1): 3}

    # 如果 bounds 不为 None，则根据边界设定 nbd、low_bnd 和 upper_bnd
    if bounds is not None:
        for i in range(0, n):
            l, u = bounds[0, i], bounds[1, i]
            if not np.isinf(l):
                low_bnd[i] = l
                l = 1
            if not np.isinf(u):
                upper_bnd[i] = u
                u = 1
            nbd[i] = bounds_map[l, u]

    # 确保 maxls 大于 0，否则抛出异常
    if not maxls > 0:
        raise ValueError('maxls must be positive.')

    # 初始化优化参数向量 x
    x = array(x0, float64)
    
    # 初始化目标函数值 f
    f = array(0.0, float64)
    
    # 初始化梯度向量 g
    g = zeros((n,), float64)
    
    # 初始化工作数组 wa
    wa = zeros(2*m*n + 5*n + 11*m*m + 8*m, float64)
    
    # 初始化整数工作数组 iwa
    iwa = zeros(3*n, fortran_int)
    
    # 初始化任务字符串数组 task
    task = zeros(1, 'S60')
    
    # 初始化字符工作数组 csave
    csave = zeros(1, 'S60')
    
    # 初始化整数工作数组 lsave
    lsave = zeros(4, fortran_int)
    
    # 初始化整数工作数组 isave
    isave = zeros(44, fortran_int)
    
    # 初始化浮点数工作数组 dsave
    dsave = zeros(29, float64)

    # 将任务状态设为 'START'
    task[:] = 'START'

    # 初始化迭代次数
    n_iterations = 0
    while 1:
        # 如果 g 是 float32 类型，可能是由用户提供的计算雅可比矩阵的函数所导致（参见 gh-18730）。
        # 底层的 Fortran 代码期望 float64 类型，因此需要将其转换为 float64 类型。
        g = g.astype(np.float64)
        
        # 调用 _lbfgsb.setulb 函数来执行 L-BFGS-B 算法的核心优化步骤。
        # 函数返回多个值，分别对应优化后的结果。
        _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,
                       pgtol, wa, iwa, task, iprint, csave, lsave,
                       isave, dsave, maxls)
        
        # 将 task 转换为字符串形式，用于判断当前优化任务的状态
        task_str = task.tobytes()
        
        if task_str.startswith(b'FG'):
            # 当任务状态以 'FG' 开头时，说明需要计算目标函数值和梯度在当前 x 处的值。
            # 调用 func_and_grad 函数获取当前 x 处的目标函数值和梯度。
            f, g = func_and_grad(x)
        elif task_str.startswith(b'NEW_X'):
            # 当任务状态以 'NEW_X' 开头时，表示进入了新的迭代。
            n_iterations += 1
            
            # 创建 OptimizeResult 对象，包含当前迭代的 x 值和目标函数值 f。
            intermediate_result = OptimizeResult(x=x, fun=f)
            
            # 检查是否需要调用回调函数 callback，并可能要求中止优化过程。
            if _call_callback_maybe_halt(callback, intermediate_result):
                task[:] = 'STOP: CALLBACK REQUESTED HALT'
            
            # 检查是否达到最大迭代次数限制。
            if n_iterations >= maxiter:
                task[:] = 'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'
            elif sf.nfev > maxfun:
                # 检查是否达到最大函数和梯度评估次数限制。
                task[:] = ('STOP: TOTAL NO. of f AND g EVALUATIONS '
                           'EXCEEDS LIMIT')
        else:
            # 如果任务状态不以 'FG' 或 'NEW_X' 开头，则跳出循环。
            break
    
    # 去除 task_str 字符串中的空字节，以获取任务状态的实际描述信息。
    task_str = task.tobytes().strip(b'\x00').strip()
    
    # 根据任务状态判断优化过程的终止原因，设置相应的 warnflag 值。
    if task_str.startswith(b'CONV'):
        warnflag = 0
    elif sf.nfev > maxfun or n_iterations >= maxiter:
        warnflag = 1
    else:
        warnflag = 2
    
    # 从 wa 数组中提取两部分工作空间，分别表示 s 和 y，具体见 lbfgsb.f 中的描述。
    s = wa[0: m*n].reshape(m, n)
    y = wa[m*n: 2*m*n].reshape(m, n)
    
    # isave(31) 表示当前迭代之前的 BFGS 更新总次数。
    n_bfgs_updates = isave[30]
    
    # 计算有效的 BFGS 更新次数，不超过 maxcor 的限制。
    n_corrs = min(n_bfgs_updates, maxcor)
    
    # 基于 s 和 y 计算逆 Hessian 矩阵的近似，用于进一步优化。
    hess_inv = LbfgsInvHessProduct(s[:n_corrs], y[:n_corrs])
    
    # 将任务状态转换为字符串形式，准备构造最终的 OptimizeResult 对象并返回。
    task_str = task_str.decode()
    return OptimizeResult(fun=f, jac=g, nfev=sf.nfev,
                          njev=sf.ngev,
                          nit=n_iterations, status=warnflag, message=task_str,
                          x=x, success=(warnflag == 0), hess_inv=hess_inv)
class LbfgsInvHessProduct(LinearOperator):
    """L-BFGS逆Hessian近似的线性操作器。

    这个操作器计算目标函数Hessian逆的近似与向量的乘积，使用L-BFGS有限存储逆Hessian的近似，
    在优化过程中积累。

    该类实现了``scipy.sparse.linalg.LinearOperator``接口。

    Parameters
    ----------
    sk : array_like, shape=(n_corr, n)
        最近`n_corr`次对解向量的更新数组。
        (参见 [1]).
    yk : array_like, shape=(n_corr, n)
        最近`n_corr`次梯度更新的数组。 (参见 [1]).

    References
    ----------
    .. [1] Nocedal, Jorge. "Updating quasi-Newton matrices with limited
       storage." Mathematics of computation 35.151 (1980): 773-782.

    """

    def __init__(self, sk, yk):
        """构造运算符。"""
        if sk.shape != yk.shape or sk.ndim != 2:
            raise ValueError('sk和yk必须具有匹配的形状 (n_corrs, n)')
        n_corrs, n = sk.shape

        super().__init__(dtype=np.float64, shape=(n, n))

        self.sk = sk
        self.yk = yk
        self.n_corrs = n_corrs
        self.rho = 1 / np.einsum('ij,ij->i', sk, yk)

    def _matvec(self, x):
        """使用BFGS矩阵进行高效矩阵-向量乘法。

        这个计算在 [1] 的第 (4) 节中描述。

        Parameters
        ----------
        x : ndarray
            形状为 (n,) 或 (n,1) 的数组。

        Returns
        -------
        y : ndarray
            矩阵-向量乘积结果

        """
        s, y, n_corrs, rho = self.sk, self.yk, self.n_corrs, self.rho
        q = np.array(x, dtype=self.dtype, copy=True)
        if q.ndim == 2 and q.shape[1] == 1:
            q = q.reshape(-1)

        alpha = np.empty(n_corrs)

        for i in range(n_corrs-1, -1, -1):
            alpha[i] = rho[i] * np.dot(s[i], q)
            q = q - alpha[i]*y[i]

        r = q
        for i in range(n_corrs):
            beta = rho[i] * np.dot(y[i], r)
            r = r + s[i] * (alpha[i] - beta)

        return r

    def todense(self):
        """返回该运算器的密集数组表示。

        Returns
        -------
        arr : ndarray, shape=(n, n)
            具有与此 `LinearOperator` 相同形状和数据的数组。

        """
        s, y, n_corrs, rho = self.sk, self.yk, self.n_corrs, self.rho
        I = np.eye(*self.shape, dtype=self.dtype)
        Hk = I

        for i in range(n_corrs):
            A1 = I - s[i][:, np.newaxis] * y[i][np.newaxis, :] * rho[i]
            A2 = I - y[i][:, np.newaxis] * s[i][np.newaxis, :] * rho[i]

            Hk = np.dot(A1, np.dot(Hk, A2)) + (rho[i] * s[i][:, np.newaxis] *
                                                        s[i][np.newaxis, :])
        return Hk
```