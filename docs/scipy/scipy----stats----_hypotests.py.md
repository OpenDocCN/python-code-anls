# `D:\src\scipysrc\scipy\scipy\stats\_hypotests.py`

```
from collections import namedtuple
from dataclasses import dataclass
from math import comb  # 导入组合函数comb
import numpy as np  # 导入NumPy库并用np作为别名
import warnings  # 导入警告模块
from itertools import combinations  # 导入组合生成函数
import scipy.stats  # 导入SciPy统计模块
from scipy.optimize import shgo  # 导入全局优化函数shgo
from . import distributions  # 从当前包中导入distributions模块
from ._common import ConfidenceInterval  # 从当前包中导入ConfidenceInterval类
from ._continuous_distns import norm  # 从当前包中导入正态分布模块中的norm函数
from scipy.special import gamma, kv, gammaln  # 导入Gamma函数及相关函数
from scipy.fft import ifft  # 导入傅里叶逆变换函数ifft
from ._stats_pythran import _a_ij_Aij_Dij2  # 从当前包中导入_stats_pythran模块中的_a_ij_Aij_Dij2函数
from ._stats_pythran import (
    _concordant_pairs as _P, _discordant_pairs as _Q
)  # 从当前包中导入_stats_pythran模块中的_concordant_pairs和_discordant_pairs函数，并分别使用_P和_Q作为别名
from ._axis_nan_policy import _axis_nan_policy_factory  # 从当前包中导入_axis_nan_policy_factory函数
from scipy.stats import _stats_py  # 导入SciPy统计模块中的_stats_py模块

__all__ = ['epps_singleton_2samp', 'cramervonmises', 'somersd',  # 设置模块公开接口列表
           'barnard_exact', 'boschloo_exact', 'cramervonmises_2samp',
           'tukey_hsd', 'poisson_means_test']

Epps_Singleton_2sampResult = namedtuple('Epps_Singleton_2sampResult',  # 定义命名元组Epps_Singleton_2sampResult
                                        ('statistic', 'pvalue'))

@_axis_nan_policy_factory(Epps_Singleton_2sampResult, n_samples=2, too_small=4)
def epps_singleton_2samp(x, y, t=(0.4, 0.8)):  # 使用装饰器进行函数修饰
    """Compute the Epps-Singleton (ES) test statistic.

    Test the null hypothesis that two samples have the same underlying
    probability distribution.

    Parameters
    ----------
    x, y : array-like
        The two samples of observations to be tested. Input must not have more
        than one dimension. Samples can have different lengths, but both
        must have at least five observations.
    t : array-like, optional
        The points (t1, ..., tn) where the empirical characteristic function is
        to be evaluated. It should be positive distinct numbers. The default
        value (0.4, 0.8) is proposed in [1]_. Input must not have more than
        one dimension.

    Returns
    -------
    statistic : float
        The test statistic.
    pvalue : float
        The associated p-value based on the asymptotic chi2-distribution.

    See Also
    --------
    ks_2samp, anderson_ksamp

    Notes
    -----
    Testing whether two samples are generated by the same underlying
    distribution is a classical question in statistics. A widely used test is
    the Kolmogorov-Smirnov (KS) test which relies on the empirical
    distribution function. Epps and Singleton introduce a test based on the
    empirical characteristic function in [1]_.

    One advantage of the ES test compared to the KS test is that is does
    not assume a continuous distribution. In [1]_, the authors conclude
    that the test also has a higher power than the KS test in many
    examples. They recommend the use of the ES test for discrete samples as
    well as continuous samples with at least 25 observations each, whereas
    `anderson_ksamp` is recommended for smaller sample sizes in the
    continuous case.

    The p-value is computed from the asymptotic distribution of the test
    statistic which follows a `chi2` distribution. If the sample size of both
    """
    """
    x 和 y 小于 25 时，应用 [1]_ 中提出的小样本校正到检验统计量中。

    默认的 t 值是根据 [1]_ 中对各种分布的考虑和高功效测试找到的良好值。[1]_ 中的表 III 给出了在该研究中测试的分布的最优值。在实现中，t 的值通过半分位数范围进行了缩放，详见 [1]_。

    References
    ----------
    .. [1] T. W. Epps and K. J. Singleton, "An omnibus test for the two-sample
       problem using the empirical characteristic function", Journal of
       Statistical Computation and Simulation 26, p. 177--203, 1986.

    .. [2] S. J. Goerg and J. Kaiser, "Nonparametric testing of distributions
       - the Epps-Singleton two-sample test using the empirical characteristic
       function", The Stata Journal 9(3), p. 454--465, 2009.

    """
    # 将 t 转换为数组，由装饰器实现
    t = np.asarray(t)
    # 检查 x 和 y 是否为有效输入
    nx, ny = len(x), len(y)
    if (nx < 5) or (ny < 5):
        raise ValueError('x 和 y 至少应有 5 个元素，但 len(x) '
                         f'= {nx}，len(y) = {ny}。')
    if not np.isfinite(x).all():
        raise ValueError('x 不应包含非有限值。')
    if not np.isfinite(y).all():
        raise ValueError('y 不应包含非有限值。')
    n = nx + ny

    # 检查 t 是否有效
    if t.ndim > 1:
        raise ValueError(f't 必须是 1 维的，但 t.ndim 等于 {t.ndim}。')
    if np.less_equal(t, 0).any():
        raise ValueError('t 必须只包含正元素。')

    # 使用 iqr 函数对半分位数范围进行缩放，以 [1]_ 中建议的方式
    from scipy.stats import iqr
    sigma = iqr(np.hstack((x, y))) / 2
    ts = np.reshape(t, (-1, 1)) / sigma

    # 计算 ES 检验的协方差估计
    gx = np.vstack((np.cos(ts*x), np.sin(ts*x))).T  # 形状为 (nx, 2*len(t))
    gy = np.vstack((np.cos(ts*y), np.sin(ts*y))).T
    cov_x = np.cov(gx.T, bias=True)  # 检验使用偏差估计的协方差
    cov_y = np.cov(gy.T, bias=True)
    est_cov = (n/nx)*cov_x + (n/ny)*cov_y
    est_cov_inv = np.linalg.pinv(est_cov)
    r = np.linalg.matrix_rank(est_cov_inv)
    if r < 2*len(t):
        warnings.warn('估计的协方差矩阵未满秩。这表明输入 t 的选择可能不佳，因此检验可能不一致。',  # 参见 [1]_ 的第 183 页
                      stacklevel=2)

    # 计算作为自由度为 r 的卡方分布的渐近分布的检验统计量 w
    g_diff = np.mean(gx, axis=0) - np.mean(gy, axis=0)
    w = n*np.dot(g_diff.T, np.dot(est_cov_inv, g_diff))

    # 应用小样本校正
    if (max(nx, ny) < 25):
        corr = 1.0/(1.0 + n**(-0.45) + 10.1*(nx**(-1.7) + ny**(-1.7)))
        w = corr * w

    # 使用 _stats_py._SimpleChi2(r) 计算 chi2 统计量
    chi2 = _stats_py._SimpleChi2(r)
    # 使用 _stats_py 模块中的 _get_pvalue 函数计算统计量 w 的 p 值
    # 使用 chi2 作为输入参数，设置 alternative='greater' 表示单侧检验，symmetric=False 表示非对称统计量，xp=np 表示使用 np 作为外部数据源
    p = _stats_py._get_pvalue(w, chi2, alternative='greater', symmetric=False, xp=np)

    # 返回一个 Epps_Singleton_2sampResult 对象，其中包含统计量 w 和计算得到的 p 值 p
    return Epps_Singleton_2sampResult(w, p)
def poisson_means_test(k1, n1, k2, n2, *, diff=0, alternative='two-sided'):
    r"""
    Performs the Poisson means test, AKA the "E-test".

    This is a test of the null hypothesis that the difference between means of
    two Poisson distributions is `diff`. The samples are provided as the
    number of events `k1` and `k2` observed within measurement intervals
    (e.g. of time, space, number of observations) of sizes `n1` and `n2`.

    Parameters
    ----------
    k1 : int
        Number of events observed from distribution 1.
    n1: float
        Size of sample from distribution 1.
    k2 : int
        Number of events observed from distribution 2.
    n2 : float
        Size of sample from distribution 2.
    diff : float, default=0
        The hypothesized difference in means between the distributions
        underlying the samples.
    alternative : {'two-sided', 'less', 'greater'}, optional
        Defines the alternative hypothesis.
        The following options are available (default is 'two-sided'):

          * 'two-sided': the difference between distribution means is not
            equal to `diff`
          * 'less': the difference between distribution means is less than
            `diff`
          * 'greater': the difference between distribution means is greater
            than `diff`

    Returns
    -------
    statistic : float
        The test statistic (see [1]_ equation 3.3).
    pvalue : float
        The probability of achieving such an extreme value of the test
        statistic under the null hypothesis.

    Notes
    -----

    Let:

    .. math:: X_1 \sim \mbox{Poisson}(\mathtt{n1}\lambda_1)

    be a random variable independent of

    .. math:: X_2  \sim \mbox{Poisson}(\mathtt{n2}\lambda_2)

    and let ``k1`` and ``k2`` be the observed values of :math:`X_1`
    and :math:`X_2`, respectively. Then `poisson_means_test` uses the number
    of observed events ``k1`` and ``k2`` from samples of size ``n1`` and
    ``n2``, respectively, to test the null hypothesis that

    .. math::
       H_0: \lambda_1 - \lambda_2 = \mathtt{diff}

    A benefit of the E-test is that it has good power for small sample sizes,
    which can reduce sampling costs [1]_. It has been evaluated and determined
    to be more powerful than the comparable C-test, sometimes referred to as
    the Poisson exact test.

    References
    ----------
    .. [1]  Krishnamoorthy, K., & Thomson, J. (2004). A more powerful test for
       comparing two Poisson means. Journal of Statistical Planning and
       Inference, 119(1), 23-35.

    .. [2]  Przyborowski, J., & Wilenski, H. (1940). Homogeneity of results in
       testing samples from Poisson series: With an application to testing
       clover seed for dodder. Biometrika, 31(3/4), 313-323.

    Examples
    --------

    Suppose that a gardener wishes to test the number of dodder (weed) seeds
    in a sack of clover seeds that they buy from a seed company. It has
    """
    # Calculate the rate estimates for each distribution
    rate1 = k1 / n1
    rate2 = k2 / n2

    # Calculate the observed difference in rates
    observed_diff = rate1 - rate2

    # Calculate the standard error of the difference in rates
    se_diff = math.sqrt(rate1 / n1 + rate2 / n2)

    # Calculate the test statistic based on the hypothesized difference
    statistic = (observed_diff - diff) / se_diff

    # Determine the appropriate cumulative distribution function based on the alternative hypothesis
    if alternative == 'two-sided':
        pvalue = 2 * (1 - norm.cdf(abs(statistic)))
    elif alternative == 'less':
        pvalue = norm.cdf(statistic)
    elif alternative == 'greater':
        pvalue = 1 - norm.cdf(statistic)
    else:
        raise ValueError("Alternative hypothesis must be 'two-sided', 'less', or 'greater'.")

    # Return the calculated statistic and p-value
    return statistic, pvalue
    """
    previously been established that the number of dodder seeds in clover
    follows the Poisson distribution.

    A 100 gram sample is drawn from the sack before being shipped to the
    gardener. The sample is analyzed, and it is found to contain no dodder
    seeds; that is, `k1` is 0. However, upon arrival, the gardener draws
    another 100 gram sample from the sack. This time, three dodder seeds are
    found in the sample; that is, `k2` is 3. The gardener would like to
    know if the difference is significant and not due to chance. The
    null hypothesis is that the difference between the two samples is merely
    due to chance, or that :math:`\lambda_1 - \lambda_2 = \mathtt{diff}`
    where :math:`\mathtt{diff} = 0`. The alternative hypothesis is that the
    difference is not due to chance, or :math:`\lambda_1 - \lambda_2 \ne 0`.
    The gardener selects a significance level of 5% to reject the null
    hypothesis in favor of the alternative [2]_.

    >>> import scipy.stats as stats
    >>> res = stats.poisson_means_test(0, 100, 3, 100)
    >>> res.statistic, res.pvalue
    (-1.7320508075688772, 0.08837900929018157)

    The p-value is .088, indicating a near 9% chance of observing a value of
    the test statistic under the null hypothesis. This exceeds 5%, so the
    gardener does not reject the null hypothesis as the difference cannot be
    regarded as significant at this level.
    """

    # 调用统计库中的假设检验函数，对两个 Poisson 分布的样本进行均值检验
    _poisson_means_test_iv(k1, n1, k2, n2, diff, alternative)

    # "for a given k_1 and k_2, an estimate of \lambda_2 is given by" [1] (3.4)
    # 计算 lambda_2 的估计值，根据给定的 k1、k2、n1、n2 和 diff
    lmbd_hat2 = ((k1 + k2) / (n1 + n2) - diff * n1 / (n1 + n2))

    # "\hat{\lambda_{2k}} may be less than or equal to zero ... and in this
    # case the null hypothesis cannot be rejected ... [and] it is not necessary
    # to compute the p-value". [1] page 26 below eq. (3.6).
    # 如果 lambda_2 的估计值小于等于零，则无法拒绝原假设，直接返回不显著结果
    if lmbd_hat2 <= 0:
        return _stats_py.SignificanceResult(0, 1)

    # The unbiased variance estimate [1] (3.2)
    # 计算无偏方差估计值，用于后续的统计量计算
    var = k1 / (n1 ** 2) + k2 / (n2 ** 2)

    # The _observed_ pivot statistic from the input. It follows the
    # unnumbered equation following equation (3.3) This is used later in
    # comparison with the computed pivot statistics in an indicator function.
    # 计算观测的枢轴统计量，用于后续与计算得到的枢轴统计量比较
    t_k1k2 = (k1 / n1 - k2 / n2 - diff) / np.sqrt(var)

    # Equation (3.5) of [1] is lengthy, so it is broken into several parts,
    # beginning here. Note that the probability mass function of poisson is
    # exp^(-\mu)*\mu^k/k!, so and this is called with shape \mu, here noted
    # here as nlmbd_hat*. The strategy for evaluating the double summation in
    # (3.5) is to create two arrays of the values of the two products inside
    # the summation and then broadcast them together into a matrix, and then
    # sum across the entire matrix.

    # Compute constants (as seen in the first and second separated products in
    # (3.5).). (This is the shape (\mu) parameter of the poisson distribution.)
    # 计算常数（用于方程式 (3.5) 中的两个分隔乘积），这是泊松分布的参数
    nlmbd_hat1 = n1 * (lmbd_hat2 + diff)
    # 计算 `nlmbd_hat2`，即 λ̂₂ * n₂
    nlmbd_hat2 = n2 * lmbd_hat2

    # 根据 λ̂₁ 计算 x₁ 的下界和上界，用于分布的尾端而不是无穷大的求和
    x1_lb, x1_ub = distributions.poisson.ppf([1e-10, 1 - 1e-16], nlmbd_hat1)
    # 根据 λ̂₂ 计算 x₂ 的下界和上界，用于分布的尾端而不是无穷大的求和
    x2_lb, x2_ub = distributions.poisson.ppf([1e-10, 1 - 1e-16], nlmbd_hat2)

    # 创建数组作为 x₁ 和 x₂ 的计数器，用于求和式（3.5）
    # x₁ 在列上，x₂ 在行上，以支持广播计算
    x1 = np.arange(x1_lb, x1_ub + 1)
    x2 = np.arange(x2_lb, x2_ub + 1)[:, None]

    # 这是方程（3.5）中的两个乘积，prob_x1 是第一个（左边），prob_x2 是第二个（右边）
    # （为了尽可能清晰：第一个包含 "+ d" 项，第二个不包含）
    prob_x1 = distributions.poisson.pmf(x1, nlmbd_hat1)
    prob_x2 = distributions.poisson.pmf(x2, nlmbd_hat2)

    # 计算用于 "pivot statistic" 的常数，如第（3.3）节后的无编号方程所示
    lmbd_x1 = x1 / n1
    lmbd_x2 = x2 / n2
    lmbds_diff = lmbd_x1 - lmbd_x2 - diff
    var_x1x2 = lmbd_x1 / n1 + lmbd_x2 / n2

    # 这是 "pivot statistic"，用于求和的指示器的左侧
    with np.errstate(invalid='ignore', divide='ignore'):
        t_x1x2 = lmbds_diff / np.sqrt(var_x1x2)

    # `[indicator]` 实现了方程（3.5）后的段落中所述的 "I[.]"，即指示函数
    if alternative == 'two-sided':
        indicator = np.abs(t_x1x2) >= np.abs(t_k1k2)
    elif alternative == 'less':
        indicator = t_x1x2 <= t_k1k2
    else:
        indicator = t_x1x2 >= t_k1k2

    # 将所有乘积组合在一起，根据指示器排除项，然后求和，实现方程（3.5）
    pvalue = np.sum((prob_x1 * prob_x2)[indicator])
    return _stats_py.SignificanceResult(t_k1k2, pvalue)
def _poisson_means_test_iv(k1, n1, k2, n2, diff, alternative):
    # 检查输入参数 `k1` 和 `k2` 是否为整数类型
    if k1 != int(k1) or k2 != int(k2):
        raise TypeError('`k1` and `k2` must be integers.')

    # 检查输入参数 `k1` 和 `k2` 是否大于等于 0
    count_err = '`k1` and `k2` must be greater than or equal to 0.'
    if k1 < 0 or k2 < 0:
        raise ValueError(count_err)

    # 检查输入参数 `n1` 和 `n2` 是否大于 0
    if n1 <= 0 or n2 <= 0:
        raise ValueError('`n1` and `n2` must be greater than 0.')

    # 检查输入参数 `diff` 是否大于等于 0
    if diff < 0:
        raise ValueError('diff must be greater than or equal to 0.')

    # 检查输入参数 `alternative` 是否在指定的备择假设列表中
    alternatives = {'two-sided', 'less', 'greater'}
    if alternative.lower() not in alternatives:
        raise ValueError(f"Alternative must be one of '{alternatives}'.")


class CramerVonMisesResult:
    def __init__(self, statistic, pvalue):
        self.statistic = statistic  # 统计量
        self.pvalue = pvalue  # p 值

    def __repr__(self):
        return (f"{self.__class__.__name__}(statistic={self.statistic}, "
                f"pvalue={self.pvalue})")


def _psi1_mod(x):
    """
    根据 Csörgő, S. 和 Faraway, J. (1996) 中的方程式 1.10 定义 psi1。
    此处实现的是一个修改版，通过排除术语 V(x) / 12（这里为 _cdf_cvm_inf(x) / 12）
    避免在 _cdf_cvm 中评估两次 _cdf_cvm_inf(x)。

    实现基于 Julian Faraway 的 MAPLE 代码和 goftest 包中函数 pCvM 的 R 代码
    （版本 v1.1.1），得到 Adrian Baddeley 的授权。此处实现的主要区别在于，
    代码会继续添加系列项，直到这些项足够小为止。
    """

    def _ed2(y):
        z = y**2 / 4
        b = kv(1/4, z) + kv(3/4, z)
        return np.exp(-z) * (y/2)**(3/2) * b / np.sqrt(np.pi)

    def _ed3(y):
        z = y**2 / 4
        c = np.exp(-z) / np.sqrt(np.pi)
        return c * (y/2)**(5/2) * (2*kv(1/4, z) + 3*kv(3/4, z) - kv(5/4, z))

    def _Ak(k, x):
        m = 2*k + 1
        sx = 2 * np.sqrt(x)
        y1 = x**(3/4)
        y2 = x**(5/4)

        e1 = m * gamma(k + 1/2) * _ed2((4 * k + 3)/sx) / (9 * y1)
        e2 = gamma(k + 1/2) * _ed3((4 * k + 1) / sx) / (72 * y2)
        e3 = 2 * (m + 2) * gamma(k + 3/2) * _ed3((4 * k + 5) / sx) / (12 * y2)
        e4 = 7 * m * gamma(k + 1/2) * _ed2((4 * k + 1) / sx) / (144 * y1)
        e5 = 7 * m * gamma(k + 1/2) * _ed2((4 * k + 5) / sx) / (144 * y1)

        return e1 + e2 + e3 + e4 + e5

    x = np.asarray(x)
    tot = np.zeros_like(x, dtype='float')
    cond = np.ones_like(x, dtype='bool')
    k = 0
    while np.any(cond):
        z = -_Ak(k, x[cond]) / (np.pi * gamma(k + 1))
        tot[cond] = tot[cond] + z
        cond[cond] = np.abs(z) >= 1e-7
        k += 1

    return tot


def _cdf_cvm_inf(x):
    """
    计算 Cramér-von Mises 统计量的累积分布函数（样本量无限大）。

    参见 Csörgő, S. 和 Faraway, J. (1996) 中的方程式 1.2。

    实现基于 Julian Faraway 的 MAPLE 代码和 goftest 包中函数 pCvM 的 R 代码
    （版本 v1.1.1），得到 Adrian Baddeley 的授权。
    """
    # 将输入参数 x 转换为 NumPy 数组，以确保能够进行向量化操作
    x = np.asarray(x)
    
    def term(x, k):
        # 计算每一项的贡献，根据 [2] 中的描述，参见公式 (1.3) 的第二行
        u = np.exp(gammaln(k + 0.5) - gammaln(k+1)) / (np.pi**1.5 * np.sqrt(x))
        # 计算系数 y
        y = 4*k + 1
        # 计算参数 q
        q = y**2 / (16*x)
        # 计算修正的 Bessel 函数
        b = kv(0.25, q)
        # 返回每一项的值
        return u * np.sqrt(y) * np.exp(-q) * b
    
    # 创建与 x 相同形状的全零数组，用于存储累加的结果
    tot = np.zeros_like(x, dtype='float')
    # 创建一个与 x 相同形状的布尔类型数组，用于判断每个元素是否满足条件
    cond = np.ones_like(x, dtype='bool')
    k = 0
    # 当仍有满足条件的元素时循环执行以下操作
    while np.any(cond):
        # 计算当前 k 值下的每一项的贡献
        z = term(x[cond], k)
        # 将计算得到的每一项的贡献加到总和数组中对应的位置
        tot[cond] = tot[cond] + z
        # 更新条件数组，将不满足精度要求的元素标记为 False
        cond[cond] = np.abs(z) >= 1e-7
        # 更新 k 值，准备计算下一项
        k += 1
    
    # 返回最终累加的结果数组
    return tot
# 计算有限样本大小下 Cramér-von Mises 统计量的累积分布函数 (cdf)。如果 n 为 None，则使用渐近 cdf (n=inf)。
# 参考 Csörgő 和 Faraway (1996) 中的方程 1.8，适用于有限样本；方程 1.2 适用于渐近 cdf。

def _cdf_cvm(x, n=None):
    # 将 x 转换为 NumPy 数组
    x = np.asarray(x)
    if n is None:
        # 如果 n 为 None，使用无限样本大小的渐近 cdf
        y = _cdf_cvm_inf(x)
    else:
        # 如果 n 不为 None，计算有限样本大小下的 cdf
        y = np.zeros_like(x, dtype='float')
        # 统计检验量的支持范围为 [12/n, n/3]，参见文献 [2] 中的 1.1 节
        sup = (1./(12*n) < x) & (x < n/3.)
        # 注意：_psi1_mod 不包括 _cdf_cvm_inf(x) / 12 这一项，因此我们需要在这里添加它
        y[sup] = _cdf_cvm_inf(x[sup]) * (1 + 1./(12*n)) + _psi1_mod(x[sup]) / n
        y[x >= n/3] = 1

    # 如果 y 是标量，返回其单元素元组
    if y.ndim == 0:
        return y[()]
    return y


# 将 Cramér-von Mises 检验的结果对象转换为元组形式
def _cvm_result_to_tuple(res):
    return res.statistic, res.pvalue


# 使用 _axis_nan_policy_factory 装饰器定义 cramervonmises 函数，用于执行单样本 Cramér-von Mises 拟合优度检验
@_axis_nan_policy_factory(CramerVonMisesResult, n_samples=1, too_small=1,
                          result_to_tuple=_cvm_result_to_tuple)
def cramervonmises(rvs, cdf, args=()):
    """执行单样本 Cramér-von Mises 拟合优度检验。

    这是一种检验累积分布函数 (cdf) :math:`F` 与观测到的随机变量 :math:`X_1, ..., X_n` 的经验分布函数 :math:`F_n` 的拟合优度。
    假设原假设是 :math:`X_i` 具有累积分布 :math:`F`。

    Parameters
    ----------
    rvs : array_like
        包含随机变量 :math:`X_i` 的观测值的一维数组。
        样本必须至少包含两个观测值。
    cdf : str or callable
        要测试观测值的累积分布函数 :math:`F`。如果是字符串，则应为 `scipy.stats` 中分布的名称。如果是可调用对象，那么会使用它来计算 cdf: ``cdf(x, *args) -> float``。
    args : tuple, optional
        分布参数。假设这些参数是已知的；参见 Notes。

    Returns
    -------
    res : 具有属性的对象
        statistic : float
            Cramér-von Mises 统计量。
        pvalue : float
            p 值。

    See Also
    --------
    kstest, cramervonmises_2samp

    Notes
    -----
    .. versionadded:: 1.6.0

    p 值依赖于文献 [2]_ 中方程 1.8 的近似值。

    """
    """
    Calculate the Cramér-von Mises test statistic and p-value for testing
    goodness of fit of a sample to a given cumulative distribution function (CDF).

    Parameters
    ----------
    rvs : array_like
        Input data to be tested.
    cdf : {str, callable}
        Cumulative distribution function to test against. If a string, must be
        the name of a distribution in `scipy.stats.distributions`. Otherwise,
        it can be a callable object representing the CDF.
    args : tuple, optional
        Additional parameters for the CDF.

    Returns
    -------
    CramerVonMisesResult
        A named tuple containing the test statistic (`statistic`) and the
        p-value (`pvalue`).

    Raises
    ------
    ValueError
        If the sample size is less than 2.

    Notes
    -----
    The Cramér-von Mises test is used to determine whether a sample comes from a
    specified distribution. The p-value indicates the probability of observing
    the test statistic or a more extreme value under the null hypothesis.

    References
    ----------
    .. [1] Cramér-von Mises criterion, Wikipedia,
           https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion
    .. [2] Csörgő, S. and Faraway, J. (1996). The Exact and Asymptotic
           Distribution of Cramér-von Mises Statistics. Journal of the
           Royal Statistical Society, pp. 221-234.
    """
    if isinstance(cdf, str):
        # If `cdf` is a string, convert it to a callable CDF function
        cdf = getattr(distributions, cdf).cdf

    # Sort the input data in ascending order
    vals = np.sort(np.asarray(rvs))

    # Check if the sample size is sufficient for testing
    if vals.size <= 1:
        raise ValueError('The sample must contain at least two observations.')

    # Compute the size of the sample
    n = len(vals)

    # Compute the CDF values of the sorted data using the specified CDF and arguments
    cdfvals = cdf(vals, *args)

    # Compute the Cramér-von Mises test statistic (w)
    u = (2*np.arange(1, n+1) - 1)/(2*n)
    w = 1/(12*n) + np.sum((u - cdfvals)**2)

    # Adjust p to ensure it is non-negative and less than or equal to 1
    p = max(0, 1. - _cdf_cvm(w, n))

    # Return a named tuple with the computed statistic and p-value
    return CramerVonMisesResult(statistic=w, pvalue=p)
def _get_wilcoxon_distr(n):
    """
    Distribution of probability of the Wilcoxon ranksum statistic r_plus (sum
    of ranks of positive differences).
    Returns an array with the probabilities of all the possible ranks
    r = 0, ..., n*(n+1)/2
    """
    # Initialize an array with a single element of 1.0 as float64
    c = np.ones(1, dtype=np.float64)
    # Loop from 1 to n (inclusive)
    for k in range(1, n + 1):
        # Store the current array 'c' in 'prev_c'
        prev_c = c
        # Initialize 'c' as an array of zeros with length k*(k+1)/2 + 1 as float64
        c = np.zeros(k * (k + 1) // 2 + 1, dtype=np.float64)
        # 'm' is the length of 'prev_c'
        m = len(prev_c)
        # Fill the first 'm' elements of 'c' with values of 'prev_c * 0.5'
        c[:m] = prev_c * 0.5
        # Add 'prev_c * 0.5' to the last 'm' elements of 'c'
        c[-m:] += prev_c * 0.5
    # Return the resulting array 'c'
    return c


def _get_wilcoxon_distr2(n):
    """
    Distribution of probability of the Wilcoxon ranksum statistic r_plus (sum
    of ranks of positive differences).
    Returns an array with the probabilities of all the possible ranks
    r = 0, ..., n*(n+1)/2
    This is a slower reference function
    References
    ----------
    .. [1] 1. Harris T, Hardin JW. Exact Wilcoxon Signed-Rank and Wilcoxon
        Mann-Whitney Ranksum Tests. The Stata Journal. 2013;13(2):337-343.
    """
    # Create an array 'ai' of shape (n, 1) containing integers from 1 to n
    ai = np.arange(1, n+1)[:, None]
    # Calculate 't' as n*(n+1)/2
    t = n*(n+1)/2
    # Calculate 'q' as 2*t
    q = 2*t
    # Create an array 'j' containing integers from 0 to q-1
    j = np.arange(q)
    # Calculate 'theta' as 2*pi*j/q
    theta = 2*np.pi/q*j
    # Calculate 'phi_sp' as the product of cosines of 'theta*ai' along axis 0
    phi_sp = np.prod(np.cos(theta*ai), axis=0)
    # Calculate 'phi_s' as exp(1j*theta*t) * phi_sp
    phi_s = np.exp(1j*theta*t) * phi_sp
    # Perform inverse discrete Fourier transform (ifft) on 'phi_s' and store in 'p'
    p = np.real(ifft(phi_s))
    # Initialize an array 'res' of length int(t)+1 filled with zeros
    res = np.zeros(int(t)+1)
    # Fill 'res' with values from 'p' at even indices, starting from 0
    res[:-1:] = p[::2]
    # Set the first element of 'res' to half of its original value
    res[0] /= 2
    # Set the last element of 'res' to the value of the first element
    res[-1] = res[0]
    # Return the resulting array 'res'
    return res


def _tau_b(A):
    """Calculate Kendall's tau-b and p-value from contingency table."""
    # Check if 'A' is truly a 2D contingency table
    if A.shape[0] == 1 or A.shape[1] == 1:
        # Return NaNs if 'A' is not 2D
        return np.nan, np.nan

    # Calculate the sum of all elements in 'A'
    NA = A.sum()
    # Calculate _P(A) and _Q(A)
    PA = _P(A)
    QA = _Q(A)
    # Calculate Sri2 and Scj2
    Sri2 = (A.sum(axis=1)**2).sum()
    Scj2 = (A.sum(axis=0)**2).sum()
    # Calculate the denominator for tau-b
    denominator = (NA**2 - Sri2)*(NA**2 - Scj2)

    # Calculate tau-b
    tau = (PA-QA)/(denominator)**0.5

    # Calculate numerator for s02_tau_b
    numerator = 4*(_a_ij_Aij_Dij2(A) - (PA - QA)**2 / NA)
    # Calculate s02_tau_b; handle case where it is zero to avoid division by zero
    s02_tau_b = numerator/denominator
    if s02_tau_b == 0:
        return tau, 0
    # Calculate Z-score
    Z = tau/s02_tau_b**0.5
    # Calculate 2-sided p-value using normal cumulative distribution function
    p = 2*norm.sf(abs(Z))

    # Return tau and p-value
    return tau, p


def _somers_d(A, alternative='two-sided'):
    """Calculate Somers' D and p-value from contingency table."""
    # Check if 'A' is truly a 2D contingency table
    if A.shape[0] <= 1 or A.shape[1] <= 1:
        return np.nan, np.nan

    # Calculate the sum of all elements in 'A'
    NA = A.sum()
    NA2 = NA**2
    # Calculate _P(A) and _Q(A)
    PA = _P(A)
    QA = _Q(A)
    # Calculate Sri2
    Sri2 = (A.sum(axis=1)**2).sum()

    # Calculate Somers' D
    d = (PA - QA)/(NA2 - Sri2)

    # Calculate _a_ij_Aij_Dij2(A)
    S = _a_ij_Aij_Dij2(A) - (PA-QA)**2/NA

    # Calculate Z-score with handling for division errors
    with np.errstate(divide='ignore'):
        Z = (PA - QA)/(4*(S))**0.5

    # Calculate p-value using _SimpleNormal and _get_pvalue functions
    norm = _stats_py._SimpleNormal()
    p = _stats_py._get_pvalue(Z, norm, alternative, xp=np)

    # Return Somers' D and p-value
    return d, p


@dataclass
class SomersDResult:
    statistic: float
    pvalue: float
    table: np.ndarray


def somersd(x, y=None, alternative='two-sided'):
    r"""Calculates Somers' D, an asymmetric measure of ordinal association.

    Like Kendall's :math:`\tau`, Somers' :math:`D` is a measure of the
    correspondence between two rankings. Both statistics consider the
    difference between the number of concordant and discordant pairs in two
    rankings :math:`X` and :math:`Y`, and both are normalized such that values
    close  to 1 indicate strong agreement and values close to -1 indicate
    strong disagreement. They differ in how they are normalized. To show the
    relationship, Somers' :math:`D` can be defined in terms of Kendall's
    :math:`\tau_a`:

    .. math::
        D(Y|X) = \frac{\tau_a(X, Y)}{\tau_a(X, X)}

    Suppose the first ranking :math:`X` has :math:`r` distinct ranks and the
    second ranking :math:`Y` has :math:`s` distinct ranks. These two lists of
    :math:`n` rankings can also be viewed as an :math:`r \times s` contingency
    table in which element :math:`i, j` is the number of rank pairs with rank
    :math:`i` in ranking :math:`X` and rank :math:`j` in ranking :math:`Y`.
    Accordingly, `somersd` also allows the input data to be supplied as a
    single, 2D contingency table instead of as two separate, 1D rankings.

    Note that the definition of Somers' :math:`D` is asymmetric: in general,
    :math:`D(Y|X) \neq D(X|Y)`. ``somersd(x, y)`` calculates Somers'
    :math:`D(Y|X)`: the "row" variable :math:`X` is treated as an independent
    variable, and the "column" variable :math:`Y` is dependent. For Somers'
    :math:`D(X|Y)`, swap the input lists or transpose the input table.

    Parameters
    ----------
    x : array_like
        1D array of rankings, treated as the (row) independent variable.
        Alternatively, a 2D contingency table.
    y : array_like, optional
        If `x` is a 1D array of rankings, `y` is a 1D array of rankings of the
        same length, treated as the (column) dependent variable.
        If `x` is 2D, `y` is ignored.
    alternative : {'two-sided', 'less', 'greater'}, optional
        Defines the alternative hypothesis. Default is 'two-sided'.
        The following options are available:
        * 'two-sided': the rank correlation is nonzero
        * 'less': the rank correlation is negative (less than zero)
        * 'greater':  the rank correlation is positive (greater than zero)

    Returns
    -------
    res : SomersDResult
        A `SomersDResult` object with the following fields:

            statistic : float
               The Somers' :math:`D` statistic.
            pvalue : float
               The p-value for a hypothesis test whose null
               hypothesis is an absence of association, :math:`D=0`.
               See notes for more information.
            table : 2D array
               The contingency table formed from rankings `x` and `y` (or the
               provided contingency table, if `x` is a 2D array)

    See Also
    --------
    kendalltau : Calculates Kendall's tau, another correlation measure.
    weightedtau : Computes a weighted version of Kendall's tau.
    spearmanr : Calculates a Spearman rank-order correlation coefficient.
    pearsonr : Calculates a Pearson correlation coefficient.

    Notes
    -----
    This function follows the contingency table approach of [2]_ and
    [3]_. *p*-values are computed based on an asymptotic approximation of
    the test statistic distribution under the null hypothesis :math:`D=0`.

    Theoretically, hypothesis tests based on Kendall's :math:`tau` and Somers'
    :math:`D` should be identical.
    However, the *p*-values returned by `kendalltau` are based
    on the null hypothesis of *independence* between :math:`X` and :math:`Y`
    (i.e. the population from which pairs in :math:`X` and :math:`Y` are
    sampled contains equal numbers of all possible pairs), which is more
    specific than the null hypothesis :math:`D=0` used here. If the null
    hypothesis of independence is desired, it is acceptable to use the
    *p*-value returned by `kendalltau` with the statistic returned by
    `somersd` and vice versa. For more information, see [2]_.

    Contingency tables are formatted according to the convention used by
    SAS and R: the first ranking supplied (``x``) is the "row" variable, and
    the second ranking supplied (``y``) is the "column" variable. This is
    opposite the convention of Somers' original paper [1]_.

    References
    ----------
    .. [1] Robert H. Somers, "A New Asymmetric Measure of Association for
           Ordinal Variables", *American Sociological Review*, Vol. 27, No. 6,
           pp. 799--811, 1962.

    .. [2] Morton B. Brown and Jacqueline K. Benedetti, "Sampling Behavior of
           Tests for Correlation in Two-Way Contingency Tables", *Journal of
           the American Statistical Association* Vol. 72, No. 358, pp.
           309--315, 1977.

    .. [3] SAS Institute, Inc., "The FREQ Procedure (Book Excerpt)",
           *SAS/STAT 9.2 User's Guide, Second Edition*, SAS Publishing, 2009.

    .. [4] Laerd Statistics, "Somers' d using SPSS Statistics", *SPSS
           Statistics Tutorials and Statistical Guides*,
           https://statistics.laerd.com/spss-tutorials/somers-d-using-spss-statistics.php,
           Accessed July 31, 2020.

    Examples
    --------
    We calculate Somers' D for the example given in [4]_, in which a hotel
    chain owner seeks to determine the association between hotel room
    cleanliness and customer satisfaction. The independent variable, hotel
    room cleanliness, is ranked on an ordinal scale: "below average (1)",
    "average (2)", or "above average (3)". The dependent variable, customer
    satisfaction, is ranked on a second scale: "very dissatisfied (1)",
    "moderately dissatisfied (2)", "neither dissatisfied nor satisfied (3)",
    "moderately satisfied (4)", or "very satisfied (5)". 189 customers
    respond to the survey, and the results are cast into a contingency table
    with the hotel room cleanliness as the "row" variable and customer
    satisfaction as the "column" variable.

    +-----+-----+-----+-----+-----+-----+
    |     | (1) | (2) | (3) | (4) | (5) |
    +=====+=====+=====+=====+=====+=====+
    x, y = np.array(x), np.array(y)
    # 将输入的 x 和 y 转换为 NumPy 数组，以便后续处理
    if x.ndim == 1:
        # 如果 x 是一维数组
        if x.size != y.size:
            # 如果 x 和 y 的长度不相等，抛出数值错误异常
            raise ValueError("Rankings must be of equal length.")
        # 根据 x 和 y 创建列联表，并取其中的第二行（Somers' D 统计量所需）
        table = scipy.stats.contingency.crosstab(x, y)[1]
    elif x.ndim == 2:
        # 如果 x 是二维数组
        if np.any(x < 0):
            # 如果 x 中有负值，抛出数值错误异常
            raise ValueError("All elements of the contingency table must be non-negative.")
        if np.any(x != x.astype(int)):
            # 如果 x 中有非整数值，抛出数值错误异常
            raise ValueError("All elements of the contingency table must be integer.")
        if x.nonzero()[0].size < 2:
            # 如果 x 中非零元素的数量小于 2，抛出数值错误异常
            raise ValueError("At least two elements of the contingency table must be nonzero.")
        # 直接使用 x 作为列联表
        table = x
    else:
        # 如果 x 不是一维或二维数组，抛出数值错误异常
        raise ValueError("x must be either a 1D or 2D array")
    # 将列联表转换为浮点数，以避免整数溢出问题
    d, p = _somers_d(table.astype(float), alternative)

    # 为了与其他相关性函数保持一致，给结果对象增加别名
    res = SomersDResult(d, p, table)
    res.correlation = d
    # 返回计算得到的 Somers' D 相关性分析结果对象
    return res
# 将一个包含 nx + ny - 1 个索引的集合分割成两个固定长度的集合的所有可能方式
def _all_partitions(nx, ny):
    """
    Partition a set of indices into two fixed-length sets in all possible ways

    Partition a set of indices 0 ... nx + ny - 1 into two sets of length nx and
    ny in all possible ways (ignoring order of elements).
    """
    # 创建包含 0 到 nx+ny-1 的数组
    z = np.arange(nx+ny)
    # 遍历 z 数组中所有长度为 nx 的组合
    for c in combinations(z, nx):
        # 将组合 c 转换为 numpy 数组 x
        x = np.array(c)
        # 创建一个布尔掩码，将 x 中的索引设为 False
        mask = np.ones(nx+ny, bool)
        mask[x] = False
        # 根据掩码 mask 创建另一个数组 y，包含 z 中非 x 部分的索引
        y = z[mask]
        # 返回生成器，每次生成一个 (x, y) 元组，表示一种分割方式
        yield x, y


# 计算所有 C(n, k) 的对数组合
def _compute_log_combinations(n):
    """Compute all log combination of C(n, k)."""
    # 计算从 0 到 n 的所有整数的 gamma 函数的对数值数组
    gammaln_arr = gammaln(np.arange(n + 1) + 1)
    # 计算 C(n, k) 的对数组合值
    return gammaln(n + 1) - gammaln_arr - gammaln_arr[::-1]


# Barnard 精确检验的结果类，包含统计量和 p 值
@dataclass
class BarnardExactResult:
    statistic: float
    pvalue: float


# 执行 Barnard 精确检验的函数，针对 2x2 列联表
def barnard_exact(table, alternative="two-sided", pooled=True, n=32):
    r"""Perform a Barnard exact test on a 2x2 contingency table.

    Parameters
    ----------
    table : array_like of ints
        A 2x2 contingency table.  Elements should be non-negative integers.

    alternative : {'two-sided', 'less', 'greater'}, optional
        Defines the null and alternative hypotheses. Default is 'two-sided'.
        Please see explanations in the Notes section below.

    pooled : bool, optional
        Whether to compute score statistic with pooled variance (as in
        Student's t-test, for example) or unpooled variance (as in Welch's
        t-test). Default is ``True``.

    n : int, optional
        Number of sampling points used in the construction of the sampling
        method. Note that this argument will automatically be converted to
        the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to
        select sample points. Default is 32. Must be positive. In most cases,
        32 points is enough to reach good precision. More points comes at
        performance cost.

    Returns
    -------
    ber : BarnardExactResult
        A result object with the following attributes.

        statistic : float
            The Wald statistic with pooled or unpooled variance, depending
            on the user choice of `pooled`.

        pvalue : float
            P-value, the probability of obtaining a distribution at least as
            extreme as the one that was actually observed, assuming that the
            null hypothesis is true.

    See Also
    --------
    chi2_contingency : Chi-square test of independence of variables in a
        contingency table.
    fisher_exact : Fisher exact test on a 2x2 contingency table.
    boschloo_exact : Boschloo's exact test on a 2x2 contingency table,
        which is an uniformly more powerful alternative to Fisher's exact test.

    Notes
    -----
    Barnard's test is an exact test used in the analysis of contingency
    tables. It examines the association of two categorical variables, and
    is a more powerful alternative than Fisher's exact test
    for 2x2 contingency tables.
    """
    # 函数注释详细描述了 Barnard 精确检验函数的输入参数和返回结果
    pass  # 实际实现在此处省略，根据具体实现进行注释
    # 定义一个2x2矩阵 :math:`X_0`，表示观察样本，每列存储一个二项试验
    # :math:`p_1, p_2` 分别定义为 :math:`x_{11}` 和 :math:`x_{12}` 的理论二项概率
    # 在使用巴纳德精确检验时，我们可以断定三种不同的零假设：
    
    # - :math:`H_0 : p_1 \geq p_2` vs :math:`H_1 : p_1 < p_2`，使用 `alternative` = "less"
    
    # - :math:`H_0 : p_1 \leq p_2` vs :math:`H_1 : p_1 > p_2`，使用 `alternative` = "greater"
    
    # - :math:`H_0 : p_1 = p_2` vs :math:`H_1 : p_1 \neq p_2`，使用 `alternative` = "two-sided"（默认）
    
    # 为了计算巴纳德精确检验，我们使用瓦尔德统计量 [3]_，带有合并或非合并方差。
    # 在默认假设下，即两个方差相等（``pooled = True``），统计量计算为：
    
    # .. math::
    
    #    T(X) = \frac{
    #        \hat{p}_1 - \hat{p}_2
    #    }{
    #        \sqrt{
    #            \hat{p}(1 - \hat{p})
    #            (\frac{1}{c_1} +
    #            \frac{1}{c_2})
    #        }
    #    }
    
    # 其中 :math:`\hat{p}_1, \hat{p}_2` 和 :math:`\hat{p}` 是 :math:`p_1, p_2` 和 :math:`p` 的估计值，
    # 后者是组合概率，假设 :math:`p_1 = p_2`。
    
    # 如果这个假设无效（``pooled = False``），统计量为：
    
    # .. math::
    
    #    T(X) = \frac{
    #        \hat{p}_1 - \hat{p}_2
    #    }{
    #        \sqrt{
    #            \frac{\hat{p}_1 (1 - \hat{p}_1)}{c_1} +
    #            \frac{\hat{p}_2 (1 - \hat{p}_2)}{c_2}
    #        }
    #    }
    
    # 然后计算 p 值为：
    
    # .. math::
    
    #    \sum
    #        \binom{c_1}{x_{11}}
    #        \binom{c_2}{x_{12}}
    #        \pi^{x_{11} + x_{12}}
    #        (1 - \pi)^{t - x_{11} - x_{12}}
    
    # 其中求和是在所有满足以下条件的 2x2列联表 :math:`X` 上进行的：
    # * 当 `alternative` = "less" 时， :math:`T(X) \leq T(X_0)`
    # * 当 `alternative` = "greater" 时， :math:`T(X) \geq T(X_0)`
    # * 当 `alternative` = "two-sided" 时， :math:`T(X) \geq |T(X_0)|`
    # 这里， :math:`c_1, c_2` 是列1和列2的总和， :math:`t` 是总样本元素的和。
    
    # 返回的 p 值是在辅助参数 :math:`\pi` 上取的最大 p 值，其中 :math:`0 \leq \pi \leq 1`。
    
    # 该函数的复杂度为 :math:`O(n c_1 c_2)`，其中 `n` 是样本点的数量。
    
    # 参考文献
    # ----------
    # .. [1] Barnard, G. A. "Significance Tests for 2x2 Tables". *Biometrika*.
    #        34.1/2 (1947): 123-138. :doi:`dpgkg3`
    
    # .. [2] Mehta, Cyrus R., and Pralay Senchaudhuri. "Conditional versus
    #        unconditional exact tests for comparing two binomials."
    #        *Cytel Software Corporation* 675 (2003): 1-5.
    
    # .. [3] "Wald Test". *Wikipedia*. https://en.wikipedia.org/wiki/Wald_test
    
    # 示例
    # --------
    # 如果输入的点数 `n` 小于等于零，抛出数值错误异常
    if n <= 0:
        raise ValueError(
            "Number of points `n` must be strictly positive, "
            f"found {n!r}"
        )

    # 将输入的表格 `table` 转换为 numpy 数组，并确保其形状为 (2, 2)
    table = np.asarray(table, dtype=np.int64)

    # 如果输入的表格 `table` 不是形状为 (2, 2)，抛出数值错误异常
    if not table.shape == (2, 2):
        raise ValueError("The input `table` must be of shape (2, 2).")

    # 如果输入表格 `table` 中有任何负数值，抛出数值错误异常
    if np.any(table < 0):
        raise ValueError("All values in `table` must be nonnegative.")

    # 如果表格 `table` 中的任意一列总和为零，返回 BarnardExactResult 对象，其统计值为 NaN，p 值为 1.0
    if 0 in table.sum(axis=0):
        # 如果列中的值都是零，那么 p 值为 1，统计值为 NaN
        return BarnardExactResult(np.nan, 1.0)

    # 计算表格 `table` 每列的总和，分别赋值给 total_col_1 和 total_col_2
    total_col_1, total_col_2 = table.sum(axis=0)
    # 创建一个包含整数序列的二维数组 x1，其行数为 total_col_1 + 1，列数为 1
    x1 = np.arange(total_col_1 + 1, dtype=np.int64).reshape(-1, 1)
    # 创建一个包含整数序列的二维数组 x2，其行数为 1，列数为 total_col_2 + 1
    x2 = np.arange(total_col_2 + 1, dtype=np.int64).reshape(1, -1)

    # 计算 p1 和 p2，分别表示 x1 和 x2 中每个元素除以总列数后的比例
    p1, p2 = x1 / total_col_1, x2 / total_col_2

    # 如果 pooled 为 True，则计算合并后的方差
    if pooled:
        # 计算合并后的比例 p
        p = (x1 + x2) / (total_col_1 + total_col_2)
        # 计算合并后的方差
        variances = p * (1 - p) * (1 / total_col_1 + 1 / total_col_2)
    else:
        # 分别计算 x1 和 x2 的方差
        variances = p1 * (1 - p1) / total_col_1 + p2 * (1 - p2) / total_col_2

    # 忽略除以 0 时的警告
    with np.errstate(divide="ignore", invalid="ignore"):
        # 计算 Wald 统计量
        wald_statistic = np.divide((p1 - p2), np.sqrt(variances))

    # 将等于条件 p1 == p2 的位置的 Wald 统计量设为 0，以移除 NaN 值
    wald_statistic[p1 == p2] = 0

    # 提取表格中给定位置 (table[0, 0], table[0, 1]) 处的 Wald 统计量
    wald_stat_obs = wald_statistic[table[0, 0], table[0, 1]]

    # 根据 alternative 的值选择相应的索引条件
    if alternative == "two-sided":
        index_arr = np.abs(wald_statistic) >= abs(wald_stat_obs)
    elif alternative == "less":
        index_arr = wald_statistic <= wald_stat_obs
    elif alternative == "greater":
        index_arr = wald_statistic >= wald_stat_obs
    else:
        # 如果 alternative 不在预期范围内，则抛出 ValueError 异常
        msg = (
            "`alternative` should be one of {'two-sided', 'less', 'greater'},"
            f" found {alternative!r}"
        )
        raise ValueError(msg)

    # 计算 x1 和 x2 的和 x1_sum_x2
    x1_sum_x2 = x1 + x2

    # 计算 x1 和 x2 对应的对数组合数
    x1_log_comb = _compute_log_combinations(total_col_1)
    x2_log_comb = _compute_log_combinations(total_col_2)
    x1_sum_x2_log_comb = x1_log_comb[x1] + x2_log_comb[x2]

    # 使用 shgo 方法计算精确的 Barnard 检验结果
    result = shgo(
        _get_binomial_log_p_value_with_nuisance_param,
        args=(x1_sum_x2, x1_sum_x2_log_comb, index_arr),
        bounds=((0, 1),),
        n=n,
        sampling_method="sobol",
    )

    # 将 result.fun（负对数 p 值）转换为 p 值，并进行剪切操作确保在 0 和 1 之间
    p_value = np.clip(np.exp(-result.fun), a_min=0, a_max=1)

    # 返回 BarnardExactResult 对象，包含 Wald 统计量和计算得到的 p 值
    return BarnardExactResult(wald_stat_obs, p_value)
@dataclass
class BoschlooExactResult:
    statistic: float
    pvalue: float


def boschloo_exact(table, alternative="two-sided", n=32):
    r"""Perform Boschloo's exact test on a 2x2 contingency table.

    Parameters
    ----------
    table : array_like of ints
        A 2x2 contingency table.  Elements should be non-negative integers.

    alternative : {'two-sided', 'less', 'greater'}, optional
        Defines the null and alternative hypotheses. Default is 'two-sided'.
        Please see explanations in the Notes section below.

    n : int, optional
        Number of sampling points used in the construction of the sampling
        method. Note that this argument will automatically be converted to
        the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to
        select sample points. Default is 32. Must be positive. In most cases,
        32 points is enough to reach good precision. More points comes at
        performance cost.

    Returns
    -------
    ber : BoschlooExactResult
        A result object with the following attributes.

        statistic : float
            The statistic used in Boschloo's test; that is, the p-value
            from Fisher's exact test.

        pvalue : float
            P-value, the probability of obtaining a distribution at least as
            extreme as the one that was actually observed, assuming that the
            null hypothesis is true.

    See Also
    --------
    chi2_contingency : Chi-square test of independence of variables in a
        contingency table.
    fisher_exact : Fisher exact test on a 2x2 contingency table.
    barnard_exact : Barnard's exact test, which is a more powerful alternative
        than Fisher's exact test for 2x2 contingency tables.

    Notes
    -----
    Boschloo's test is an exact test used in the analysis of contingency
    tables. It examines the association of two categorical variables, and
    is a uniformly more powerful alternative to Fisher's exact test
    for 2x2 contingency tables.

    Boschloo's exact test uses the p-value of Fisher's exact test as a
    statistic, and Boschloo's p-value is the probability under the null
    hypothesis of observing such an extreme value of this statistic.

    Let's define :math:`X_0` a 2x2 matrix representing the observed sample,
    where each column stores the binomial experiment, as in the example
    below. Let's also define :math:`p_1, p_2` the theoretical binomial
    probabilities for  :math:`x_{11}` and :math:`x_{12}`. When using
    Boschloo exact test, we can assert three different alternative hypotheses:

    - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 < p_2`,
      with `alternative` = "less"

    - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 > p_2`,
      with `alternative` = "greater"

    - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 \neq p_2`,
      with `alternative` = "two-sided" (default)

    There are multiple conventions for computing a two-sided p-value when the
    null hypothesis is true. Boschloo's test offers a more powerful alternative
    compared to Fisher's exact test for 2x2 tables.
    """
    这段代码是关于Boschloo精确检验（一种用于2x2表的假设检验）的文档字符串说明。

    .. versionadded:: 1.7.0
        添加于SciPy版本1.7.0。

    References
    ----------
    .. [1] R.D. Boschloo. "Raised conditional level of significance for the
       2 x 2-table when testing the equality of two probabilities",
       Statistica Neerlandica, 24(1), 1970
       Boschloo提出的提升条件显著水平的方法，用于测试两个概率是否相等。

    .. [2] "Boschloo's test", Wikipedia,
       https://en.wikipedia.org/wiki/Boschloo%27s_test
       Boschloo检验的维基百科链接，详细解释了该检验的背景和原理。

    .. [3] Lise M. Saari et al. "Employee attitudes and job satisfaction",
       Human Resource Management, 43(4), 395-407, 2004,
       :doi:`10.1002/hrm.20032`.
       论文引用，探讨了员工态度和工作满意度的相关研究，提供了实际例子。

    Examples
    --------
    下面的示例探讨了“Employee attitudes and job satisfaction” [3]_
    这篇文章中的数据，比较了科学家和大学教授对工作满意度的差异。

    这里我们进行了假设检验，使用了Boschloo精确检验来比较两组数据。
    通过检验结果的统计量和p值，我们得出了在5%显著性水平下的结论。

    >>> import scipy.stats as stats
    >>> res = stats.boschloo_exact([[74, 31], [43, 32]], alternative="greater")
    >>> res.statistic
    0.0483
    >>> res.pvalue
    0.0355

    在零假设下，即科学家在工作中更满意的假设下，观察到如此极端的数据结果的概率约为3.55%。
    由于这个p值小于我们选择的显著性水平，我们有证据拒绝零假设 :math:`H_0`，支持备择假设。

    """
    hypergeom = distributions.hypergeom

    if n <= 0:
        raise ValueError(
            "Number of points `n` must be strictly positive,"
            f" found {n!r}"
        )
    # 将输入的表格转换为NumPy数组，并确保数据类型为int64
    table = np.asarray(table, dtype=np.int64)

    # 检查表格形状是否为(2, 2)，如果不是则抛出数值错误异常
    if not table.shape == (2, 2):
        raise ValueError("The input `table` must be of shape (2, 2).")

    # 检查表格中是否有负值，如果有则抛出数值错误异常
    if np.any(table < 0):
        raise ValueError("All values in `table` must be nonnegative.")

    # 检查第一列和第二列的总和是否有零，如果有则返回一个特定的BoschlooExactResult对象
    if 0 in table.sum(axis=0):
        # 如果两列的值都是零，p值为1，统计值为NaN
        return BoschlooExactResult(np.nan, np.nan)

    # 计算第一列和第二列的总和
    total_col_1, total_col_2 = table.sum(axis=0)
    total = total_col_1 + total_col_2

    # 创建第一列和第二列可能的取值范围
    x1 = np.arange(total_col_1 + 1, dtype=np.int64).reshape(1, -1)
    x2 = np.arange(total_col_2 + 1, dtype=np.int64).reshape(-1, 1)

    # 计算x1和x2的和
    x1_sum_x2 = x1 + x2

    # 根据alternative参数选择不同的计算方式
    if alternative == 'less':
        # 如果alternative为'less'，使用超几何分布的累积分布函数计算p值
        pvalues = hypergeom.cdf(x1, total, x1_sum_x2, total_col_1).T
    elif alternative == 'greater':
        # 如果alternative为'greater'，使用超几何分布的累积分布函数计算p值（对第二列）
        pvalues = hypergeom.cdf(x2, total, x1_sum_x2, total_col_2).T
    elif alternative == 'two-sided':
        # 如果alternative为'two-sided'，分别计算两个方向上的Boschloo精确检验结果
        boschloo_less = boschloo_exact(table, alternative="less", n=n)
        boschloo_greater = boschloo_exact(table, alternative="greater", n=n)

        # 根据p值大小选择较小的结果
        res = (
            boschloo_less if boschloo_less.pvalue < boschloo_greater.pvalue
            else boschloo_greater
        )

        # 计算双侧检验的p值，定义为两个单侧检验p值的最小值的两倍
        pvalue = np.clip(2 * res.pvalue, a_min=0, a_max=1)
        return BoschlooExactResult(res.statistic, pvalue)
    else:
        # 如果alternative不在支持的选项中，抛出值错误异常
        msg = (
            f"`alternative` should be one of {'two-sided', 'less', 'greater'},"
            f" found {alternative!r}"
        )
        raise ValueError(msg)

    # 根据表格中的值获取Fisher精确检验的统计量
    fisher_stat = pvalues[table[0, 0], table[0, 1]]

    # 通过乘以（1+1e-13）来避免小数值误差，相当于使用相对容差为1e-13和绝对容差为0的np.isclose
    index_arr = pvalues <= fisher_stat * (1+1e-13)

    # 转置x1、x2和x1_sum_x2，准备计算对数组合数
    x1, x2, x1_sum_x2 = x1.T, x2.T, x1_sum_x2.T

    # 计算x1、x2和x1_sum_x2的对数组合数
    x1_log_comb = _compute_log_combinations(total_col_1)
    x2_log_comb = _compute_log_combinations(total_col_2)
    x1_sum_x2_log_comb = x1_log_comb[x1] + x2_log_comb[x2]

    # 使用shgo函数进行优化，计算具有干扰参数的二项式对数p值
    result = shgo(
        _get_binomial_log_p_value_with_nuisance_param,
        args=(x1_sum_x2, x1_sum_x2_log_comb, index_arr),
        bounds=((0, 1),),
        n=n,
        sampling_method="sobol",
    )

    # 将结果中的负对数p值转换为实际p值，并进行裁剪确保在0到1之间
    p_value = np.clip(np.exp(-result.fun), a_min=0, a_max=1)
    return BoschlooExactResult(fisher_stat, p_value)
def _get_binomial_log_p_value_with_nuisance_param(
    nuisance_param, x1_sum_x2, x1_sum_x2_log_comb, index_arr
):
    r"""
    Compute the log pvalue in respect of a nuisance parameter considering
    a 2x2 sample space.

    Parameters
    ----------
    nuisance_param : float
        nuisance parameter used in the computation of the maximisation of
        the p-value. Must be between 0 and 1

    x1_sum_x2 : ndarray
        Sum of x1 and x2 inside barnard_exact

    x1_sum_x2_log_comb : ndarray
        sum of the log combination of x1 and x2

    index_arr : ndarray of boolean

    Returns
    -------
    p_value : float
        Return the maximum p-value considering every nuisance parameter
        between 0 and 1

    Notes
    -----

    Both Barnard's test and Boschloo's test iterate over a nuisance parameter
    :math:`\pi \in [0, 1]` to find the maximum p-value. To search this
    maxima, this function return the negative log pvalue with respect to the
    nuisance parameter passed in params. This negative log p-value is then
    used in `shgo` to find the minimum negative pvalue which is our maximum
    pvalue.

    Also, to compute the different combination used in the
    p-values' computation formula, this function uses `gammaln` which is
    more tolerant for large value than `scipy.special.comb`. `gammaln` gives
    a log combination. For the little precision loss, performances are
    improved a lot.
    """
    # Determine the dimensions of the input array x1_sum_x2
    t1, t2 = x1_sum_x2.shape
    # Calculate the total number of trials in the binomial test
    n = t1 + t2 - 2
    # Suppress division by zero warnings during logarithmic operations
    with np.errstate(divide="ignore", invalid="ignore"):
        # Compute logarithm of nuisance_param, handling where nuisance_param >= 0
        log_nuisance = np.log(
            nuisance_param,
            out=np.zeros_like(nuisance_param),
            where=nuisance_param >= 0,
        )
        # Compute logarithm of 1 - nuisance_param, handling where 1 - nuisance_param >= 0
        log_1_minus_nuisance = np.log(
            1 - nuisance_param,
            out=np.zeros_like(nuisance_param),
            where=1 - nuisance_param >= 0,
        )

        # Compute the influence of nuisance parameter on x1_sum_x2 and n - x1_sum_x2
        nuisance_power_x1_x2 = log_nuisance * x1_sum_x2
        nuisance_power_x1_x2[(x1_sum_x2 == 0)[:, :]] = 0

        nuisance_power_n_minus_x1_x2 = log_1_minus_nuisance * (n - x1_sum_x2)
        nuisance_power_n_minus_x1_x2[(x1_sum_x2 == n)[:, :]] = 0

        # Sum up logarithmic values for different combinations
        tmp_log_values_arr = (
            x1_sum_x2_log_comb
            + nuisance_power_x1_x2
            + nuisance_power_n_minus_x1_x2
        )

    # Select logarithmic values based on index_arr
    tmp_values_from_index = tmp_log_values_arr[index_arr]

    # Find the maximum value among selected logarithmic values
    max_value = tmp_values_from_index.max()

    # Take logarithm of p-value to enhance precision within [0, 1] interval
    # Logarithm operation transforms the interval to [-inf, 0], aiding in precision
    # 设置 numpy 的错误状态，忽略除以零和无效操作的警告
    with np.errstate(divide="ignore", invalid="ignore"):
        # 计算 tmp_values_from_index 减去 max_value 后的指数，然后求和
        log_probs = np.exp(tmp_values_from_index - max_value).sum()
        
        # 计算对数概率值 log_probs，并将结果存储在 log_pvalue 中
        # 使用 np.log 函数计算对数，同时指定当 log_probs 大于 0 时，将结果写入输出数组
        log_pvalue = max_value + np.log(
            log_probs,
            out=np.full_like(log_probs, -np.inf),  # 输出数组，初始化为 -inf
            where=log_probs > 0,  # 仅在 log_probs 大于 0 时有效
        )

    # 由于 shgo 寻找最小值，因此返回的是 -log_pvalue
    return -log_pvalue
def _pval_cvm_2samp_exact(s, m, n):
    """
    Compute the exact p-value of the Cramer-von Mises two-sample test
    for a given value s of the test statistic.
    m and n are the sizes of the samples.

    [1] Y. Xiao, A. Gordon, and A. Yakovlev, "A C++ Program for
        the Cramér-Von Mises Two-Sample Test", J. Stat. Soft.,
        vol. 17, no. 8, pp. 1-15, Dec. 2006.
    [2] T. W. Anderson "On the Distribution of the Two-Sample Cramer-von Mises
        Criterion," The Annals of Mathematical Statistics, Ann. Math. Statist.
        33(3), 1148-1159, (September, 1962)
    """

    # 计算 m 和 n 的最小公倍数
    lcm = np.lcm(m, n)
    # 在文献 [1] 的第 4 页，方程式 3 下面
    a = lcm // m
    b = lcm // n
    # 结合文献 [2] 中的方程式 9 和文献 [1] 中的方程式 2，并解出 ζ
    # 提示：`s` 是文献 [2] 中的 U，文献 [1] 中的 T_2 是文献 [2] 中的 T
    mn = m * n
    zeta = lcm ** 2 * (m + n) * (6 * s - mn * (4 * mn - 1)) // (6 * mn ** 2)

    # 绑定可能出现在 `gs` 中的最大值（记住两行！）
    zeta_bound = lcm**2 * (m + n)  # 第一行中的元素上限
    combinations = comb(m + n, m)  # 第二行的和
    max_gs = max(zeta_bound, combinations)
    dtype = np.min_scalar_type(max_gs)

    # 文献 [1] 第 6 页中定义的 $g_{u, v}^+$ 的频率表
    gs = ([np.array([[0], [1]], dtype=dtype)]
          + [np.empty((2, 0), dtype=dtype) for _ in range(m)])
    for u in range(n + 1):
        next_gs = []
        tmp = np.empty((2, 0), dtype=dtype)
        for v, g in enumerate(gs):
            # 使用文献 [1] 中的方程式 11 递归计算 g。尽管看起来不像，这也包括算法 1 的所有内容的 12/13
            vi, i0, i1 = np.intersect1d(tmp[0], g[0], return_indices=True)
            tmp = np.concatenate([
                np.stack([vi, tmp[1, i0] + g[1, i1]]),
                np.delete(tmp, i0, 1),
                np.delete(g, i1, 1)
            ], 1)
            res = (a * v - b * u) ** 2
            tmp[0] += res.astype(dtype)
            next_gs.append(tmp)
        gs = next_gs
    value, freq = gs[m]
    return np.float64(np.sum(freq[value >= zeta]) / combinations)


@_axis_nan_policy_factory(CramerVonMisesResult, n_samples=2, too_small=1,
                          result_to_tuple=_cvm_result_to_tuple)
def cramervonmises_2samp(x, y, method='auto'):
    """Perform the two-sample Cramér-von Mises test for goodness of fit.

    This is the two-sample version of the Cramér-von Mises test ([1]_):
    for two independent samples :math:`X_1, ..., X_n` and
    :math:`Y_1, ..., Y_m`, the null hypothesis is that the samples
    come from the same (unspecified) continuous distribution.

    Parameters
    ----------
    x : array_like
        A 1-D array of observed values of the random variables :math:`X_i`.
        Must contain at least two observations.
    y : array_like
        A 1-D array of observed values of the random variables :math:`Y_i`.
        Must contain at least two observations.
    """
    method : {'auto', 'asymptotic', 'exact'}, optional
        # 方法参数，用于指定计算 p 值的方法，可选值为 'auto', 'asymptotic', 'exact'，参见下文说明。
        The method used to compute the p-value, see Notes for details.
        # 计算 p 值的方法，详细内容请参阅注释部分的说明。

    Returns
    -------
    res : object with attributes
        statistic : float
            # 统计量，即克拉默-冯·米塞斯统计量。
            Cramér-von Mises statistic.
        pvalue : float
            # p 值。
            The p-value.

    See Also
    --------
    cramervonmises, anderson_ksamp, epps_singleton_2samp, ks_2samp

    Notes
    -----
    .. versionadded:: 1.7.0
        # 添加版本信息，从版本 1.7.0 开始支持该函数。

    The statistic is computed according to equation 9 in [2]_. The
    calculation of the p-value depends on the keyword `method`:

    - ``asymptotic``: The p-value is approximated by using the limiting
      distribution of the test statistic.
        # 当 method='asymptotic' 时，使用测试统计量的极限分布来近似计算 p 值。

    - ``exact``: The exact p-value is computed by enumerating all
      possible combinations of the test statistic, see [2]_.
        # 当 method='exact' 时，通过枚举所有可能的测试统计量组合来精确计算 p 值。

    If ``method='auto'``, the exact approach is used
    if both samples contain equal to or less than 20 observations,
    otherwise the asymptotic distribution is used.
        # 当 method='auto' 时，如果两个样本的观测值都小于或等于 20，则使用精确方法；否则使用渐近分布。

    If the underlying distribution is not continuous, the p-value is likely to
    be conservative (Section 6.2 in [3]_). When ranking the data to compute
    the test statistic, midranks are used if there are ties.
        # 如果底层分布不连续，则 p 值可能是保守的。在计算测试统计量时，如果存在并列的情况，使用中位秩。

    References
    ----------
    .. [1] https://en.wikipedia.org/wiki/Cramer-von_Mises_criterion
    .. [2] Anderson, T.W. (1962). On the distribution of the two-sample
           Cramer-von-Mises criterion. The Annals of Mathematical
           Statistics, pp. 1148-1159.
    .. [3] Conover, W.J., Practical Nonparametric Statistics, 1971.

    Examples
    --------

    Suppose we wish to test whether two samples generated by
    ``scipy.stats.norm.rvs`` have the same distribution. We choose a
    significance level of alpha=0.05.

    >>> import numpy as np
    >>> from scipy import stats
    >>> rng = np.random.default_rng()
    >>> x = stats.norm.rvs(size=100, random_state=rng)
    >>> y = stats.norm.rvs(size=70, random_state=rng)
    >>> res = stats.cramervonmises_2samp(x, y)
    >>> res.statistic, res.pvalue
    (0.29376470588235293, 0.1412873014573014)

    The p-value exceeds our chosen significance level, so we do not
    reject the null hypothesis that the observed samples are drawn from the
    same distribution.

    For small sample sizes, one can compute the exact p-values:

    >>> x = stats.norm.rvs(size=7, random_state=rng)
    >>> y = stats.t.rvs(df=2, size=6, random_state=rng)
    >>> res = stats.cramervonmises_2samp(x, y, method='exact')
    >>> res.statistic, res.pvalue
    (0.197802197802198, 0.31643356643356646)

    The p-value based on the asymptotic distribution is a good approximation
    even though the sample size is small.

    >>> res = stats.cramervonmises_2samp(x, y, method='asymptotic')
    >>> res.statistic, res.pvalue
    (0.197802197802198, 0.2966041181527128)

    Independent of the method, one would not reject the null hypothesis at the
    chosen significance level in this example.
    # 将输入的列表 x 和 y 转换为 NumPy 数组，并按升序排序
    xa = np.sort(np.asarray(x))
    ya = np.sort(np.asarray(y))

    # 检查排序后的数组长度，若任一数组长度小于等于1，则抛出数值错误异常
    if xa.size <= 1 or ya.size <= 1:
        raise ValueError('x and y must contain at least two observations.')
    
    # 检查方法参数是否在指定的列表中，若不在则抛出数值错误异常
    if method not in ['auto', 'exact', 'asymptotic']:
        raise ValueError('method must be either auto, exact or asymptotic.')

    # 计算排序后数组的长度
    nx = len(xa)
    ny = len(ya)

    # 自动选择方法：若最大长度超过20，则选择渐近方法；否则选择精确方法
    if method == 'auto':
        if max(nx, ny) > 20:
            method = 'asymptotic'
        else:
            method = 'exact'

    # 将 x 和 y 的排序合并到一个数组 z 中
    z = np.concatenate([xa, ya])
    # 计算 z 中元素的秩，使用平均秩处理并分割为 rx 和 ry
    r = scipy.stats.rankdata(z, method='average')
    rx = r[:nx]
    ry = r[nx:]

    # 计算统计量 U (文献 [2] 中的方程 10)
    u = nx * np.sum((rx - np.arange(1, nx+1))**2)
    u += ny * np.sum((ry - np.arange(1, ny+1))**2)

    # 计算统计量 T (文献 [2] 中的方程 9)
    k, N = nx*ny, nx + ny
    t = u / (k*N) - (4*k - 1)/(6*N)

    # 根据选择的方法计算 p 值
    if method == 'exact':
        p = _pval_cvm_2samp_exact(u, nx, ny)
    else:
        # 计算 T 的期望值和方差 (文献 [2] 中的方程 11 和 14)
        et = (1 + 1/N)/6
        vt = (N+1) * (4*k*N - 3*(nx**2 + ny**2) - 2*k)
        vt = vt / (45 * N**2 * 4 * k)

        # 计算标准化的统计量 tn (文献 [2] 中的方程 15)
        tn = 1/6 + (t - et) / np.sqrt(45 * vt)

        # 近似 tn 的分布与单样本检验统计量的极限分布
        # 若 tn < 0.003，则 _cdf_cvm_inf(tn) < 1.28*1e-18，直接返回 1.0
        if tn < 0.003:
            p = 1.0
        else:
            p = max(0, 1. - _cdf_cvm_inf(tn))

    # 返回 Cramer-Von Mises 检验的结果，包括统计量 t 和 p 值
    return CramerVonMisesResult(statistic=t, pvalue=p)
    # TukeyHSDResult 类，用于存储 `scipy.stats.tukey_hsd` 函数的结果
    class TukeyHSDResult:
        """Result of `scipy.stats.tukey_hsd`.

        Attributes
        ----------
        statistic : float ndarray
            The computed statistic of the test for each comparison. The element
            at index ``(i, j)`` is the statistic for the comparison between groups
            ``i`` and ``j``.
        pvalue : float ndarray
            The associated p-value from the studentized range distribution. The
            element at index ``(i, j)`` is the p-value for the comparison
            between groups ``i`` and ``j``.

        Notes
        -----
        The string representation of this object displays the most recently
        calculated confidence interval, and if none have been previously
        calculated, it will evaluate ``confidence_interval()``.

        References
        ----------
        .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, "7.4.7.1. Tukey's
               Method."
               https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,
               28 November 2020.
        """

        # 初始化方法，设置统计量、p 值及其他属性
        def __init__(self, statistic, pvalue, _nobs, _ntreatments, _stand_err):
            self.statistic = statistic
            self.pvalue = pvalue
            self._ntreatments = _ntreatments
            self._nobs = _nobs
            self._stand_err = _stand_err
            self._ci = None
            self._ci_cl = None

        # 返回对象的字符串表示形式，显示最近计算的置信区间
        def __str__(self):
            # 注意：`__str__` 方法会打印最近调用的 `confidence_interval` 的置信区间。
            # 如果尚未调用过，则会使用默认的置信水平 .95 进行调用。
            if self._ci is None:
                self.confidence_interval(confidence_level=.95)
            s = ("Tukey's HSD Pairwise Group Comparisons"
                 f" ({self._ci_cl*100:.1f}% Confidence Interval)\n")
            s += "Comparison  Statistic  p-value  Lower CI  Upper CI\n"
            # 遍历 p 值数组的维度，生成每对组之间的比较统计信息
            for i in range(self.pvalue.shape[0]):
                for j in range(self.pvalue.shape[0]):
                    if i != j:
                        s += (f" ({i} - {j}) {self.statistic[i, j]:>10.3f}"
                              f"{self.pvalue[i, j]:>10.3f}"
                              f"{self._ci.low[i, j]:>10.3f}"
                              f"{self._ci.high[i, j]:>10.3f}\n")
            return s


    # 内部函数 `_tukey_hsd_iv`，用于验证输入参数并返回处理后的参数列表
    def _tukey_hsd_iv(args):
        if (len(args)) < 2:
            raise ValueError("There must be more than 1 treatment.")
        # 将参数列表中的每个参数转换为 NumPy 数组
        args = [np.asarray(arg) for arg in args]
        # 检查每个参数是否为一维数组
        for arg in args:
            if arg.ndim != 1:
                raise ValueError("Input samples must be one-dimensional.")
            # 检查每个参数数组的大小是否大于 1
            if arg.size <= 1:
                raise ValueError("Input sample size must be greater than one.")
            # 检查每个参数数组是否包含无穷大值
            if np.isinf(arg).any():
                raise ValueError("Input samples must be finite.")
        return args


    # 函数 `tukey_hsd`，执行 Tukey's HSD 测试以比较多个处理的均值差异
    def tukey_hsd(*args):
        """Perform Tukey's HSD test for equality of means over multiple treatments.

        Tukey's honestly significant difference (HSD) test performs pairwise
        comparison of means for a set of samples. Whereas ANOVA (e.g. `f_oneway`)
        assesses whether the true means underlying each sample are identical,
        """
    # Tukey's HSD (Honestly Significant Difference) test is used for post hoc
    # comparisons of means among multiple samples, assessing whether each
    # sample mean significantly differs from others.

    # The null hypothesis assumes all sample distributions have identical means.
    # The test statistic for each pair of samples is the mean difference. Each
    # p-value represents the probability under the null hypothesis of observing
    # such a large statistic, considering multiple comparisons. Confidence
    # intervals for mean differences are also computed.

    Parameters
    ----------
    sample1, sample2, ... : array_like
        Measurements of each sample group; at least two groups are required.

    Returns
    -------
    result : `~scipy.stats._result_classes.TukeyHSDResult` instance
        Returns an object with:
        statistic : float ndarray
            Test statistics for each group comparison.
        pvalue : float ndarray
            P-values for each group comparison.

        Methods:
        confidence_interval(confidence_level=0.95):
            Computes confidence intervals for mean differences.

    See Also
    --------
    dunnett : Compares means against a control group.

    Notes
    -----
    Assumptions for valid use:
    1. Observations are independent within and among groups.
    2. Observations within each group follow a normal distribution.
    3. Sample distributions have equal finite variance.

    The original test formulation assumes equal sample sizes; for unequal sizes,
    Tukey-Kramer method is employed.

    References
    ----------
    .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, "7.4.7.1. Tukey's
           Method."
           https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,
           Accessed 28 November 2020.
    .. [2] Abdi, Herve & Williams, Lynne. (2021). "Tukey's Honestly Significant
           Difference (HSD) Test."
           https://personal.utdallas.edu/~herve/abdi-HSD2010-pretty.pdf
    .. [3] "One-Way ANOVA Using SAS PROC ANOVA & PROC GLM." SAS Tutorials, 2007,
           www.stattutorials.com/SAS/TUTORIAL-PROC-GLM.htm.
    # 引入所需的库和函数
    >>> import numpy as np
    >>> from scipy.stats import tukey_hsd
    
    # 创建三组数据，分别表示三种头痛药品的缓解时间（单位：分钟）
    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]
    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]
    >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]
    
    # 绘制箱线图以直观比较各组数据的分布情况
    >>> import matplotlib.pyplot as plt
    >>> fig, ax = plt.subplots(1, 1)
    >>> ax.boxplot([group0, group1, group2])
    >>> ax.set_xticklabels(["group0", "group1", "group2"]) # 跳过测试：显示组标签
    >>> ax.set_ylabel("mean") # 跳过测试：显示坐标轴标签
    >>> plt.show()
    
    # 从箱线图可以看出第一组与第二组、第三组在四分位范围有重叠，接下来我们可以使用tukey_hsd测试来确定各组均值之间是否存在显著差异。
    # 我们设置显著性水平为0.05以拒绝零假设。
    
    # 对三组数据进行Tukey's HSD（Honestly Significant Difference）多重比较检验
    >>> res = tukey_hsd(group0, group1, group2)
    
    # 打印Tukey's HSD检验结果，显示每两组之间的比较统计量、p值以及置信区间
    >>> print(res)
    Tukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)
    Comparison  Statistic  p-value   Lower CI   Upper CI
    (0 - 1)     -4.600      0.014     -8.249     -0.951
    (0 - 2)     -0.260      0.980     -3.909      3.389
    (1 - 0)      4.600      0.014      0.951      8.249
    (1 - 2)      4.340      0.020      0.691      7.989
    (2 - 0)      0.260      0.980     -3.389      3.909
    (2 - 1)     -4.340      0.020     -7.989     -0.691
    
    # 零假设是每组数据具有相同的均值。对比“group0”和“group1”以及“group1”和“group2”的p值均不超过0.05，因此我们拒绝它们具有相同均值的零假设。
    # 对比“group0”和“group2”的p值超过0.05，因此我们接受它们的零假设，即它们的均值没有显著差异。
    
    # 我们还可以计算与所选置信水平相关联的置信区间。
    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]
    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]
    # 调用 Tukey HSD 方法对多个组进行多重比较分析，返回 TukeyHSDResult 对象
    args = _tukey_hsd_iv(args)
    # 获取组的数量
    ntreatments = len(args)
    # 计算每个组的均值并转换为 NumPy 数组
    means = np.asarray([np.mean(arg) for arg in args])
    # 获取每个组的样本数量并转换为 NumPy 数组
    nsamples_treatments = np.asarray([a.size for a in args])
    # 计算总样本数
    nobs = np.sum(nsamples_treatments)

    # 计算均方误差（mean square error），有时称为组内均方误差
    mse = (np.sum([np.var(arg, ddof=1) for arg in args] *
                  (nsamples_treatments - 1)) / (nobs - ntreatments))

    # 当组大小不同时，标准误差的计算方式有所不同
    if np.unique(nsamples_treatments).size == 1:
        # 如果所有输入组的长度相同，则只需计算一个值
        normalize = 2 / nsamples_treatments[0]
    else:
        # 对于不同大小的组，必须计算每个单独比较的方差值
        # 使用广播机制获取结果矩阵
        normalize = 1 / nsamples_treatments + 1 / nsamples_treatments[None].T

    # 标准误差在计算 Tukey 准则和查找 p 值时使用
    stand_err = np.sqrt(normalize * mse / 2)

    # 均值差异是测试统计量
    mean_differences = means[None].T - means

    # 计算 t 统计量，用于在 studentized range 的生存函数中获取 p 值
    t_stat = np.abs(mean_differences) / stand_err

    # 准备参数以计算 studentized range 的生存函数，从而得到 p 值
    params = t_stat, ntreatments, nobs - ntreatments
    # 使用 studentized range 的生存函数计算 p 值
    pvalues = distributions.studentized_range.sf(*params)

    # 返回 TukeyHSDResult 对象，包括均值差异、p 值、组数、总样本数和标准误差
    return TukeyHSDResult(mean_differences, pvalues, ntreatments,
                          nobs, stand_err)
```