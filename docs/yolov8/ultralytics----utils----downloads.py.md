# `.\yolov8\ultralytics\utils\downloads.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

# å¯¼å…¥å¿…è¦çš„åº“
import contextlib  # æä¾›ä¸Šä¸‹æ–‡ç®¡ç†å·¥å…·çš„æ ‡å‡†åº“
import re  # æä¾›æ­£åˆ™è¡¨è¾¾å¼æ“ä½œçš„æ¨¡å—
import shutil  # æä¾›é«˜çº§æ–‡ä»¶æ“ä½œçš„æ¨¡å—
import subprocess  # æä¾›è¿è¡Œå¤–éƒ¨å‘½ä»¤çš„åŠŸèƒ½
from itertools import repeat  # æä¾›è¿­ä»£å·¥å…·å‡½æ•°
from multiprocessing.pool import ThreadPool  # æä¾›å¤šçº¿ç¨‹æ± çš„åŠŸèƒ½
from pathlib import Path  # æä¾›å¤„ç†æ–‡ä»¶è·¯å¾„çš„ç±»å’Œå‡½æ•°
from urllib import parse, request  # æä¾›å¤„ç† URL ç›¸å…³çš„æ¨¡å—

import requests  # æä¾›è¿›è¡Œ HTTP è¯·æ±‚çš„æ¨¡å—
import torch  # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶

# ä» Ultralytics çš„ utils æ¨¡å—ä¸­å¯¼å…¥ç‰¹å®šå‡½æ•°å’Œç±»
from ultralytics.utils import LOGGER, TQDM, checks, clean_url, emojis, is_online, url2file

# å®šä¹‰ Ultralytics GitHub ä¸Šçš„èµ„æºä»“åº“å’Œæ–‡ä»¶ååˆ—è¡¨
GITHUB_ASSETS_REPO = "ultralytics/assets"
GITHUB_ASSETS_NAMES = (
    [f"yolov8{k}{suffix}.pt" for k in "nsmlx" for suffix in ("", "-cls", "-seg", "-pose", "-obb", "-oiv7")]
    + [f"yolov5{k}{resolution}u.pt" for k in "nsmlx" for resolution in ("", "6")]
    + [f"yolov3{k}u.pt" for k in ("", "-spp", "-tiny")]
    + [f"yolov8{k}-world.pt" for k in "smlx"]
    + [f"yolov8{k}-worldv2.pt" for k in "smlx"]
    + [f"yolov9{k}.pt" for k in "tsmce"]
    + [f"yolov10{k}.pt" for k in "nsmblx"]
    + [f"yolo_nas_{k}.pt" for k in "sml"]
    + [f"sam_{k}.pt" for k in "bl"]
    + [f"FastSAM-{k}.pt" for k in "sx"]
    + [f"rtdetr-{k}.pt" for k in "lx"]
    + ["mobile_sam.pt"]
    + ["calibration_image_sample_data_20x128x128x3_float32.npy.zip"]
)
GITHUB_ASSETS_STEMS = [Path(k).stem for k in GITHUB_ASSETS_NAMES]


def is_url(url, check=False):
    """
    éªŒè¯ç»™å®šçš„å­—ç¬¦ä¸²æ˜¯å¦ä¸º URLï¼Œå¹¶å¯é€‰æ‹©æ£€æŸ¥è¯¥ URL æ˜¯å¦åœ¨çº¿å¯ç”¨ã€‚

    Args:
        url (str): è¦éªŒè¯ä¸º URL çš„å­—ç¬¦ä¸²ã€‚
        check (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™é¢å¤–æ£€æŸ¥ URL æ˜¯å¦åœ¨çº¿å¯ç”¨ã€‚é»˜è®¤ä¸º Trueã€‚

    Returns:
        bool: å¦‚æœæ˜¯æœ‰æ•ˆçš„ URL è¿”å› Trueã€‚å¦‚æœ 'check' ä¸º Trueï¼Œåˆ™åŒæ—¶æ£€æŸ¥ URL åœ¨çº¿æ˜¯å¦å¯ç”¨ã€‚å¦åˆ™è¿”å› Falseã€‚

    Example:
        ```py
        valid = is_url("https://www.example.com")
        ```
    """
    with contextlib.suppress(Exception):
        url = str(url)
        result = parse.urlparse(url)
        assert all([result.scheme, result.netloc])  # æ£€æŸ¥æ˜¯å¦ä¸º URL
        if check:
            with request.urlopen(url) as response:
                return response.getcode() == 200  # æ£€æŸ¥æ˜¯å¦åœ¨çº¿å¯ç”¨
        return True
    return False


def delete_dsstore(path, files_to_delete=(".DS_Store", "__MACOSX")):
    """
    åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ ".DS_Store" æ–‡ä»¶ã€‚

    Args:
        path (str, optional): åº”åˆ é™¤ ".DS_Store" æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
        files_to_delete (tuple): è¦åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨ã€‚

    Example:
        ```py
        from ultralytics.utils.downloads import delete_dsstore

        delete_dsstore('path/to/dir')
        ```

    Note:
        ".DS_Store" æ–‡ä»¶ç”±è‹¹æœæ“ä½œç³»ç»Ÿåˆ›å»ºï¼ŒåŒ…å«å…³äºæ–‡ä»¶å’Œæ–‡ä»¶å¤¹çš„å…ƒæ•°æ®ã€‚å®ƒä»¬æ˜¯éšè—çš„ç³»ç»Ÿæ–‡ä»¶ï¼Œåœ¨ä¸åŒæ“ä½œç³»ç»Ÿé—´ä¼ è¾“æ–‡ä»¶æ—¶å¯èƒ½ä¼šå¼•èµ·é—®é¢˜ã€‚
    """
    # éå†éœ€è¦åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨
    for file in files_to_delete:
        # ä½¿ç”¨è·¯å¾„å¯¹è±¡æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…æŒ‡å®šæ–‡ä»¶åçš„æ–‡ä»¶
        matches = list(Path(path).rglob(file))
        # è®°å½•æ—¥å¿—ä¿¡æ¯ï¼ŒæŒ‡ç¤ºæ­£åœ¨åˆ é™¤å“ªäº›æ–‡ä»¶
        LOGGER.info(f"Deleting {file} files: {matches}")
        # éå†æ¯ä¸€ä¸ªæ‰¾åˆ°çš„æ–‡ä»¶è·¯å¾„ï¼Œå¹¶åˆ é™¤æ–‡ä»¶
        for f in matches:
            f.unlink()
# è§£å‹ç¼©ä¸€ä¸ª ZIP æ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„ï¼Œæ’é™¤åœ¨æ’é™¤åˆ—è¡¨ä¸­çš„æ–‡ä»¶
def unzip_file(file, path=None, exclude=(".DS_Store", "__MACOSX"), exist_ok=False, progress=True):
    """
    Unzips a *.zip file to the specified path, excluding files containing strings in the exclude list.

    If the zipfile does not contain a single top-level directory, the function will create a new
    directory with the same name as the zipfile (without the extension) to extract its contents.
    If a path is not provided, the function will use the parent directory of the zipfile as the default path.

    Args:
        file (str): The path to the zipfile to be extracted.
        path (str, optional): The path to extract the zipfile to. Defaults to None.
        exclude (tuple, optional): A tuple of filename strings to be excluded. Defaults to ('.DS_Store', '__MACOSX').
        exist_ok (bool, optional): Whether to overwrite existing contents if they exist. Defaults to False.
        progress (bool, optional): Whether to display a progress bar. Defaults to True.

    Raises:
        BadZipFile: If the provided file does not exist or is not a valid zipfile.

    Returns:
        (Path): The path to the directory where the zipfile was extracted.

    Example:
        ```py
        from ultralytics.utils.downloads import unzip_file

        dir = unzip_file('path/to/file.zip')
        ```
    """
    from zipfile import ZipFile, BadZipFile
    from pathlib import Path

    # åˆ é™¤ç›®å½•ä¸­çš„ .DS_Store æ–‡ä»¶
    delete_dsstore(directory)
    # è½¬æ¢è¾“å…¥çš„è·¯å¾„ä¸º Path å¯¹è±¡
    directory = Path(directory)
    # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡º FileNotFoundError å¼‚å¸¸
    if not directory.is_dir():
        raise FileNotFoundError(f"Directory '{directory}' does not exist.")

    # æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰ä¸åœ¨æ’é™¤åˆ—è¡¨ä¸­çš„æ–‡ä»¶å¹¶å‹ç¼©
    files_to_zip = [f for f in directory.rglob("*") if f.is_file() and all(x not in f.name for x in exclude)]
    # è®¾å®šå‹ç¼©åçš„æ–‡ä»¶åä¸ºç›®å½•ååŠ  .zip åç¼€
    zip_file = directory.with_suffix(".zip")
    # è®¾å®šå‹ç¼©æ–¹å¼ï¼Œæ ¹æ® compress å‚æ•°é€‰æ‹© ZIP_DEFLATED æˆ– ZIP_STORED
    compression = ZIP_DEFLATED if compress else ZIP_STORED
    # ä½¿ç”¨ ZipFile å¯¹è±¡æ‰“å¼€ zip_fileï¼Œä»¥å†™å…¥æ¨¡å¼åˆ›å»ºå‹ç¼©æ–‡ä»¶
    with ZipFile(zip_file, "w", compression) as f:
        # ä½¿ç”¨ TQDM æ˜¾ç¤ºå‹ç¼©è¿›åº¦æ¡ï¼Œéå† files_to_zip åˆ—è¡¨ä¸­çš„æ–‡ä»¶
        for file in TQDM(files_to_zip, desc=f"Zipping {directory} to {zip_file}...", unit="file", disable=not progress):
            # å°†æ–‡ä»¶å†™å…¥å‹ç¼©æ–‡ä»¶ä¸­ï¼Œæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ä»¥ç›®å½•ä¸ºåŸºå‡†
            f.write(file, file.relative_to(directory))

    # è¿”å›å‹ç¼©æ–‡ä»¶çš„è·¯å¾„
    return zip_file  # return path to zip file
    from zipfile import BadZipFile, ZipFile, is_zipfile

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæœ‰æ•ˆçš„ ZIP æ–‡ä»¶
    if not (Path(file).exists() and is_zipfile(file)):
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è€…ä¸æ˜¯æœ‰æ•ˆçš„ ZIP æ–‡ä»¶ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        raise BadZipFile(f"File '{file}' does not exist or is a bad zip file.")
    
    if path is None:
        path = Path(file).parent  # é»˜è®¤è·¯å¾„ä¸ºæ–‡ä»¶æ‰€åœ¨ç›®å½•

    # è§£å‹ç¼©æ–‡ä»¶å†…å®¹
    with ZipFile(file) as zipObj:
        # ä»æ‰€æœ‰æ–‡ä»¶ä¸­ç­›é€‰å‡ºä¸åŒ…å«æŒ‡å®šæ’é™¤é¡¹çš„æ–‡ä»¶
        files = [f for f in zipObj.namelist() if all(x not in f for x in exclude)]
        
        # è·å–é¡¶å±‚ç›®å½•åˆ—è¡¨
        top_level_dirs = {Path(f).parts[0] for f in files}

        # å†³å®šæ˜¯ç›´æ¥è§£å‹ç¼©è¿˜æ˜¯è§£å‹ç¼©åˆ°ä¸€ä¸ªç›®å½•
        unzip_as_dir = len(top_level_dirs) == 1  # åˆ¤æ–­æ˜¯å¦åªæœ‰ä¸€ä¸ªé¡¶å±‚ç›®å½•
        if unzip_as_dir:
            # è‹¥ ZIP æ–‡ä»¶åªæœ‰ä¸€ä¸ªé¡¶å±‚ç›®å½•ï¼Œåˆ™è§£å‹åˆ°æŒ‡å®šçš„è·¯å¾„ä¸‹
            extract_path = path
            path = Path(path) / list(top_level_dirs)[0]  # å°†é¡¶å±‚ç›®å½•æ·»åŠ åˆ°è·¯å¾„ä¸­
        else:
            # è‹¥ ZIP æ–‡ä»¶æœ‰å¤šä¸ªæ–‡ä»¶åœ¨é¡¶å±‚ï¼Œåˆ™è§£å‹ç¼©åˆ°å•ç‹¬çš„å­ç›®å½•ä¸­
            path = extract_path = Path(path) / Path(file).stem  # åˆ›å»ºä¸€ä¸ªæ–°çš„å­ç›®å½•

        # æ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²ç»å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œå¦‚æœä¸å…è®¸è¦†ç›–ï¼Œåˆ™ç›´æ¥è¿”å›ç›®å½•è·¯å¾„
        if path.exists() and any(path.iterdir()) and not exist_ok:
            LOGGER.warning(f"WARNING âš ï¸ Skipping {file} unzip as destination directory {path} is not empty.")
            return path

        # éå†æ–‡ä»¶åˆ—è¡¨ï¼Œé€ä¸ªè§£å‹æ–‡ä»¶
        for f in TQDM(files, desc=f"Unzipping {file} to {Path(path).resolve()}...", unit="file", disable=not progress):
            # ç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨æŒ‡å®šçš„è§£å‹è·¯å¾„å†…ï¼Œé¿å…è·¯å¾„éå†å®‰å…¨æ¼æ´
            if ".." in Path(f).parts:
                LOGGER.warning(f"Potentially insecure file path: {f}, skipping extraction.")
                continue
            zipObj.extract(f, extract_path)

    return path  # è¿”å›è§£å‹åçš„ç›®å½•è·¯å¾„
# æ ¹æ®ç»™å®šçš„ URL è·å–æ–‡ä»¶çš„å¤´éƒ¨ä¿¡æ¯
try:
    r = requests.head(url)  # å‘èµ· HEAD è¯·æ±‚è·å–æ–‡ä»¶ä¿¡æ¯
    assert r.status_code < 400, f"URL error for {url}: {r.status_code} {r.reason}"  # æ£€æŸ¥å“åº”çŠ¶æ€ç 
except Exception:
    return True  # è¯·æ±‚å‡ºç°é—®é¢˜ï¼Œé»˜è®¤è¿”å› True

# è®¡ç®—æ¯ä¸ª GiBï¼ˆ2^30 å­—èŠ‚ï¼‰
gib = 1 << 30  # æ¯ä¸ª GiB çš„å­—èŠ‚æ•°
# è®¡ç®—è¦ä¸‹è½½æ–‡ä»¶çš„å¤§å°ï¼ˆGBï¼‰
data = int(r.headers.get("Content-Length", 0)) / gib  # æ–‡ä»¶å¤§å°ï¼ˆGBï¼‰

# è·å–æŒ‡å®šè·¯å¾„çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ
total, used, free = (x / gib for x in shutil.disk_usage(path))  # æ€»ç©ºé—´ã€å·²ç”¨ç©ºé—´ã€å‰©ä½™ç©ºé—´ï¼ˆGBï¼‰

# æ£€æŸ¥å‰©ä½™ç©ºé—´æ˜¯å¦è¶³å¤Ÿ
if data * sf < free:
    return True  # ç©ºé—´è¶³å¤Ÿ

# ç£ç›˜ç©ºé—´ä¸è¶³çš„æƒ…å†µ
text = (
    f"WARNING âš ï¸ Insufficient free disk space {free:.1f} GB < {data * sf:.3f} GB required, "
    f"Please free {data * sf - free:.1f} GB additional disk space and try again."
)
if hard:
    raise MemoryError(text)  # æŠ›å‡ºå†…å­˜é”™è¯¯å¼‚å¸¸
LOGGER.warning(text)  # è®°å½•è­¦å‘Šæ—¥å¿—
return False  # è¿”å›ç©ºé—´ä¸è¶³
    # ä½¿ç”¨ requests åº“åˆ›å»ºä¸€ä¸ªä¼šè¯å¯¹è±¡
    with requests.Session() as session:
        # å‘é€ GET è¯·æ±‚åˆ°æŒ‡å®šçš„ Google Drive URLï¼Œå¹¶å…è®¸æµå¼ä¼ è¾“
        response = session.get(drive_url, stream=True)
        
        # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦åŒ…å« "quota exceeded"ï¼Œå¦‚æœæ˜¯åˆ™æŠ›å‡ºè¿æ¥é”™è¯¯å¼‚å¸¸
        if "quota exceeded" in str(response.content.lower()):
            raise ConnectionError(
                emojis(
                    f"âŒ  Google Drive file download quota exceeded. "
                    f"Please try again later or download this file manually at {link}."
                )
            )
        
        # éå†å“åº”ä¸­çš„ cookies
        for k, v in response.cookies.items():
            # å¦‚æœ cookie çš„é”®ä»¥ "download_warning" å¼€å¤´ï¼Œå°† token æ·»åŠ åˆ° drive_url ä¸­
            if k.startswith("download_warning"):
                drive_url += f"&confirm={v}"  # v æ˜¯ token
        
        # è·å–å“åº”å¤´ä¸­çš„ content-disposition å±æ€§
        cd = response.headers.get("content-disposition")
        
        # å¦‚æœ content-disposition å­˜åœ¨
        if cd:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æå‡ºæ–‡ä»¶å
            filename = re.findall('filename="(.+)"', cd)[0]
    
    # è¿”å›æ›´æ–°åçš„ drive_url å’Œè§£æå‡ºçš„æ–‡ä»¶å filename
    return drive_url, filename
# å®šä¹‰ä¸€ä¸ªå®‰å…¨ä¸‹è½½å‡½æ•°ï¼Œä»æŒ‡å®šçš„ URL ä¸‹è½½æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§é€‰é¡¹å¦‚é‡è¯•ã€è§£å‹å’Œåˆ é™¤å·²ä¸‹è½½æ–‡ä»¶ç­‰

def safe_download(
    url,
    file=None,
    dir=None,
    unzip=True,
    delete=False,
    curl=False,
    retry=3,
    min_bytes=1e0,
    exist_ok=False,
    progress=True,
):
    """
    Downloads files from a URL, with options for retrying, unzipping, and deleting the downloaded file.

    Args:
        url (str): The URL of the file to be downloaded.
        file (str, optional): The filename of the downloaded file.
            If not provided, the file will be saved with the same name as the URL.
        dir (str, optional): The directory to save the downloaded file.
            If not provided, the file will be saved in the current working directory.
        unzip (bool, optional): Whether to unzip the downloaded file. Default: True.
        delete (bool, optional): Whether to delete the downloaded file after unzipping. Default: False.
        curl (bool, optional): Whether to use curl command line tool for downloading. Default: False.
        retry (int, optional): The number of times to retry the download in case of failure. Default: 3.
        min_bytes (float, optional): The minimum number of bytes that the downloaded file should have, to be considered
            a successful download. Default: 1E0.
        exist_ok (bool, optional): Whether to overwrite existing contents during unzipping. Defaults to False.
        progress (bool, optional): Whether to display a progress bar during the download. Default: True.

    Example:
        ```py
        from ultralytics.utils.downloads import safe_download

        link = "https://ultralytics.com/assets/bus.jpg"
        path = safe_download(link)
        ```
    """

    gdrive = url.startswith("https://drive.google.com/")  # æ£€æŸ¥ URL æ˜¯å¦æ˜¯è°·æ­Œé©±åŠ¨å™¨çš„é“¾æ¥
    if gdrive:
        url, file = get_google_drive_file_info(url)  # å¦‚æœæ˜¯è°·æ­Œé©±åŠ¨å™¨é“¾æ¥ï¼Œè·å–æ–‡ä»¶ä¿¡æ¯

    f = Path(dir or ".") / (file or url2file(url))  # æ„é€ æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤åœ¨å½“å‰ç›®å½•ä¸‹ç”Ÿæˆæˆ–æŒ‡å®šç›®å½•
    if "://" not in str(url) and Path(url).is_file():  # æ£€æŸ¥ URL æ˜¯å¦å­˜åœ¨ï¼ˆåœ¨ Windows Python<3.10 ä¸­éœ€è¦æ£€æŸ¥ '://'ï¼‰
        f = Path(url)  # å¦‚æœ URL æ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œåˆ™ç›´æ¥ä½¿ç”¨è¯¥è·¯å¾„ä½œä¸ºæ–‡ä»¶å
    elif not f.is_file():  # å¦‚æœ URL æˆ–æ–‡ä»¶ä¸å­˜åœ¨
        uri = (url if gdrive else clean_url(url)).replace(  # æ¸…ç†å’Œæ›¿æ¢çš„ URL
            "https://github.com/ultralytics/assets/releases/download/v0.0.0/",
            "https://ultralytics.com/assets/",  # æ›¿æ¢ä¸ºçš„ URL åˆ«å
        )
        desc = f"Downloading {uri} to '{f}'"  # ä¸‹è½½æè¿°ä¿¡æ¯
        LOGGER.info(f"{desc}...")  # è®°å½•ä¸‹è½½ä¿¡æ¯åˆ°æ—¥å¿—
        f.parent.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        check_disk_space(url, path=f.parent)  # æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦è¶³å¤Ÿ
        for i in range(retry + 1):  # é‡è¯•ä¸‹è½½çš„æ¬¡æ•°èŒƒå›´
            try:
                if curl or i > 0:  # ä½¿ç”¨ curl ä¸‹è½½å¹¶æ”¯æŒé‡è¯•
                    s = "sS" * (not progress)  # æ˜¯å¦é™é»˜ä¸‹è½½
                    r = subprocess.run(["curl", "-#", f"-{s}L", url, "-o", f, "--retry", "3", "-C", "-"]).returncode  # æ‰§è¡Œ curl å‘½ä»¤ä¸‹è½½æ–‡ä»¶
                    assert r == 0, f"Curl return value {r}"  # ç¡®ä¿ curl å‘½ä»¤è¿”å›å€¼ä¸º 0ï¼Œè¡¨ç¤ºä¸‹è½½æˆåŠŸ
                else:  # ä½¿ç”¨ urllib ä¸‹è½½
                    method = "torch"
                    if method == "torch":
                        torch.hub.download_url_to_file(url, f, progress=progress)  # ä½¿ç”¨ torch æ¨¡å—ä¸‹è½½æ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„
                    else:
                        with request.urlopen(url) as response, TQDM(  # ä½¿ç”¨ urllib æ‰“å¼€ URL å¹¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                            total=int(response.getheader("Content-Length", 0)),
                            desc=desc,
                            disable=not progress,
                            unit="B",
                            unit_scale=True,
                            unit_divisor=1024,
                        ) as pbar:
                            with open(f, "wb") as f_opened:  # æ‰“å¼€æ–‡ä»¶å¹¶å†™å…¥ä¸‹è½½çš„æ•°æ®
                                for data in response:
                                    f_opened.write(data)
                                    pbar.update(len(data))  # æ›´æ–°ä¸‹è½½è¿›åº¦æ¡

                if f.exists():  # å¦‚æœæ–‡ä»¶å­˜åœ¨
                    if f.stat().st_size > min_bytes:  # å¦‚æœæ–‡ä»¶å¤§å°å¤§äºæŒ‡å®šçš„æœ€å°å­—èŠ‚æ•°
                        break  # æˆåŠŸä¸‹è½½ï¼Œé€€å‡ºå¾ªç¯
                    f.unlink()  # åˆ é™¤éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
            except Exception as e:
                if i == 0 and not is_online():  # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å°è¯•ä¸”æœªè”ç½‘
                    raise ConnectionError(emojis(f"âŒ  Download failure for {uri}. Environment is not online.")) from e  # æŠ›å‡ºè¿æ¥é”™è¯¯å¼‚å¸¸
                elif i >= retry:  # å¦‚æœé‡è¯•æ¬¡æ•°è¶…è¿‡è®¾å®šçš„å€¼
                    raise ConnectionError(emojis(f"âŒ  Download failure for {uri}. Retry limit reached.")) from e  # æŠ›å‡ºè¿æ¥é”™è¯¯å¼‚å¸¸
                LOGGER.warning(f"âš ï¸ Download failure, retrying {i + 1}/{retry} {uri}...")  # è®°å½•ä¸‹è½½å¤±è´¥å¹¶é‡è¯•çš„è­¦å‘Šä¿¡æ¯

    if unzip and f.exists() and f.suffix in {"", ".zip", ".tar", ".gz"}:  # å¦‚æœéœ€è¦è§£å‹ä¸”æ–‡ä»¶å­˜åœ¨ä¸”æ–‡ä»¶åç¼€åˆæ³•
        from zipfile import is_zipfile

        unzip_dir = (dir or f.parent).resolve()  # å¦‚æœæä¾›äº†ç›®å½•åˆ™è§£å‹åˆ°æŒ‡å®šç›®å½•ï¼Œå¦åˆ™è§£å‹åˆ°æ–‡ä»¶æ‰€åœ¨ç›®å½•
        if is_zipfile(f):  # å¦‚æœæ˜¯ ZIP æ–‡ä»¶
            unzip_dir = unzip_file(file=f, path=unzip_dir, exist_ok=exist_ok, progress=progress)  # è§£å‹ ZIP æ–‡ä»¶
        elif f.suffix in {".tar", ".gz"}:  # å¦‚æœæ˜¯ .tar æˆ– .gz æ–‡ä»¶
            LOGGER.info(f"Unzipping {f} to {unzip_dir}...")  # è®°å½•è§£å‹ä¿¡æ¯åˆ°æ—¥å¿—
            subprocess.run(["tar", "xf" if f.suffix == ".tar" else "xfz", f, "--directory", unzip_dir], check=True)  # ä½¿ç”¨ tar å‘½ä»¤è§£å‹æ–‡ä»¶
        if delete:
            f.unlink()  # åˆ é™¤åŸå§‹å‹ç¼©æ–‡ä»¶
        return unzip_dir  # è¿”å›è§£å‹åçš„ç›®å½•è·¯å¾„
# ä» GitHub ä»“åº“ä¸­è·å–æŒ‡å®šç‰ˆæœ¬çš„æ ‡ç­¾å’Œèµ„äº§åˆ—è¡¨ã€‚å¦‚æœæœªæŒ‡å®šç‰ˆæœ¬ï¼Œåˆ™è·å–æœ€æ–°å‘å¸ƒçš„èµ„äº§ã€‚
def get_github_assets(repo="ultralytics/assets", version="latest", retry=False):
    # å¦‚æœç‰ˆæœ¬ä¸æ˜¯æœ€æ–°ï¼Œå°†ç‰ˆæœ¬å·æ ¼å¼åŒ–ä¸º 'tags/version'ï¼Œä¾‹å¦‚ 'tags/v6.2'
    if version != "latest":
        version = f"tags/{version}"
    # æ„å»º GitHub API è¯·æ±‚çš„ URL
    url = f"https://api.github.com/repos/{repo}/releases/{version}"
    # å‘é€ GET è¯·æ±‚è·å–æ•°æ®
    r = requests.get(url)  # github api
    # å¦‚æœè¯·æ±‚å¤±è´¥ä¸”ä¸æ˜¯å› ä¸º 403 çŠ¶æ€ç é™åˆ¶ï¼Œå¹¶ä¸”è®¾ç½®äº†é‡è¯•æ ‡å¿—ï¼Œåˆ™å†æ¬¡å°è¯•è¯·æ±‚
    if r.status_code != 200 and r.reason != "rate limit exceeded" and retry:
        r = requests.get(url)  # try again
    # å¦‚æœè¯·æ±‚ä»ç„¶å¤±è´¥ï¼Œè®°å½•è­¦å‘Šæ—¥å¿—å¹¶è¿”å›ç©ºå­—ç¬¦ä¸²å’Œç©ºåˆ—è¡¨
    if r.status_code != 200:
        LOGGER.warning(f"âš ï¸ GitHub assets check failure for {url}: {r.status_code} {r.reason}")
        return "", []
    # è§£æ JSON æ•°æ®ï¼Œè¿”å›æ ‡ç­¾åå’Œèµ„äº§åç§°åˆ—è¡¨
    data = r.json()
    return data["tag_name"], [x["name"] for x in data["assets"]]  # tag, assets i.e. ['yolov8n.pt', 'yolov8s.pt', ...]


# å°è¯•ä» GitHub å‘å¸ƒèµ„äº§ä¸­ä¸‹è½½æ–‡ä»¶ï¼Œå¦‚æœæœ¬åœ°ä¸å­˜åœ¨ã€‚é¦–å…ˆæ£€æŸ¥æœ¬åœ°æ–‡ä»¶ï¼Œç„¶åå°è¯•ä»æŒ‡å®šçš„ GitHub ä»“åº“ç‰ˆæœ¬ä¸‹è½½ã€‚
def attempt_download_asset(file, repo="ultralytics/assets", release="v8.2.0", **kwargs):
    from ultralytics.utils import SETTINGS  # ç”¨äºè§£å†³å¾ªç¯å¯¼å…¥é—®é¢˜çš„å±€éƒ¨å¼•å…¥

    # å¯¹æ–‡ä»¶åè¿›è¡Œ YOLOv5u æ–‡ä»¶åæ£€æŸ¥å’Œæ›´æ–°
    file = str(file)
    file = checks.check_yolov5u_filename(file)
    file = Path(file.strip().replace("'", ""))
    # å¦‚æœæ–‡ä»¶å­˜åœ¨äºæœ¬åœ°ï¼Œç›´æ¥è¿”å›æ–‡ä»¶è·¯å¾„
    if file.exists():
        return str(file)
    # å¦‚æœæ–‡ä»¶å­˜åœ¨äºè®¾ç½®ä¸­æŒ‡å®šçš„æƒé‡ç›®å½•ä¸­ï¼Œç›´æ¥è¿”å›æ–‡ä»¶è·¯å¾„
    elif (SETTINGS["weights_dir"] / file).exists():
        return str(SETTINGS["weights_dir"] / file)
    else:
        # å¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œåˆ™æ˜¯URL
        name = Path(parse.unquote(str(file))).name  # è§£ç æ–‡ä»¶è·¯å¾„ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œå¦‚ '%2F' è§£ç ä¸º '/'
        download_url = f"https://github.com/{repo}/releases/download"
        
        if str(file).startswith(("http:/", "https:/")):  # å¦‚æœæ˜¯ä»¥ http:/ æˆ– https:/ å¼€å¤´çš„URLï¼Œåˆ™ä¸‹è½½æ–‡ä»¶
            url = str(file).replace(":/", "://")  # ä¿®æ­£URLæ ¼å¼ï¼ŒPathlib ä¼šå°† :// è½¬æ¢ä¸º :/
            file = url2file(name)  # è§£æURLä¸­çš„è®¤è¯ä¿¡æ¯ï¼Œä¾‹å¦‚ https://url.com/file.txt?auth...
            
            if Path(file).is_file():
                LOGGER.info(f"Found {clean_url(url)} locally at {file}")  # æ–‡ä»¶å·²å­˜åœ¨äºæœ¬åœ°
            else:
                safe_download(url=url, file=file, min_bytes=1e5, **kwargs)  # å®‰å…¨ä¸‹è½½æ–‡ä»¶

        elif repo == GITHUB_ASSETS_REPO and name in GITHUB_ASSETS_NAMES:
            # å¦‚æœæ˜¯ GitHub çš„èµ„æºä»“åº“ä¸”æ–‡ä»¶ååœ¨é¢„å®šä¹‰çš„èµ„æºåç§°åˆ—è¡¨ä¸­ï¼Œåˆ™å®‰å…¨ä¸‹è½½
            safe_download(url=f"{download_url}/{release}/{name}", file=file, min_bytes=1e5, **kwargs)

        else:
            # å¦åˆ™ï¼Œè·å–æŒ‡å®šä»“åº“å’Œå‘å¸ƒç‰ˆæœ¬çš„ GitHub èµ„æºæ ‡ç­¾å’Œæ–‡ä»¶åˆ—è¡¨
            tag, assets = get_github_assets(repo, release)
            if not assets:
                tag, assets = get_github_assets(repo)  # è·å–æœ€æ–°çš„å‘å¸ƒç‰ˆæœ¬
            if name in assets:
                # å¦‚æœæ–‡ä»¶ååœ¨èµ„æºåˆ—è¡¨ä¸­ï¼Œåˆ™å®‰å…¨ä¸‹è½½å¯¹åº”æ–‡ä»¶
                safe_download(url=f"{download_url}/{tag}/{name}", file=file, min_bytes=1e5, **kwargs)

        return str(file)  # è¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆæœ¬åœ°æ–‡ä»¶æˆ–ä¸‹è½½åçš„æ–‡ä»¶è·¯å¾„ï¼‰
# å®šä¹‰äº†ä¸€ä¸ªä¸‹è½½å‡½æ•°ï¼Œç”¨äºä»æŒ‡å®šçš„ URL ä¸‹è½½æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•ã€‚æ”¯æŒå¹¶å‘ä¸‹è½½å¦‚æœæŒ‡å®šäº†å¤šä¸ªçº¿ç¨‹ã€‚
def download(url, dir=Path.cwd(), unzip=True, delete=False, curl=False, threads=1, retry=3, exist_ok=False):
    """
    Downloads files from specified URLs to a given directory. Supports concurrent downloads if multiple threads are
    specified.

    Args:
        url (str | list): The URL or list of URLs of the files to be downloaded.
        dir (Path, optional): The directory where the files will be saved. Defaults to the current working directory.
        unzip (bool, optional): Flag to unzip the files after downloading. Defaults to True.
        delete (bool, optional): Flag to delete the zip files after extraction. Defaults to False.
        curl (bool, optional): Flag to use curl for downloading. Defaults to False.
        threads (int, optional): Number of threads to use for concurrent downloads. Defaults to 1.
        retry (int, optional): Number of retries in case of download failure. Defaults to 3.
        exist_ok (bool, optional): Whether to overwrite existing contents during unzipping. Defaults to False.

    Example:
        ```py
        download('https://ultralytics.com/assets/example.zip', dir='path/to/dir', unzip=True)
        ```
    """
    dir = Path(dir)  # å°†ç›®å½•å‚æ•°è½¬æ¢ä¸º Path å¯¹è±¡
    dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•ï¼Œå¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™é€’å½’åˆ›å»º

    if threads > 1:
        # å¦‚æœæŒ‡å®šäº†å¤šä¸ªçº¿ç¨‹ï¼Œåˆ™ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
        with ThreadPool(threads) as pool:
            pool.map(
                lambda x: safe_download(
                    url=x[0],  # å•ä¸ªæ–‡ä»¶çš„ä¸‹è½½ URL
                    dir=x[1],  # ä¸‹è½½æ–‡ä»¶ä¿å­˜çš„ç›®å½•
                    unzip=unzip,  # æ˜¯å¦è§£å‹ç¼©
                    delete=delete,  # æ˜¯å¦åˆ é™¤å‹ç¼©æ–‡ä»¶
                    curl=curl,  # æ˜¯å¦ä½¿ç”¨ curl ä¸‹è½½
                    retry=retry,  # ä¸‹è½½å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°
                    exist_ok=exist_ok,  # æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
                    progress=threads <= 1,  # æ˜¯å¦æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                ),
                zip(url, repeat(dir)),  # å°† URL å’Œç›®å½•å‚æ•°è¿›è¡Œç»„åˆ
            )
            pool.close()  # å…³é—­çº¿ç¨‹æ± 
            pool.join()  # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ä»»åŠ¡å®Œæˆ
    else:
        # å¦‚æœåªæœ‰å•ä¸ªçº¿ç¨‹ï¼Œé¡ºåºä¸‹è½½æ¯ä¸ªæ–‡ä»¶
        for u in [url] if isinstance(url, (str, Path)) else url:
            safe_download(url=u, dir=dir, unzip=unzip, delete=delete, curl=curl, retry=retry, exist_ok=exist_ok)
```