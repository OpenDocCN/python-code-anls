# `.\yolov8\examples\YOLOv8-Segmentation-ONNXRuntime-Python\main.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—

import cv2  # å¯¼å…¥ OpenCV åº“
import numpy as np  # å¯¼å…¥ NumPy åº“
import onnxruntime as ort  # å¯¼å…¥ ONNX Runtime åº“

from ultralytics.utils import ASSETS, yaml_load  # ä» ultralytics.utils ä¸­å¯¼å…¥ ASSETS å’Œ yaml_load å‡½æ•°
from ultralytics.utils.checks import check_yaml  # ä» ultralytics.utils.checks ä¸­å¯¼å…¥ check_yaml å‡½æ•°
from ultralytics.utils.plotting import Colors  # ä» ultralytics.utils.plotting ä¸­å¯¼å…¥ Colors ç±»


class YOLOv8Seg:
    """YOLOv8 segmentation model."""

    def __init__(self, onnx_model):
        """
        Initialization.

        Args:
            onnx_model (str): Path to the ONNX model.
        """

        # Build Ort session
        self.session = ort.InferenceSession(
            onnx_model,
            providers=["CUDAExecutionProvider", "CPUExecutionProvider"]
            if ort.get_device() == "GPU"
            else ["CPUExecutionProvider"],
        )

        # Numpy dtype: support both FP32 and FP16 onnx model
        self.ndtype = np.half if self.session.get_inputs()[0].type == "tensor(float16)" else np.single

        # Get model width and height(YOLOv8-seg only has one input)
        self.model_height, self.model_width = [x.shape for x in self.session.get_inputs()][0][-2:]

        # Load COCO class names
        self.classes = yaml_load(check_yaml("coco8.yaml"))["names"]

        # Create color palette
        self.color_palette = Colors()

    def __call__(self, im0, conf_threshold=0.4, iou_threshold=0.45, nm=32):
        """
        The whole pipeline: pre-process -> inference -> post-process.

        Args:
            im0 (Numpy.ndarray): original input image.
            conf_threshold (float): confidence threshold for filtering predictions.
            iou_threshold (float): iou threshold for NMS.
            nm (int): the number of masks.

        Returns:
            boxes (List): list of bounding boxes.
            segments (List): list of segments.
            masks (np.ndarray): [N, H, W], output masks.
        """

        # Pre-process
        im, ratio, (pad_w, pad_h) = self.preprocess(im0)  # è°ƒç”¨ preprocess æ–¹æ³•è¿›è¡Œå›¾åƒé¢„å¤„ç†

        # Ort inference
        preds = self.session.run(None, {self.session.get_inputs()[0].name: im})  # ä½¿ç”¨ ONNX Runtime è¿›è¡Œæ¨ç†

        # Post-process
        boxes, segments, masks = self.postprocess(
            preds,
            im0=im0,
            ratio=ratio,
            pad_w=pad_w,
            pad_h=pad_h,
            conf_threshold=conf_threshold,
            iou_threshold=iou_threshold,
            nm=nm,
        )  # è°ƒç”¨ postprocess æ–¹æ³•è¿›è¡Œåå¤„ç†
        return boxes, segments, masks
    def preprocess(self, img):
        """
        Pre-processes the input image.

        Args:
            img (Numpy.ndarray): image about to be processed.

        Returns:
            img_process (Numpy.ndarray): image preprocessed for inference.
            ratio (tuple): width, height ratios in letterbox.
            pad_w (float): width padding in letterbox.
            pad_h (float): height padding in letterbox.
        """

        # ä½¿ç”¨ letterbox() å‡½æ•°è°ƒæ•´è¾“å…¥å›¾åƒçš„å¤§å°å¹¶å¡«å……
        shape = img.shape[:2]  # åŸå§‹å›¾åƒçš„å½¢çŠ¶
        new_shape = (self.model_height, self.model_width)
        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
        ratio = r, r  # è®¡ç®—å®½é«˜æ¯”ä¾‹
        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))  # è®¡ç®—è°ƒæ•´åçš„å°ºå¯¸
        pad_w, pad_h = (new_shape[1] - new_unpad[0]) / 2, (new_shape[0] - new_unpad[1]) / 2  # è®¡ç®—å¡«å……çš„å®½åº¦å’Œé«˜åº¦
        if shape[::-1] != new_unpad:  # å¦‚æœå°ºå¯¸ä¸ä¸€è‡´ï¼Œåˆ™è¿›è¡Œ resize æ“ä½œ
            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
        top, bottom = int(round(pad_h - 0.1)), int(round(pad_h + 0.1))
        left, right = int(round(pad_w - 0.1)), int(round(pad_w + 0.1))
        # ä½¿ç”¨æŒ‡å®šé¢œè‰²è¿›è¡Œè¾¹ç•Œå¡«å……
        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))

        # å›¾åƒè½¬æ¢æµç¨‹ï¼šHWC è½¬æ¢ä¸º CHW -> BGR è½¬æ¢ä¸º RGB -> å½’ä¸€åŒ–å¤„ç† -> è¿ç»­åŒ–å¤„ç† -> æ·»åŠ é¢å¤–ç»´åº¦ï¼ˆå¯é€‰ï¼‰
        img = np.ascontiguousarray(np.einsum("HWC->CHW", img)[::-1], dtype=self.ndtype) / 255.0
        img_process = img[None] if len(img.shape) == 3 else img  # æ·»åŠ é¢å¤–ç»´åº¦ä»¥é€‚åº”ç½‘ç»œè¾“å…¥
        return img_process, ratio, (pad_w, pad_h)
    def postprocess(self, preds, im0, ratio, pad_w, pad_h, conf_threshold, iou_threshold, nm=32):
        """
        Post-process the prediction.

        Args:
            preds (Numpy.ndarray): predictions come from ort.session.run().
            im0 (Numpy.ndarray): [h, w, c] original input image.
            ratio (tuple): width, height ratios in letterbox.
            pad_w (float): width padding in letterbox.
            pad_h (float): height padding in letterbox.
            conf_threshold (float): conf threshold.
            iou_threshold (float): iou threshold.
            nm (int): the number of masks.

        Returns:
            boxes (List): list of bounding boxes.
            segments (List): list of segments.
            masks (np.ndarray): [N, H, W], output masks.
        """
        x, protos = preds[0], preds[1]  # Two outputs: predictions and protos

        # Transpose the first output: (Batch_size, xywh_conf_cls_nm, Num_anchors) -> (Batch_size, Num_anchors, xywh_conf_cls_nm)
        x = np.einsum("bcn->bnc", x)

        # Predictions filtering by conf-threshold
        x = x[np.amax(x[..., 4:-nm], axis=-1) > conf_threshold]

        # Create a new matrix which merge these(box, score, cls, nm) into one
        # For more details about `numpy.c_()`: https://numpy.org/doc/1.26/reference/generated/numpy.c_.html
        x = np.c_[x[..., :4], np.amax(x[..., 4:-nm], axis=-1), np.argmax(x[..., 4:-nm], axis=-1), x[..., -nm:]]

        # NMS filtering
        x = x[cv2.dnn.NMSBoxes(x[:, :4], x[:, 4], conf_threshold, iou_threshold)]

        # Decode and return
        if len(x) > 0:
            # Bounding boxes format change: cxcywh -> xyxy
            x[..., [0, 1]] -= x[..., [2, 3]] / 2
            x[..., [2, 3]] += x[..., [0, 1]]

            # Rescales bounding boxes from model shape(model_height, model_width) to the shape of original image
            x[..., :4] -= [pad_w, pad_h, pad_w, pad_h]
            x[..., :4] /= min(ratio)

            # Bounding boxes boundary clamp
            x[..., [0, 2]] = x[:, [0, 2]].clip(0, im0.shape[1])
            x[..., [1, 3]] = x[:, [1, 3]].clip(0, im0.shape[0])

            # Process masks
            masks = self.process_mask(protos[0], x[:, 6:], x[:, :4], im0.shape)

            # Masks -> Segments(contours)
            segments = self.masks2segments(masks)
            return x[..., :6], segments, masks  # boxes, segments, masks
        else:
            return [], [], []


æ³¨é‡Šï¼š

        x, protos = preds[0], preds[1]  # ä»é¢„æµ‹ç»“æœä¸­åˆ†ç¦»å‡ºä¸¤ä¸ªè¾“å‡ºï¼šé¢„æµ‹å’ŒåŸå‹
        x = np.einsum("bcn->bnc", x)  # è½¬ç½®ç¬¬ä¸€ä¸ªè¾“å‡ºï¼š(Batch_size, xywh_conf_cls_nm, Num_anchors) -> (Batch_size, Num_anchors, xywh_conf_cls_nm)
        x = x[np.amax(x[..., 4:-nm], axis=-1) > conf_threshold]  # æ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤é¢„æµ‹ç»“æœ
        x = np.c_[x[..., :4], np.amax(x[..., 4:-nm], axis=-1), np.argmax(x[..., 4:-nm], axis=-1), x[..., -nm:]]  # å°†(box, score, cls, nm)åˆå¹¶æˆä¸€ä¸ªæ–°çŸ©é˜µ
        x = x[cv2.dnn.NMSBoxes(x[:, :4], x[:, 4], conf_threshold, iou_threshold)]  # ä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶ç­›é€‰è¾¹ç•Œæ¡†
        if len(x) > 0:
            x[..., [0, 1]] -= x[..., [2, 3]] / 2  # è¾¹ç•Œæ¡†æ ¼å¼ä»cxcywhè½¬æ¢ä¸ºxyxy
            x[..., [2, 3]] += x[..., [0, 1]]  # è°ƒæ•´è¾¹ç•Œæ¡†åæ ‡
            x[..., :4] -= [pad_w, pad_h, pad_w, pad_h]  # å°†è¾¹ç•Œæ¡†ä»æ¨¡å‹å°ºå¯¸è½¬æ¢ä¸ºåŸå§‹å›¾åƒå°ºå¯¸
            x[..., :4] /= min(ratio)  # æ ¹æ®å›¾åƒç¼©æ”¾æ¯”ä¾‹é‡æ–°ç¼©æ”¾è¾¹ç•Œæ¡†
            x[..., [0, 2]] = x[:, [0, 2]].clip(0, im0.shape[1])  # å¯¹è¾¹ç•Œæ¡†çš„xåæ ‡è¿›è¡Œè¾¹ç•Œé™åˆ¶
            x[..., [1, 3]] = x[:, [1, 3]].clip(0, im0.shape[0])  # å¯¹è¾¹ç•Œæ¡†çš„yåæ ‡è¿›è¡Œè¾¹ç•Œé™åˆ¶
            masks = self.process_mask(protos[0], x[:, 6:], x[:, :4], im0.shape)  # å¤„ç†ç”Ÿæˆæ©ç 
            segments = self.masks2segments(masks)  # å°†æ©ç è½¬æ¢ä¸ºè½®å»“
            return x[..., :6], segments, masks  # è¿”å›è¾¹ç•Œæ¡†ã€è½®å»“å’Œæ©ç 
        else:
            return [], [], []  # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è¾¹ç•Œæ¡†ï¼Œè¿”å›ç©ºåˆ—è¡¨
    def process_mask(self, protos, masks_in, bboxes, im0_shape):
        """
        Takes the output of the mask head, and applies the mask to the bounding boxes. This produces masks of higher quality
        but is slower. (Borrowed from https://github.com/ultralytics/ultralytics/blob/465df3024f44fa97d4fad9986530d5a13cdabdca/ultralytics/utils/ops.py#L618)

        Args:
            protos (numpy.ndarray): [mask_dim, mask_h, mask_w].
                ç”¨äºç”Ÿæˆæ©ç çš„åŸå‹å¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸º [æ©ç ç»´åº¦, æ©ç é«˜åº¦, æ©ç å®½åº¦]ã€‚
            masks_in (numpy.ndarray): [n, mask_dim], n is number of masks after nms.
                ç»è¿‡éæå¤§å€¼æŠ‘åˆ¶åçš„æ©ç å¼ é‡ï¼Œå½¢çŠ¶ä¸º [n, æ©ç ç»´åº¦]ã€‚
            bboxes (numpy.ndarray): bboxes re-scaled to original image shape.
                é‡æ–°ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå½¢çŠ¶çš„è¾¹ç•Œæ¡†åæ ‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º [n, 4]ã€‚
            im0_shape (tuple): the size of the input image (h,w,c).
                è¾“å…¥å›¾åƒçš„å°ºå¯¸ï¼Œä»¥å…ƒç»„å½¢å¼è¡¨ç¤º (é«˜åº¦, å®½åº¦, é€šé“æ•°)ã€‚

        Returns:
            (numpy.ndarray): The upsampled masks.
                è¿”å›ç»å¤„ç†çš„æ©ç ï¼Œå½¢çŠ¶ä¸º [n, mask_h, mask_w]ã€‚
        """
        c, mh, mw = protos.shape
        # ä½¿ç”¨åŸå‹å¼ é‡å’Œæ©ç è¾“å…¥æ‰§è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå¾—åˆ°æ©ç çš„é«˜è´¨é‡ç‰ˆæœ¬
        masks = np.matmul(masks_in, protos.reshape((c, -1))).reshape((-1, mh, mw)).transpose(1, 2, 0)  # HWN
        masks = np.ascontiguousarray(masks)
        # å°†æ©ç ä» P3 å½¢çŠ¶é‡æ–°ç¼©æ”¾åˆ°åŸå§‹è¾“å…¥å›¾åƒå½¢çŠ¶
        masks = self.scale_mask(masks, im0_shape)
        # å¯¹æ©ç è¿›è¡Œè½¬ç½®ï¼Œä» HWN å½¢çŠ¶è½¬æ¢ä¸º NHW å½¢çŠ¶
        masks = np.einsum("HWN -> NHW", masks)
        # æ ¹æ®è¾¹ç•Œæ¡†è£å‰ªæ©ç 
        masks = self.crop_mask(masks, bboxes)
        # å°†æ©ç ä¸­å¤§äº0.5çš„éƒ¨åˆ†è®¾ç½®ä¸ºTrueï¼Œå°äºç­‰äº0.5çš„éƒ¨åˆ†è®¾ç½®ä¸ºFalse
        return np.greater(masks, 0.5)
    def scale_mask(masks, im0_shape, ratio_pad=None):
        """
        Takes a mask, and resizes it to the original image size. (Borrowed from
        https://github.com/ultralytics/ultralytics/blob/465df3024f44fa97d4fad9986530d5a13cdabdca/ultralytics/utils/ops.py#L305)

        Args:
            masks (np.ndarray): resized and padded masks/images, [h, w, num]/[h, w, 3].
            im0_shape (tuple): the original image shape.
            ratio_pad (tuple): the ratio of the padding to the original image.

        Returns:
            masks (np.ndarray): The masks that are being returned.
        """
        # è·å–å½“å‰ masks çš„å½¢çŠ¶ï¼Œå–å‰ä¸¤ä¸ªç»´åº¦ï¼ˆé«˜åº¦å’Œå®½åº¦ï¼‰
        im1_shape = masks.shape[:2]
        
        # å¦‚æœ ratio_pad ä¸º Noneï¼Œåˆ™æ ¹æ® im0_shape è®¡ç®—æ¯”ä¾‹
        if ratio_pad is None:
            gain = min(im1_shape[0] / im0_shape[0], im1_shape[1] / im0_shape[1])  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ gain = old / new
            pad = (im1_shape[1] - im0_shape[1] * gain) / 2, (im1_shape[0] - im0_shape[0] * gain) / 2  # è®¡ç®—å¡«å……é‡ pad = (width_padding, height_padding)
        else:
            pad = ratio_pad[1]  # å¦åˆ™ç›´æ¥å– ratio_pad çš„ç¬¬äºŒä¸ªå…ƒç´ ä½œä¸º pad

        # è®¡ç®— mask çš„ top-left å’Œ bottom-right è¾¹ç•Œ
        top, left = int(round(pad[1] - 0.1)), int(round(pad[0] - 0.1))  # è®¡ç®—é¡¶éƒ¨å’Œå·¦ä¾§è¾¹ç•Œ
        bottom, right = int(round(im1_shape[0] - pad[1] + 0.1)), int(round(im1_shape[1] - pad[0] + 0.1))  # è®¡ç®—åº•éƒ¨å’Œå³ä¾§è¾¹ç•Œ
        
        # å¦‚æœ masks çš„å½¢çŠ¶ç»´åº¦å°äº 2ï¼Œåˆ™æŠ›å‡º ValueError å¼‚å¸¸
        if len(masks.shape) < 2:
            raise ValueError(f'"len of masks shape" should be 2 or 3, but got {len(masks.shape)}')
        
        # æ ¹æ®è®¡ç®—å¾—åˆ°çš„è¾¹ç•Œè£å‰ª masks
        masks = masks[top:bottom, left:right]
        
        # ä½¿ç”¨ OpenCV è¿›è¡Œå›¾åƒç¼©æ”¾è‡³åŸå§‹å›¾åƒå¤§å°
        masks = cv2.resize(
            masks, (im0_shape[1], im0_shape[0]), interpolation=cv2.INTER_LINEAR
        )  # ä½¿ç”¨çº¿æ€§æ’å€¼è¿›è¡Œç¼©æ”¾ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ä½¿ç”¨ INTER_CUBIC

        # å¦‚æœ masks çš„å½¢çŠ¶ç»´åº¦ä¸º 2ï¼Œåˆ™æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œä½¿å…¶å˜ä¸ºä¸‰ç»´
        if len(masks.shape) == 2:
            masks = masks[:, :, None]
        
        # è¿”å›è°ƒæ•´å¤§å°åçš„ masks
        return masks
    def draw_and_visualize(self, im, bboxes, segments, vis=False, save=True):
        """
        Draw and visualize results.

        Args:
            im (np.ndarray): original image, shape [h, w, c].
            bboxes (numpy.ndarray): [n, 4], n is number of bboxes.
            segments (List): list of segment masks.
            vis (bool): imshow using OpenCV.
            save (bool): save image annotated.

        Returns:
            None
        """

        # å¤åˆ¶åŸå§‹å›¾åƒä½œä¸ºç»˜å›¾ç”»å¸ƒ
        im_canvas = im.copy()

        # éå†è¾¹ç•Œæ¡†å’Œåˆ†å‰²æ©ç 
        for (*box, conf, cls_), segment in zip(bboxes, segments):
            # ç»˜åˆ¶å¤šè¾¹å½¢è¾¹ç•Œï¼Œå¹¶å¡«å……åˆ†å‰²æ©ç 
            cv2.polylines(im, np.int32([segment]), True, (255, 255, 255), 2)  # ç™½è‰²è¾¹ç•Œçº¿
            cv2.fillPoly(im_canvas, np.int32([segment]), self.color_palette(int(cls_), bgr=True))

            # ç»˜åˆ¶è¾¹ç•Œæ¡†çŸ©å½¢
            cv2.rectangle(
                im,
                (int(box[0]), int(box[1])),
                (int(box[2]), int(box[3])),
                self.color_palette(int(cls_), bgr=True),
                1,
                cv2.LINE_AA,
            )

            # æ·»åŠ è¾¹ç•Œæ¡†æ ‡ç­¾
            cv2.putText(
                im,
                f"{self.classes[cls_]}: {conf:.3f}",
                (int(box[0]), int(box[1] - 9)),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                self.color_palette(int(cls_), bgr=True),
                2,
                cv2.LINE_AA,
            )

        # æ··åˆåŸå§‹å›¾åƒå’Œç»˜å›¾ç»“æœ
        im = cv2.addWeighted(im_canvas, 0.3, im, 0.7, 0)

        # æ˜¾ç¤ºå›¾åƒ
        if vis:
            cv2.imshow("demo", im)
            cv2.waitKey(0)
            cv2.destroyAllWindows()

        # ä¿å­˜å›¾åƒ
        if save:
            cv2.imwrite("demo.jpg", im)
# å¦‚æœå½“å‰è„šæœ¬è¢«ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€çš„å‚æ•°ï¼šæ¨¡å‹æ–‡ä»¶çš„è·¯å¾„
    parser.add_argument("--model", type=str, required=True, help="Path to ONNX model")
    # æ·»åŠ å¯é€‰å‚æ•°ï¼šè¾“å…¥å›¾åƒçš„è·¯å¾„ï¼Œé»˜è®¤ä¸º ASSETS ç›®å½•ä¸‹çš„ bus.jpg
    parser.add_argument("--source", type=str, default=str(ASSETS / "bus.jpg"), help="Path to input image")
    # æ·»åŠ å¯é€‰å‚æ•°ï¼šç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤ä¸º 0.25
    parser.add_argument("--conf", type=float, default=0.25, help="Confidence threshold")
    # æ·»åŠ å¯é€‰å‚æ•°ï¼šIoU é˜ˆå€¼ï¼Œé»˜è®¤ä¸º 0.45
    parser.add_argument("--iou", type=float, default=0.45, help="NMS IoU threshold")
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # æ„å»ºæ¨¡å‹å®ä¾‹ï¼Œä½¿ç”¨æŒ‡å®šçš„ ONNX æ¨¡å‹è·¯å¾„
    model = YOLOv8Seg(args.model)

    # ä½¿ç”¨ OpenCV è¯»å–æŒ‡å®šè·¯å¾„çš„å›¾åƒæ–‡ä»¶
    img = cv2.imread(args.source)

    # è¿›è¡Œæ¨ç†
    boxes, segments, _ = model(img, conf_threshold=args.conf, iou_threshold=args.iou)

    # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œå¤šè¾¹å½¢
    if len(boxes) > 0:
        # åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†å’Œå¤šè¾¹å½¢ï¼Œå¹¶æ ¹æ®éœ€è¦ä¿å­˜æˆ–æ˜¾ç¤º
        model.draw_and_visualize(img, boxes, segments, vis=False, save=True)
```