# `.\yolov8\ultralytics\utils\ops.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import contextlib  # å¯¼å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç›¸å…³çš„æ¨¡å—
import math  # å¯¼å…¥æ•°å­¦å‡½æ•°æ¨¡å—
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
import time  # å¯¼å…¥æ—¶é—´æ¨¡å—

import cv2  # å¯¼å…¥OpenCVåº“
import numpy as np  # å¯¼å…¥NumPyåº“
import torch  # å¯¼å…¥PyTorchåº“
import torch.nn.functional as F  # å¯¼å…¥PyTorchçš„å‡½æ•°æ¨¡å—

from ultralytics.utils import LOGGER  # ä»ultralytics.utilsä¸­å¯¼å…¥LOGGERå¯¹è±¡
from ultralytics.utils.metrics import batch_probiou  # ä»ultralytics.utils.metricsä¸­å¯¼å…¥batch_probiouå‡½æ•°


class Profile(contextlib.ContextDecorator):
    """
    YOLOv8 Profile class. Use as a decorator with @Profile() or as a context manager with 'with Profile():'.

    Example:
        ```py
        from ultralytics.utils.ops import Profile

        with Profile(device=device) as dt:
            pass  # slow operation here

        print(dt)  # prints "Elapsed time is 9.5367431640625e-07 s"
        ```
    """

    def __init__(self, t=0.0, device: torch.device = None):
        """
        Initialize the Profile class.

        Args:
            t (float): Initial time. Defaults to 0.0.
            device (torch.device): Devices used for model inference. Defaults to None (cpu).
        """
        self.t = t  # åˆå§‹åŒ–ç´¯è®¡æ—¶é—´
        self.device = device  # åˆå§‹åŒ–è®¾å¤‡
        self.cuda = bool(device and str(device).startswith("cuda"))  # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨CUDAåŠ é€Ÿ

    def __enter__(self):
        """Start timing."""
        self.start = self.time()  # è®°å½•å¼€å§‹æ—¶é—´
        return self

    def __exit__(self, type, value, traceback):  # noqa
        """Stop timing."""
        self.dt = self.time() - self.start  # è®¡ç®—è€—æ—¶
        self.t += self.dt  # ç´¯åŠ è€—æ—¶åˆ°æ€»æ—¶é—´

    def __str__(self):
        """Returns a human-readable string representing the accumulated elapsed time in the profiler."""
        return f"Elapsed time is {self.t} s"  # è¿”å›ç´¯è®¡çš„è€—æ—¶ä¿¡æ¯

    def time(self):
        """Get current time."""
        if self.cuda:
            torch.cuda.synchronize(self.device)  # åŒæ­¥CUDAæµ
        return time.time()  # è¿”å›å½“å‰æ—¶é—´æˆ³


def segment2box(segment, width=640, height=640):
    """
    Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy).

    Args:
        segment (torch.Tensor): the segment label
        width (int): the width of the image. Defaults to 640
        height (int): The height of the image. Defaults to 640

    Returns:
        (np.ndarray): the minimum and maximum x and y values of the segment.
    """
    x, y = segment.T  # æå–segmentçš„xyåæ ‡
    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)  # å†…éƒ¨çº¦æŸæ¡ä»¶
    x = x[inside]  # è¿‡æ»¤ç¬¦åˆçº¦æŸæ¡ä»¶çš„xåæ ‡
    y = y[inside]  # è¿‡æ»¤ç¬¦åˆçº¦æŸæ¡ä»¶çš„yåæ ‡
    return (
        np.array([x.min(), y.min(), x.max(), y.max()], dtype=segment.dtype)
        if any(x)
        else np.zeros(4, dtype=segment.dtype)
    )  # è¿”å›segmentçš„æœ€å°å’Œæœ€å¤§xyåæ ‡ï¼Œå¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ç‚¹åˆ™è¿”å›å…¨é›¶æ•°ç»„


def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True, xywh=False):
    """
    Rescales bounding boxes (in the format of xyxy by default) from the shape of the image they were originally
    specified in (img1_shape) to the shape of a different image (img0_shape).

    Args:
        img1_shape (tuple): Shape of the original image (height, width).
        boxes (torch.Tensor): Bounding boxes in format xyxy.
        img0_shape (tuple): Shape of the new image (height, width).
        ratio_pad (tuple): Aspect ratio and padding.
        padding (bool): Whether to pad bounding boxes or not.
        xywh (bool): Whether the boxes are in xywh format or not. Defaults to False.
    """
    """
        Args:
            img1_shape (tuple): ç›®æ ‡å›¾åƒçš„å½¢çŠ¶ï¼Œæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)
            boxes (torch.Tensor): å›¾åƒä¸­ç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º (x1, y1, x2, y2)
            img0_shape (tuple): åŸå§‹å›¾åƒçš„å½¢çŠ¶ï¼Œæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)
            ratio_pad (tuple): ä¸€ä¸ªå…ƒç»„ (ratio, pad)ï¼Œç”¨äºç¼©æ”¾è¾¹ç•Œæ¡†ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™æ ¹æ®ä¸¤ä¸ªå›¾åƒçš„å¤§å°å·®å¼‚è®¡ç®— ratio å’Œ pad
            padding (bool): å¦‚æœä¸º Trueï¼Œåˆ™å‡è®¾è¾¹ç•Œæ¡†åŸºäº YOLO æ ·å¼å¢å¼ºçš„å›¾åƒã€‚å¦‚æœä¸º Falseï¼Œåˆ™è¿›è¡Œå¸¸è§„çš„é‡æ–°ç¼©æ”¾
            xywh (bool): è¾¹ç•Œæ¡†æ ¼å¼æ˜¯å¦ä¸º xywhï¼Œ é»˜è®¤ä¸º False
    
        Returns:
            boxes (torch.Tensor): ç¼©æ”¾åçš„è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º (x1, y1, x2, y2)
    """
    if ratio_pad is None:  # å¦‚æœæœªæä¾› ratio_padï¼Œåˆ™ä» img0_shape è®¡ç®—
        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ gain = ç›®æ ‡å›¾åƒå°ºå¯¸ / åŸå§‹å›¾åƒå°ºå¯¸
        pad = (
            round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1),  # è®¡ç®—å®½åº¦æ–¹å‘çš„å¡«å……é‡
            round((img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1),  # è®¡ç®—é«˜åº¦æ–¹å‘çš„å¡«å……é‡
        )
    else:
        gain = ratio_pad[0][0]  # ä½¿ç”¨æä¾›çš„ ratio_pad ä¸­çš„ç¼©æ”¾æ¯”ä¾‹
        pad = ratio_pad[1]  # ä½¿ç”¨æä¾›çš„ ratio_pad ä¸­çš„å¡«å……é‡
    
    if padding:
        boxes[..., 0] -= pad[0]  # å‡å» x æ–¹å‘çš„å¡«å……é‡
        boxes[..., 1] -= pad[1]  # å‡å» y æ–¹å‘çš„å¡«å……é‡
        if not xywh:
            boxes[..., 2] -= pad[0]  # å¯¹äºé xywh æ ¼å¼çš„è¾¹ç•Œæ¡†ï¼Œå†æ¬¡å‡å» x æ–¹å‘çš„å¡«å……é‡
            boxes[..., 3] -= pad[1]  # å¯¹äºé xywh æ ¼å¼çš„è¾¹ç•Œæ¡†ï¼Œå†æ¬¡å‡å» y æ–¹å‘çš„å¡«å……é‡
    
    boxes[..., :4] /= gain  # ç¼©æ”¾è¾¹ç•Œæ¡†åæ ‡
    return clip_boxes(boxes, img0_shape)  # è°ƒç”¨ clip_boxes å‡½æ•°ï¼Œç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒå†…éƒ¨
# æ‰§è¡Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰æ“ä½œï¼Œç”¨äºä¸€ç»„è¾¹ç•Œæ¡†ï¼Œæ”¯æŒæ©ç å’Œæ¯ä¸ªæ¡†å¤šä¸ªæ ‡ç­¾ã€‚
def non_max_suppression(
    prediction,
    conf_thres=0.25,  # ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤é˜ˆå€¼çš„æ¡†å°†è¢«å¿½ç•¥
    iou_thres=0.45,  # IoUï¼ˆäº¤å¹¶æ¯”ï¼‰é˜ˆå€¼ï¼Œç”¨äºåˆ¤æ–­é‡å æ¡†ä¹‹é—´æ˜¯å¦åˆå¹¶
    classes=None,  # ç±»åˆ«åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ç‰¹å®šç±»åˆ«çš„æ¡†
    agnostic=False,  # æ˜¯å¦å¿½ç•¥é¢„æµ‹æ¡†çš„ç±»åˆ«ä¿¡æ¯
    multi_label=False,  # æ˜¯å¦æ”¯æŒå¤šæ ‡ç­¾è¾“å‡º
    labels=(),  # æ ‡ç­¾åˆ—è¡¨ï¼ŒæŒ‡å®šè¦ä¿ç•™çš„æ ‡ç­¾
    max_det=300,  # æœ€å¤§æ£€æµ‹æ¡†æ•°
    nc=0,  # ç±»åˆ«æ•°é‡ï¼ˆå¯é€‰ï¼‰
    max_time_img=0.05,  # æœ€å¤§å›¾åƒå¤„ç†æ—¶é—´
    max_nms=30000,  # æœ€å¤§NMSæ“ä½œæ•°
    max_wh=7680,  # æœ€å¤§å®½åº¦å’Œé«˜åº¦
    in_place=True,  # æ˜¯å¦å°±åœ°ä¿®æ”¹
    rotated=False,  # æ˜¯å¦ä¸ºæ—‹è½¬æ¡†
):
    """
    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.
    """
    # å¦‚æœé¢„æµ‹ä¸ºç©ºï¼Œè¿”å›ä¸€ä¸ªç©ºçš„numpyæ•°ç»„
    if len(prediction) == 0:
        return np.empty((0,), dtype=np.int8)
    
    # æ ¹æ®ç½®ä¿¡åº¦å¯¹é¢„æµ‹æ¡†è¿›è¡Œé™åºæ’åº
    sorted_idx = torch.argsort(prediction[:, 4], descending=True)
    prediction = prediction[sorted_idx]
    
    # è®¡ç®—æ‰€æœ‰æ¡†ä¸¤ä¸¤ä¹‹é—´çš„probiouå¾—åˆ†çŸ©é˜µï¼Œå¹¶å–å…¶ä¸Šä¸‰è§’éƒ¨åˆ†
    ious = batch_probiou(prediction, prediction).triu_(diagonal=1)
    
    # æ ¹æ®IoUé˜ˆå€¼è¿›è¡Œéæå¤§å€¼æŠ‘åˆ¶ï¼Œä¿ç•™ç¬¦åˆæ¡ä»¶çš„æ¡†ç´¢å¼•
    pick = torch.nonzero(ious.max(dim=0)[0] < iou_thres).squeeze(-1)
    
    # è¿”å›æŒ‰ç…§é™åºæ’åˆ—çš„è¢«é€‰æ¡†çš„ç´¢å¼•
    return sorted_idx[pick]
    import torchvision  # å¼•å…¥torchvisionæ¨¡å—ï¼Œç”¨äºåŠ å¿«â€œimport ultralyticsâ€çš„é€Ÿåº¦

    # æ£€æŸ¥ç½®ä¿¡åº¦é˜ˆå€¼çš„æœ‰æ•ˆæ€§ï¼Œå¿…é¡»åœ¨0åˆ°1ä¹‹é—´
    assert 0 <= conf_thres <= 1, f"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0"
    # æ£€æŸ¥IoUé˜ˆå€¼çš„æœ‰æ•ˆæ€§ï¼Œå¿…é¡»åœ¨0åˆ°1ä¹‹é—´
    assert 0 <= iou_thres <= 1, f"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0"

    # å¦‚æœpredictionæ˜¯ä¸€ä¸ªåˆ—è¡¨æˆ–å…ƒç»„ï¼ˆä¾‹å¦‚YOLOv8æ¨¡å‹åœ¨éªŒè¯æ¨¡å¼ä¸‹çš„è¾“å‡ºï¼‰ï¼Œé€‰æ‹©æ¨æ–­è¾“å‡ºéƒ¨åˆ†
    if isinstance(prediction, (list, tuple)):
        prediction = prediction[0]  # é€‰æ‹©æ¨æ–­è¾“å‡º

    # å¦‚æœæŒ‡å®šäº†classesï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºä¸predictionè®¾å¤‡ç›¸åŒçš„torchå¼ é‡
    if classes is not None:
        classes = torch.tensor(classes, device=prediction.device)

    # å¦‚æœpredictionçš„æœ€åä¸€ä¸ªç»´åº¦ä¸º6ï¼Œè¯´æ˜æ˜¯ç«¯åˆ°ç«¯æ¨¡å‹çš„è¾“å‡ºï¼ˆBNCæ ¼å¼ï¼Œå³1,300,6ï¼‰
    if prediction.shape[-1] == 6:
        # å¯¹æ¯ä¸ªé¢„æµ‹ç»“æœè¿›è¡Œç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤
        output = [pred[pred[:, 4] > conf_thres] for pred in prediction]
        # å¦‚æœæŒ‡å®šäº†classesï¼Œåˆ™è¿›ä¸€æ­¥æ ¹æ®classesè¿›è¡Œè¿‡æ»¤
        if classes is not None:
            output = [pred[(pred[:, 5:6] == classes).any(1)] for pred in output]
        return output

    # è·å–batch sizeï¼ˆBCNæ ¼å¼ï¼Œå³1,84,6300ï¼‰
    bs = prediction.shape[0]
    # å¦‚æœæœªæŒ‡å®šncï¼ˆç±»åˆ«æ•°é‡ï¼‰ï¼Œåˆ™æ ¹æ®predictionçš„å½¢çŠ¶æ¨æ–­ç±»åˆ«æ•°é‡
    nc = nc or (prediction.shape[1] - 4)  # number of classes
    # è®¡ç®—é¢„æµ‹ç»“æœä¸­çš„æ©ç æ•°é‡
    nm = prediction.shape[1] - nc - 4  # number of masks
    
    # ç¡®å®šæ©ç èµ·å§‹ç´¢å¼•
    mi = 4 + nc  # mask start index
    
    # æ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼ç¡®å®šå€™é€‰é¡¹
    xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates

    # è®¾ç½®æ—¶é—´é™åˆ¶
    time_limit = 2.0 + max_time_img * bs  # seconds to quit after
    
    # è‹¥å¤šæ ‡ç­¾è®¾ç½®ä¸ºçœŸï¼Œåˆ™æ¯ä¸ªæ¡†å¯èƒ½æœ‰å¤šä¸ªæ ‡ç­¾ï¼ˆå¢åŠ 0.5ms/å›¾åƒï¼‰
    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)

    # è°ƒæ•´é¢„æµ‹ç»“æœçš„ç»´åº¦é¡ºåºï¼Œå°†æœ€åä¸¤ä¸ªç»´åº¦äº’æ¢
    prediction = prediction.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)
    
    # å¦‚æœä¸æ˜¯æ—‹è½¬æ¡†ï¼Œæ ¹æ®éœ€æ±‚å°†é¢„æµ‹çš„è¾¹ç•Œæ¡†æ ¼å¼ä»xywhè½¬æ¢ä¸ºxyxy
    if not rotated:
        if in_place:
            prediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy in-place modification
        else:
            # åœ¨éåŸåœ°æ“ä½œæ—¶ï¼Œå°†è¾¹ç•Œæ¡†å’Œå…¶ä»–é¢„æµ‹ç»“æœè¿æ¥èµ·æ¥ï¼Œè½¬æ¢ä¸ºxyxyæ ¼å¼
            prediction = torch.cat((xywh2xyxy(prediction[..., :4]), prediction[..., 4:]), dim=-1)  # xywh to xyxy

    # è®°å½•å½“å‰æ—¶é—´
    t = time.time()
    
    # åˆå§‹åŒ–è¾“å‡ºåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªç©ºçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º(0, 6 + nm)ï¼Œåœ¨æŒ‡å®šè®¾å¤‡ä¸Š
    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs
    for xi, x in enumerate(prediction):  # å¯¹æ¯ä¸ªé¢„æµ‹ç»“æœè¿›è¡Œéå†ï¼Œxiæ˜¯ç´¢å¼•ï¼Œxæ˜¯é¢„æµ‹ç»“æœ
        # Apply constraints
        # åº”ç”¨çº¦æŸæ¡ä»¶
        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height
        # å¯¹é¢„æµ‹ç»“æœä¸­çš„å®½åº¦å’Œé«˜åº¦è¿›è¡Œçº¦æŸï¼Œå°†ä¸æ»¡è¶³æ¡ä»¶çš„ç½®ä¸º0

        x = x[xc[xi]]  # confidence
        # æ ¹æ®ç½®ä¿¡åº¦ç´¢å¼•è·å–é¢„æµ‹ç»“æœçš„å­é›†

        # Cat apriori labels if autolabelling
        # å¦‚æœè‡ªåŠ¨æ ‡æ³¨ï¼Œåˆå¹¶å…ˆéªŒæ ‡ç­¾
        if labels and len(labels[xi]) and not rotated:
            lb = labels[xi]
            v = torch.zeros((len(lb), nc + nm + 4), device=x.device)
            v[:, :4] = xywh2xyxy(lb[:, 1:5])  # box
            v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls
            x = torch.cat((x, v), 0)
            # å°†å…ˆéªŒæ ‡ç­¾ä¸é¢„æµ‹ç»“æœåˆå¹¶ï¼Œå½¢æˆæ–°çš„é¢„æµ‹ç»“æœ

        # If none remain process next image
        # å¦‚æœæ²¡æœ‰å‰©ä½™çš„é¢„æµ‹ç»“æœï¼Œåˆ™å¤„ç†ä¸‹ä¸€å¼ å›¾åƒ
        if not x.shape[0]:
            continue

        # Detections matrix nx6 (xyxy, conf, cls)
        # æ£€æµ‹çŸ©é˜µï¼Œå¤§å°ä¸ºnx6ï¼ˆxyxyåæ ‡ï¼Œç½®ä¿¡åº¦ï¼Œç±»åˆ«ï¼‰
        box, cls, mask = x.split((4, nc, nm), 1)

        if multi_label:
            i, j = torch.where(cls > conf_thres)
            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)
            # å¦‚æœæ”¯æŒå¤šæ ‡ç­¾ï¼Œæ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼ç­›é€‰ç±»åˆ«ï¼Œå¹¶å½¢æˆæ–°çš„é¢„æµ‹ç»“æœ
        else:  # best class only
            conf, j = cls.max(1, keepdim=True)
            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]
            # å¦åˆ™ï¼Œé€‰æ‹©æœ€é«˜ç½®ä¿¡åº¦çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœ

        # Filter by class
        # æ ¹æ®ç±»åˆ«è¿›è¡Œè¿‡æ»¤
        if classes is not None:
            x = x[(x[:, 5:6] == classes).any(1)]
            # å¦‚æœæŒ‡å®šäº†ç±»åˆ«ï¼Œåªä¿ç•™åŒ¹é…æŒ‡å®šç±»åˆ«çš„é¢„æµ‹ç»“æœ

        # Check shape
        # æ£€æŸ¥é¢„æµ‹ç»“æœçš„å½¢çŠ¶
        n = x.shape[0]  # number of boxes
        # nä¸ºç›’å­ï¼ˆè¾¹ç•Œæ¡†ï¼‰çš„æ•°é‡
        if not n:  # no boxes
            continue
        if n > max_nms:  # excess boxes
            x = x[x[:, 4].argsort(descending=True)[:max_nms]]
            # å¦‚æœç›’å­æ•°é‡è¶…è¿‡è®¾å®šçš„æœ€å¤§NMSæ•°é‡ï¼Œåˆ™æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶ä¿ç•™å‰max_nmsä¸ªç›’å­

        # Batched NMS
        # æ‰¹å¤„ç†çš„éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰
        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes
        scores = x[:, 4]  # scores
        if rotated:
            boxes = torch.cat((x[:, :2] + c, x[:, 2:4], x[:, -1:]), dim=-1)
            i = nms_rotated(boxes, scores, iou_thres)
            # å¦‚æœå¯ç”¨äº†æ—‹è½¬NMSï¼Œå¯¹æ—‹è½¬è¾¹ç•Œæ¡†è¿›è¡ŒNMSå¤„ç†
        else:
            boxes = x[:, :4] + c
            i = torchvision.ops.nms(boxes, scores, iou_thres)
            # å¦åˆ™ï¼Œå¯¹æ ‡å‡†è¾¹ç•Œæ¡†è¿›è¡ŒNMSå¤„ç†
        i = i[:max_det]  # limit detections
        # é™åˆ¶æœ€ç»ˆçš„æ£€æµ‹ç»“æœæ•°é‡

        output[xi] = x[i]
        # å°†å¤„ç†åçš„é¢„æµ‹ç»“æœå­˜å…¥è¾“å‡ºä¸­çš„å¯¹åº”ä½ç½®
        if (time.time() - t) > time_limit:
            LOGGER.warning(f"WARNING âš ï¸ NMS time limit {time_limit:.3f}s exceeded")
            break  # time limit exceeded
            # å¦‚æœè¶…è¿‡äº†NMSå¤„ç†æ—¶é—´é™åˆ¶ï¼Œè®°å½•è­¦å‘Šå¹¶è·³å‡ºå¾ªç¯

    return output
def scale_image(masks, im0_shape, ratio_pad=None):
    """
    Takes a mask, and resizes it to the original image size.

    Args:
        masks (np.ndarray): resized and padded masks/images, [h, w, num]/[h, w, 3].
        im0_shape (tuple): the original image shape
        ratio_pad (tuple): the ratio of the padding to the original image.

    Returns:
        masks (np.ndarray): The masks that are being returned with shape [h, w, num].
    """
    # è·å–å½“å‰ masks çš„å½¢çŠ¶
    im1_shape = masks.shape
    
    # å¦‚æœå½“å‰ masks å½¢çŠ¶ä¸åŸå§‹å›¾ç‰‡å½¢çŠ¶ç›¸åŒï¼Œåˆ™ç›´æ¥è¿”å› masksï¼Œæ— éœ€è°ƒæ•´å¤§å°
    if im1_shape[:2] == im0_shape[:2]:
        return masks
    
    # å¦‚æœæœªæŒ‡å®š ratio_padï¼Œåˆ™æ ¹æ® im0_shape è®¡ç®— gain å’Œ pad
    if ratio_pad is None:
        # è®¡ç®— gainï¼Œå³ç¼©æ”¾æ¯”ä¾‹
        gain = min(im1_shape[0] / im0_shape[0], im1_shape[1] / im0_shape[1])
        # è®¡ç®— padding çš„å®½åº¦å’Œé«˜åº¦
        pad = (im1_shape[1] - im0_shape[1] * gain) / 2, (im1_shape[0] - im0_shape[0] * gain) / 2
    else:
        pad = ratio_pad[1]  # ä½¿ç”¨æŒ‡å®šçš„ ratio_pad ä¸­çš„ padding å€¼
    
    # å°† pad è½¬æ¢ä¸ºæ•´æ•°ï¼Œè¡¨ç¤ºä¸Šã€å·¦ã€ä¸‹ã€å³çš„è¾¹ç•Œ
    top, left = int(pad[1]), int(pad[0])  # y, x
    bottom, right = int(im1_shape[0] - pad[1]), int(im1_shape[1] - pad[0])
    
    # å¦‚æœ masks çš„ç»´åº¦å°äº 2ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
    if len(masks.shape) < 2:
        raise ValueError(f'"len of masks shape" should be 2 or 3, but got {len(masks.shape)}')
    
    # å¯¹ masks è¿›è¡Œè£å‰ªï¼ŒæŒ‰ç…§è®¡ç®—å¾—åˆ°çš„è¾¹ç•Œè¿›è¡Œè£å‰ª
    masks = masks[top:bottom, left:right]
    
    # å°†è£å‰ªåçš„ masks è°ƒæ•´å¤§å°è‡³åŸå§‹å›¾ç‰‡å¤§å°
    masks = cv2.resize(masks, (im0_shape[1], im0_shape[0]))
    # æ£€æŸ¥ masks çš„ç»´åº¦æ˜¯å¦ä¸º 2
    if len(masks.shape) == 2:
        # å¦‚æœæ˜¯ï¼Œæ·»åŠ ä¸€ä¸ªé¢å¤–çš„ç»´åº¦ï¼Œä½¿å…¶å˜ä¸ºä¸‰ç»´
        masks = masks[:, :, None]

    # è¿”å›å¤„ç†åçš„ masks å˜é‡
    return masks
def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):
    """
    Convert bounding box coordinates from (x1, y1, x2, y2) format to normalized (x, y, width, height) format,
    relative to image dimensions and optionally clip the values.

    Args:
        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.
        w (int): Width of the image. Defaults to 640.
        h (int): Height of the image. Defaults to 640.
        clip (bool): Whether to clip the normalized coordinates to [0, 1]. Defaults to False.
        eps (float): Epsilon value for numerical stability. Defaults to 0.0.

    Returns:
        y (np.ndarray | torch.Tensor): The bounding box coordinates in normalized (x, y, width, height) format.
    """
    assert x.shape[-1] == 4, f"input shape last dimension expected 4 but input shape is {x.shape}"
    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy
    half_w = w / 2.0
    half_h = h / 2.0
    y[..., 0] = (x[..., 0] + x[..., 2]) / (2 * w)  # center x normalized
    y[..., 1] = (x[..., 1] + x[..., 3]) / (2 * h)  # center y normalized
    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width normalized
    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height normalized

    if clip:
        y = torch.clamp(y, min=eps, max=1.0 - eps) if isinstance(y, torch.Tensor) else np.clip(y, eps, 1.0 - eps)

    return y
    # å°†è¾¹ç•Œæ¡†åæ ‡ä» (x1, y1, x2, y2) æ ¼å¼è½¬æ¢ä¸º (x, y, width, height, normalized) æ ¼å¼ã€‚å…¶ä¸­ x, y, width å’Œ height å‡å·²å½’ä¸€åŒ–è‡³å›¾åƒå°ºå¯¸ã€‚
    
    Args:
        x (np.ndarray | torch.Tensor): è¾“å…¥çš„è¾¹ç•Œæ¡†åæ ‡ï¼Œæ ¼å¼ä¸º (x1, y1, x2, y2)ã€‚
        w (int): å›¾åƒçš„å®½åº¦ã€‚é»˜è®¤ä¸º 640ã€‚
        h (int): å›¾åƒçš„é«˜åº¦ã€‚é»˜è®¤ä¸º 640ã€‚
        clip (bool): å¦‚æœä¸º Trueï¼Œåˆ™å°†è¾¹ç•Œæ¡†è£å‰ªåˆ°å›¾åƒè¾¹ç•Œå†…ã€‚é»˜è®¤ä¸º Falseã€‚
        eps (float): è¾¹ç•Œæ¡†å®½åº¦å’Œé«˜åº¦çš„æœ€å°å€¼ã€‚é»˜è®¤ä¸º 0.0ã€‚
    
    Returns:
        y (np.ndarray | torch.Tensor): æ ¼å¼ä¸º (x, y, width, height, normalized) çš„è¾¹ç•Œæ¡†åæ ‡ã€‚
    """
    if clip:
        # è°ƒç”¨ clip_boxes å‡½æ•°ï¼Œå°†è¾¹ç•Œæ¡† x è£å‰ªåˆ°å›¾åƒè¾¹ç•Œå†…ï¼Œè¾¹ç•Œä¸º (h - eps, w - eps)
        x = clip_boxes(x, (h - eps, w - eps))
    assert x.shape[-1] == 4, f"input shape last dimension expected 4 but input shape is {x.shape}"
    # æ ¹æ®è¾“å…¥ x çš„ç±»å‹åˆ›å»ºä¸ä¹‹ç›¸åŒç±»å‹çš„ç©ºæ•°ç»„ yï¼Œç›¸æ¯” clone/copy æ“ä½œæ›´å¿«
    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)
    # è®¡ç®— x ä¸­æ¯ä¸ªè¾¹ç•Œæ¡†çš„ä¸­å¿ƒç‚¹ x åæ ‡ï¼Œå¹¶å°†å…¶å½’ä¸€åŒ–åˆ°å›¾åƒå®½åº¦ w
    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center
    # è®¡ç®— x ä¸­æ¯ä¸ªè¾¹ç•Œæ¡†çš„ä¸­å¿ƒç‚¹ y åæ ‡ï¼Œå¹¶å°†å…¶å½’ä¸€åŒ–åˆ°å›¾åƒé«˜åº¦ h
    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center
    # è®¡ç®— x ä¸­æ¯ä¸ªè¾¹ç•Œæ¡†çš„å®½åº¦ï¼Œå¹¶å°†å…¶å½’ä¸€åŒ–åˆ°å›¾åƒå®½åº¦ w
    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width
    # è®¡ç®— x ä¸­æ¯ä¸ªè¾¹ç•Œæ¡†çš„é«˜åº¦ï¼Œå¹¶å°†å…¶å½’ä¸€åŒ–åˆ°å›¾åƒé«˜åº¦ h
    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height
    # è¿”å›æ ¼å¼ä¸º (x, y, width, height, normalized) çš„è¾¹ç•Œæ¡†åæ ‡ y
    return y
def xywh2ltwh(x):
    """
    å°†è¾¹ç•Œæ¡†æ ¼å¼ä» [x, y, w, h] è½¬æ¢ä¸º [x1, y1, w, h]ï¼Œå…¶ä¸­ x1, y1 æ˜¯å·¦ä¸Šè§’åæ ‡ã€‚

    Args:
        x (np.ndarray | torch.Tensor): è¾“å…¥å¼ é‡ï¼ŒåŒ…å« xywh æ ¼å¼çš„è¾¹ç•Œæ¡†åæ ‡

    Returns:
        y (np.ndarray | torch.Tensor): è¾“å‡ºå¼ é‡ï¼ŒåŒ…å« xyltwh æ ¼å¼çš„è¾¹ç•Œæ¡†åæ ‡
    """
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    y[..., 0] = x[..., 0] - x[..., 2] / 2  # å·¦ä¸Šè§’ x åæ ‡
    y[..., 1] = x[..., 1] - x[..., 3] / 2  # å·¦ä¸Šè§’ y åæ ‡
    return y


def xyxy2ltwh(x):
    """
    å°†å¤šä¸ª [x1, y1, x2, y2] æ ¼å¼çš„è¾¹ç•Œæ¡†è½¬æ¢ä¸º [x1, y1, w, h] æ ¼å¼ï¼Œå…¶ä¸­ xy1 æ˜¯å·¦ä¸Šè§’ï¼Œxy2 æ˜¯å³ä¸‹è§’ã€‚

    Args:
        x (np.ndarray | torch.Tensor): è¾“å…¥å¼ é‡ï¼ŒåŒ…å« xyxy æ ¼å¼çš„è¾¹ç•Œæ¡†åæ ‡

    Returns:
        y (np.ndarray | torch.Tensor): è¾“å‡ºå¼ é‡ï¼ŒåŒ…å« xyltwh æ ¼å¼çš„è¾¹ç•Œæ¡†åæ ‡
    """
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    y[..., 2] = x[..., 2] - x[..., 0]  # å®½åº¦
    y[..., 3] = x[..., 3] - x[..., 1]  # é«˜åº¦
    return y


def ltwh2xywh(x):
    """
    å°† [x1, y1, w, h] æ ¼å¼çš„è¾¹ç•Œæ¡†è½¬æ¢ä¸º [x, y, w, h] æ ¼å¼ï¼Œå…¶ä¸­ xy1 æ˜¯å·¦ä¸Šè§’ï¼Œxy æ˜¯ä¸­å¿ƒåæ ‡ã€‚

    Args:
        x (torch.Tensor): è¾“å…¥å¼ é‡

    Returns:
        y (np.ndarray | torch.Tensor): è¾“å‡ºå¼ é‡ï¼ŒåŒ…å« xywh æ ¼å¼çš„è¾¹ç•Œæ¡†åæ ‡
    """
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    y[..., 0] = x[..., 0] + x[..., 2] / 2  # ä¸­å¿ƒ x åæ ‡
    y[..., 1] = x[..., 1] + x[..., 3] / 2  # ä¸­å¿ƒ y åæ ‡
    return y


def xyxyxyxy2xywhr(x):
    """
    å°†æ‰¹é‡çš„æ–¹å‘è¾¹ç•Œæ¡† (OBB) ä» [xy1, xy2, xy3, xy4] æ ¼å¼è½¬æ¢ä¸º [cx, cy, w, h, rotation] æ ¼å¼ã€‚
    æ—‹è½¬è§’åº¦çš„èŒƒå›´æ˜¯ä» 0 åˆ° 90 åº¦ã€‚

    Args:
        x (numpy.ndarray | torch.Tensor): è¾“å…¥çš„è§’ç‚¹æ•°ç»„ [xy1, xy2, xy3, xy4]ï¼Œå½¢çŠ¶ä¸º (n, 8)ã€‚

    Returns:
        (numpy.ndarray | torch.Tensor): è½¬æ¢åçš„æ•°æ®ï¼Œå½¢çŠ¶ä¸º (n, 5)ï¼ŒåŒ…å« [cx, cy, w, h, rotation] æ ¼å¼ã€‚
    """
    is_torch = isinstance(x, torch.Tensor)
    points = x.cpu().numpy() if is_torch else x
    points = points.reshape(len(x), -1, 2)
    rboxes = []
    for pts in points:
        # æ³¨æ„: ä½¿ç”¨ cv2.minAreaRect æ¥è·å–å‡†ç¡®çš„ xywhr æ ¼å¼ï¼Œ
        # ç‰¹åˆ«æ˜¯å½“æ•°æ®åŠ è½½å™¨ä¸­çš„ä¸€äº›å¯¹è±¡å› å¢å¼ºè€Œè¢«è£å‰ªæ—¶ã€‚
        (cx, cy), (w, h), angle = cv2.minAreaRect(pts)
        rboxes.append([cx, cy, w, h, angle / 180 * np.pi])
    return torch.tensor(rboxes, device=x.device, dtype=x.dtype) if is_torch else np.asarray(rboxes)


def xywhr2xyxyxyxy(x):
    """
    å°†æ‰¹é‡çš„æ–¹å‘è¾¹ç•Œæ¡† (OBB) ä» [cx, cy, w, h, rotation] æ ¼å¼è½¬æ¢ä¸º [xy1, xy2, xy3, xy4] æ ¼å¼ã€‚
    æ—‹è½¬è§’åº¦çš„èŒƒå›´åº”ä¸º 0 åˆ° 90 åº¦ã€‚

    Args:
        x (numpy.ndarray | torch.Tensor): è¾“å…¥çš„è§’ç‚¹æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n, 5) æˆ– (b, n, 5)ã€‚

    Returns:
        (numpy.ndarray | torch.Tensor): è½¬æ¢åçš„è§’ç‚¹æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n, 4, 2) æˆ– (b, n, 4, 2)ã€‚
    """
    # è¿™ä¸ªå‡½æ•°æ²¡æœ‰å®ç°ä¸»ä½“éƒ¨åˆ†ï¼Œå› æ­¤ä¸éœ€è¦æ·»åŠ æ³¨é‡Šã€‚
    pass
    # æ ¹æ®è¾“å…¥çš„å¼ é‡ç±»å‹é€‰æ‹©å¯¹åº”çš„æ•°å­¦å‡½æ•°åº“
    cos, sin, cat, stack = (
        (torch.cos, torch.sin, torch.cat, torch.stack)
        if isinstance(x, torch.Tensor)
        else (np.cos, np.sin, np.concatenate, np.stack)
    )

    # æå–å¼ é‡ x çš„ä¸­å¿ƒåæ ‡
    ctr = x[..., :2]
    # æå–å¼ é‡ x çš„å®½åº¦ã€é«˜åº¦å’Œè§’åº¦ä¿¡æ¯
    w, h, angle = (x[..., i : i + 1] for i in range(2, 5))
    # è®¡ç®—è§’åº¦çš„ä½™å¼¦å’Œæ­£å¼¦å€¼
    cos_value, sin_value = cos(angle), sin(angle)
    # è®¡ç®—ç¬¬ä¸€ä¸ªå‘é‡ vec1
    vec1 = [w / 2 * cos_value, w / 2 * sin_value]
    # è®¡ç®—ç¬¬äºŒä¸ªå‘é‡ vec2
    vec2 = [-h / 2 * sin_value, h / 2 * cos_value]
    # åˆå¹¶å‘é‡ vec1 çš„ä¸¤ä¸ªåˆ†é‡
    vec1 = cat(vec1, -1)
    # åˆå¹¶å‘é‡ vec2 çš„ä¸¤ä¸ªåˆ†é‡
    vec2 = cat(vec2, -1)
    # è®¡ç®—çŸ©å½¢çš„å››ä¸ªé¡¶ç‚¹
    pt1 = ctr + vec1 + vec2
    pt2 = ctr + vec1 - vec2
    pt3 = ctr - vec1 - vec2
    pt4 = ctr - vec1 + vec2
    # å°†å››ä¸ªé¡¶ç‚¹æŒ‰è¡Œå †å å½¢æˆæ–°çš„å¼ é‡ï¼Œå¹¶æ²¿ç€å€’æ•°ç¬¬äºŒä¸ªç»´åº¦å †å 
    return stack([pt1, pt2, pt3, pt4], -2)
def ltwh2xyxy(x):
    """
    å°†è¾¹ç•Œæ¡†ä»[x1, y1, w, h]è½¬æ¢ä¸º[x1, y1, x2, y2]ï¼Œå…¶ä¸­xy1ä¸ºå·¦ä¸Šè§’ï¼Œxy2ä¸ºå³ä¸‹è§’ã€‚

    Args:
        x (np.ndarray | torch.Tensor): è¾“å…¥çš„å›¾åƒæˆ–å¼ é‡

    Returns:
        y (np.ndarray | torch.Tensor): è¾¹ç•Œæ¡†çš„xyxyåæ ‡
    """
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    y[..., 2] = x[..., 2] + x[..., 0]  # è®¡ç®—å®½åº¦
    y[..., 3] = x[..., 3] + x[..., 1]  # è®¡ç®—é«˜åº¦
    return y


def segments2boxes(segments):
    """
    å°†åˆ†æ®µæ ‡ç­¾è½¬æ¢ä¸ºæ¡†æ ‡ç­¾ï¼Œå³(cls, xy1, xy2, ...)è½¬æ¢ä¸º(cls, xywh)

    Args:
        segments (list): åˆ†æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªåˆ†æ®µæ˜¯ä¸€ä¸ªç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªç‚¹æ˜¯ä¸€ä¸ªåŒ…å«x, yåæ ‡çš„åˆ—è¡¨

    Returns:
        (np.ndarray): è¾¹ç•Œæ¡†çš„xywhåæ ‡
    """
    boxes = []
    for s in segments:
        x, y = s.T  # æå–åˆ†æ®µçš„xyåæ ‡
        boxes.append([x.min(), y.min(), x.max(), y.max()])  # è®¡ç®—xyxyåæ ‡
    return xyxy2xywh(np.array(boxes))  # è½¬æ¢ä¸ºxywhåæ ‡


def resample_segments(segments, n=1000):
    """
    å°†åˆ†æ®µåˆ—è¡¨(samples,2)è¾“å…¥å¹¶å°†å…¶ä¸Šé‡‡æ ·åˆ°æ¯ä¸ªnç‚¹çš„åˆ†æ®µåˆ—è¡¨(samples,2)ã€‚

    Args:
        segments (list): åŒ…å«(samples,2)æ•°ç»„çš„åˆ—è¡¨ï¼Œå…¶ä¸­samplesæ˜¯åˆ†æ®µä¸­çš„ç‚¹æ•°ã€‚
        n (int): è¦ä¸Šé‡‡æ ·åˆ°çš„ç‚¹æ•°ï¼Œé»˜è®¤ä¸º1000ã€‚

    Returns:
        segments (list): ä¸Šé‡‡æ ·åçš„åˆ†æ®µåˆ—è¡¨ã€‚
    """
    for i, s in enumerate(segments):
        s = np.concatenate((s, s[0:1, :]), axis=0)  # é¦–å°¾ç›¸æ¥ï¼Œé—­åˆåˆ†æ®µ
        x = np.linspace(0, len(s) - 1, n)
        xp = np.arange(len(s))
        segments[i] = (
            np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)], dtype=np.float32).reshape(2, -1).T
        )  # æ’å€¼è·å–ä¸Šé‡‡æ ·ç‚¹
    return segments


def crop_mask(masks, boxes):
    """
    æ ¹æ®è¾¹ç•Œæ¡†è£å‰ªæ©æ¨¡ï¼Œå¹¶è¿”å›è£å‰ªåçš„æ©æ¨¡ã€‚

    Args:
        masks (torch.Tensor): [n, h, w] æ©æ¨¡å¼ é‡
        boxes (torch.Tensor): [n, 4] ç›¸å¯¹ç‚¹å½¢å¼çš„è¾¹ç•Œæ¡†åæ ‡

    Returns:
        (torch.Tensor): è£å‰ªåçš„æ©æ¨¡
    """
    _, h, w = masks.shape
    x1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # åˆ†ç¦»è¾¹ç•Œæ¡†åæ ‡
    r = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # è¡Œç´¢å¼•
    c = torch.arange(h, device=masks.device, dtype=x1.dtype)[None, :, None]  # åˆ—ç´¢å¼•

    return masks * ((r >= x1) * (r < x2) * (c >= y1) * (c < y2))


def process_mask(protos, masks_in, bboxes, shape, upsample=False):
    """
    ä½¿ç”¨æ©æ¨¡å¤´éƒ¨çš„è¾“å‡ºï¼Œå°†æ©æ¨¡åº”ç”¨äºè¾¹ç•Œæ¡†ã€‚

    Args:
        protos: æœªæŒ‡å®š
        masks_in (torch.Tensor): [n, h, w] æ©æ¨¡å¼ é‡
        bboxes (torch.Tensor): [n, 4] è¾¹ç•Œæ¡†åæ ‡
        shape: æœªæŒ‡å®š
        upsample (bool): æ˜¯å¦ä¸Šé‡‡æ ·ï¼Œé»˜è®¤ä¸ºFalse

    Returns:
        unspecified
    """
    # å‡½æ•°ä½“æœªæä¾›
    pass
    # è·å– protos å¼ é‡çš„å½¢çŠ¶ä¿¡æ¯ï¼Œåˆ†åˆ«èµ‹å€¼ç»™ c, mh, mw
    c, mh, mw = protos.shape  # CHW
    
    # è§£æ„ shape å…ƒç»„ï¼Œè·å–è¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ä¿¡æ¯ï¼Œåˆ†åˆ«èµ‹å€¼ç»™ ih, iw
    ih, iw = shape
    
    # è®¡ç®—æ¯ä¸ª mask çš„è¾“å‡ºï¼Œé€šè¿‡ masks_in ä¸ protos çš„çŸ©é˜µä¹˜æ³•ï¼Œå†é‡æ–° reshape æˆ [n, mh, mw] çš„å½¢çŠ¶
    masks = (masks_in @ protos.float().view(c, -1)).view(-1, mh, mw)  # CHW
    
    # è®¡ç®—å®½åº¦å’Œé«˜åº¦çš„æ¯”ç‡ï¼Œç”¨äºå°† bounding boxes æŒ‰æ¯”ä¾‹ç¼©æ”¾
    width_ratio = mw / iw
    height_ratio = mh / ih
    
    # å¤åˆ¶ bounding boxes å¼ é‡ï¼ŒæŒ‰ç…§æ¯”ç‡è°ƒæ•´å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡
    downsampled_bboxes = bboxes.clone()
    downsampled_bboxes[:, 0] *= width_ratio
    downsampled_bboxes[:, 2] *= width_ratio
    downsampled_bboxes[:, 3] *= height_ratio
    downsampled_bboxes[:, 1] *= height_ratio
    
    # è£å‰ª masksï¼Œæ ¹æ® downsampled_bboxes ä¸­çš„è¾¹ç•Œæ¡†ä¿¡æ¯è¿›è¡Œè£å‰ªï¼Œè¾“å‡ºç»“æœçš„å½¢çŠ¶ä¿æŒä¸º CHW
    masks = crop_mask(masks, downsampled_bboxes)  # CHW
    
    # å¦‚æœ upsample æ ‡å¿—ä¸º Trueï¼Œåˆ™å¯¹ masks è¿›è¡ŒåŒçº¿æ€§æ’å€¼ï¼Œå°†å…¶å°ºå¯¸è°ƒæ•´ä¸º shapeï¼Œæœ€ç»ˆå½¢çŠ¶ä¸º [1, h, w]
    if upsample:
        masks = F.interpolate(masks[None], shape, mode="bilinear", align_corners=False)[0]  # CHW
    
    # è¿”å› masks å¼ é‡ä¸­å¤§äº 0.0 çš„å…ƒç´ ï¼Œå³äºŒå€¼åŒ–åçš„äºŒè¿›åˆ¶ mask å¼ é‡ï¼Œå½¢çŠ¶ä¸º [n, h, w]
    return masks.gt_(0.0)
# å®šä¹‰å‡½æ•° process_mask_nativeï¼Œå¤„ç†åŸç”Ÿæ©æ¨¡çš„é€»è¾‘
def process_mask_native(protos, masks_in, bboxes, shape):
    """
    It takes the output of the mask head, and crops it after upsampling to the bounding boxes.

    Args:
        protos (torch.Tensor): [mask_dim, mask_h, mask_w]ï¼ŒåŸå‹æ©æ¨¡çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º [æ©æ¨¡ç»´åº¦, é«˜åº¦, å®½åº¦]
        masks_in (torch.Tensor): [n, mask_dim]ï¼Œç» NMS åçš„æ©æ¨¡å¼ é‡ï¼Œå½¢çŠ¶ä¸º [n, æ©æ¨¡ç»´åº¦]ï¼Œn ä¸ºç»è¿‡ NMS åçš„æ©æ¨¡æ•°é‡
        bboxes (torch.Tensor): [n, 4]ï¼Œç» NMS åçš„è¾¹ç•Œæ¡†å¼ é‡ï¼Œå½¢çŠ¶ä¸º [n, 4]ï¼Œn ä¸ºç»è¿‡ NMS åçš„æ©æ¨¡æ•°é‡
        shape (tuple): è¾“å…¥å›¾åƒçš„å°ºå¯¸ (é«˜åº¦, å®½åº¦)

    Returns:
        masks (torch.Tensor): å¤„ç†åçš„æ©æ¨¡å¼ é‡ï¼Œå½¢çŠ¶ä¸º [é«˜åº¦, å®½åº¦, n]
    """
    c, mh, mw = protos.shape  # è·å–åŸå‹æ©æ¨¡çš„é€šé“æ•°ã€é«˜åº¦ã€å®½åº¦
    masks = (masks_in @ protos.float().view(c, -1)).view(-1, mh, mw)  # è®¡ç®—æ©æ¨¡å¼ é‡ï¼Œè¿›è¡Œä¸Šé‡‡æ ·åè£å‰ªåˆ°è¾¹ç•Œæ¡†å¤§å°
    masks = scale_masks(masks[None], shape)[0]  # å¯¹æ©æ¨¡è¿›è¡Œå°ºå¯¸ç¼©æ”¾
    masks = crop_mask(masks, bboxes)  # æ ¹æ®è¾¹ç•Œæ¡†è£å‰ªæ©æ¨¡
    return masks.gt_(0.0)  # è¿”å›æ©æ¨¡å¼ é‡ï¼Œåº”ç”¨å¤§äºé›¶çš„é˜ˆå€¼å¤„ç†


# å®šä¹‰å‡½æ•° scale_masksï¼Œå°†åˆ†æ®µæ©æ¨¡å°ºå¯¸ç¼©æ”¾åˆ°æŒ‡å®šå½¢çŠ¶
def scale_masks(masks, shape, padding=True):
    """
    Rescale segment masks to shape.

    Args:
        masks (torch.Tensor): (N, C, H, W)ï¼Œæ©æ¨¡å¼ é‡ï¼Œå½¢çŠ¶ä¸º (æ‰¹é‡å¤§å°, é€šé“æ•°, é«˜åº¦, å®½åº¦)
        shape (tuple): ç›®æ ‡é«˜åº¦å’Œå®½åº¦
        padding (bool): å¦‚æœä¸º Trueï¼Œåˆ™å‡è®¾è¾¹ç•Œæ¡†åŸºäº YOLO æ ·å¼å¢å¼ºçš„å›¾åƒã€‚å¦‚æœä¸º Falseï¼Œåˆ™è¿›è¡Œå¸¸è§„å°ºå¯¸ç¼©æ”¾ã€‚

    Returns:
        masks (torch.Tensor): ç¼©æ”¾åçš„æ©æ¨¡å¼ é‡
    """
    mh, mw = masks.shape[2:]  # è·å–æ©æ¨¡å¼ é‡çš„é«˜åº¦å’Œå®½åº¦
    gain = min(mh / shape[0], mw / shape[1])  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ gain = æ—§å°ºå¯¸ / æ–°å°ºå¯¸
    pad = [mw - shape[1] * gain, mh - shape[0] * gain]  # è®¡ç®—é«˜åº¦å’Œå®½åº¦çš„å¡«å……å€¼

    if padding:
        pad[0] /= 2  # å®½åº¦å¡«å……å‡åŠ
        pad[1] /= 2  # é«˜åº¦å¡«å……å‡åŠ

    top, left = (int(pad[1]), int(pad[0])) if padding else (0, 0)  # è®¡ç®—é¡¶éƒ¨å’Œå·¦ä¾§å¡«å……ä½ç½®
    bottom, right = (int(mh - pad[1]), int(mw - pad[0]))  # è®¡ç®—åº•éƒ¨å’Œå³ä¾§å¡«å……ä½ç½®
    masks = masks[..., top:bottom, left:right]  # å¯¹æ©æ¨¡è¿›è¡Œè£å‰ª

    masks = F.interpolate(masks, shape, mode="bilinear", align_corners=False)  # ä½¿ç”¨åŒçº¿æ€§æ’å€¼å¯¹æ©æ¨¡è¿›è¡Œå°ºå¯¸ç¼©æ”¾
    return masks


# å®šä¹‰å‡½æ•° scale_coordsï¼Œå°†å›¾åƒ 1 çš„åˆ†å‰²åæ ‡ç¼©æ”¾åˆ°å›¾åƒ 0 çš„å°ºå¯¸
def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None, normalize=False, padding=True):
    """
    Rescale segment coordinates (xy) from img1_shape to img0_shape.

    Args:
        img1_shape (tuple): åæ ‡æ‰€åœ¨å›¾åƒçš„å°ºå¯¸ã€‚
        coords (torch.Tensor): éœ€è¦ç¼©æ”¾çš„åæ ‡ï¼Œå½¢çŠ¶ä¸º n,2ã€‚
        img0_shape (tuple): åº”ç”¨åˆ†å‰²çš„ç›®æ ‡å›¾åƒçš„å°ºå¯¸ã€‚
        ratio_pad (tuple): å›¾åƒå°ºå¯¸ä¸å¡«å……å›¾åƒå°ºå¯¸çš„æ¯”ä¾‹ã€‚
        normalize (bool): å¦‚æœä¸º Trueï¼Œåˆ™å°†åæ ‡å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´å†…ã€‚é»˜è®¤ä¸º Falseã€‚
        padding (bool): å¦‚æœä¸º Trueï¼Œåˆ™å‡è®¾è¾¹ç•Œæ¡†åŸºäº YOLO æ ·å¼å¢å¼ºçš„å›¾åƒã€‚å¦‚æœä¸º Falseï¼Œåˆ™è¿›è¡Œå¸¸è§„å°ºå¯¸ç¼©æ”¾ã€‚

    Returns:
        coords (torch.Tensor): ç¼©æ”¾åçš„åæ ‡ã€‚
    """
    if ratio_pad is None:  # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¯”ä¾‹ï¼Œåˆ™æ ¹æ®å›¾åƒ 0 çš„å°ºå¯¸è®¡ç®—
        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ gain = æ—§å°ºå¯¸ / æ–°å°ºå¯¸
        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # è®¡ç®—é«˜åº¦å’Œå®½åº¦çš„å¡«å……å€¼
    else:
        gain = ratio_pad[0][0]  # è·å–å¡«å……æ¯”ä¾‹çš„ç¼©æ”¾å¢ç›Š
        pad = ratio_pad[1]  # è·å–å¡«å……å€¼

    if padding:
        coords[..., 0] -= pad[0]  # å‡å» x æ–¹å‘çš„å¡«å……å€¼
        coords[..., 1] -= pad[1]  # å‡å» y æ–¹å‘çš„å¡«å……å€¼

    coords[..., 0] /= gain  # æ ¹æ®ç¼©æ”¾å¢ç›Šè¿›è¡Œ x åæ ‡ç¼©æ”¾
    coords[..., 1] /= gain  # æ ¹æ®ç¼©æ”¾å¢ç›Šè¿›è¡Œ y åæ ‡ç¼©æ”¾
    coords = clip_coords(coords, img0_shape)  # è°ƒç”¨ clip_coords å‡½æ•°å¯¹åæ ‡è¿›è¡Œè£å‰ª
    # å¦‚æœ normalize å‚æ•°ä¸º Trueï¼Œåˆ™è¿›è¡Œåæ ‡å½’ä¸€åŒ–å¤„ç†
    if normalize:
        # å°†æ‰€æœ‰åæ ‡ç‚¹çš„ x å€¼é™¤ä»¥å›¾åƒå®½åº¦ï¼Œå®ç° x åæ ‡çš„å½’ä¸€åŒ–
        coords[..., 0] /= img0_shape[1]  # width
        # å°†æ‰€æœ‰åæ ‡ç‚¹çš„ y å€¼é™¤ä»¥å›¾åƒé«˜åº¦ï¼Œå®ç° y åæ ‡çš„å½’ä¸€åŒ–
        coords[..., 1] /= img0_shape[0]  # height
    # è¿”å›å½’ä¸€åŒ–åçš„åæ ‡æ•°ç»„
    return coords
# Regularize rotated boxes in range [0, pi/2].
def regularize_rboxes(rboxes):
    x, y, w, h, t = rboxes.unbind(dim=-1)
    # Swap edge and angle if h >= w
    w_ = torch.where(w > h, w, h)  # Determine the maximum edge length
    h_ = torch.where(w > h, h, w)  # Determine the minimum edge length
    t = torch.where(w > h, t, t + math.pi / 2) % math.pi  # Adjust angle if height is greater than width
    return torch.stack([x, y, w_, h_, t], dim=-1)  # Stack the regularized boxes


# It takes a list of masks(n,h,w) and returns a list of segments(n,xy)
def masks2segments(masks, strategy="largest"):
    segments = []
    for x in masks.int().cpu().numpy().astype("uint8"):
        # Find contours in the mask image
        c = cv2.findContours(x, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]
        if c:
            if strategy == "concat":  # concatenate all segments
                c = np.concatenate([x.reshape(-1, 2) for x in c])
            elif strategy == "largest":  # select largest segment
                c = np.array(c[np.array([len(x) for x in c]).argmax()]).reshape(-1, 2)
        else:
            c = np.zeros((0, 2))  # no segments found
        segments.append(c.astype("float32"))
    return segments


# Convert a batch of FP32 torch tensors (0.0-1.0) to a NumPy uint8 array (0-255), changing from BCHW to BHWC layout.
def convert_torch2numpy_batch(batch: torch.Tensor) -> np.ndarray:
    return (batch.permute(0, 2, 3, 1).contiguous() * 255).clamp(0, 255).to(torch.uint8).cpu().numpy()


# Cleans a string by replacing special characters with underscore _
def clean_str(s):
    return re.sub(pattern="[|@#!Â¡Â·$â‚¬%&()=?Â¿^*;:,Â¨Â´><+]", repl="_", string=s)
```