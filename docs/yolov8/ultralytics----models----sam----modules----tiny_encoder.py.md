# `.\yolov8\ultralytics\models\sam\modules\tiny_encoder.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

# --------------------------------------------------------
# TinyViT Model Architecture
# Copyright (c) 2022 Microsoft
# Adapted from LeViT and Swin Transformer
#   LeViT: (https://github.com/facebookresearch/levit)
#   Swin: (https://github.com/microsoft/swin-transformer)
# Build the TinyViT Model
# --------------------------------------------------------

import itertools  # å¯¼å…¥ itertools åº“ï¼Œç”¨äºè¿­ä»£æ“ä½œ
from typing import Tuple  # å¯¼å…¥ Tuple ç±»å‹æç¤ºï¼Œç”¨äºæŒ‡å®šå…ƒç»„ç±»å‹

import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ åº“
import torch.nn as nn  # å¯¼å…¥ PyTorch ç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F  # å¯¼å…¥ PyTorch ç¥ç»ç½‘ç»œå‡½æ•°æ¨¡å—
import torch.utils.checkpoint as checkpoint  # å¯¼å…¥ PyTorch æ£€æŸ¥ç‚¹æ¨¡å—ï¼Œç”¨äºå†…å­˜ä¼˜åŒ–

from ultralytics.utils.instance import to_2tuple  # ä» ultralytics.utils.instance æ¨¡å—ä¸­å¯¼å…¥ to_2tuple å‡½æ•°


class Conv2d_BN(torch.nn.Sequential):
    """A sequential container that performs 2D convolution followed by batch normalization."""

    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1, groups=1, bn_weight_init=1):
        """Initializes the MBConv model with given input channels, output channels, expansion ratio, activation, and
        drop path.
        """
        super().__init__()
        # æ·»åŠ  2D å·ç§¯å±‚ï¼Œä¸ä½¿ç”¨åç½®å‚æ•°
        self.add_module("c", torch.nn.Conv2d(a, b, ks, stride, pad, dilation, groups, bias=False))
        # æ·»åŠ æ‰¹å½’ä¸€åŒ–å±‚ï¼Œå¹¶åˆå§‹åŒ–æƒé‡ä¸º bn_weight_initï¼Œåç½®ä¸º 0
        bn = torch.nn.BatchNorm2d(b)
        torch.nn.init.constant_(bn.weight, bn_weight_init)
        torch.nn.init.constant_(bn.bias, 0)
        self.add_module("bn", bn)


class PatchEmbed(nn.Module):
    """Embeds images into patches and projects them into a specified embedding dimension."""

    def __init__(self, in_chans, embed_dim, resolution, activation):
        """Initialize the PatchMerging class with specified input, output dimensions, resolution and activation
        function.
        """
        super().__init__()
        img_size: Tuple[int, int] = to_2tuple(resolution)
        self.patches_resolution = (img_size[0] // 4, img_size[1] // 4)
        self.num_patches = self.patches_resolution[0] * self.patches_resolution[1]
        self.in_chans = in_chans
        self.embed_dim = embed_dim
        n = embed_dim
        # æ„å»ºåºåˆ—æ¨¡å‹ï¼ŒåŒ…å«ä¸¤ä¸ª Conv2d_BN å±‚å’Œæ¿€æ´»å‡½æ•°
        self.seq = nn.Sequential(
            Conv2d_BN(in_chans, n // 2, 3, 2, 1),  # ç¬¬ä¸€ä¸ªå·ç§¯ + æ‰¹å½’ä¸€åŒ–å±‚
            activation(),  # æ¿€æ´»å‡½æ•°
            Conv2d_BN(n // 2, n, 3, 2, 1),  # ç¬¬äºŒä¸ªå·ç§¯ + æ‰¹å½’ä¸€åŒ–å±‚
        )

    def forward(self, x):
        """Runs input tensor 'x' through the PatchMerging model's sequence of operations."""
        return self.seq(x)


class MBConv(nn.Module):
    """Mobile Inverted Bottleneck Conv (MBConv) layer, part of the EfficientNet architecture."""
    def __init__(self, in_chans, out_chans, expand_ratio, activation, drop_path):
        """
        Initializes a convolutional layer with specified dimensions, input resolution, depth, and activation
        function.
        """
        super().__init__()
        
        # è®¾ç½®è¾“å…¥é€šé“æ•°
        self.in_chans = in_chans
        # è®¡ç®—éšè—å±‚é€šé“æ•°ï¼Œæ ¹æ®æ‰©å±•æ¯”ä¾‹
        self.hidden_chans = int(in_chans * expand_ratio)
        # è®¾ç½®è¾“å‡ºé€šé“æ•°
        self.out_chans = out_chans

        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼ŒåŒ…æ‹¬å·ç§¯å’Œæ‰¹å½’ä¸€åŒ–
        self.conv1 = Conv2d_BN(in_chans, self.hidden_chans, ks=1)
        # ç¬¬ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œæ ¹æ®ç»™å®šçš„æ¿€æ´»å‡½æ•°ç±»å®ä¾‹åŒ–
        self.act1 = activation()

        # ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼ŒåŒ…æ‹¬å·ç§¯ã€æ‰¹å½’ä¸€åŒ–å’Œåˆ†ç»„å·ç§¯ï¼ˆæ ¹æ®éšè—é€šé“æ•°ï¼‰
        self.conv2 = Conv2d_BN(self.hidden_chans, self.hidden_chans, ks=3, stride=1, pad=1, groups=self.hidden_chans)
        # ç¬¬äºŒä¸ªæ¿€æ´»å‡½æ•°ï¼ŒåŒæ ·æ ¹æ®ç»™å®šçš„æ¿€æ´»å‡½æ•°ç±»å®ä¾‹åŒ–
        self.act2 = activation()

        # ç¬¬ä¸‰ä¸ªå·ç§¯å±‚ï¼ŒåŒ…æ‹¬å·ç§¯ã€æ‰¹å½’ä¸€åŒ–
        self.conv3 = Conv2d_BN(self.hidden_chans, out_chans, ks=1, bn_weight_init=0.0)
        # ç¬¬ä¸‰ä¸ªæ¿€æ´»å‡½æ•°ï¼Œä½¿ç”¨ç»™å®šçš„æ¿€æ´»å‡½æ•°ç±»å®ä¾‹åŒ–
        self.act3 = activation()

        # åœ¨è®­ç»ƒæ—¶ï¼Œæ ¹æ®æ˜¯å¦éœ€è¦è¿›è¡Œ DropPath æ“ä½œæ¥å†³å®šæ˜¯å¦ä½¿ç”¨ DropPath å±‚
        # NOTE: `DropPath` is needed only for training.
        self.drop_path = nn.Identity()  # å¦‚æœ drop_path <= 0ï¼Œä½¿ç”¨æ’ç­‰æ˜ å°„ä½œä¸º drop_path
        # self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        """
        Implements the forward pass for the model architecture.
        """
        # å°†è¾“å…¥ä½œä¸ºå¿«æ·è¿æ¥ï¼ˆshortcutï¼‰
        shortcut = x
        # ç¬¬ä¸€å±‚å·ç§¯æ“ä½œ
        x = self.conv1(x)
        # ç¬¬ä¸€å±‚æ¿€æ´»å‡½æ•°
        x = self.act1(x)
        # ç¬¬äºŒå±‚å·ç§¯æ“ä½œ
        x = self.conv2(x)
        # ç¬¬äºŒå±‚æ¿€æ´»å‡½æ•°
        x = self.act2(x)
        # ç¬¬ä¸‰å±‚å·ç§¯æ“ä½œ
        x = self.conv3(x)
        # DropPath æ“ä½œï¼ˆåœ¨è®­ç»ƒæ—¶å¯èƒ½ä¼šå¯¹ x è¿›è¡Œæ“ä½œï¼‰
        x = self.drop_path(x)
        # åŠ ä¸Šå¿«æ·è¿æ¥
        x += shortcut
        # æœ€åä¸€å±‚æ¿€æ´»å‡½æ•°
        return self.act3(x)
class PatchMerging(nn.Module):
    """Merges neighboring patches in the feature map and projects to a new dimension."""

    def __init__(self, input_resolution, dim, out_dim, activation):
        """Initializes the PatchMerging module with specified parameters.

        Args:
            input_resolution (tuple): Resolution of the input feature map (H, W).
            dim (int): Input dimensionality of the feature map.
            out_dim (int): Output dimensionality after merging and projection.
            activation (torch.nn.Module): Activation function instance.
        """
        super().__init__()

        self.input_resolution = input_resolution  # Store input resolution (H, W)
        self.dim = dim  # Store input dimensionality
        self.out_dim = out_dim  # Store output dimensionality
        self.act = activation()  # Initialize activation function instance
        self.conv1 = Conv2d_BN(dim, out_dim, 1, 1, 0)  # 1x1 convolution layer
        stride_c = 1 if out_dim in {320, 448, 576} else 2
        self.conv2 = Conv2d_BN(out_dim, out_dim, 3, stride_c, 1, groups=out_dim)  # Depthwise separable convolution
        self.conv3 = Conv2d_BN(out_dim, out_dim, 1, 1, 0)  # 1x1 convolution layer

    def forward(self, x):
        """Performs forward pass through the PatchMerging module.

        Args:
            x (torch.Tensor): Input tensor, expected to have dimensions (B, C, H, W) or (B, H, W, C).

        Returns:
            torch.Tensor: Flattened and transposed tensor after convolution operations.
        """
        if x.ndim == 3:
            H, W = self.input_resolution
            B = len(x)
            # Reshape input tensor to (B, C, H, W) format if initially in (B, H, W, C)
            x = x.view(B, H, W, -1).permute(0, 3, 1, 2)

        x = self.conv1(x)  # Apply first convolution layer
        x = self.act(x)  # Apply activation function

        x = self.conv2(x)  # Apply second convolution layer
        x = self.act(x)  # Apply activation function
        x = self.conv3(x)  # Apply third convolution layer

        return x.flatten(2).transpose(1, 2)  # Flatten and transpose output tensor


class ConvLayer(nn.Module):
    """
    Convolutional Layer featuring multiple MobileNetV3-style inverted bottleneck convolutions (MBConv).

    Optionally applies downsample operations to the output, and provides support for gradient checkpointing.
    """

    def __init__(
        self,
        dim,
        input_resolution,
        depth,
        activation,
        drop_path=0.0,
        downsample=None,
        use_checkpoint=False,
        out_dim=None,
        conv_expand_ratio=4.0,
    ):
        """Initializes the ConvLayer module with specified parameters.

        Args:
            dim (int): Input dimensionality for the convolutional layer.
            input_resolution (tuple): Resolution of the input feature map (H, W).
            depth (int): Depth of the convolutional layer.
            activation (torch.nn.Module): Activation function instance.
            drop_path (float, optional): Dropout probability. Defaults to 0.0.
            downsample (str or None, optional): Downsample operation type. Defaults to None.
            use_checkpoint (bool, optional): Flag to use gradient checkpointing. Defaults to False.
            out_dim (int or None, optional): Output dimensionality. Defaults to None.
            conv_expand_ratio (float, optional): Expansion ratio for convolution layers. Defaults to 4.0.
        """
        super().__init__()

        # Initialize module attributes
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.activation = activation
        self.drop_path = drop_path
        self.downsample = downsample
        self.use_checkpoint = use_checkpoint
        self.out_dim = out_dim
        self.conv_expand_ratio = conv_expand_ratio
    ):
        """
        Initializes the ConvLayer with the given dimensions and settings.

        Args:
            dim (int): The dimensionality of the input and output.
            input_resolution (Tuple[int, int]): The resolution of the input image.
            depth (int): The number of MBConv layers in the block.
            activation (Callable): Activation function applied after each convolution.
            drop_path (Union[float, List[float]]): Drop path rate. Single float or a list of floats for each MBConv.
            downsample (Optional[Callable]): Function for downsampling the output. None to skip downsampling.
            use_checkpoint (bool): Whether to use gradient checkpointing to save memory.
            out_dim (Optional[int]): The dimensionality of the output. None means it will be the same as `dim`.
            conv_expand_ratio (float): Expansion ratio for the MBConv layers.
        """
        super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•

        self.dim = dim  # è®¾ç½®è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦
        self.input_resolution = input_resolution  # è®¾ç½®è¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡
        self.depth = depth  # è®¾ç½® MBConv å—ä¸­çš„å±‚æ•°
        self.use_checkpoint = use_checkpoint  # è®¾ç½®æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ¥èŠ‚çœå†…å­˜

        # æ„å»ºå—
        self.blocks = nn.ModuleList(
            [
                MBConv(
                    dim,
                    dim,
                    conv_expand_ratio,
                    activation,
                    drop_path[i] if isinstance(drop_path, list) else drop_path,
                )
                for i in range(depth)
            ]
        )

        # Patch merging layer
        self.downsample = (
            None  # å¦‚æœæ²¡æœ‰æŒ‡å®š downsample å‡½æ•°ï¼Œåˆ™è®¾ç½®ä¸º None
            if downsample is None
            else downsample(input_resolution, dim=dim, out_dim=out_dim, activation=activation)  # å¦åˆ™è°ƒç”¨ downsample å‡½æ•°è¿›è¡Œä¸‹é‡‡æ ·
        )

    def forward(self, x):
        """Processes the input through a series of convolutional layers and returns the activated output."""
        for blk in self.blocks:
            x = checkpoint.checkpoint(blk, x) if self.use_checkpoint else blk(x)  # ä¾æ¬¡å¯¹è¾“å…¥ x åº”ç”¨æ¯ä¸ª MBConv å—
        return x if self.downsample is None else self.downsample(x)  # å¦‚æœå®šä¹‰äº† downsample å‡½æ•°ï¼Œåˆ™å¯¹æœ€ç»ˆè¾“å‡º x è¿›è¡Œä¸‹é‡‡æ ·å¤„ç†
        """
        Initializes the Multi-head Attention module with given parameters.

        Args:
            dim (int): Dimensionality of input embeddings.
            key_dim (int): Dimensionality of key and query vectors.
            num_heads (int, optional): Number of attention heads. Default is 8.
            attn_ratio (int, optional): Ratio of total spatial positions to use as attention biases. Default is 4.
            resolution (tuple, optional): Spatial resolution of the input. Default is (14, 14).
        """
        super().__init__()
        # Calculate the size of each head in the attention mechanism
        head_dim = key_dim // num_heads
        # Initialize the linear transformation of input into query, key, and value
        self.to_qkv = nn.Linear(dim, 3 * key_dim)
        # Cache the number of attention heads
        self.num_heads = num_heads
        # Set the spatial bias ratio for attention mechanism
        self.attn_ratio = attn_ratio
        # Determine the size of spatial grid for the attention biases
        self.resolution = resolution
        # Initialize cached attention biases for inference, to be deleted during training
        self.ab = None
    ):
        """
        Initializes the Attention module.

        Args:
            dim (int): The dimensionality of the input and output.
            key_dim (int): The dimensionality of the keys and queries.
            num_heads (int, optional): Number of attention heads. Default is 8.
            attn_ratio (float, optional): Attention ratio, affecting the dimensions of the value vectors. Default is 4.
            resolution (Tuple[int, int], optional): Spatial resolution of the input feature map. Default is (14, 14).

        Raises:
            AssertionError: If `resolution` is not a tuple of length 2.
        """
        super().__init__()

        # æ£€æŸ¥å¹¶ç¡®ä¿ `resolution` æ˜¯é•¿åº¦ä¸º 2 çš„å…ƒç»„
        assert isinstance(resolution, tuple) and len(resolution) == 2, "'resolution' argument not tuple of length 2"
        
        # è®¾ç½®æ¨¡å—çš„å±æ€§
        self.num_heads = num_heads
        self.scale = key_dim**-0.5
        self.key_dim = key_dim
        self.nh_kd = nh_kd = key_dim * num_heads
        self.d = int(attn_ratio * key_dim)
        self.dh = int(attn_ratio * key_dim) * num_heads
        self.attn_ratio = attn_ratio
        
        # è®¡ç®— `h`ï¼Œä½œä¸ºåç»­çº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦
        h = self.dh + nh_kd * 2

        # Layer normalization å±‚
        self.norm = nn.LayerNorm(dim)
        
        # çº¿æ€§å˜æ¢å±‚ï¼Œå°†è¾“å…¥è½¬æ¢ä¸º `h` ç»´åº¦
        self.qkv = nn.Linear(dim, h)
        
        # è¾“å‡ºæŠ•å½±å±‚ï¼Œå°†æ³¨æ„åŠ›å¤´çš„ç»“æœæŠ•å½±å› `dim` ç»´åº¦
        self.proj = nn.Linear(self.dh, dim)

        # ç”Ÿæˆæ‰€æœ‰ç©ºé—´ä½ç½®çš„åç§»é‡å¯¹åº”çš„ç´¢å¼•
        points = list(itertools.product(range(resolution[0]), range(resolution[1])))
        N = len(points)
        attention_offsets = {}
        idxs = []
        for p1 in points:
            for p2 in points:
                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))
                if offset not in attention_offsets:
                    attention_offsets[offset] = len(attention_offsets)
                idxs.append(attention_offsets[offset])
        
        # åˆå§‹åŒ–æ³¨æ„åŠ›åç½®å‚æ•°ï¼Œå¹¶æ³¨å†Œä¸ºæ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°
        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, len(attention_offsets)))
        self.register_buffer("attention_bias_idxs", torch.LongTensor(idxs).view(N, N), persistent=False)

    @torch.no_grad()
    def train(self, mode=True):
        """Sets the module in training mode and handles attribute 'ab' based on the mode."""
        # è°ƒç”¨çˆ¶ç±»çš„ `train` æ–¹æ³•ï¼Œè®¾ç½®æ¨¡å—çš„è®­ç»ƒæ¨¡å¼
        super().train(mode)
        
        # æ ¹æ®è®­ç»ƒæ¨¡å¼å¤„ç† `ab` å±æ€§
        if mode and hasattr(self, "ab"):
            del self.ab  # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ä¸”å­˜åœ¨ `ab` å±æ€§ï¼Œåˆ™åˆ é™¤å®ƒ
        else:
            self.ab = self.attention_biases[:, self.attention_bias_idxs]
            # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼æˆ– `ab` å±æ€§ä¸å­˜åœ¨ï¼Œåˆ™å°† `attention_biases` ä¸ `attention_bias_idxs` ç»“åˆèµ·æ¥å­˜å‚¨åœ¨ `ab` ä¸­
    def forward(self, x):  # x
        """å¯¹è¾“å…¥å¼ é‡ 'x' æ‰§è¡Œå‰å‘ä¼ æ’­ï¼ŒåŒ…æ‹¬å½’ä¸€åŒ–å’ŒæŸ¥è¯¢é”®/å€¼æ“ä½œã€‚"""
        B, N, _ = x.shape  # B, N, C

        # å½’ä¸€åŒ–å¤„ç†
        x = self.norm(x)

        # æŸ¥è¯¢é”®å€¼å¯¹
        qkv = self.qkv(x)
        # å°†ç»“æœé‡å¡‘ä¸º (B, N, num_heads, d)ï¼Œå¹¶åˆ†å‰²ä¸º q, k, v
        q, k, v = qkv.view(B, N, self.num_heads, -1).split([self.key_dim, self.key_dim, self.d], dim=3)
        # å°†ç»´åº¦é‡æ–°æ’åˆ—ä¸º (B, num_heads, N, d)
        q = q.permute(0, 2, 1, 3)
        k = k.permute(0, 2, 1, 3)
        v = v.permute(0, 2, 1, 3)

        # å°† attention_biases è½¬ç§»åˆ°åˆé€‚çš„è®¾å¤‡ä¸Š
        self.ab = self.ab.to(self.attention_biases.device)

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ŒåŒ…æ‹¬ç¼©æ”¾å’Œåç½®é¡¹
        attn = (q @ k.transpose(-2, -1)) * self.scale + (
            self.attention_biases[:, self.attention_bias_idxs] if self.training else self.ab
        )
        attn = attn.softmax(dim=-1)
        
        # è®¡ç®—åŠ æƒåçš„å€¼
        x = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)
        
        # åº”ç”¨æŠ•å½±å±‚å¹¶è¿”å›ç»“æœ
        return self.proj(x)
class TinyViTBlock(nn.Module):
    """TinyViT Block that applies self-attention and a local convolution to the input."""

    def __init__(
        self,
        dim,
        input_resolution,
        num_heads,
        window_size=7,
        mlp_ratio=4.0,
        drop=0.0,
        drop_path=0.0,
        local_conv_size=3,
        activation=nn.GELU,
    ):
        """
        Initializes the TinyViTBlock.

        Args:
            dim (int): The dimensionality of the input and output.
            input_resolution (Tuple[int, int]): Spatial resolution of the input feature map.
            num_heads (int): Number of attention heads.
            window_size (int, optional): Window size for attention. Default is 7.
            mlp_ratio (float, optional): Ratio of mlp hidden dim to embedding dim. Default is 4.
            drop (float, optional): Dropout rate. Default is 0.
            drop_path (float, optional): Stochastic depth rate. Default is 0.
            local_conv_size (int, optional): The kernel size of the local convolution. Default is 3.
            activation (torch.nn, optional): Activation function for MLP. Default is nn.GELU.

        Raises:
            AssertionError: If `window_size` is not greater than 0.
            AssertionError: If `dim` is not divisible by `num_heads`.
        """
        super().__init__()
        self.dim = dim  # è®¾ç½®è¾“å…¥è¾“å‡ºçš„ç»´åº¦
        self.input_resolution = input_resolution  # è®¾ç½®è¾“å…¥ç‰¹å¾å›¾çš„ç©ºé—´åˆ†è¾¨ç‡
        self.num_heads = num_heads  # è®¾ç½®æ³¨æ„åŠ›å¤´çš„æ•°é‡
        assert window_size > 0, "window_size must be greater than 0"  # æ–­è¨€çª—å£å¤§å°å¿…é¡»å¤§äº0
        self.window_size = window_size  # è®¾ç½®æ³¨æ„åŠ›æœºåˆ¶çš„çª—å£å¤§å°
        self.mlp_ratio = mlp_ratio  # è®¾ç½®MLPéšè—å±‚ç»´åº¦ä¸åµŒå…¥ç»´åº¦çš„æ¯”ä¾‹

        # NOTE: `DropPath` is needed only for training.
        # self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.drop_path = nn.Identity()  # è®¾ç½®DropPathå±‚ï¼Œç”¨äºè®­ç»ƒæ—¶çš„éšæœºæ·±åº¦ï¼ˆå¦‚æœdrop_pathå¤§äº0ï¼‰

        assert dim % num_heads == 0, "dim must be divisible by num_heads"  # æ–­è¨€ç»´åº¦å¿…é¡»èƒ½å¤Ÿè¢«æ³¨æ„åŠ›å¤´æ•°æ•´é™¤
        head_dim = dim // num_heads  # è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦

        window_resolution = (window_size, window_size)
        self.attn = Attention(dim, head_dim, num_heads, attn_ratio=1, resolution=window_resolution)
        # åˆå§‹åŒ–æ³¨æ„åŠ›å±‚ï¼Œä¼ å…¥ç»´åº¦ã€å¤´éƒ¨ç»´åº¦ã€å¤´éƒ¨æ•°é‡ã€æ³¨æ„åŠ›æ¯”ä¾‹å’Œçª—å£åˆ†è¾¨ç‡

        mlp_hidden_dim = int(dim * mlp_ratio)
        mlp_activation = activation
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=mlp_activation, drop=drop)
        # åˆå§‹åŒ–MLPå±‚ï¼Œä¼ å…¥è¾“å…¥ç‰¹å¾ç»´åº¦ã€éšè—å±‚ç‰¹å¾ç»´åº¦ã€æ¿€æ´»å‡½æ•°å’Œdropoutç‡

        pad = local_conv_size // 2
        self.local_conv = Conv2d_BN(dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim)
        # åˆå§‹åŒ–æœ¬åœ°å·ç§¯å±‚ï¼Œä¼ å…¥è¾“å…¥å’Œè¾“å‡ºç‰¹å¾ç»´åº¦ã€å·ç§¯æ ¸å¤§å°ã€æ­¥é•¿ã€å¡«å……ã€åˆ†ç»„æ•°
    def forward(self, x):
        """å¯¹è¾“å…¥çš„ 'x' è¿›è¡ŒåŸºäºæ³¨æ„åŠ›çš„è½¬æ¢æˆ–å¡«å……ï¼Œç„¶åé€šè¿‡æœ¬åœ°å·ç§¯ä¼ é€’ã€‚

        Args:
            x (tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch, height*width, channels]ã€‚

        Returns:
            tensor: ç»è¿‡å¤„ç†åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch, height*width, channels]ã€‚
        """
        h, w = self.input_resolution
        b, hw, c = x.shape  # batch, height*width, channels
        assert hw == h * w, "input feature has wrong size"  # æ–­è¨€è¾“å…¥ç‰¹å¾çš„å°ºå¯¸æ˜¯å¦æ­£ç¡®
        res_x = x  # ä¿ç•™åŸå§‹è¾“å…¥å¼ é‡

        # å¦‚æœè¾“å…¥åˆ†è¾¨ç‡ç­‰äºçª—å£å°ºå¯¸ï¼Œåˆ™ç›´æ¥åº”ç”¨æ³¨æ„åŠ›æ¨¡å—
        if h == self.window_size and w == self.window_size:
            x = self.attn(x)
        else:
            # å¦åˆ™ï¼Œå¯¹è¾“å…¥è¿›è¡Œé‡å¡‘ä»¥ä¾¿è¿›è¡Œå¡«å……
            x = x.view(b, h, w, c)
            pad_b = (self.window_size - h % self.window_size) % self.window_size
            pad_r = (self.window_size - w % self.window_size) % self.window_size
            padding = pad_b > 0 or pad_r > 0  # æ£€æŸ¥æ˜¯å¦éœ€è¦å¡«å……

            # å¦‚æœéœ€è¦å¡«å……ï¼Œåˆ™å¯¹è¾“å…¥è¿›è¡Œå¡«å……æ“ä½œ
            if padding:
                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))

            pH, pW = h + pad_b, w + pad_r
            nH = pH // self.window_size
            nW = pW // self.window_size

            # çª—å£åˆ†å‰²
            x = (
                x.view(b, nH, self.window_size, nW, self.window_size, c)
                .transpose(2, 3)
                .reshape(b * nH * nW, self.window_size * self.window_size, c)
            )
            x = self.attn(x)  # åº”ç”¨æ³¨æ„åŠ›æ¨¡å—

            # çª—å£é‡ç»„
            x = x.view(b, nH, nW, self.window_size, self.window_size, c).transpose(2, 3).reshape(b, pH, pW, c)
            if padding:
                x = x[:, :h, :w].contiguous()  # ç§»é™¤å¡«å……éƒ¨åˆ†

            x = x.view(b, hw, c)  # æ¢å¤åŸå§‹å½¢çŠ¶

        x = res_x + self.drop_path(x)  # åŠ å…¥æ®‹å·®è¿æ¥å’ŒDropPathæ“ä½œ
        x = x.transpose(1, 2).reshape(b, c, h, w)  # è½¬ç½®å’Œé‡å¡‘å¼ é‡å½¢çŠ¶
        x = self.local_conv(x)  # åº”ç”¨æœ¬åœ°å·ç§¯
        x = x.view(b, c, hw).transpose(1, 2)  # é‡å¡‘å¼ é‡å½¢çŠ¶

        return x + self.drop_path(self.mlp(x))  # åŠ å…¥æ®‹å·®è¿æ¥å’ŒMLPæ“ä½œ

    def extra_repr(self) -> str:
        """è¿”å›ä¸€ä¸ªæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºTinyViTBlockçš„å‚æ•°ï¼šç»´åº¦ã€è¾“å…¥åˆ†è¾¨ç‡ã€æ³¨æ„åŠ›å¤´æ•°ã€çª—å£å°ºå¯¸å’ŒMLPæ¯”ä¾‹ã€‚

        Returns:
            str: æ ¼å¼åŒ–åçš„å‚æ•°ä¿¡æ¯å­—ç¬¦ä¸²ã€‚
        """
        return (
            f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, "
            f"window_size={self.window_size}, mlp_ratio={self.mlp_ratio}"
        )
# å®šä¹‰ä¸€ä¸ªåä¸º BasicLayer çš„ç±»ï¼Œç”¨äº TinyViT æ¶æ„ä¸­çš„ä¸€ä¸ªé˜¶æ®µçš„åŸºæœ¬å±‚æ¬¡
class BasicLayer(nn.Module):
    """A basic TinyViT layer for one stage in a TinyViT architecture."""

    def __init__(
        self,
        dim,
        input_resolution,
        depth,
        num_heads,
        window_size,
        mlp_ratio=4.0,
        drop=0.0,
        drop_path=0.0,
        downsample=None,
        use_checkpoint=False,
        local_conv_size=3,
        activation=nn.GELU,
        out_dim=None,
    ):
        """
        Initializes the BasicLayer.

        Args:
            dim (int): The dimensionality of the input and output.
            input_resolution (Tuple[int, int]): Spatial resolution of the input feature map.
            depth (int): Number of TinyViT blocks.
            num_heads (int): Number of attention heads.
            window_size (int): Local window size.
            mlp_ratio (float, optional): Ratio of mlp hidden dim to embedding dim. Default is 4.
            drop (float, optional): Dropout rate. Default is 0.
            drop_path (float | tuple[float], optional): Stochastic depth rate. Default is 0.
            downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default is None.
            use_checkpoint (bool, optional): Whether to use checkpointing to save memory. Default is False.
            local_conv_size (int, optional): Kernel size of the local convolution. Default is 3.
            activation (torch.nn, optional): Activation function for MLP. Default is nn.GELU.
            out_dim (int | None, optional): The output dimension of the layer. Default is None.

        Raises:
            ValueError: If `drop_path` is a list of float but its length doesn't match `depth`.
        """
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()
        # è®¾ç½®ç±»çš„å±æ€§
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # æ„å»º TinyViTBlock ç»„æˆçš„æ¨¡å—åˆ—è¡¨
        self.blocks = nn.ModuleList(
            [
                TinyViTBlock(
                    dim=dim,
                    input_resolution=input_resolution,
                    num_heads=num_heads,
                    window_size=window_size,
                    mlp_ratio=mlp_ratio,
                    drop=drop,
                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                    local_conv_size=local_conv_size,
                    activation=activation,
                )
                for i in range(depth)  # æ ¹æ® depth å‚æ•°å¾ªç¯åˆ›å»º TinyViTBlock
            ]
        )

        # å¦‚æœæŒ‡å®šäº† downsample å‚æ•°ï¼Œåˆ™åˆ›å»ºå¯¹åº”çš„ä¸‹é‡‡æ ·å±‚
        self.downsample = (
            None
            if downsample is None
            else downsample(input_resolution, dim=dim, out_dim=out_dim, activation=activation)
        )
    # æ‰§è¡Œè¾“å…¥å¼ é‡çš„å‰å‘ä¼ æ’­ï¼Œå¹¶è¿”å›ä¸€ä¸ªè§„èŒƒåŒ–çš„å¼ é‡
    def forward(self, x):
        # éå†ç½‘ç»œä¸­çš„æ¯ä¸ªå—è¿›è¡Œå‰å‘ä¼ æ’­
        for blk in self.blocks:
            # å¦‚æœä½¿ç”¨äº†æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œåˆ™é€šè¿‡æ£€æŸ¥ç‚¹æ‰§è¡Œå—çš„å‰å‘ä¼ æ’­ï¼Œå¦åˆ™ç›´æ¥è°ƒç”¨å—çš„å‰å‘ä¼ æ’­
            x = checkpoint.checkpoint(blk, x) if self.use_checkpoint else blk(x)
        # å¦‚æœå­˜åœ¨ä¸‹é‡‡æ ·å‡½æ•°ï¼Œåˆ™å¯¹è¾“å‡ºå¼ é‡è¿›è¡Œä¸‹é‡‡æ ·æ“ä½œ
        return x if self.downsample is None else self.downsample(x)

    # è¿”å›ä¸€ä¸ªæè¿°å±‚å‚æ•°çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼
    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"
    class LayerNorm2d(nn.Module):
        """A PyTorch implementation of Layer Normalization in 2D."""

        def __init__(self, num_channels: int, eps: float = 1e-6) -> None:
            """Initialize LayerNorm2d with the number of channels and an optional epsilon."""
            super().__init__()
            # Define learnable parameters for scaling and shifting
            self.weight = nn.Parameter(torch.ones(num_channels))
            self.bias = nn.Parameter(torch.zeros(num_channels))
            self.eps = eps

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """Perform a forward pass, normalizing the input tensor."""
            # Compute mean and standard deviation across channels
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            # Normalize the input tensor
            x = (x - u) / torch.sqrt(s + self.eps)
            # Scale and shift the normalized tensor
            return self.weight[:, None, None] * x + self.bias[:, None, None]


    class TinyViT(nn.Module):
        """
        The TinyViT architecture for vision tasks.

        Attributes:
            img_size (int): Input image size.
            in_chans (int): Number of input channels.
            num_classes (int): Number of classification classes.
            embed_dims (List[int]): List of embedding dimensions for each layer.
            depths (List[int]): List of depths for each layer.
            num_heads (List[int]): List of number of attention heads for each layer.
            window_sizes (List[int]): List of window sizes for each layer.
            mlp_ratio (float): Ratio of MLP hidden dimension to embedding dimension.
            drop_rate (float): Dropout rate for drop layers.
            drop_path_rate (float): Drop path rate for stochastic depth.
            use_checkpoint (bool): Use checkpointing for efficient memory usage.
            mbconv_expand_ratio (float): Expansion ratio for MBConv layer.
            local_conv_size (int): Local convolution kernel size.
            layer_lr_decay (float): Layer-wise learning rate decay.

        Note:
            This implementation is generalized to accept a list of depths, attention heads,
            embedding dimensions and window sizes, which allows you to create a
            "stack" of TinyViT models of varying configurations.
        """

        def __init__(
            self,
            img_size=224,
            in_chans=3,
            num_classes=1000,
            embed_dims=(96, 192, 384, 768),
            depths=(2, 2, 6, 2),
            num_heads=(3, 6, 12, 24),
            window_sizes=(7, 7, 14, 7),
            mlp_ratio=4.0,
            drop_rate=0.0,
            drop_path_rate=0.1,
            use_checkpoint=False,
            mbconv_expand_ratio=4.0,
            local_conv_size=3,
            layer_lr_decay=1.0,
    def set_layer_lr_decay(self, layer_lr_decay):
        """Sets the learning rate decay for each layer in the TinyViT model."""
        decay_rate = layer_lr_decay  # è®¾ç½®æ¯ä¸ªå±‚çš„å­¦ä¹ ç‡è¡°å‡ç‡

        # Layers -> blocks (depth)
        depth = sum(self.depths)  # è®¡ç®—æ€»çš„å±‚æ·±åº¦
        lr_scales = [decay_rate ** (depth - i - 1) for i in range(depth)]  # è®¡ç®—æ¯ä¸ªå±‚çš„å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹

        def _set_lr_scale(m, scale):
            """Sets the learning rate scale for each layer in the model based on the layer's depth."""
            for p in m.parameters():
                p.lr_scale = scale  # è®¾ç½®æ¯ä¸ªæ¨¡å‹å±‚çš„å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹

        self.patch_embed.apply(lambda x: _set_lr_scale(x, lr_scales[0]))  # å¯¹è¾“å…¥åµŒå…¥å±‚è®¾ç½®å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹
        i = 0
        for layer in self.layers:
            for block in layer.blocks:
                block.apply(lambda x: _set_lr_scale(x, lr_scales[i]))  # å¯¹æ¯ä¸ªå±‚å—è®¾ç½®å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹
                i += 1
            if layer.downsample is not None:
                layer.downsample.apply(lambda x: _set_lr_scale(x, lr_scales[i - 1]))  # å¯¹ä¸‹é‡‡æ ·å±‚è®¾ç½®å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹
        assert i == depth  # ç¡®ä¿è®¾ç½®äº†æ‰€æœ‰å±‚çš„å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹
        for m in [self.norm_head, self.head]:
            m.apply(lambda x: _set_lr_scale(x, lr_scales[-1]))  # å¯¹å½’ä¸€åŒ–å¤´éƒ¨å’Œå¤´éƒ¨è®¾ç½®å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹

        for k, p in self.named_parameters():
            p.param_name = k  # ä¸ºæ¯ä¸ªå‚æ•°è®¾ç½®å‚æ•°åå±æ€§

        def _check_lr_scale(m):
            """Checks if the learning rate scale attribute is present in module's parameters."""
            for p in m.parameters():
                assert hasattr(p, "lr_scale"), p.param_name  # æ£€æŸ¥æ¨¡å—å‚æ•°ä¸­æ˜¯å¦å­˜åœ¨å­¦ä¹ ç‡ç¼©æ”¾å±æ€§

        self.apply(_check_lr_scale)  # åº”ç”¨æ£€æŸ¥å­¦ä¹ ç‡ç¼©æ”¾å±æ€§çš„å‡½æ•°åˆ°æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨¡å—

    def _init_weights(self, m):
        """Initializes weights for linear layers and layer normalization in the given module."""
        if isinstance(m, nn.Linear):
            # NOTE: This initialization is needed only for training.
            # trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)  # åˆå§‹åŒ–çº¿æ€§å±‚çš„åç½®ä¸ºå¸¸æ•°0
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)  # åˆå§‹åŒ–å±‚å½’ä¸€åŒ–çš„åç½®ä¸ºå¸¸æ•°0
            nn.init.constant_(m.weight, 1.0)  # åˆå§‹åŒ–å±‚å½’ä¸€åŒ–çš„æƒé‡ä¸ºå¸¸æ•°1.0

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        """Returns a dictionary of parameter names where weight decay should not be applied."""
        return {"attention_biases"}  # è¿”å›ä¸åº”ç”¨æƒé‡è¡°å‡çš„å‚æ•°åç§°å­—å…¸

    def forward_features(self, x):
        """Runs the input through the model layers and returns the transformed output."""
        x = self.patch_embed(x)  # x input is (N, C, H, W)

        x = self.layers[0](x)  # å¯¹è¾“å…¥åº”ç”¨ç¬¬ä¸€ä¸ªå±‚
        start_i = 1

        for i in range(start_i, len(self.layers)):
            layer = self.layers[i]
            x = layer(x)  # ä¾æ¬¡å¯¹æ¯ä¸ªå±‚åº”ç”¨è¾“å…¥
        batch, _, channel = x.shape
        x = x.view(batch, 64, 64, channel)  # è°ƒæ•´è¾“å‡ºçš„å½¢çŠ¶
        x = x.permute(0, 3, 1, 2)  # è°ƒæ•´è¾“å‡ºçš„ç»´åº¦é¡ºåº
        return self.neck(x)  # è¿”å›ç»è¿‡é¢ˆéƒ¨å¤„ç†åçš„è¾“å‡º

    def forward(self, x):
        """Executes a forward pass on the input tensor through the constructed model layers."""
        return self.forward_features(x)  # æ‰§è¡Œè¾“å…¥å¼ é‡é€šè¿‡æ„å»ºæ¨¡å‹å±‚çš„å‰å‘ä¼ æ’­
```