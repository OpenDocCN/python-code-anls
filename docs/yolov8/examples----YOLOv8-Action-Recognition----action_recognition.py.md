# `.\yolov8\examples\YOLOv8-Action-Recognition\action_recognition.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import time  # å¯¼å…¥æ—¶é—´æ¨¡å—
from collections import defaultdict  # å¯¼å…¥é»˜è®¤å­—å…¸æ¨¡å—
from typing import List, Optional, Tuple  # å¯¼å…¥ç±»å‹æç¤ºç›¸å…³æ¨¡å—
from urllib.parse import urlparse  # å¯¼å…¥ URL è§£ææ¨¡å—

import cv2  # å¯¼å…¥ OpenCV å›¾åƒå¤„ç†åº“
import numpy as np  # å¯¼å…¥ NumPy æ•°å­¦è®¡ç®—åº“
import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ åº“
from transformers import AutoModel, AutoProcessor  # å¯¼å…¥ Hugging Face Transformers æ¨¡å—

from ultralytics import YOLO  # å¯¼å…¥ Ultralytics YOLO ç›®æ ‡æ£€æµ‹æ¨¡å—
from ultralytics.data.loaders import get_best_youtube_url  # å¯¼å…¥è·å–æœ€ä½³ YouTube URL çš„å‡½æ•°
from ultralytics.utils.plotting import Annotator  # å¯¼å…¥å›¾åƒæ ‡æ³¨å·¥å…·ç±»
from ultralytics.utils.torch_utils import select_device  # å¯¼å…¥é€‰æ‹©è®¾å¤‡çš„å·¥å…·å‡½æ•°

class TorchVisionVideoClassifier:
    """Classifies videos using pretrained TorchVision models; see https://pytorch.org/vision/stable/."""

    from torchvision.models.video import (
        MViT_V1_B_Weights,
        MViT_V2_S_Weights,
        R3D_18_Weights,
        S3D_Weights,
        Swin3D_B_Weights,
        Swin3D_T_Weights,
        mvit_v1_b,
        mvit_v2_s,
        r3d_18,
        s3d,
        swin3d_b,
        swin3d_t,
    )

    model_name_to_model_and_weights = {
        "s3d": (s3d, S3D_Weights.DEFAULT),
        "r3d_18": (r3d_18, R3D_18_Weights.DEFAULT),
        "swin3d_t": (swin3d_t, Swin3D_T_Weights.DEFAULT),
        "swin3d_b": (swin3d_b, Swin3D_B_Weights.DEFAULT),
        "mvit_v1_b": (mvit_v1_b, MViT_V1_B_Weights.DEFAULT),
        "mvit_v2_s": (mvit_v2_s, MViT_V2_S_Weights.DEFAULT),
    }

    def __init__(self, model_name: str, device: str or torch.device = ""):
        """
        Initialize the VideoClassifier with the specified model name and device.

        Args:
            model_name (str): The name of the model to use.
            device (str or torch.device, optional): The device to run the model on. Defaults to "".

        Raises:
            ValueError: If an invalid model name is provided.
        """
        if model_name not in self.model_name_to_model_and_weights:
            raise ValueError(f"Invalid model name '{model_name}'. Available models: {self.available_model_names()}")
        model, self.weights = self.model_name_to_model_and_weights[model_name]
        self.device = select_device(device)  # é€‰æ‹©è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
        self.model = model(weights=self.weights).to(self.device).eval()  # åˆå§‹åŒ–æ¨¡å‹å¹¶å°†å…¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡

    @staticmethod
    def available_model_names() -> List[str]:
        """
        Get the list of available model names.

        Returns:
            list: List of available model names.
        """
        return list(TorchVisionVideoClassifier.model_name_to_model_and_weights.keys())
    # å¯¹è§†é¢‘åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¸€ç»„è£å‰ªå›¾åƒè¿›è¡Œé¢„å¤„ç†
    def preprocess_crops_for_video_cls(self, crops: List[np.ndarray], input_size: list = None) -> torch.Tensor:
        """
        Preprocess a list of crops for video classification.

        Args:
            crops (List[np.ndarray]): List of crops to preprocess. Each crop should have dimensions (H, W, C)
            input_size (tuple, optional): The target input size for the model. Defaults to (224, 224).

        Returns:
            torch.Tensor: Preprocessed crops as a tensor with dimensions (1, T, C, H, W).
        """
        # å¦‚æœæœªæä¾›è¾“å…¥å¤§å°ï¼Œåˆ™é»˜è®¤ä¸º (224, 224)
        if input_size is None:
            input_size = [224, 224]
        # å¯¼å…¥ torchvision.transforms.v2 æ¨¡å—ï¼Œå¹¶åˆ›å»ºå˜æ¢åºåˆ— transform
        from torchvision.transforms import v2

        transform = v2.Compose(
            [
                # å°†å›¾åƒæ•°æ®ç±»å‹è½¬æ¢ä¸º float32ï¼Œå¹¶è¿›è¡Œå°ºåº¦ç¼©æ”¾
                v2.ToDtype(torch.float32, scale=True),
                # è°ƒæ•´å›¾åƒå¤§å°åˆ°æŒ‡å®šçš„ input_sizeï¼Œä½¿ç”¨æŠ—é”¯é½¿æ–¹æ³•
                v2.Resize(input_size, antialias=True),
                # æ ¹æ®é¢„å…ˆå®šä¹‰çš„å‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œå›¾åƒå½’ä¸€åŒ–
                v2.Normalize(mean=self.weights.transforms().mean, std=self.weights.transforms().std),
            ]
        )

        # å¯¹æ¯ä¸ªè£å‰ªå›¾åƒåº”ç”¨ transform å˜æ¢ï¼Œè½¬æ¢ä¸ºå¼ é‡å¹¶é‡æ–°æ’åˆ—ç»´åº¦
        processed_crops = [transform(torch.from_numpy(crop).permute(2, 0, 1)) for crop in crops]
        # å°†å¤„ç†åçš„è£å‰ªå›¾åƒå †å æˆä¸€ä¸ªå¼ é‡ï¼Œæ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œé‡æ–°æ’åˆ—ç»´åº¦ä»¥é€‚åº”æ¨¡å‹è¾“å…¥æ ¼å¼ï¼Œå¹¶å°†ç»“æœç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ä¸Š
        return torch.stack(processed_crops).unsqueeze(0).permute(0, 2, 1, 3, 4).to(self.device)

    # è°ƒç”¨å¯¹è±¡ä½œä¸ºå‡½æ•°æ—¶æ‰§è¡Œçš„æ–¹æ³•ï¼Œç”¨äºåœ¨ç»™å®šåºåˆ—ä¸Šè¿›è¡Œæ¨æ–­
    def __call__(self, sequences: torch.Tensor):
        """
        Perform inference on the given sequences.

        Args:
            sequences (torch.Tensor): The input sequences for the model. The expected input dimensions are
                                      (B, T, C, H, W) for batched video frames or (T, C, H, W) for single video frames.

        Returns:
            torch.Tensor: The model's output.
        """
        # è¿›å…¥æ¨æ–­æ¨¡å¼ï¼Œç¡®ä¿ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—
        with torch.inference_mode():
            # è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œè¿”å›æ¨¡å‹çš„è¾“å‡ºç»“æœ
            return self.model(sequences)

    # å¯¹æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œå¾—åˆ°é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾å’Œç½®ä¿¡åº¦
    def postprocess(self, outputs: torch.Tensor) -> Tuple[List[str], List[float]]:
        """
        Postprocess the model's batch output.

        Args:
            outputs (torch.Tensor): The model's output.

        Returns:
            List[str]: The predicted labels.
            List[float]: The predicted confidences.
        """
        # åˆå§‹åŒ–é¢„æµ‹æ ‡ç­¾åˆ—è¡¨å’Œç½®ä¿¡åº¦åˆ—è¡¨
        pred_labels = []
        pred_confs = []
        # éå†æ¨¡å‹è¾“å‡ºçš„æ¯ä¸ªæ ·æœ¬
        for output in outputs:
            # æ‰¾åˆ°è¾“å‡ºå¼ é‡ä¸­æœ€é«˜ç½®ä¿¡åº¦çš„ç±»åˆ«ç´¢å¼•
            pred_class = output.argmax(0).item()
            # æ ¹æ®ç´¢å¼•ä»é¢„å…ˆå®šä¹‰çš„ç±»åˆ«å­—å…¸ä¸­è·å–é¢„æµ‹æ ‡ç­¾
            pred_label = self.weights.meta["categories"][pred_class]
            # å°†é¢„æµ‹æ ‡ç­¾æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            pred_labels.append(pred_label)
            # è®¡ç®—å¹¶è·å–è¯¥ç±»åˆ«çš„ç½®ä¿¡åº¦å€¼
            pred_conf = output.softmax(0)[pred_class].item()
            # å°†ç½®ä¿¡åº¦å€¼æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            pred_confs.append(pred_conf)

        # è¿”å›é¢„æµ‹æ ‡ç­¾åˆ—è¡¨å’Œç½®ä¿¡åº¦åˆ—è¡¨ä½œä¸ºå…ƒç»„
        return pred_labels, pred_confs
# å®šä¹‰ä¸€ä¸ªè§†é¢‘åˆ†ç±»å™¨ç±»ï¼Œä½¿ç”¨ Hugging Face æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ï¼Œé€‚ç”¨äºå¤šç§è®¾å¤‡
class HuggingFaceVideoClassifier:
    """Zero-shot video classifier using Hugging Face models for various devices."""

    def __init__(
        self,
        labels: List[str],
        model_name: str = "microsoft/xclip-base-patch16-zero-shot",
        device: str or torch.device = "",
        fp16: bool = False,
    ):
        """
        Initialize the HuggingFaceVideoClassifier with the specified model name.

        Args:
            labels (List[str]): List of labels for zero-shot classification.
            model_name (str): The name of the model to use. Defaults to "microsoft/xclip-base-patch16-zero-shot".
            device (str or torch.device, optional): The device to run the model on. Defaults to "".
            fp16 (bool, optional): Whether to use FP16 for inference. Defaults to False.
        """
        # è®¾ç½®æ˜¯å¦ä½¿ç”¨ FP16 è¿›è¡Œæ¨æ–­
        self.fp16 = fp16
        # å­˜å‚¨åˆ†ç±»å™¨çš„æ ‡ç­¾åˆ—è¡¨
        self.labels = labels
        # é€‰æ‹©è®¾å¤‡å¹¶å°†å…¶åˆ†é…ç»™ self.device
        self.device = select_device(device)
        # ä»é¢„è®­ç»ƒæ¨¡å‹åç§°åŠ è½½å¤„ç†å™¨
        self.processor = AutoProcessor.from_pretrained(model_name)
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶å°†å…¶ç§»è‡³æ‰€é€‰è®¾å¤‡
        model = AutoModel.from_pretrained(model_name).to(self.device)
        # å¦‚æœä½¿ç”¨ FP16ï¼Œåˆ™å°†æ¨¡å‹è½¬æ¢ä¸º FP16 æ ¼å¼
        if fp16:
            model = model.half()
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model = model.eval()

    def preprocess_crops_for_video_cls(self, crops: List[np.ndarray], input_size: list = None) -> torch.Tensor:
        """
        Preprocess a list of crops for video classification.

        Args:
            crops (List[np.ndarray]): List of crops to preprocess. Each crop should have dimensions (H, W, C)
            input_size (tuple, optional): The target input size for the model. Defaults to (224, 224).

        Returns:
            torch.Tensor: Preprocessed crops as a tensor (1, T, C, H, W).
        """
        # å¦‚æœæœªæä¾›è¾“å…¥å°ºå¯¸ï¼Œåˆ™é»˜è®¤ä¸º (224, 224)
        if input_size is None:
            input_size = [224, 224]
        # å¯¼å…¥ torchvision ä¸­çš„ transforms æ¨¡å—
        from torchvision import transforms

        # å®šä¹‰å›¾åƒé¢„å¤„ç†ç®¡é“
        transform = transforms.Compose(
            [
                transforms.Lambda(lambda x: x.float() / 255.0),  # å°†åƒç´ å€¼ç¼©æ”¾åˆ° [0, 1]
                transforms.Resize(input_size),  # è°ƒæ•´å›¾åƒå¤§å°è‡³æŒ‡å®šå°ºå¯¸
                transforms.Normalize(
                    mean=self.processor.image_processor.image_mean,  # æ ¹æ®å¤„ç†å™¨å®šä¹‰çš„å‡å€¼è¿›è¡Œå½’ä¸€åŒ–
                    std=self.processor.image_processor.image_std  # æ ¹æ®å¤„ç†å™¨å®šä¹‰çš„æ ‡å‡†å·®è¿›è¡Œå½’ä¸€åŒ–
                ),
            ]
        )

        # å¯¹è¾“å…¥çš„æ¯ä¸ª crop è¿›è¡Œé¢„å¤„ç†
        processed_crops = [transform(torch.from_numpy(crop).permute(2, 0, 1)) for crop in crops]  # (T, C, H, W)
        # å°†é¢„å¤„ç†åçš„ crop å †å æˆä¸€ä¸ªå¼ é‡ï¼Œå¹¶åœ¨æœ€å‰é¢å¢åŠ ä¸€ä¸ªç»´åº¦è¡¨ç¤ºæ‰¹å¤„ç†
        output = torch.stack(processed_crops).unsqueeze(0).to(self.device)  # (1, T, C, H, W)
        # å¦‚æœä½¿ç”¨ FP16ï¼Œåˆ™å°†è¾“å‡ºå¼ é‡è½¬æ¢ä¸º FP16 æ ¼å¼
        if self.fp16:
            output = output.half()
        return output
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œä½¿å¯¹è±¡å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨ï¼Œæ‰§è¡Œæ¨æ–­æ“ä½œ
    def __call__(self, sequences: torch.Tensor) -> torch.Tensor:
        """
        Perform inference on the given sequences.

        Args:
            sequences (torch.Tensor): The input sequences for the model. Batched video frames with shape (B, T, H, W, C).

        Returns:
            torch.Tensor: The model's output.
        """

        # ä½¿ç”¨å¤„ç†å™¨ï¼ˆprocessorï¼‰å¤„ç†æ ‡ç­¾ï¼Œè¿”å›åŒ…å«è¾“å…¥idsçš„PyTorchå¼ é‡ï¼Œå¡«å……æ•°æ®ä¸ºTrue
        input_ids = self.processor(text=self.labels, return_tensors="pt", padding=True)["input_ids"].to(self.device)

        # æ„å»ºè¾“å…¥å­—å…¸ï¼ŒåŒ…å«åƒç´ å€¼ï¼ˆsequencesï¼‰å’Œè¾“å…¥idsï¼ˆinput_idsï¼‰
        inputs = {"pixel_values": sequences, "input_ids": input_ids}

        # è¿›å…¥æ¨æ–­æ¨¡å¼
        with torch.inference_mode():
            # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œä¼ å…¥inputså­—å…¸ä½œä¸ºå‚æ•°
            outputs = self.model(**inputs)

        # è¿”å›æ¨¡å‹è¾“å‡ºä¸­çš„logits_per_video
        return outputs.logits_per_video

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºåå¤„ç†æ¨¡å‹çš„æ‰¹é‡è¾“å‡º
    def postprocess(self, outputs: torch.Tensor) -> Tuple[List[List[str]], List[List[float]]]:
        """
        Postprocess the model's batch output.

        Args:
            outputs (torch.Tensor): The model's output.

        Returns:
            List[List[str]]: The predicted top3 labels.
            List[List[float]]: The predicted top3 confidences.
        """
        # åˆå§‹åŒ–é¢„æµ‹æ ‡ç­¾å’Œç½®ä¿¡åº¦åˆ—è¡¨
        pred_labels = []
        pred_confs = []

        # ä½¿ç”¨torch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå…³é—­æ¢¯åº¦è®¡ç®—
        with torch.no_grad():
            # å‡è®¾outputså·²ç»æ˜¯logitså¼ é‡
            logits_per_video = outputs

            # å¯¹logitsè¿›è¡Œsoftmaxæ“ä½œï¼Œå°†å…¶è½¬æ¢ä¸ºæ¦‚ç‡
            probs = logits_per_video.softmax(dim=-1)

        # éå†æ¯ä¸ªè§†é¢‘çš„æ¦‚ç‡åˆ†å¸ƒ
        for prob in probs:
            # è·å–æ¦‚ç‡æœ€é«˜çš„ä¸¤ä¸ªç´¢å¼•
            top2_indices = prob.topk(2).indices.tolist()

            # æ ¹æ®ç´¢å¼•è·å–å¯¹åº”çš„æ ‡ç­¾å’Œç½®ä¿¡åº¦ï¼Œå¹¶è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼
            top2_labels = [self.labels[idx] for idx in top2_indices]
            top2_confs = prob[top2_indices].tolist()

            # å°†é¢„æµ‹çš„top2æ ‡ç­¾å’Œç½®ä¿¡åº¦æ·»åŠ åˆ°å¯¹åº”çš„åˆ—è¡¨ä¸­
            pred_labels.append(top2_labels)
            pred_confs.append(top2_confs)

        # è¿”å›é¢„æµ‹çš„top3æ ‡ç­¾åˆ—è¡¨å’Œç½®ä¿¡åº¦åˆ—è¡¨
        return pred_labels, pred_confs
# åˆå§‹åŒ–è£å‰ªå¹¶å¡«å……å‡½æ•°ï¼Œç”¨äºä»è§†é¢‘å¸§ä¸­è£å‰ªæŒ‡å®šåŒºåŸŸå¹¶æ·»åŠ è¾¹è·ï¼Œè¿”å›å°ºå¯¸ä¸º 224x224 çš„è£å‰ªå›¾åƒ
def crop_and_pad(frame, box, margin_percent):
    """Crop box with margin and take square crop from frame."""
    # è§£ææ¡†æ¡†çš„åæ ‡
    x1, y1, x2, y2 = map(int, box)
    # è®¡ç®—æ¡†æ¡†çš„å®½åº¦å’Œé«˜åº¦
    w, h = x2 - x1, y2 - y1

    # æ·»åŠ è¾¹è·
    margin_x, margin_y = int(w * margin_percent / 100), int(h * margin_percent / 100)
    # è°ƒæ•´æ¡†æ¡†çš„ä½ç½®ï¼Œç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
    x1, y1 = max(0, x1 - margin_x), max(0, y1 - margin_y)
    x2, y2 = min(frame.shape[1], x2 + margin_x), min(frame.shape[0], y2 + margin_y)

    # ä»å›¾åƒä¸­å¿ƒè·å–æ­£æ–¹å½¢è£å‰ª
    size = max(y2 - y1, x2 - x1)
    center_y, center_x = (y1 + y2) // 2, (x1 + x2) // 2
    half_size = size // 2
    # è£å‰ªå‡ºæ­£æ–¹å½¢åŒºåŸŸ
    square_crop = frame[
        max(0, center_y - half_size) : min(frame.shape[0], center_y + half_size),
        max(0, center_x - half_size) : min(frame.shape[1], center_x + half_size),
    ]

    # å°†è£å‰ªçš„å›¾åƒå¤§å°è°ƒæ•´ä¸º 224x224 åƒç´ 
    return cv2.resize(square_crop, (224, 224), interpolation=cv2.INTER_LINEAR)


def run(
    weights: str = "yolov8n.pt",
    device: str = "",
    source: str = "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
    output_path: Optional[str] = None,
    crop_margin_percentage: int = 10,
    num_video_sequence_samples: int = 8,
    skip_frame: int = 2,
    video_cls_overlap_ratio: float = 0.25,
    fp16: bool = False,
    video_classifier_model: str = "microsoft/xclip-base-patch32",
    labels: List[str] = None,
) -> None:
    """
    Run action recognition on a video source using YOLO for object detection and a video classifier.

    Args:
        weights (str): Path to the YOLO model weights. Defaults to "yolov8n.pt".
        device (str): Device to run the model on. Use 'cuda' for NVIDIA GPU, 'mps' for Apple Silicon, or 'cpu'. Defaults to auto-detection.
        source (str): Path to mp4 video file or YouTube URL. Defaults to a sample YouTube video.
        output_path (Optional[str], optional): Path to save the output video. Defaults to None.
        crop_margin_percentage (int, optional): Percentage of margin to add around detected objects. Defaults to 10.
        num_video_sequence_samples (int, optional): Number of video frames to use for classification. Defaults to 8.
        skip_frame (int, optional): Number of frames to skip between detections. Defaults to 4.
        video_cls_overlap_ratio (float, optional): Overlap ratio between video sequences. Defaults to 0.25.
        fp16 (bool, optional): Whether to use half-precision floating point. Defaults to False.
        video_classifier_model (str, optional): Name or path of the video classifier model. Defaults to "microsoft/xclip-base-patch32".
        labels (List[str], optional): List of labels for zero-shot classification. Defaults to predefined list.

    Returns:
        None
    """
    # å¦‚æœæ ‡ç­¾åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é¢„å®šä¹‰çš„åŠ¨ä½œæ ‡ç­¾
    if labels is None:
        labels = [
            "walking",
            "running",
            "brushing teeth",
            "looking into phone",
            "weight lifting",
            "cooking",
            "sitting",
        ]
    
    # åˆå§‹åŒ–æ¨¡å‹å’Œè®¾å¤‡
    device = select_device(device)  # é€‰æ‹©è¿è¡Œçš„è®¾å¤‡
    yolo_model = YOLO(weights).to(device)  # åŠ è½½å¹¶ç§»åŠ¨ YOLO æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
    # å¦‚æœè§†é¢‘åˆ†ç±»æ¨¡å‹åœ¨ TorchVisionVideoClassifier å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­
    if video_classifier_model in TorchVisionVideoClassifier.available_model_names():
        # æ‰“å°è­¦å‘Šä¿¡æ¯ï¼ŒæŒ‡å‡º 'fp16' ä¸æ”¯æŒ TorchVisionVideoClassifierï¼Œå°†å…¶è®¾ç½®ä¸º False
        print("'fp16' is not supported for TorchVisionVideoClassifier. Setting fp16 to False.")
        # æ‰“å°è­¦å‘Šä¿¡æ¯ï¼ŒæŒ‡å‡º 'labels' åœ¨ TorchVisionVideoClassifier ä¸­ä¸ä½¿ç”¨ï¼Œå¿½ç•¥æä¾›çš„æ ‡ç­¾å¹¶ä½¿ç”¨ Kinetics-400 æ ‡ç­¾
        print(
            "'labels' is not used for TorchVisionVideoClassifier. Ignoring the provided labels and using Kinetics-400 labels."
        )
        # ä½¿ç”¨ TorchVisionVideoClassifier åˆå§‹åŒ–è§†é¢‘åˆ†ç±»å™¨å¯¹è±¡ï¼Œè®¾å¤‡ä¸ºç»™å®šè®¾å¤‡
        video_classifier = TorchVisionVideoClassifier(video_classifier_model, device=device)
    else:
        # ä½¿ç”¨ HuggingFaceVideoClassifier åˆå§‹åŒ–è§†é¢‘åˆ†ç±»å™¨å¯¹è±¡
        video_classifier = HuggingFaceVideoClassifier(
            labels, model_name=video_classifier_model, device=device, fp16=fp16
        )
    
    # åˆå§‹åŒ–è§†é¢‘æ•è·å¯¹è±¡
    # å¦‚æœæºåœ°å€ä»¥ "http" å¼€å¤´ä¸”ä¸»æœºåæ˜¯ YouTube ç›¸å…³çš„åœ°å€ï¼Œåˆ™è·å–æœ€ä½³çš„ YouTube è§†é¢‘åœ°å€
    if source.startswith("http") and urlparse(source).hostname in {"www.youtube.com", "youtube.com", "youtu.be"}:
        source = get_best_youtube_url(source)
    # å¦åˆ™ï¼Œå¦‚æœæºåœ°å€ä¸æ˜¯ä»¥ ".mp4" ç»“å°¾ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯å¼‚å¸¸
    elif not source.endswith(".mp4"):
        raise ValueError("Invalid source. Supported sources are YouTube URLs and MP4 files.")
    # ä½¿ç”¨ OpenCV æ‰“å¼€è§†é¢‘æ•è·å¯¹è±¡
    cap = cv2.VideoCapture(source)
    
    # è·å–è§†é¢‘çš„å±æ€§ä¿¡æ¯ï¼šå¸§å®½åº¦ã€å¸§é«˜åº¦ã€å¸§ç‡
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œåˆ™åˆå§‹åŒ–è§†é¢‘å†™å…¥å¯¹è±¡
    if output_path is not None:
        # ä½¿ç”¨ mp4v ç¼–è§£ç å™¨åˆ›å»ºè§†é¢‘å†™å…¥å¯¹è±¡
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))
    
    # åˆå§‹åŒ–è·Ÿè¸ªå†å²å­—å…¸å’Œå¸§è®¡æ•°å™¨
    track_history = defaultdict(list)
    frame_counter = 0
    
    # åˆå§‹åŒ–éœ€è¦æ¨æ–­çš„è·Ÿè¸ª IDã€éœ€è¦æ¨æ–­çš„è£å‰ªå›¾åƒã€é¢„æµ‹æ ‡ç­¾å’Œç½®ä¿¡åº¦åˆ—è¡¨
    track_ids_to_infer = []
    crops_to_infer = []
    pred_labels = []
    pred_confs = []
    
    # é‡Šæ”¾è§†é¢‘æ•è·å¯¹è±¡
    cap.release()
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œåˆ™é‡Šæ”¾è§†é¢‘å†™å…¥å¯¹è±¡
    if output_path is not None:
        out.release()
    
    # å…³é—­æ‰€æœ‰ OpenCV çª—å£
    cv2.destroyAllWindows()
# è§£æå‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def parse_opt():
    """Parse command line arguments."""
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šæƒé‡æ–‡ä»¶çš„è·¯å¾„ï¼Œé»˜è®¤ä¸º"yolov8n.pt"
    parser.add_argument("--weights", type=str, default="yolov8n.pt", help="ultralytics detector model path")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šè®¾å¤‡ç±»å‹ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œæ”¯æŒ cuda è®¾å¤‡ï¼ˆå¦‚ '0' æˆ– '0,1,2,3'ï¼‰ã€cpu æˆ– mpsï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
    parser.add_argument("--device", default="", help='cuda device, i.e. 0 or 0,1,2,3 or cpu/mps, "" for auto-detection')
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šè§†é¢‘æ–‡ä»¶è·¯å¾„æˆ– YouTube URLï¼Œé»˜è®¤ä¸º Rick Astley çš„è§†é¢‘é“¾æ¥
    parser.add_argument(
        "--source",
        type=str,
        default="https://www.youtube.com/watch?v=dQw4w9WgXcQ",
        help="video file path or youtube URL",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šè¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º"output_video.mp4"
    parser.add_argument("--output-path", type=str, default="output_video.mp4", help="output video file path")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šæ£€æµ‹åˆ°çš„å¯¹è±¡å‘¨å›´æ·»åŠ çš„è£å‰ªè¾¹è·ç™¾åˆ†æ¯”ï¼Œé»˜è®¤ä¸º10%
    parser.add_argument(
        "--crop-margin-percentage", type=int, default=10, help="percentage of margin to add around detected objects"
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šç”¨äºåˆ†ç±»çš„è§†é¢‘å¸§æ ·æœ¬æ•°é‡ï¼Œé»˜è®¤ä¸º8å¸§
    parser.add_argument(
        "--num-video-sequence-samples", type=int, default=8, help="number of video frames to use for classification"
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šåœ¨æ£€æµ‹ä¹‹é—´è·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º2å¸§
    parser.add_argument("--skip-frame", type=int, default=2, help="number of frames to skip between detections")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šè§†é¢‘åºåˆ—ä¹‹é—´çš„é‡å æ¯”ç‡ï¼Œé»˜è®¤ä¸º0.25
    parser.add_argument(
        "--video-cls-overlap-ratio", type=float, default=0.25, help="overlap ratio between video sequences"
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šæ˜¯å¦ä½¿ç”¨ FP16 è¿›è¡Œæ¨æ–­ï¼Œé»˜è®¤ä¸º False
    parser.add_argument("--fp16", action="store_true", help="use FP16 for inference")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šè§†é¢‘åˆ†ç±»å™¨æ¨¡å‹çš„åç§°ï¼Œé»˜è®¤ä¸º"microsoft/xclip-base-patch32"
    parser.add_argument(
        "--video-classifier-model", type=str, default="microsoft/xclip-base-patch32", help="video classifier model name"
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼šç”¨äºé›¶æ ·æœ¬è§†é¢‘åˆ†ç±»çš„æ ‡ç­¾åˆ—è¡¨ï¼Œé»˜è®¤ä¸º["dancing", "singing a song"]
    parser.add_argument(
        "--labels",
        nargs="+",
        type=str,
        default=["dancing", "singing a song"],
        help="labels for zero-shot video classification",
    )
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ç»“æœ
    return parser.parse_args()


# ä¸»å‡½æ•°ï¼Œè¿è¡Œæ—¶æ¥å—ä¸€ä¸ªå‚æ•° opt
def main(opt):
    """Main function."""
    # å°† opt è§£åŒ…åä½œä¸ºå…³é”®å­—å‚æ•°ä¼ é€’ç»™ run å‡½æ•°
    run(**vars(opt))


# å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç 
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶èµ‹å€¼ç»™ opt
    opt = parse_opt()
    # è°ƒç”¨ä¸»å‡½æ•°ï¼Œä¼ å…¥è§£æåçš„å‚æ•° opt
    main(opt)
```