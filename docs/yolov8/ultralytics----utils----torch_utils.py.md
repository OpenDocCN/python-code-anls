# `.\yolov8\ultralytics\utils\torch_utils.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import gc  # å¯¼å…¥åƒåœ¾å›æ”¶æ¨¡å—
import math  # å¯¼å…¥æ•°å­¦æ¨¡å—
import os  # å¯¼å…¥æ“ä½œç³»ç»Ÿæ¨¡å—
import random  # å¯¼å…¥éšæœºæ•°æ¨¡å—
import time  # å¯¼å…¥æ—¶é—´æ¨¡å—
from contextlib import contextmanager  # å¯¼å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¨¡å—
from copy import deepcopy  # å¯¼å…¥æ·±æ‹·è´å‡½æ•°
from datetime import datetime  # å¯¼å…¥æ—¥æœŸæ—¶é—´æ¨¡å—
from pathlib import Path  # å¯¼å…¥è·¯å¾„æ¨¡å—
from typing import Union  # å¯¼å…¥ç±»å‹æ³¨è§£

import numpy as np  # å¯¼å…¥NumPyåº“
import torch  # å¯¼å…¥PyTorchåº“
import torch.distributed as dist  # å¯¼å…¥PyTorchåˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—
import torch.nn as nn  # å¯¼å…¥PyTorchç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F  # å¯¼å…¥PyTorchå‡½æ•°æ¨¡å—

from ultralytics.utils import (  # å¯¼å…¥Ultralyticså·¥å…·å‡½æ•°
    DEFAULT_CFG_DICT,  # é»˜è®¤é…ç½®å­—å…¸
    DEFAULT_CFG_KEYS,  # é»˜è®¤é…ç½®é”®åˆ—è¡¨
    LOGGER,  # æ—¥å¿—è®°å½•å™¨
    NUM_THREADS,  # çº¿ç¨‹æ•°
    PYTHON_VERSION,  # Pythonç‰ˆæœ¬
    TORCHVISION_VERSION,  # TorchVisionç‰ˆæœ¬
    __version__,  # Ultralyticsç‰ˆæœ¬
    colorstr,  # å­—ç¬¦ä¸²é¢œè‰²åŒ–å‡½æ•°
)
from ultralytics.utils.checks import check_version  # å¯¼å…¥ç‰ˆæœ¬æ£€æŸ¥å‡½æ•°

try:
    import thop  # å°è¯•å¯¼å…¥thopåº“
except ImportError:
    thop = None  # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè®¾ä¸ºNone

# Version checks (all default to version>=min_version)
TORCH_1_9 = check_version(torch.__version__, "1.9.0")  # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦>=1.9.0
TORCH_1_13 = check_version(torch.__version__, "1.13.0")  # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦>=1.13.0
TORCH_2_0 = check_version(torch.__version__, "2.0.0")  # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦>=2.0.0
TORCHVISION_0_10 = check_version(TORCHVISION_VERSION, "0.10.0")  # æ£€æŸ¥TorchVisionç‰ˆæœ¬æ˜¯å¦>=0.10.0
TORCHVISION_0_11 = check_version(TORCHVISION_VERSION, "0.11.0")  # æ£€æŸ¥TorchVisionç‰ˆæœ¬æ˜¯å¦>=0.11.0
TORCHVISION_0_13 = check_version(TORCHVISION_VERSION, "0.13.0")  # æ£€æŸ¥TorchVisionç‰ˆæœ¬æ˜¯å¦>=0.13.0
TORCHVISION_0_18 = check_version(TORCHVISION_VERSION, "0.18.0")  # æ£€æŸ¥TorchVisionç‰ˆæœ¬æ˜¯å¦>=0.18.0


@contextmanager
def torch_distributed_zero_first(local_rank: int):
    """Ensures all processes in distributed training wait for the local master (rank 0) to complete a task first."""
    initialized = dist.is_available() and dist.is_initialized()  # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†åˆ†å¸ƒå¼è®­ç»ƒä¸”æ˜¯å¦å·²åˆå§‹åŒ–
    if initialized and local_rank not in {-1, 0}:  # å¦‚æœåˆå§‹åŒ–ä¸”å½“å‰è¿›ç¨‹ä¸æ˜¯ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰
        dist.barrier(device_ids=[local_rank])  # ç­‰å¾…æœ¬åœ°ä¸»èŠ‚ç‚¹ï¼ˆrank 0ï¼‰å®Œæˆä»»åŠ¡
    yield  # æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„ä¸»ä½“éƒ¨åˆ†
    if initialized and local_rank == 0:  # å¦‚æœåˆå§‹åŒ–ä¸”å½“å‰è¿›ç¨‹æ˜¯ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰
        dist.barrier(device_ids=[0])  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœ¨ç»§ç»­ä¹‹å‰éƒ½ç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆ


def smart_inference_mode():
    """Applies torch.inference_mode() decorator if torch>=1.9.0 else torch.no_grad() decorator."""
    
    def decorate(fn):
        """Applies appropriate torch decorator for inference mode based on torch version."""
        if TORCH_1_9 and torch.is_inference_mode_enabled():
            return fn  # å¦‚æœå·²å¯ç”¨æ¨æ–­æ¨¡å¼ï¼Œç›´æ¥è¿”å›å‡½æ•°
        else:
            return (torch.inference_mode if TORCH_1_9 else torch.no_grad)()(fn)  # æ ¹æ®ç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„æ¨æ–­æ¨¡å¼è£…é¥°å™¨

    return decorate


def autocast(enabled: bool, device: str = "cuda"):
    """
    Get the appropriate autocast context manager based on PyTorch version and AMP setting.

    This function returns a context manager for automatic mixed precision (AMP) training that is compatible with both
    older and newer versions of PyTorch. It handles the differences in the autocast API between PyTorch versions.

    Args:
        enabled (bool): Whether to enable automatic mixed precision.
        device (str, optional): The device to use for autocast. Defaults to 'cuda'.

    Returns:
        (torch.amp.autocast): The appropriate autocast context manager.

    Note:
        - For PyTorch versions 1.13 and newer, it uses `torch.amp.autocast`.
        - For older versions, it uses `torch.cuda.autocast`.

    Example:
        ```py
        with autocast(amp=True):
            # Your mixed precision operations here
            pass
        ```
    """
    # å¦‚æœ TORCH_1_13 å˜é‡ä¸ºçœŸï¼Œä½¿ç”¨ torch.amp.autocast æ–¹æ³•å¼€å¯è‡ªåŠ¨æ··åˆç²¾åº¦æ¨¡å¼
    if TORCH_1_13:
        return torch.amp.autocast(device, enabled=enabled)
    # å¦‚æœ TORCH_1_13 å˜é‡ä¸ºå‡ï¼Œä½¿ç”¨ torch.cuda.amp.autocast æ–¹æ³•å¼€å¯è‡ªåŠ¨æ··åˆç²¾åº¦æ¨¡å¼
    else:
        return torch.cuda.amp.autocast(enabled)
def get_cpu_info():
    """Return a string with system CPU information, i.e. 'Apple M2'."""
    import cpuinfo  # å¯¼å…¥cpuinfoåº“ï¼Œç”¨äºè·å–CPUä¿¡æ¯ï¼Œéœ€ä½¿ç”¨pipå®‰è£…py-cpuinfo

    k = "brand_raw", "hardware_raw", "arch_string_raw"  # æŒ‰ä¼˜å…ˆé¡ºåºåˆ—å‡ºä¿¡æ¯é”®ï¼ˆå¹¶éæ‰€æœ‰é”®å§‹ç»ˆå¯ç”¨ï¼‰
    info = cpuinfo.get_cpu_info()  # è·å–CPUä¿¡æ¯çš„å­—å…¸
    string = info.get(k[0] if k[0] in info else k[1] if k[1] in info else k[2], "unknown")  # æå–CPUä¿¡æ¯å­—ç¬¦ä¸²
    return string.replace("(R)", "").replace("CPU ", "").replace("@ ", "")  # å¤„ç†ç‰¹æ®Šå­—ç¬¦åè¿”å›CPUä¿¡æ¯å­—ç¬¦ä¸²


def select_device(device="", batch=0, newline=False, verbose=True):
    """
    Selects the appropriate PyTorch device based on the provided arguments.

    The function takes a string specifying the device or a torch.device object and returns a torch.device object
    representing the selected device. The function also validates the number of available devices and raises an
    exception if the requested device(s) are not available.

    Args:
        device (str | torch.device, optional): Device string or torch.device object.
            Options are 'None', 'cpu', or 'cuda', or '0' or '0,1,2,3'. Defaults to an empty string, which auto-selects
            the first available GPU, or CPU if no GPU is available.
        batch (int, optional): Batch size being used in your model. Defaults to 0.
        newline (bool, optional): If True, adds a newline at the end of the log string. Defaults to False.
        verbose (bool, optional):
    elif device:  # é CPU è®¾å¤‡è¯·æ±‚æ—¶æ‰§è¡Œä»¥ä¸‹æ“ä½œ
        if device == "cuda":
            device = "0"
        visible = os.environ.get("CUDA_VISIBLE_DEVICES", None)
        os.environ["CUDA_VISIBLE_DEVICES"] = device  # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¿…é¡»åœ¨æ£€æŸ¥å¯ç”¨æ€§ä¹‹å‰è®¾ç½®
        if not (torch.cuda.is_available() and torch.cuda.device_count() >= len(device.split(","))):
            LOGGER.info(s)  # è®°å½•ä¿¡æ¯åˆ°æ—¥å¿—
            install = (
                "See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no "
                "CUDA devices are seen by torch.\n"
                if torch.cuda.device_count() == 0
                else ""
            )
            raise ValueError(
                f"Invalid CUDA 'device={device}' requested."
                f" Use 'device=cpu' or pass valid CUDA device(s) if available,"
                f" i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n"
                f"\ntorch.cuda.is_available(): {torch.cuda.is_available()}"
                f"\ntorch.cuda.device_count(): {torch.cuda.device_count()}"
                f"\nos.environ['CUDA_VISIBLE_DEVICES']: {visible}\n"
                f"{install}"
            )

    if not cpu and not mps and torch.cuda.is_available():  # å¦‚æœå¯ç”¨ä¸”æœªè¯·æ±‚ CPU æˆ– MPS
        devices = device.split(",") if device else "0"  # å®šä¹‰è®¾å¤‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸º "0"
        n = len(devices)  # è®¾å¤‡æ•°é‡
        if n > 1:  # å¤š GPU æƒ…å†µ
            if batch < 1:
                raise ValueError(
                    "AutoBatch with batch<1 not supported for Multi-GPU training, "
                    "please specify a valid batch size, i.e. batch=16."
                )
            if batch >= 0 and batch % n != 0:  # æ£€æŸ¥ batch_size æ˜¯å¦å¯ä»¥è¢«è®¾å¤‡æ•°é‡æ•´é™¤
                raise ValueError(
                    f"'batch={batch}' must be a multiple of GPU count {n}. Try 'batch={batch // n * n}' or "
                    f"'batch={batch // n * n + n}', the nearest batch sizes evenly divisible by {n}."
                )
        space = " " * (len(s) + 1)  # åˆ›å»ºç©ºæ ¼ä¸²
        for i, d in enumerate(devices):
            p = torch.cuda.get_device_properties(i)
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\n"  # å­—ç¬¦ä¸²æ‹¼æ¥ GPU ä¿¡æ¯
        arg = "cuda:0"  # è®¾ç½® CUDA è®¾å¤‡ä¸ºé»˜è®¤å€¼ "cuda:0"
    elif mps and TORCH_2_0 and torch.backends.mps.is_available():
        # å¦‚æœæ”¯æŒ MPS å¹¶ä¸”æ»¡è¶³æ¡ä»¶ï¼Œåˆ™ä¼˜å…ˆé€‰æ‹© MPS
        s += f"MPS ({get_cpu_info()})\n"  # æ·»åŠ  MPS ä¿¡æ¯åˆ°å­—ç¬¦ä¸²
        arg = "mps"  # è®¾ç½®è®¾å¤‡ç±»å‹ä¸º "mps"
    else:  # å¦åˆ™ï¼Œé»˜è®¤ä½¿ç”¨ CPU
        s += f"CPU ({get_cpu_info()})\n"  # æ·»åŠ  CPU ä¿¡æ¯åˆ°å­—ç¬¦ä¸²
        arg = "cpu"  # è®¾ç½®è®¾å¤‡ç±»å‹ä¸º "cpu"

    if arg in {"cpu", "mps"}:
        torch.set_num_threads(NUM_THREADS)  # è®¾ç½® CPU è®­ç»ƒçš„çº¿ç¨‹æ•°
    if verbose:
        LOGGER.info(s if newline else s.rstrip())  # å¦‚æœéœ€è¦è¯¦ç»†è¾“å‡ºï¼Œåˆ™è®°å½•è¯¦ç»†ä¿¡æ¯åˆ°æ—¥å¿—
    return torch.device(arg)  # è¿”å›å¯¹åº”çš„ Torch è®¾å¤‡å¯¹è±¡
# è¿”å›å½“å‰ç³»ç»Ÿæ—¶é—´ï¼Œç¡®ä¿åœ¨ä½¿ç”¨ PyTorch æ—¶ç²¾ç¡®åŒæ­¥æ—¶é—´
def time_sync():
    """PyTorch-accurate time."""
    # å¦‚æœ CUDA å¯ç”¨ï¼ŒåŒæ­¥ CUDA è®¡ç®—çš„æ—¶é—´
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    # è¿”å›å½“å‰æ—¶é—´æˆ³
    return time.time()


# å°† Conv2d() å’Œ BatchNorm2d() å±‚èåˆï¼Œå®ç°ä¼˜åŒ– https://tehnokv.com/posts/fusing-batchnorm-and-conv/
def fuse_conv_and_bn(conv, bn):
    """Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/."""
    # åˆ›å»ºèåˆåçš„å·ç§¯å±‚å¯¹è±¡
    fusedconv = (
        nn.Conv2d(
            conv.in_channels,
            conv.out_channels,
            kernel_size=conv.kernel_size,
            stride=conv.stride,
            padding=conv.padding,
            dilation=conv.dilation,
            groups=conv.groups,
            bias=True,
        )
        .requires_grad_(False)  # ç¦ç”¨æ¢¯åº¦è¿½è¸ªï¼Œä¸éœ€è¦åå‘ä¼ æ’­è®­ç»ƒ
        .to(conv.weight.device)  # å°†èåˆåçš„å·ç§¯å±‚ç§»åˆ°ä¸è¾“å…¥å·ç§¯å±‚ç›¸åŒçš„è®¾å¤‡ä¸Š
    )

    # å‡†å¤‡å·ç§¯å±‚çš„æƒé‡
    w_conv = conv.weight.clone().view(conv.out_channels, -1)
    # è®¡ç®—èåˆåçš„æƒé‡
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # å‡†å¤‡ç©ºé—´åç½®é¡¹
    b_conv = torch.zeros(conv.weight.shape[0], device=conv.weight.device) if conv.bias is None else conv.bias
    # è®¡ç®—èåˆåçš„åç½®é¡¹
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv


# å°† ConvTranspose2d() å’Œ BatchNorm2d() å±‚èåˆ
def fuse_deconv_and_bn(deconv, bn):
    """Fuse ConvTranspose2d() and BatchNorm2d() layers."""
    # åˆ›å»ºèåˆåçš„åå·ç§¯å±‚å¯¹è±¡
    fuseddconv = (
        nn.ConvTranspose2d(
            deconv.in_channels,
            deconv.out_channels,
            kernel_size=deconv.kernel_size,
            stride=deconv.stride,
            padding=deconv.padding,
            output_padding=deconv.output_padding,
            dilation=deconv.dilation,
            groups=deconv.groups,
            bias=True,
        )
        .requires_grad_(False)  # ç¦ç”¨æ¢¯åº¦è¿½è¸ªï¼Œä¸éœ€è¦åå‘ä¼ æ’­è®­ç»ƒ
        .to(deconv.weight.device)  # å°†èåˆåçš„åå·ç§¯å±‚ç§»åˆ°ä¸è¾“å…¥åå·ç§¯å±‚ç›¸åŒçš„è®¾å¤‡ä¸Š
    )

    # å‡†å¤‡åå·ç§¯å±‚çš„æƒé‡
    w_deconv = deconv.weight.clone().view(deconv.out_channels, -1)
    # è®¡ç®—èåˆåçš„æƒé‡
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fuseddconv.weight.copy_(torch.mm(w_bn, w_deconv).view(fuseddconv.weight.shape))

    # å‡†å¤‡ç©ºé—´åç½®é¡¹
    b_conv = torch.zeros(deconv.weight.shape[1], device=deconv.weight.device) if deconv.bias is None else deconv.bias
    # è®¡ç®—èåˆåçš„åç½®é¡¹
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fuseddconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fuseddconv


# è¾“å‡ºæ¨¡å‹çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‚æ•°æ•°é‡ã€æ¢¯åº¦æ•°é‡å’Œå±‚çš„æ•°é‡
def model_info(model, detailed=False, verbose=True, imgsz=640):
    """
    Model information.

    imgsz may be int or list, i.e. imgsz=640 or imgsz=[640, 320].
    """
    # å¦‚æœä¸éœ€è¦è¯¦ç»†ä¿¡æ¯ï¼Œåˆ™ç›´æ¥è¿”å›
    if not verbose:
        return
    # è·å–æ¨¡å‹çš„å‚æ•°æ•°é‡
    n_p = get_num_params(model)  # number of parameters
    # è·å–æ¨¡å‹çš„æ¢¯åº¦æ•°é‡
    n_g = get_num_gradients(model)  # number of gradients
    # è·å–æ¨¡å‹çš„å±‚æ•°é‡
    n_l = len(list(model.modules()))  # number of layers
    # å¦‚æœ detailed å‚æ•°ä¸º Trueï¼Œåˆ™è¾“å‡ºè¯¦ç»†çš„æ¨¡å‹å‚æ•°ä¿¡æ¯
    if detailed:
        # ä½¿ç”¨ LOGGER è®°å½•æ¨¡å‹å‚æ•°çš„è¯¦ç»†ä¿¡æ¯è¡¨å¤´ï¼ŒåŒ…æ‹¬å±‚ç¼–å·ã€åç§°ã€æ¢¯åº¦æ˜¯å¦è®¡ç®—ã€å‚æ•°æ•°é‡ã€å½¢çŠ¶ã€å¹³å‡å€¼ã€æ ‡å‡†å·®å’Œæ•°æ®ç±»å‹
        LOGGER.info(
            f"{'layer':>5} {'name':>40} {'gradient':>9} {'parameters':>12} {'shape':>20} {'mu':>10} {'sigma':>10}"
        )
        # éå†æ¨¡å‹çš„æ‰€æœ‰å‘½åå‚æ•°ï¼Œå¹¶ç»™æ¯ä¸ªå‚æ•°åˆ†é…ä¸€ä¸ªåºå· i
        for i, (name, p) in enumerate(model.named_parameters()):
            # å»é™¤å‚æ•°åä¸­çš„ "module_list." å­—ç¬¦ä¸²
            name = name.replace("module_list.", "")
            # ä½¿ç”¨ LOGGER è®°å½•æ¯ä¸ªå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬åºå·ã€åç§°ã€æ˜¯å¦éœ€è¦æ¢¯åº¦ã€å‚æ•°æ•°é‡ã€å½¢çŠ¶ã€å¹³å‡å€¼ã€æ ‡å‡†å·®å’Œæ•°æ®ç±»å‹
            LOGGER.info(
                "%5g %40s %9s %12g %20s %10.3g %10.3g %10s"
                % (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std(), p.dtype)
            )

    # è®¡ç®—æ¨¡å‹çš„æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰
    flops = get_flops(model, imgsz)
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒèåˆè®¡ç®—ï¼Œå¦‚æœæ”¯æŒï¼Œåˆ™æ·»åŠ  " (fused)" åˆ°è¾“å‡ºä¸­
    fused = " (fused)" if getattr(model, "is_fused", lambda: False)() else ""
    # å¦‚æœè®¡ç®—å¾—åˆ°çš„ FLOPs ä¸ä¸ºç©ºï¼Œåˆ™æ·»åŠ åˆ°è¾“å‡ºä¸­
    fs = f", {flops:.1f} GFLOPs" if flops else ""
    # è·å–æ¨¡å‹çš„ YAML æ–‡ä»¶è·¯å¾„æˆ–è€…ç›´æ¥ä»æ¨¡å‹å±æ€§ä¸­è·å– YAML æ–‡ä»¶è·¯å¾„ï¼Œå¹¶å»é™¤è·¯å¾„ä¸­çš„ "yolo" æ›¿æ¢ä¸º "YOLO"ï¼Œæˆ–é»˜è®¤ä¸º "Model"
    yaml_file = getattr(model, "yaml_file", "") or getattr(model, "yaml", {}).get("yaml_file", "")
    model_name = Path(yaml_file).stem.replace("yolo", "YOLO") or "Model"
    # ä½¿ç”¨ LOGGER è®°å½•æ¨¡å‹çš„æ€»ç»“ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹åç§°ã€å±‚æ•°é‡ã€å‚æ•°æ•°é‡ã€æ¢¯åº¦æ•°é‡å’Œè®¡ç®—é‡ä¿¡æ¯
    LOGGER.info(f"{model_name} summary{fused}: {n_l:,} layers, {n_p:,} parameters, {n_g:,} gradients{fs}")
    # è¿”å›æ¨¡å‹çš„å±‚æ•°é‡ã€å‚æ•°æ•°é‡ã€æ¢¯åº¦æ•°é‡å’Œè®¡ç®—é‡
    return n_l, n_p, n_g, flops
# è¿”å› YOLO æ¨¡å‹ä¸­çš„æ€»å‚æ•°æ•°é‡
def get_num_params(model):
    return sum(x.numel() for x in model.parameters())


# è¿”å› YOLO æ¨¡å‹ä¸­å…·æœ‰æ¢¯åº¦çš„å‚æ•°æ€»æ•°
def get_num_gradients(model):
    return sum(x.numel() for x in model.parameters() if x.requires_grad)


# ä¸ºæ—¥å¿—è®°å½•å™¨è¿”å›åŒ…å«æœ‰ç”¨æ¨¡å‹ä¿¡æ¯çš„å­—å…¸
def model_info_for_loggers(trainer):
    if trainer.args.profile:  # å¦‚æœéœ€è¦è¿›è¡Œ ONNX å’Œ TensorRT çš„æ€§èƒ½åˆ†æ
        from ultralytics.utils.benchmarks import ProfileModels

        # ä½¿ç”¨ ProfileModels è¿›è¡Œæ¨¡å‹æ€§èƒ½åˆ†æï¼Œè·å–ç»“æœ
        results = ProfileModels([trainer.last], device=trainer.device).profile()[0]
        results.pop("model/name")  # ç§»é™¤ç»“æœä¸­çš„æ¨¡å‹åç§°
    else:  # å¦åˆ™ä»…è¿”å›æœ€è¿‘éªŒè¯çš„ PyTorch æ—¶é—´ä¿¡æ¯
        results = {
            "model/parameters": get_num_params(trainer.model),  # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
            "model/GFLOPs": round(get_flops(trainer.model), 3),  # è®¡ç®—æ¨¡å‹çš„ GFLOPs
        }
    results["model/speed_PyTorch(ms)"] = round(trainer.validator.speed["inference"], 3)  # è®°å½• PyTorch æ¨ç†é€Ÿåº¦
    return results


# è¿”å› YOLO æ¨¡å‹çš„ FLOPsï¼ˆæµ®ç‚¹è¿ç®—æ•°ï¼‰
def get_flops(model, imgsz=640):
    if not thop:
        return 0.0  # å¦‚æœ thop åŒ…æœªå®‰è£…ï¼Œè¿”å› 0.0 GFLOPs

    try:
        model = de_parallel(model)  # å–æ¶ˆæ¨¡å‹çš„å¹¶è¡ŒåŒ–
        p = next(model.parameters())
        if not isinstance(imgsz, list):
            imgsz = [imgsz, imgsz]  # å¦‚æœ imgsz æ˜¯ int æˆ– floatï¼Œæ‰©å±•ä¸ºåˆ—è¡¨

        try:
            stride = max(int(model.stride.max()), 32) if hasattr(model, "stride") else 32  # è·å–è¾“å…¥å¼ é‡çš„æ­¥å¹…å¤§å°
            im = torch.empty((1, p.shape[1], stride, stride), device=p.device)  # åˆ›å»ºè¾“å…¥å›¾åƒå¼ é‡
            flops = thop.profile(deepcopy(model), inputs=[im], verbose=False)[0] / 1e9 * 2  # ä½¿ç”¨ thop è®¡ç®— GFLOPs
            return flops * imgsz[0] / stride * imgsz[1] / stride  # è®¡ç®—åŸºäºå›¾åƒå°ºå¯¸çš„ GFLOPs
        except Exception:
            im = torch.empty((1, p.shape[1], *imgsz), device=p.device)  # åˆ›å»ºè¾“å…¥å›¾åƒå¼ é‡
            return thop.profile(deepcopy(model), inputs=[im], verbose=False)[0] / 1e9 * 2  # è®¡ç®—åŸºäºå›¾åƒå°ºå¯¸çš„ GFLOPs
    except Exception:
        return 0.0  # å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å› 0.0 GFLOPs


# ä½¿ç”¨ Torch åˆ†æå™¨è®¡ç®—æ¨¡å‹çš„ FLOPsï¼ˆthop åŒ…çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é€Ÿåº¦é€šå¸¸è¾ƒæ…¢ 2-10 å€ï¼‰
def get_flops_with_torch_profiler(model, imgsz=640):
    if not TORCH_2_0:  # å¦‚æœ Torch ç‰ˆæœ¬ä½äº 2.0ï¼Œè¿”å› 0.0
        return 0.0
    model = de_parallel(model)  # å–æ¶ˆæ¨¡å‹çš„å¹¶è¡ŒåŒ–
    p = next(model.parameters())
    if not isinstance(imgsz, list):
        imgsz = [imgsz, imgsz]  # å¦‚æœ imgsz æ˜¯ int æˆ– floatï¼Œæ‰©å±•ä¸ºåˆ—è¡¨
    try:
        # ä½¿ç”¨æ¨¡å‹çš„æ­¥å¹…å¤§å°æ¥ç¡®å®šè¾“å…¥å¼ é‡çš„æ­¥å¹…
        stride = (max(int(model.stride.max()), 32) if hasattr(model, "stride") else 32) * 2  # æœ€å¤§æ­¥å¹…
        # åˆ›å»ºä¸€ä¸ªç©ºçš„å¼ é‡ä½œä¸ºè¾“å…¥å›¾åƒï¼Œæ ¼å¼ä¸ºBCHW
        im = torch.empty((1, p.shape[1], stride, stride), device=p.device)
        with torch.profiler.profile(with_flops=True) as prof:
            # å¯¹æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè®°å½•æ€§èƒ½æŒ‡æ ‡
            model(im)
        # è®¡ç®—æ¨¡å‹çš„æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰
        flops = sum(x.flops for x in prof.key_averages()) / 1e9
        # æ ¹æ®è¾“å…¥å›¾åƒå¤§å°è°ƒæ•´è®¡ç®—çš„FLOPsï¼Œä¾‹å¦‚ 640x640 GFLOPs
        flops = flops * imgsz[0] / stride * imgsz[1] / stride
    except Exception:
        # å¯¹äºRTDETRæ¨¡å‹ï¼Œä½¿ç”¨å®é™…å›¾åƒå¤§å°ä½œä¸ºè¾“å…¥å¼ é‡çš„å¤§å°
        im = torch.empty((1, p.shape[1], *imgsz), device=p.device)  # è¾“å…¥å›¾åƒä¸ºBCHWæ ¼å¼
        with torch.profiler.profile(with_flops=True) as prof:
            # å¯¹æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè®°å½•æ€§èƒ½æŒ‡æ ‡
            model(im)
        # è®¡ç®—æ¨¡å‹çš„æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰
        flops = sum(x.flops for x in prof.key_averages()) / 1e9
    # è¿”å›è®¡ç®—å¾—åˆ°çš„FLOPs
    return flops
def initialize_weights(model):
    """Initialize model weights to random values."""
    # Iterate over all modules in the model
    for m in model.modules():
        t = type(m)
        # Check if the module is a 2D convolutional layer
        if t is nn.Conv2d:
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        # Check if the module is a 2D batch normalization layer
        elif t is nn.BatchNorm2d:
            # Set epsilon (eps) and momentum parameters
            m.eps = 1e-3
            m.momentum = 0.03
        # Check if the module is one of the specified activation functions
        elif t in {nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU}:
            # Enable inplace operation for the activation function
            m.inplace = True


def scale_img(img, ratio=1.0, same_shape=False, gs=32):
    """Scales and pads an image tensor of shape img(bs,3,y,x) based on given ratio and grid size gs, optionally
    retaining the original shape.
    """
    # If ratio is 1.0, return the original image tensor
    if ratio == 1.0:
        return img
    # Retrieve height and width from the image tensor shape
    h, w = img.shape[2:]
    # Compute the new scaled size based on the given ratio
    s = (int(h * ratio), int(w * ratio))  # new size
    # Resize the image tensor using bilinear interpolation
    img = F.interpolate(img, size=s, mode="bilinear", align_corners=False)  # resize
    # If not retaining the original shape, pad or crop the image tensor
    if not same_shape:
        # Calculate the padded height and width based on the ratio and grid size
        h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))
    # Pad the image tensor to match the calculated dimensions
    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean


def copy_attr(a, b, include=(), exclude=()):
    """Copies attributes from object 'b' to object 'a', with options to include/exclude certain attributes."""
    # Iterate through attributes in object 'b'
    for k, v in b.__dict__.items():
        # Skip attributes based on conditions: not in include list, starts with '_', or in exclude list
        if (len(include) and k not in include) or k.startswith("_") or k in exclude:
            continue
        else:
            # Set attribute 'k' in object 'a' to the value 'v' from object 'b'
            setattr(a, k, v)


def get_latest_opset():
    """Return the second-most recent ONNX opset version supported by this version of PyTorch, adjusted for maturity."""
    # Check if using PyTorch version 1.13 or newer
    if TORCH_1_13:
        # Dynamically compute the second-most recent ONNX opset version supported
        return max(int(k[14:]) for k in vars(torch.onnx) if "symbolic_opset" in k) - 1
    # For PyTorch versions <= 1.12, return predefined opset versions
    version = torch.onnx.producer_version.rsplit(".", 1)[0]  # i.e. '2.3'
    return {"1.12": 15, "1.11": 14, "1.10": 13, "1.9": 12, "1.8": 12}.get(version, 12)


def intersect_dicts(da, db, exclude=()):
    """Returns a dictionary of intersecting keys with matching shapes, excluding 'exclude' keys, using da values."""
    # Create a dictionary comprehension to filter keys based on conditions
    return {k: v for k, v in da.items() if k in db and all(x not in k for x in exclude) and v.shape == db[k].shape}


def is_parallel(model):
    """Returns True if model is of type DP or DDP."""
    # Check if the model is an instance of DataParallel or DistributedDataParallel
    return isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel))


def de_parallel(model):
    """De-parallelize a model: returns single-GPU model if model is of type DP or DDP."""
    # Return the underlying module of a DataParallel or DistributedDataParallel model
    return model.module if is_parallel(model) else model


def one_cycle(y1=0.0, y2=1.0, steps=100):
    """Returns a lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf."""
    # Generate a lambda function that implements a sinusoidal ramp
    return lambda x: max((1 - math.cos(x * math.pi / steps)) / 2, 0) * (y2 - y1) + y1


def init_seeds(seed=0, deterministic=False):
    """Initialize random number generator seeds."""
    # This function initializes seeds for random number generators
    # It is intended to be implemented further, but the current snippet does not contain the complete implementation.
    pass
    # åˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆRNGï¼‰ç§å­ï¼Œä»¥ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ https://pytorch.org/docs/stable/notes/randomness.html.
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # ç”¨äºå¤šGPUæƒ…å†µä¸‹çš„ç§å­è®¾ç½®ï¼Œç¡®ä¿å¼‚å¸¸å®‰å…¨æ€§
    # torch.backends.cudnn.benchmark = True  # AutoBatché—®é¢˜ https://github.com/ultralytics/yolov5/issues/9287
    # å¦‚æœéœ€è¦ç¡®å®šæ€§è¡Œä¸ºï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if deterministic:
        if TORCH_2_0:
            # ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼Œå¹¶åœ¨ä¸å¯ç¡®å®šæ—¶å‘å‡ºè­¦å‘Š
            torch.use_deterministic_algorithms(True, warn_only=True)
            torch.backends.cudnn.deterministic = True
            # è®¾ç½®CUBLASå·¥ä½œç©ºé—´å¤§å°çš„é…ç½®
            os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
            os.environ["PYTHONHASHSEED"] = str(seed)
        else:
            # æç¤ºå‡çº§åˆ°torch>=2.0.0ä»¥å®ç°ç¡®å®šæ€§è®­ç»ƒ
            LOGGER.warning("WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.")
    else:
        # å…³é—­ç¡®å®šæ€§ç®—æ³•ï¼Œå…è®¸éç¡®å®šæ€§è¡Œä¸º
        torch.use_deterministic_algorithms(False)
        torch.backends.cudnn.deterministic = False
class ModelEMA:
    """
    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models. Keeps a moving
    average of everything in the model state_dict (parameters and buffers)

    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage

    To disable EMA set the `enabled` attribute to `False`.
    """

    def __init__(self, model, decay=0.9999, tau=2000, updates=0):
        """Initialize EMA for 'model' with given arguments."""
        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA
        self.updates = updates  # number of EMA updates
        self.decay = lambda x: decay * (1 - math.exp(-x / tau))  # decay exponential ramp (to help early epochs)
        for p in self.ema.parameters():
            p.requires_grad_(False)
        self.enabled = True

    def update(self, model):
        """Update EMA parameters."""
        if self.enabled:
            self.updates += 1
            d = self.decay(self.updates)

            msd = de_parallel(model).state_dict()  # model state_dict
            for k, v in self.ema.state_dict().items():
                if v.dtype.is_floating_point:  # true for FP16 and FP32
                    v *= d
                    v += (1 - d) * msd[k].detach()
                    # assert v.dtype == msd[k].dtype == torch.float32, f'{k}: EMA {v.dtype},  model {msd[k].dtype}'

    def update_attr(self, model, include=(), exclude=("process_group", "reducer")):
        """Updates attributes and saves stripped model with optimizer removed."""
        if self.enabled:
            copy_attr(self.ema, model, include, exclude)


def strip_optimizer(f: Union[str, Path] = "best.pt", s: str = "") -> None:
    """
    Strip optimizer from 'f' to finalize training, optionally save as 's'.

    Args:
        f (str): file path to model to strip the optimizer from. Default is 'best.pt'.
        s (str): file path to save the model with stripped optimizer to. If not provided, 'f' will be overwritten.

    Returns:
        None

    Example:
        ```py
        from pathlib import Path
        from ultralytics.utils.torch_utils import strip_optimizer

        for f in Path('path/to/model/checkpoints').rglob('*.pt'):
            strip_optimizer(f)
        ```

    Note:
        Use `ultralytics.nn.torch_safe_load` for missing modules with `x = torch_safe_load(f)[0]`
    """
    try:
        x = torch.load(f, map_location=torch.device("cpu"))
        assert isinstance(x, dict), "checkpoint is not a Python dictionary"
        assert "model" in x, "'model' missing from checkpoint"
    except Exception as e:
        LOGGER.warning(f"WARNING âš ï¸ Skipping {f}, not a valid Ultralytics model: {e}")
        return

    updates = {
        "date": datetime.now().isoformat(),
        "version": __version__,
        "license": "AGPL-3.0 License (https://ultralytics.com/license)",
        "docs": "https://docs.ultralytics.com",
    }

    # Update model
    # å¦‚æœå­—å…¸ x ä¸­æœ‰ "ema" é”®ï¼Œåˆ™å°† "model" é”®çš„å€¼è®¾ä¸º "ema" çš„å€¼ï¼Œæ›¿æ¢æ¨¡å‹ä¸º EMA æ¨¡å‹
    if x.get("ema"):
        x["model"] = x["ema"]  # replace model with EMA
    
    # å¦‚æœ "model" å¯¹è±¡å…·æœ‰ "args" å±æ€§ï¼Œå°†å…¶è½¬æ¢ä¸ºå­—å…¸ç±»å‹ï¼Œä» IterableSimpleNamespace è½¬æ¢ä¸º dict
    if hasattr(x["model"], "args"):
        x["model"].args = dict(x["model"].args)  # convert from IterableSimpleNamespace to dict
    
    # å¦‚æœ "model" å¯¹è±¡å…·æœ‰ "criterion" å±æ€§ï¼Œå°†å…¶è®¾ç½®ä¸º Noneï¼Œå»é™¤æŸå¤±å‡½æ•°çš„æ ‡å‡†
    if hasattr(x["model"], "criterion"):
        x["model"].criterion = None  # strip loss criterion
    
    # å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦æµ®ç‚¹æ•°è¡¨ç¤ºï¼Œå³ FP16
    x["model"].half()  # to FP16
    
    # å°†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°è®¾ç½®ä¸ºä¸éœ€è¦æ¢¯åº¦è®¡ç®—
    for p in x["model"].parameters():
        p.requires_grad = False

    # æ›´æ–°å­—å…¸ä¸­çš„å…¶ä»–é”®
    args = {**DEFAULT_CFG_DICT, **x.get("train_args", {})}  # å°† DEFAULT_CFG_DICT å’Œ x ä¸­çš„ "train_args" åˆå¹¶ä¸ºä¸€ä¸ªå­—å…¸
    for k in "optimizer", "best_fitness", "ema", "updates":  # éå†æŒ‡å®šçš„é”®
        x[k] = None  # å°†å­—å…¸ x ä¸­æŒ‡å®šé”®çš„å€¼è®¾ä¸º None
    x["epoch"] = -1  # å°† epoch é”®çš„å€¼è®¾ä¸º -1
    # åˆ›å»ºä¸€ä¸ªæ–°å­—å…¸ï¼Œå…¶ä¸­ä»…åŒ…å« DEFAULT_CFG_KEYS ä¸­å­˜åœ¨çš„é”®å€¼å¯¹ï¼Œå¹¶å°†å…¶èµ‹ç»™ "train_args"
    x["train_args"] = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # strip non-default keys
    # x['model'].args = x['train_args']  # æ­¤è¡Œä»£ç è¢«æ³¨é‡Šæ‰äº†ï¼Œä¸å†ä½¿ç”¨

    # å°† updates å’Œ x ä¸­çš„å†…å®¹åˆå¹¶ä¸ºä¸€ä¸ªå­—å…¸ï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œä¸ä½¿ç”¨ dill åºåˆ—åŒ–
    torch.save({**updates, **x}, s or f, use_dill=False)  # combine dicts (prefer to the right)
    
    # è·å–æ–‡ä»¶çš„å¤§å°ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå…†å­—èŠ‚ï¼ˆMBï¼‰
    mb = os.path.getsize(s or f) / 1e6  # file size
    
    # è®°å½•æ—¥å¿—ï¼Œæ˜¾ç¤ºä¼˜åŒ–å™¨å·²ä»æ–‡ä»¶ä¸­å‰¥ç¦»ï¼ŒåŒæ—¶æ˜¾ç¤ºæ–‡ä»¶åå’Œæ–‡ä»¶å¤§å°
    LOGGER.info(f"Optimizer stripped from {f},{f' saved as {s},' if s else ''} {mb:.1f}MB")
# å°†ç»™å®šä¼˜åŒ–å™¨çš„çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºFP16æ ¼å¼ï¼Œé‡ç‚¹åœ¨äºè½¬æ¢'torch.Tensor'ç±»å‹çš„æ•°æ®
def convert_optimizer_state_dict_to_fp16(state_dict):
    # éå†ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸ä¸­çš„'state'é”®å¯¹åº”çš„æ‰€æœ‰çŠ¶æ€
    for state in state_dict["state"].values():
        # éå†æ¯ä¸ªçŠ¶æ€çš„é”®å€¼å¯¹
        for k, v in state.items():
            # æ’é™¤é”®ä¸º"step"ä¸”å€¼ä¸º'torch.Tensor'ç±»å‹ä¸”æ•°æ®ç±»å‹ä¸ºtorch.float32çš„æƒ…å†µ
            if k != "step" and isinstance(v, torch.Tensor) and v.dtype is torch.float32:
                # å°†ç¬¦åˆæ¡ä»¶çš„Tensorç±»å‹æ•°æ®è½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆFP16ï¼‰
                state[k] = v.half()

    # è¿”å›è½¬æ¢åçš„çŠ¶æ€å­—å…¸
    return state_dict


# Ultralyticsé€Ÿåº¦ã€å†…å­˜å’ŒFLOPsï¼ˆæµ®ç‚¹è¿ç®—æ•°ï¼‰åˆ†æå™¨
def profile(input, ops, n=10, device=None):
    # ç»“æœå­˜å‚¨åˆ—è¡¨
    results = []
    # å¦‚æœè®¾å¤‡å‚æ•°ä¸æ˜¯torch.deviceç±»å‹ï¼Œåˆ™é€‰æ‹©è®¾å¤‡
    if not isinstance(device, torch.device):
        device = select_device(device)
    # æ‰“å°æ—¥å¿—ä¿¡æ¯ï¼ŒåŒ…æ‹¬å„é¡¹å‚æ•°
    LOGGER.info(
        f"{'Params':>12s}{'GFLOPs':>12s}{'GPU_mem (GB)':>14s}{'forward (ms)':>14s}{'backward (ms)':>14s}"
        f"{'input':>24s}{'output':>24s}"
    )
    for x in input if isinstance(input, list) else [input]:
        # å¦‚æœè¾“å…¥æ˜¯åˆ—è¡¨ï¼Œåˆ™éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼›å¦åˆ™å°†è¾“å…¥æ”¾å…¥åˆ—è¡¨ä¸­å¹¶éå†
        x = x.to(device)
        # å°†å½“å‰å…ƒç´ ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Šï¼ˆå¦‚GPUï¼‰
        x.requires_grad = True
        # è®¾ç½®å½“å‰å…ƒç´ çš„æ¢¯åº¦è·Ÿè¸ªä¸ºTrue

        for m in ops if isinstance(ops, list) else [ops]:
            # å¦‚æœæ“ä½œæ˜¯åˆ—è¡¨ï¼Œåˆ™éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ“ä½œï¼›å¦åˆ™å°†æ“ä½œæ”¾å…¥åˆ—è¡¨ä¸­å¹¶éå†
            m = m.to(device) if hasattr(m, "to") else m
            # å¦‚æœæ“ä½œå…·æœ‰"to"æ–¹æ³•ï¼Œåˆ™å°†å…¶ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Šï¼›å¦åˆ™ä¿æŒä¸å˜
            m = m.half() if hasattr(m, "half") and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m
            # å¦‚æœæ“ä½œå…·æœ‰"half"æ–¹æ³•ï¼Œå¹¶ä¸”è¾“å…¥æ˜¯torch.Tensorç±»å‹ä¸”æ•°æ®ç±»å‹ä¸ºtorch.float16ï¼Œåˆ™å°†æ“ä½œè½¬æ¢ä¸ºåŠç²¾åº¦ï¼ˆfloat16ï¼‰ï¼›å¦åˆ™ä¿æŒä¸å˜
            tf, tb, t = 0, 0, [0, 0, 0]
            # åˆå§‹åŒ–æ—¶é—´è®°å½•å˜é‡ï¼šå‰å‘ä¼ æ’­æ—¶é—´ï¼Œåå‘ä¼ æ’­æ—¶é—´ï¼Œæ—¶é—´è®°å½•åˆ—è¡¨

            try:
                flops = thop.profile(m, inputs=[x], verbose=False)[0] / 1e9 * 2 if thop else 0
                # ä½¿ç”¨thopåº“å¯¹æ“ä½œè¿›è¡Œæµ®ç‚¹æ“ä½œè®¡ç®—ï¼ˆFLOPsï¼‰ï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸ºGFLOPsï¼ˆåäº¿æ¬¡æµ®ç‚¹æ“ä½œæ¯ç§’ï¼‰
            except Exception:
                flops = 0
                # å¦‚æœè®¡ç®—FLOPså‡ºç°å¼‚å¸¸ï¼Œåˆ™å°†FLOPsè®¾ç½®ä¸º0

            try:
                for _ in range(n):
                    t[0] = time_sync()
                    # è®°å½•å‰å‘ä¼ æ’­å¼€å§‹æ—¶é—´
                    y = m(x)
                    # æ‰§è¡Œæ“ä½œçš„å‰å‘ä¼ æ’­
                    t[1] = time_sync()
                    # è®°å½•å‰å‘ä¼ æ’­ç»“æŸæ—¶é—´
                    try:
                        (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
                        # è®¡ç®—è¾“å‡ºyçš„æ€»å’Œï¼Œå¹¶å¯¹æ€»å’Œè¿›è¡Œåå‘ä¼ æ’­
                        t[2] = time_sync()
                        # è®°å½•åå‘ä¼ æ’­ç»“æŸæ—¶é—´
                    except Exception:  # no backward method
                        t[2] = float("nan")
                        # å¦‚æœæ²¡æœ‰åå‘ä¼ æ’­æ–¹æ³•ï¼Œåˆ™å°†åå‘ä¼ æ’­æ—¶é—´è®¾ç½®ä¸ºNaN
                    tf += (t[1] - t[0]) * 1000 / n
                    # è®¡ç®—æ¯ä¸ªæ“ä½œçš„å¹³å‡å‰å‘ä¼ æ’­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
                    tb += (t[2] - t[1]) * 1000 / n
                    # è®¡ç®—æ¯ä¸ªæ“ä½œçš„å¹³å‡åå‘ä¼ æ’­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
                mem = torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0
                # å¦‚æœCUDAå¯ç”¨ï¼Œåˆ™è®¡ç®—å½“å‰GPUä¸Šçš„å†…å­˜ä½¿ç”¨é‡ï¼ˆå•ä½ï¼šGBï¼‰
                s_in, s_out = (tuple(x.shape) if isinstance(x, torch.Tensor) else "list" for x in (x, y))
                # è·å–è¾“å…¥xå’Œè¾“å‡ºyçš„å½¢çŠ¶ä¿¡æ¯
                p = sum(x.numel() for x in m.parameters()) if isinstance(m, nn.Module) else 0
                # è®¡ç®—æ“ä½œmä¸­çš„å‚æ•°æ•°é‡
                LOGGER.info(f"{p:12}{flops:12.4g}{mem:>14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):>24s}{str(s_out):>24s}")
                # å°†ç»“æœè®°å½•åˆ°æ—¥å¿—ä¸­ï¼ŒåŒ…æ‹¬å‚æ•°æ•°é‡ã€FLOPsã€å†…å­˜å ç”¨ã€æ—¶é—´ç­‰ä¿¡æ¯
                results.append([p, flops, mem, tf, tb, s_in, s_out])
                # å°†ç»“æœæ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
            except Exception as e:
                LOGGER.info(e)
                # è®°å½•å¼‚å¸¸ä¿¡æ¯åˆ°æ—¥å¿—ä¸­
                results.append(None)
                # å°†ç©ºç»“æœæ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
            gc.collect()
            # å°è¯•é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜
            torch.cuda.empty_cache()
            # æ¸…ç©ºCUDAç¼“å­˜

    return results
    # è¿”å›æ‰€æœ‰æ“ä½œçš„ç»“æœåˆ—è¡¨
class EarlyStopping:
    """Early stopping class that stops training when a specified number of epochs have passed without improvement."""

    def __init__(self, patience=50):
        """
        Initialize early stopping object.

        Args:
            patience (int, optional): Number of epochs to wait after fitness stops improving before stopping.
        """
        self.best_fitness = 0.0  # åˆå§‹åŒ–æœ€ä½³é€‚åº”åº¦ä¸º0.0ï¼Œå³æœ€ä½³å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰
        self.best_epoch = 0  # åˆå§‹åŒ–æœ€ä½³è½®æ¬¡ä¸º0
        self.patience = patience or float("inf")  # è®¾ç½®ç­‰å¾…é€‚åº”åº¦åœæ­¢æé«˜çš„è½®æ¬¡æ•°ï¼Œè‹¥æœªæä¾›åˆ™è®¾ä¸ºæ— ç©·å¤§
        self.possible_stop = False  # æ˜¯å¦å¯èƒ½åœ¨ä¸‹ä¸€ä¸ªè½®æ¬¡åœæ­¢è®­ç»ƒçš„æ ‡å¿—

    def __call__(self, epoch, fitness):
        """
        Check whether to stop training.

        Args:
            epoch (int): Current epoch of training
            fitness (float): Fitness value of current epoch

        Returns:
            (bool): True if training should stop, False otherwise
        """
        if fitness is None:  # æ£€æŸ¥é€‚åº”åº¦æ˜¯å¦ä¸ºNoneï¼ˆå½“val=Falseæ—¶ä¼šå‘ç”Ÿï¼‰
            return False

        if fitness >= self.best_fitness:  # å¦‚æœå½“å‰é€‚åº”åº¦å¤§äºæˆ–ç­‰äºæœ€ä½³é€‚åº”åº¦
            self.best_epoch = epoch  # æ›´æ–°æœ€ä½³è½®æ¬¡ä¸ºå½“å‰è½®æ¬¡
            self.best_fitness = fitness  # æ›´æ–°æœ€ä½³é€‚åº”åº¦ä¸ºå½“å‰é€‚åº”åº¦
        delta = epoch - self.best_epoch  # è®¡ç®—æœªæ”¹å–„çš„è½®æ¬¡æ•°
        self.possible_stop = delta >= (self.patience - 1)  # æ›´æ–°å¯èƒ½åœ¨ä¸‹ä¸€ä¸ªè½®æ¬¡åœæ­¢è®­ç»ƒçš„æ ‡å¿—
        stop = delta >= self.patience  # è‹¥æœªæ”¹å–„çš„è½®æ¬¡æ•°è¶…è¿‡è®¾å®šçš„ç­‰å¾…è½®æ¬¡æ•°ï¼Œåˆ™åœæ­¢è®­ç»ƒ
        if stop:
            prefix = colorstr("EarlyStopping: ")  # è®¾ç½®è¾“å‡ºå‰ç¼€
            LOGGER.info(
                f"{prefix}Training stopped early as no improvement observed in last {self.patience} epochs. "
                f"Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n"
                f"To update EarlyStopping(patience={self.patience}) pass a new patience value, "
                f"i.e. `patience=300` or use `patience=0` to disable EarlyStopping."
            )  # è¾“å‡ºåœæ­¢è®­ç»ƒä¿¡æ¯
        return stop  # è¿”å›æ˜¯å¦åœæ­¢è®­ç»ƒçš„æ ‡å¿—
```