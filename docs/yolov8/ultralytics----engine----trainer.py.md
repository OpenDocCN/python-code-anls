# `.\yolov8\ultralytics\engine\trainer.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""
Train a model on a dataset.

Usage:
    $ yolo mode=train model=yolov8n.pt data=coco8.yaml imgsz=640 epochs=100 batch=16
"""

# å¯¼å…¥éœ€è¦çš„åº“å’Œæ¨¡å—
import gc  # åƒåœ¾å›æ”¶æ¨¡å—
import math  # æ•°å­¦è®¡ç®—æ¨¡å—
import os  # ç³»ç»Ÿæ“ä½œæ¨¡å—
import subprocess  # å­è¿›ç¨‹ç®¡ç†æ¨¡å—
import time  # æ—¶é—´æ“ä½œæ¨¡å—
import warnings  # è­¦å‘Šå¤„ç†æ¨¡å—
from copy import deepcopy  # æ·±æ‹·è´å‡½æ•°
from datetime import datetime, timedelta  # æ—¥æœŸæ—¶é—´æ“ä½œæ¨¡å—
from pathlib import Path  # è·¯å¾„æ“ä½œæ¨¡å—

import numpy as np  # æ•°ç»„æ“ä½œæ¨¡å—
import torch  # PyTorchæ·±åº¦å­¦ä¹ åº“
from torch import distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—
from torch import nn, optim  # ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œä¼˜åŒ–å™¨æ¨¡å—

# å¯¼å…¥Ultralyticsåº“ä¸­çš„å„ç§åŠŸèƒ½å’Œå·¥å…·
from ultralytics.cfg import get_cfg, get_save_dir  # è·å–é…ç½®å’Œä¿å­˜ç›®å½•
from ultralytics.data.utils import check_cls_dataset, check_det_dataset  # æ£€æŸ¥åˆ†ç±»å’Œæ£€æµ‹æ•°æ®é›†çš„å®ç”¨å·¥å…·
from ultralytics.nn.tasks import attempt_load_one_weight, attempt_load_weights  # åŠ è½½æ¨¡å‹æƒé‡çš„å‡½æ•°
from ultralytics.utils import (
    DEFAULT_CFG,  # é»˜è®¤é…ç½®
    LOGGER,  # æ—¥å¿—è®°å½•å™¨
    RANK,  # æ’å
    TQDM,  # è¿›åº¦æ¡
    __version__,  # Ultralyticsç‰ˆæœ¬å·
    callbacks,  # å›è°ƒå‡½æ•°
    clean_url,  # æ¸…ç†URL
    colorstr,  # å­—ç¬¦ä¸²é¢œè‰²å¤„ç†
    emojis,  # è¡¨æƒ…ç¬¦å·
    yaml_save,  # ä¿å­˜YAMLæ–‡ä»¶
)
from ultralytics.utils.autobatch import check_train_batch_size  # æ£€æŸ¥è®­ç»ƒæ‰¹æ¬¡å¤§å°
from ultralytics.utils.checks import (
    check_amp,  # æ£€æŸ¥AMPï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰æ”¯æŒ
    check_file,  # æ£€æŸ¥æ–‡ä»¶
    check_imgsz,  # æ£€æŸ¥å›¾åƒå°ºå¯¸
    check_model_file_from_stem,  # ä»stemæ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    print_args,  # æ‰“å°å‚æ•°
)
from ultralytics.utils.dist import ddp_cleanup, generate_ddp_command  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ¸…ç†å’Œå‘½ä»¤ç”Ÿæˆ
from ultralytics.utils.files import get_latest_run  # è·å–æœ€æ–°è¿è¡Œæ–‡ä»¶
from ultralytics.utils.torch_utils import (
    EarlyStopping,  # æ—©åœ
    ModelEMA,  # æ¨¡å‹æŒ‡æ•°ç§»åŠ¨å¹³å‡
    autocast,  # è‡ªåŠ¨ç±»å‹è½¬æ¢
    convert_optimizer_state_dict_to_fp16,  # å°†ä¼˜åŒ–å™¨çŠ¶æ€è½¬æ¢ä¸ºFP16
    init_seeds,  # åˆå§‹åŒ–éšæœºç§å­
    one_cycle,  # å•å¾ªç¯å­¦ä¹ ç‡ç­–ç•¥
    select_device,  # é€‰æ‹©è®¾å¤‡
    strip_optimizer,  # å‰¥ç¦»ä¼˜åŒ–å™¨
    torch_distributed_zero_first,  # åˆ†å¸ƒå¼è®­ç»ƒçš„é¦–æ¬¡æ¸…é›¶
)

# å®šä¹‰åŸºç¡€è®­ç»ƒå™¨ç±»
class BaseTrainer:
    """
    BaseTrainer.

    A base class for creating trainers.

    Attributes:
        args (SimpleNamespace): Configuration for the trainer.
        validator (BaseValidator): Validator instance.
        model (nn.Module): Model instance.
        callbacks (defaultdict): Dictionary of callbacks.
        save_dir (Path): Directory to save results.
        wdir (Path): Directory to save weights.
        last (Path): Path to the last checkpoint.
        best (Path): Path to the best checkpoint.
        save_period (int): Save checkpoint every x epochs (disabled if < 1).
        batch_size (int): Batch size for training.
        epochs (int): Number of epochs to train for.
        start_epoch (int): Starting epoch for training.
        device (torch.device): Device to use for training.
        amp (bool): Flag to enable AMP (Automatic Mixed Precision).
        scaler (amp.GradScaler): Gradient scaler for AMP.
        data (str): Path to data.
        trainset (torch.utils.data.Dataset): Training dataset.
        testset (torch.utils.data.Dataset): Testing dataset.
        ema (nn.Module): EMA (Exponential Moving Average) of the model.
        resume (bool): Resume training from a checkpoint.
        lf (nn.Module): Loss function.
        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.
        best_fitness (float): The best fitness value achieved.
        fitness (float): Current fitness value.
        loss (float): Current loss value.
        tloss (float): Total loss value.
        loss_names (list): List of loss names.
        csv (Path): Path to results CSV file.
    """
    # åˆå§‹åŒ– BaseTrainer ç±»çš„æ„é€ å‡½æ•°
    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        Initializes the BaseTrainer class.

        Args:
            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.
            overrides (dict, optional): Configuration overrides. Defaults to None.
            _callbacks (list, optional): List of callbacks. Defaults to None.
        """
        # è·å–é…ç½®å‚æ•°
        self.args = get_cfg(cfg, overrides)
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒçŠ¶æ€
        self.check_resume(overrides)
        # é€‰æ‹©è®¾å¤‡
        self.device = select_device(self.args.device, self.args.batch)
        self.validator = None  # åˆå§‹åŒ–éªŒè¯å™¨ä¸º None
        self.metrics = None  # åˆå§‹åŒ–æŒ‡æ ‡ä¸º None
        self.plots = {}  # åˆå§‹åŒ–å›¾å½¢å­—å…¸ä¸ºç©º

        # åˆå§‹åŒ–éšæœºç§å­
        init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)

        # ç›®å½•è®¾ç½®
        self.save_dir = get_save_dir(self.args)  # è·å–ä¿å­˜ç›®å½•
        self.args.name = self.save_dir.name  # æ›´æ–°æ—¥å¿—è®°å½•å™¨çš„åç§°
        self.wdir = self.save_dir / "weights"  # æƒé‡ä¿å­˜ç›®å½•
        if RANK in {-1, 0}:
            self.wdir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•
            self.args.save_dir = str(self.save_dir)  # è®¾ç½®ä¿å­˜ç›®å½•è·¯å¾„
            yaml_save(self.save_dir / "args.yaml", vars(self.args))  # ä¿å­˜è¿è¡Œå‚æ•°åˆ° YAML æ–‡ä»¶
        self.last, self.best = self.wdir / "last.pt", self.wdir / "best.pt"  # æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        self.save_period = self.args.save_period  # ä¿å­˜å‘¨æœŸè®¾ç½®

        self.batch_size = self.args.batch  # æ‰¹é‡å¤§å°
        self.epochs = self.args.epochs  # è®­ç»ƒå‘¨æœŸæ•°
        self.start_epoch = 0  # èµ·å§‹å‘¨æœŸ
        if RANK == -1:
            print_args(vars(self.args))  # æ‰“å°å‚æ•°ä¿¡æ¯

        # è®¾å¤‡è®¾ç½®
        if self.device.type in {"cpu", "mps"}:
            self.args.workers = 0  # è‹¥ä½¿ç”¨ CPU æˆ–è€… mps è®¾å¤‡ï¼Œè®¾ç½® workers ä¸º 0ï¼Œæå‡è®­ç»ƒé€Ÿåº¦

        # æ¨¡å‹å’Œæ•°æ®é›†
        self.model = check_model_file_from_stem(self.args.model)  # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼Œæ·»åŠ åç¼€ï¼ˆä¾‹å¦‚ yolov8n -> yolov8n.ptï¼‰
        with torch_distributed_zero_first(RANK):  # é¿å…å¤šæ¬¡è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†
            self.trainset, self.testset = self.get_dataset()  # è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        self.ema = None  # æŒ‡æ•°ç§»åŠ¨å¹³å‡å‚æ•°åˆå§‹åŒ–ä¸º None

        # ä¼˜åŒ–å™¨å·¥å…·åˆå§‹åŒ–
        self.lf = None  # æŸå¤±å‡½æ•°åˆå§‹åŒ–ä¸º None
        self.scheduler = None  # è°ƒåº¦å™¨åˆå§‹åŒ–ä¸º None

        # æ¯ä¸ªå‘¨æœŸçš„æŒ‡æ ‡
        self.best_fitness = None  # æœ€ä½³é€‚åº”åº¦åˆå§‹åŒ–ä¸º None
        self.fitness = None  # é€‚åº”åº¦åˆå§‹åŒ–ä¸º None
        self.loss = None  # æŸå¤±åˆå§‹åŒ–ä¸º None
        self.tloss = None  # æ€»æŸå¤±åˆå§‹åŒ–ä¸º None
        self.loss_names = ["Loss"]  # æŸå¤±åç§°åˆ—è¡¨
        self.csv = self.save_dir / "results.csv"  # CSV æ–‡ä»¶è·¯å¾„
        self.plot_idx = [0, 1, 2]  # ç»˜å›¾ç´¢å¼•

        # HUB
        self.hub_session = None  # HUB ä¼šè¯åˆå§‹åŒ–ä¸º None

        # å›è°ƒå‡½æ•°
        self.callbacks = _callbacks or callbacks.get_default_callbacks()  # è·å–é»˜è®¤æˆ–è‡ªå®šä¹‰å›è°ƒå‡½æ•°åˆ—è¡¨
        if RANK in {-1, 0}:
            callbacks.add_integration_callbacks(self)  # æ·»åŠ é›†æˆå›è°ƒå‡½æ•°

    def add_callback(self, event: str, callback):
        """Appends the given callback to the event."""
        self.callbacks[event].append(callback)  # å°†ç»™å®šçš„å›è°ƒå‡½æ•°æ·»åŠ åˆ°äº‹ä»¶å¯¹åº”çš„å›è°ƒå‡½æ•°åˆ—è¡¨

    def set_callback(self, event: str, callback):
        """Sets the given callback for the event, overriding any existing callbacks."""
        self.callbacks[event] = [callback]  # è®¾ç½®ç»™å®šäº‹ä»¶çš„å›è°ƒå‡½æ•°ï¼Œè¦†ç›–ç°æœ‰çš„æ‰€æœ‰å›è°ƒå‡½æ•°

    def run_callbacks(self, event: str):
        """Runs all existing callbacks associated with the given event."""
        for callback in self.callbacks.get(event, []):  # éå†ç»™å®šäº‹ä»¶å…³è”çš„æ‰€æœ‰å›è°ƒå‡½æ•°
            callback(self)  # æ‰§è¡Œå›è°ƒå‡½æ•°
    def train(self):
        """
        Allow device='', device=None on Multi-GPU systems to default to device=0.
        """
        # Determine the number of GPUs to be used based on the provided device configuration
        if isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'
            world_size = len(self.args.device.split(","))
        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)
            world_size = len(self.args.device)
        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=number
            world_size = 1  # default to device 0
        else:  # i.e. device='cpu' or 'mps'
            world_size = 0

        # Run subprocess if DDP (Distributed Data Parallel) training is enabled and not in a subprocess
        if world_size > 1 and "LOCAL_RANK" not in os.environ:
            # Argument checks and adjustments for Multi-GPU training
            if self.args.rect:
                LOGGER.warning("WARNING âš ï¸ 'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'")
                self.args.rect = False
            if self.args.batch < 1.0:
                LOGGER.warning(
                    "WARNING âš ï¸ 'batch<1' for AutoBatch is incompatible with Multi-GPU training, setting "
                    "default 'batch=16'"
                )
                self.args.batch = 16

            # Generate and execute the DDP command
            cmd, file = generate_ddp_command(world_size, self)
            try:
                LOGGER.info(f'{colorstr("DDP:")} debug command {" ".join(cmd)}')
                subprocess.run(cmd, check=True)
            except Exception as e:
                raise e
            finally:
                # Clean up DDP resources
                ddp_cleanup(self, str(file))

        else:
            # Perform single-GPU or CPU training
            self._do_train(world_size)

    def _setup_scheduler(self):
        """
        Initialize training learning rate scheduler based on arguments.
        """
        if self.args.cos_lr:
            # Use cosine learning rate schedule
            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1->hyp['lrf']
        else:
            # Use linear learning rate schedule
            self.lf = lambda x: max(1 - x / self.epochs, 0) * (1.0 - self.args.lrf) + self.args.lrf  # linear
        # Initialize LambdaLR scheduler with the defined learning rate function
        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)

    def _setup_ddp(self, world_size):
        """
        Initializes and configures DistributedDataParallel (DDP) for training.
        """
        # Set CUDA device for the current process rank
        torch.cuda.set_device(RANK)
        self.device = torch.device("cuda", RANK)
        # Configure environment for DDP training
        os.environ["TORCH_NCCL_BLOCKING_WAIT"] = "1"  # set to enforce timeout
        # Initialize the process group for distributed training
        dist.init_process_group(
            backend="nccl" if dist.is_nccl_available() else "gloo",
            timeout=timedelta(seconds=10800),  # 3 hours
            rank=RANK,
            world_size=world_size,
        )
    def save_model(self):
        """
        Save model training checkpoints with additional metadata.
        """
        import io  # å¯¼å…¥ioæ¨¡å—ï¼Œç”¨äºå¤„ç†å­—èŠ‚æµæ“ä½œ

        import pandas as pd  # å¯¼å…¥pandasæ¨¡å—ï¼Œç”¨äºå¤„ç†CSVæ–‡ä»¶å’Œæ•°æ®ç»“æ„

        # Serialize ckpt to a byte buffer once (faster than repeated torch.save() calls)
        buffer = io.BytesIO()  # åˆ›å»ºä¸€ä¸ªå­—èŠ‚æµç¼“å†²åŒºå¯¹è±¡
        torch.save(
            {
                "epoch": self.epoch,  # ä¿å­˜å½“å‰è®­ç»ƒè½®æ¬¡
                "best_fitness": self.best_fitness,  # ä¿å­˜æœ€ä½³è®­ç»ƒæ•ˆæœæŒ‡æ ‡
                "model": None,  # æ¨¡å‹ä¿¡æ¯ï¼ˆç”±EMAæ´¾ç”Ÿçš„æ¢å¤å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹ï¼‰
                "ema": deepcopy(self.ema.ema).half(),  # æ·±åº¦å¤åˆ¶EMAå¯¹è±¡çš„EMAéƒ¨åˆ†ï¼Œå¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                "updates": self.ema.updates,  # ä¿å­˜EMAå¯¹è±¡çš„æ›´æ–°æ¬¡æ•°
                "optimizer": convert_optimizer_state_dict_to_fp16(deepcopy(self.optimizer.state_dict())),  # è½¬æ¢ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸ä¸ºåŠç²¾åº¦
                "train_args": vars(self.args),  # ä¿å­˜è®­ç»ƒå‚æ•°çš„å­—å…¸å½¢å¼
                "train_metrics": {**self.metrics, **{"fitness": self.fitness}},  # ä¿å­˜è®­ç»ƒæŒ‡æ ‡å’Œé€‚åº”åº¦æŒ‡æ ‡
                "train_results": {k.strip(): v for k, v in pd.read_csv(self.csv).to_dict(orient="list").items()},  # è¯»å–CSVæ–‡ä»¶å¹¶å°†ç»“æœä¿å­˜ä¸ºå­—å…¸
                "date": datetime.now().isoformat(),  # ä¿å­˜å½“å‰æ—¶é—´çš„ISOæ ¼å¼å­—ç¬¦ä¸²
                "version": __version__,  # ä¿å­˜å½“å‰ç‰ˆæœ¬å·
                "license": "AGPL-3.0 (https://ultralytics.com/license)",  # è®¸å¯è¯ä¿¡æ¯
                "docs": "https://docs.ultralytics.com",  # æ–‡æ¡£é“¾æ¥
            },
            buffer,
        )
        serialized_ckpt = buffer.getvalue()  # è·å–åºåˆ—åŒ–åçš„æ£€æŸ¥ç‚¹å†…å®¹ä»¥ä¿å­˜

        # Save checkpoints
        self.last.write_bytes(serialized_ckpt)  # ä¿å­˜æœ€åçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆlast.ptï¼‰
        if self.best_fitness == self.fitness:
            self.best.write_bytes(serialized_ckpt)  # è‹¥å½“å‰é€‚åº”åº¦ç­‰äºæœ€ä½³é€‚åº”åº¦ï¼Œåˆ™ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆbest.ptï¼‰
        if (self.save_period > 0) and (self.epoch % self.save_period == 0):
            (self.wdir / f"epoch{self.epoch}.pt").write_bytes(serialized_ckpt)  # æ ¹æ®å½“å‰è½®æ¬¡ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¦‚'epoch3.pt'

    def get_dataset(self):
        """
        Get train, val path from data dict if it exists.

        Returns None if data format is not recognized.
        """
        try:
            if self.args.task == "classify":  # å¦‚æœä»»åŠ¡ä¸ºåˆ†ç±»
                data = check_cls_dataset(self.args.data)  # æ£€æŸ¥å¹¶è·å–åˆ†ç±»æ•°æ®é›†
            elif self.args.data.split(".")[-1] in {"yaml", "yml"} or self.args.task in {
                "detect",
                "segment",
                "pose",
                "obb",
            }:  # å¦‚æœæ•°æ®æ–‡ä»¶æ ¼å¼ä¸ºyamlæˆ–ymlï¼Œæˆ–è€…ä»»åŠ¡æ˜¯æ£€æµ‹ã€åˆ†å‰²ã€å§¿æ€æˆ–obb
                data = check_det_dataset(self.args.data)  # æ£€æŸ¥å¹¶è·å–æ£€æµ‹æ•°æ®é›†
                if "yaml_file" in data:
                    self.args.data = data["yaml_file"]  # æ›´æ–°self.args.dataä»¥éªŒè¯'yolo train data=url.zip'çš„ä½¿ç”¨
        except Exception as e:
            raise RuntimeError(emojis(f"Dataset '{clean_url(self.args.data)}' error âŒ {e}")) from e  # æŠ›å‡ºæ•°æ®é›†é”™è¯¯å¼‚å¸¸
        self.data = data  # å°†è·å–çš„æ•°æ®é›†ä¿å­˜åˆ°self.data
        return data["train"], data.get("val") or data.get("test")  # è¿”å›è®­ç»ƒé›†è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›éªŒè¯é›†æˆ–æµ‹è¯•é›†è·¯å¾„
    def setup_model(self):
        """
        Load/create/download model for any task.

        Checks if a model is already loaded. If not, attempts to load weights from a '.pt' file or a specified pretrained path,
        then initializes the model using the loaded configuration and weights.
        """
        if isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed
            return

        cfg, weights = self.model, None
        ckpt = None
        if str(self.model).endswith(".pt"):
            # Attempt to load weights and configuration from a '.pt' file
            weights, ckpt = attempt_load_one_weight(self.model)
            cfg = weights.yaml
        elif isinstance(self.args.pretrained, (str, Path)):
            # Attempt to load weights from a specified pretrained path
            weights, _ = attempt_load_one_weight(self.args.pretrained)
        
        # Initialize the model using the retrieved configuration and weights
        self.model = self.get_model(cfg=cfg, weights=weights, verbose=RANK == -1)  # calls Model(cfg, weights)
        return ckpt

    def optimizer_step(self):
        """
        Perform a single step of the training optimizer with gradient clipping and EMA update.

        Unscales gradients, clips gradients to a maximum norm of 10.0, performs optimizer step, updates scaler,
        and zeroes the gradients of the optimizer. If an Exponential Moving Average (EMA) is enabled, updates the EMA.
        """
        self.scaler.unscale_(self.optimizer)  # unscale gradients
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradients
        self.scaler.step(self.optimizer)
        self.scaler.update()
        self.optimizer.zero_grad()
        if self.ema:
            self.ema.update(self.model)

    def preprocess_batch(self, batch):
        """
        Allows custom preprocessing model inputs and ground truths depending on task type.

        This function can be overridden to implement custom preprocessing logic for batches of data before feeding them
        into the model.
        """
        return batch

    def validate(self):
        """
        Runs validation on test set using self.validator.

        Returns metrics and fitness. Updates the best fitness if a new best fitness is found during validation.
        """
        metrics = self.validator(self)
        fitness = metrics.pop("fitness", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found
        if not self.best_fitness or self.best_fitness < fitness:
            self.best_fitness = fitness
        return metrics, fitness

    def get_model(self, cfg=None, weights=None, verbose=True):
        """
        Get model and raise NotImplementedError for loading cfg files.

        Raises NotImplementedError indicating that loading configuration files (cfg) is not supported by this method.
        """
        raise NotImplementedError("This task trainer doesn't support loading cfg files")

    def get_validator(self):
        """
        Returns a NotImplementedError when the get_validator function is called.

        Raises NotImplementedError indicating that the get_validator function is not implemented in this trainer.
        """
        raise NotImplementedError("get_validator function not implemented in trainer")

    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode="train"):
        """
        Returns dataloader derived from torch.data.Dataloader.

        Raises NotImplementedError indicating that the get_dataloader function is not implemented in this trainer.
        """
        raise NotImplementedError("get_dataloader function not implemented in trainer")

    def build_dataset(self, img_path, mode="train", batch=None):
        """
        Build dataset.

        Raises NotImplementedError indicating that the build_dataset function is not implemented in this trainer.
        """
        raise NotImplementedError("build_dataset function not implemented in trainer")

    def label_loss_items(self, loss_items=None, prefix="train"):
        """
        Returns a loss dict with labelled training loss items tensor.

        This function provides a labeled dictionary of loss items. It's particularly useful for tasks like segmentation and detection.
        """
        return {"loss": loss_items} if loss_items is not None else ["loss"]
    # è®¾ç½®æˆ–æ›´æ–°æ¨¡å‹è®­ç»ƒå‰çš„å‚æ•°
    def set_model_attributes(self):
        """To set or update model parameters before training."""
        self.model.names = self.data["names"]

    # æ„å»ºç”¨äºè®­ç»ƒ YOLO æ¨¡å‹çš„ç›®æ ‡å¼ é‡
    def build_targets(self, preds, targets):
        """Builds target tensors for training YOLO model."""
        pass

    # è¿”å›æè¿°è®­ç»ƒè¿›åº¦çš„å­—ç¬¦ä¸²
    def progress_string(self):
        """Returns a string describing training progress."""
        return ""

    # TODO: å¯èƒ½éœ€è¦å°†ä»¥ä¸‹å‡½æ•°æ”¾å…¥å›è°ƒå‡½æ•°ä¸­
    # ç»˜åˆ¶ YOLO è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®­ç»ƒæ ·æœ¬å›¾åƒ
    def plot_training_samples(self, batch, ni):
        """Plots training samples during YOLO training."""
        pass

    # ç»˜åˆ¶ YOLO æ¨¡å‹çš„è®­ç»ƒæ ‡ç­¾
    def plot_training_labels(self):
        """Plots training labels for YOLO model."""
        pass

    # å°†è®­ç»ƒæŒ‡æ ‡ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­
    def save_metrics(self, metrics):
        """Saves training metrics to a CSV file."""
        keys, vals = list(metrics.keys()), list(metrics.values())
        n = len(metrics) + 1  # number of cols
        s = "" if self.csv.exists() else (("%23s," * n % tuple(["epoch"] + keys)).rstrip(",") + "\n")  # header
        with open(self.csv, "a") as f:
            f.write(s + ("%23.5g," * n % tuple([self.epoch + 1] + vals)).rstrip(",") + "\n")

    # ç»˜åˆ¶å¹¶å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡
    def plot_metrics(self):
        """Plot and display metrics visually."""
        pass

    # æ³¨å†Œç»˜å›¾ä¿¡æ¯ï¼ˆä¾‹å¦‚ä¾›å›è°ƒå‡½æ•°ä½¿ç”¨ï¼‰
    def on_plot(self, name, data=None):
        """Registers plots (e.g. to be consumed in callbacks)"""
        path = Path(name)
        self.plots[path] = {"data": data, "timestamp": time.time()}

    # æ‰§è¡Œç‰©ä½“æ£€æµ‹ YOLO æ¨¡å‹çš„æœ€ç»ˆè¯„ä¼°å’ŒéªŒè¯
    def final_eval(self):
        """Performs final evaluation and validation for object detection YOLO model."""
        for f in self.last, self.best:
            if f.exists():
                strip_optimizer(f)  # å»é™¤ä¼˜åŒ–å™¨ä¿¡æ¯
                if f is self.best:
                    LOGGER.info(f"\nValidating {f}...")
                    self.validator.args.plots = self.args.plots
                    self.metrics = self.validator(model=f)
                    self.metrics.pop("fitness", None)
                    self.run_callbacks("on_fit_epoch_end")
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¢å¤æ£€æŸ¥ç‚¹ï¼Œå¹¶æ ¹æ®éœ€è¦æ›´æ–°å‚æ•°
    def check_resume(self, overrides):
        # è·å–æ¢å¤è·¯å¾„
        resume = self.args.resume
        # å¦‚æœæœ‰æ¢å¤è·¯å¾„
        if resume:
            try:
                # æ£€æŸ¥æ¢å¤è·¯å¾„æ˜¯å¦æ˜¯å­—ç¬¦ä¸²æˆ–è·¯å¾„å¯¹è±¡å¹¶ä¸”å­˜åœ¨
                exists = isinstance(resume, (str, Path)) and Path(resume).exists()
                # è·å–æœ€æ–°çš„è¿è¡Œæ£€æŸ¥ç‚¹è·¯å¾„
                last = Path(check_file(resume) if exists else get_latest_run())

                # æ£€æŸ¥æ¢å¤çš„æ•°æ® YAML æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦åˆ™å¼ºåˆ¶é‡æ–°ä¸‹è½½æ•°æ®é›†
                ckpt_args = attempt_load_weights(last).args
                if not Path(ckpt_args["data"]).exists():
                    ckpt_args["data"] = self.args.data

                # æ ‡è®°ä¸ºéœ€è¦æ¢å¤
                resume = True
                # è·å–é…ç½®å¯¹è±¡ï¼Œå¹¶æ›´æ–°å‚æ•°
                self.args = get_cfg(ckpt_args)
                self.args.model = self.args.resume = str(last)  # æ¢å¤æ¨¡å‹è·¯å¾„
                # éå†å¯èƒ½éœ€è¦æ›´æ–°çš„å‚æ•°ï¼Œä¾‹å¦‚å‡å°‘å†…å­˜å ç”¨æˆ–æ›´æ–°è®¾å¤‡
                for k in "imgsz", "batch", "device":
                    if k in overrides:
                        setattr(self.args, k, overrides[k])

            except Exception as e:
                # å¦‚æœå‡ºç°å¼‚å¸¸ï¼ŒæŠ›å‡ºæ–‡ä»¶æœªæ‰¾åˆ°çš„é”™è¯¯
                raise FileNotFoundError(
                    "Resume checkpoint not found. Please pass a valid checkpoint to resume from, "
                    "i.e. 'yolo train resume model=path/to/last.pt'"
                ) from e
        # å°†æ¢å¤çŠ¶æ€æ›´æ–°åˆ°å¯¹è±¡ä¸­
        self.resume = resume

    # ä»ç»™å®šçš„æ£€æŸ¥ç‚¹æ¢å¤ YOLO è®­ç»ƒ
    def resume_training(self, ckpt):
        # å¦‚æœæ£€æŸ¥ç‚¹ä¸ºç©ºæˆ–è€…æ²¡æœ‰è®¾ç½®æ¢å¤æ ‡å¿—ï¼Œåˆ™ç›´æ¥è¿”å›
        if ckpt is None or not self.resume:
            return
        # åˆå§‹åŒ–æœ€ä½³é€‚åº”åº¦å’Œå¼€å§‹çš„ epoch
        best_fitness = 0.0
        start_epoch = ckpt.get("epoch", -1) + 1
        # å¦‚æœæ£€æŸ¥ç‚¹ä¸­åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œåˆ™åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if ckpt.get("optimizer", None) is not None:
            self.optimizer.load_state_dict(ckpt["optimizer"])  # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            best_fitness = ckpt["best_fitness"]
        # å¦‚æœå­˜åœ¨ EMA å¹¶ä¸”æ£€æŸ¥ç‚¹ä¸­åŒ…å« EMA æ•°æ®ï¼Œåˆ™åŠ è½½ EMA çŠ¶æ€
        if self.ema and ckpt.get("ema"):
            self.ema.ema.load_state_dict(ckpt["ema"].float().state_dict())  # åŠ è½½ EMA çŠ¶æ€
            self.ema.updates = ckpt["updates"]
        # ç¡®ä¿å¼€å§‹çš„ epoch å¤§äº 0ï¼Œå¦åˆ™è¾“å‡ºé”™è¯¯ä¿¡æ¯
        assert start_epoch > 0, (
            f"{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\n"
            f"Start a new training without resuming, i.e. 'yolo train model={self.args.model}'"
        )
        # è®°å½•æ¢å¤è®­ç»ƒçš„ä¿¡æ¯
        LOGGER.info(f"Resuming training {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs")
        # å¦‚æœæ€» epoch å°äºå¼€å§‹çš„ epochï¼Œåˆ™è¾“å‡ºè°ƒè¯•ä¿¡æ¯è¿›è¡Œå¾®è°ƒ
        if self.epochs < start_epoch:
            LOGGER.info(
                f"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs."
            )
            self.epochs += ckpt["epoch"]  # è¿›è¡Œé¢å¤–çš„å¾®è°ƒ epoch
        # æ›´æ–°æœ€ä½³é€‚åº”åº¦å’Œå¼€å§‹çš„ epoch
        self.best_fitness = best_fitness
        self.start_epoch = start_epoch
        # å¦‚æœå¼€å§‹çš„ epoch å¤§äºæ€» epoch å‡å»å…³é—­æ··åˆå›¾å—çš„æ•°é‡ï¼Œåˆ™å…³é—­æ•°æ®åŠ è½½å™¨ä¸­çš„æ··åˆå›¾å—
        if start_epoch > (self.epochs - self.args.close_mosaic):
            self._close_dataloader_mosaic()
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå…³é—­æ•°æ®åŠ è½½å™¨çš„é©¬èµ›å…‹å¢å¼ºåŠŸèƒ½
    def _close_dataloader_mosaic(self):
        """Update dataloaders to stop using mosaic augmentation."""
        # æ£€æŸ¥è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ•°æ®é›†æ˜¯å¦å…·æœ‰ 'mosaic' å±æ€§
        if hasattr(self.train_loader.dataset, "mosaic"):
            # å°†æ•°æ®é›†çš„ 'mosaic' å±æ€§è®¾ç½®ä¸º Falseï¼Œåœæ­¢ä½¿ç”¨é©¬èµ›å…‹å¢å¼º
            self.train_loader.dataset.mosaic = False
        # å†æ¬¡æ£€æŸ¥è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ•°æ®é›†æ˜¯å¦å…·æœ‰ 'close_mosaic' æ–¹æ³•
        if hasattr(self.train_loader.dataset, "close_mosaic"):
            # è®°å½•ä¿¡æ¯åˆ°æ—¥å¿—ï¼ŒæŒ‡ç¤ºæ­£åœ¨å…³é—­æ•°æ®åŠ è½½å™¨çš„é©¬èµ›å…‹å¢å¼º
            LOGGER.info("Closing dataloader mosaic")
            # è°ƒç”¨æ•°æ®é›†çš„ 'close_mosaic' æ–¹æ³•ï¼Œå…³é—­é©¬èµ›å…‹å¢å¼ºï¼Œå¹¶ä¼ é€’é¢å¤–çš„å‚æ•° 'hyp=self.args'
            self.train_loader.dataset.close_mosaic(hyp=self.args)
```