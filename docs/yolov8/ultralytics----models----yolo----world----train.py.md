# `.\yolov8\ultralytics\models\yolo\world\train.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import itertools  # å¯¼å…¥ itertools æ¨¡å—

from ultralytics.data import build_yolo_dataset  # ä» ultralytics.data æ¨¡å—å¯¼å…¥ build_yolo_dataset å‡½æ•°
from ultralytics.models import yolo  # ä» ultralytics.models æ¨¡å—å¯¼å…¥ yolo æ¨¡å‹
from ultralytics.nn.tasks import WorldModel  # ä» ultralytics.nn.tasks å¯¼å…¥ WorldModel ç±»
from ultralytics.utils import DEFAULT_CFG, RANK, checks  # ä» ultralytics.utils å¯¼å…¥ DEFAULT_CFG, RANK, checks
from ultralytics.utils.torch_utils import de_parallel  # ä» ultralytics.utils.torch_utils å¯¼å…¥ de_parallel å‡½æ•°


def on_pretrain_routine_end(trainer):
    """Callback."""
    if RANK in {-1, 0}:
        # NOTE: for evaluation
        # ä» trainer.test_loader.dataset.data["names"] ä¸­è·å–æ‰€æœ‰åç§°ï¼Œä»…ä¿ç•™ç¬¬ä¸€ä¸ªæ–œæ ä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºåç§°
        names = [name.split("/")[0] for name in list(trainer.test_loader.dataset.data["names"].values())]
        # è®¾ç½® trainer.ema.ema ä¸­çš„ç±»åˆ«ä¸º namesï¼Œä¸ç¼“å­˜å‰ªè¾‘æ¨¡å‹
        de_parallel(trainer.ema.ema).set_classes(names, cache_clip_model=False)
    device = next(trainer.model.parameters()).device  # è·å– trainer.model ä¸­ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡ä¿¡æ¯
    # ä½¿ç”¨æŒ‡å®šè®¾å¤‡åŠ è½½ ViT-B/32 æ¨¡å‹åˆ° trainer.text_model ä¸­
    trainer.text_model, _ = trainer.clip.load("ViT-B/32", device=device)
    # å°† trainer.text_model ä¸­æ‰€æœ‰å‚æ•°è®¾ä¸ºä¸éœ€è¦æ¢¯åº¦è®¡ç®—
    for p in trainer.text_model.parameters():
        p.requires_grad_(False)


class WorldTrainer(yolo.detect.DetectionTrainer):
    """
    A class to fine-tune a world model on a close-set dataset.

    Example:
        ```py
        from ultralytics.models.yolo.world import WorldModel

        args = dict(model='yolov8s-world.pt', data='coco8.yaml', epochs=3)
        trainer = WorldTrainer(overrides=args)
        trainer.train()
        ```
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """Initialize a WorldTrainer object with given arguments."""
        if overrides is None:
            overrides = {}
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°åˆå§‹åŒ–å¯¹è±¡
        super().__init__(cfg, overrides, _callbacks)

        # Import and assign clip
        try:
            import clip
        except ImportError:
            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† CLIP åº“ï¼Œå¦‚æœæœªå®‰è£…ï¼Œåˆ™å®‰è£…è¯¥åº“
            checks.check_requirements("git+https://github.com/ultralytics/CLIP.git")
            import clip
        self.clip = clip  # å°† clip æ¨¡å—èµ‹å€¼ç»™ self.clip

    def get_model(self, cfg=None, weights=None, verbose=True):
        """Return WorldModel initialized with specified config and weights."""
        # NOTE: This `nc` here is the max number of different text samples in one image, rather than the actual `nc`.
        # NOTE: Following the official config, nc hard-coded to 80 for now.
        # ä½¿ç”¨ cfg å’Œ weights å‚æ•°åˆå§‹åŒ– WorldModelï¼Œè®¾ç½® nc ä¸ºæ•°æ®é›†ä¸­çš„æœ€å¤§æ–‡æœ¬æ ·æœ¬æ•°å’Œ 80 ä¸­çš„æœ€å°å€¼
        model = WorldModel(
            cfg["yaml_file"] if isinstance(cfg, dict) else cfg,
            ch=3,
            nc=min(self.data["nc"], 80),
            verbose=verbose and RANK == -1,
        )
        if weights:
            model.load(weights)  # å¦‚æœæä¾›äº† weights å‚æ•°ï¼Œåˆ™åŠ è½½æ¨¡å‹æƒé‡
        self.add_callback("on_pretrain_routine_end", on_pretrain_routine_end)  # æ·»åŠ å›è°ƒå‡½æ•° on_pretrain_routine_end åˆ°å¯¹è±¡

        return model  # è¿”å›åˆå§‹åŒ–çš„ WorldModel å¯¹è±¡
    # è·å–å½“å‰æ¨¡å‹çš„æœ€å¤§æ­¥é•¿ï¼Œå¦‚æœæ¨¡å‹å­˜åœ¨åˆ™è·å–æœ€å¤§æ­¥é•¿ï¼Œå¦åˆ™è¿”å›0ï¼Œå¹¶è½¬ä¸ºæ•´æ•°
    gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)
    # è°ƒç”¨å‡½æ•°æ„å»º YOLO æ•°æ®é›†ï¼Œè¿”å›æ„å»ºçš„æ•°æ®é›†å¯¹è±¡
    return build_yolo_dataset(
        self.args, img_path, batch, self.data, mode=mode, rect=mode == "val", stride=gs, multi_modal=mode == "train"
    )


```py    
    # è°ƒç”¨çˆ¶ç±»æ–¹æ³•å¤„ç†å›¾åƒæ‰¹æ¬¡
    batch = super().preprocess_batch(batch)

    # NOTE: add text features
    # å°†æ‰€æœ‰å›¾åƒæ‰¹æ¬¡ä¸­çš„æ–‡æœ¬åˆå¹¶ä¸ºä¸€ä¸ªåˆ—è¡¨
    texts = list(itertools.chain(*batch["texts"]))
    # ä½¿ç”¨ CLIP æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°ä¸å›¾åƒæ‰¹æ¬¡ç›¸åŒçš„è®¾å¤‡ä¸Š
    text_token = self.clip.tokenize(texts).to(batch["img"].device)
    # ä½¿ç”¨æ–‡æœ¬æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶è½¬æ¢ä¸ºä¸å›¾åƒæ‰¹æ¬¡ç›¸åŒçš„æ•°æ®ç±»å‹ï¼ˆtorch.float32ï¼‰
    txt_feats = self.text_model.encode_text(text_token).to(dtype=batch["img"].dtype)
    # å¯¹ç¼–ç åçš„æ–‡æœ¬ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)
    # å°†å¤„ç†åçš„æ–‡æœ¬ç‰¹å¾é‡å¡‘ä¸ºé€‚åˆæ‰¹æ¬¡çš„å½¢çŠ¶ï¼Œå¹¶å­˜å‚¨åœ¨æ‰¹æ¬¡å­—å…¸ä¸­
    batch["txt_feats"] = txt_feats.reshape(len(batch["texts"]), -1, txt_feats.shape[-1])
    # è¿”å›é¢„å¤„ç†åçš„æ‰¹æ¬¡æ•°æ®
    return batch
```