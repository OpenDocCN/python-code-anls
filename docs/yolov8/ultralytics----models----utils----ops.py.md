# `.\yolov8\ultralytics\models\utils\ops.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import torch  # å¯¼å…¥PyTorchåº“
import torch.nn as nn  # å¯¼å…¥PyTorchç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F  # å¯¼å…¥PyTorchå‡½æ•°æ¨¡å—
from scipy.optimize import linear_sum_assignment  # å¯¼å…¥SciPyåº“ä¸­çš„linear_sum_assignmentå‡½æ•°

from ultralytics.utils.metrics import bbox_iou  # å¯¼å…¥Ultralyticså·¥å…·åŒ…ä¸­çš„bbox_iouå‡½æ•°
from ultralytics.utils.ops import xywh2xyxy, xyxy2xywh  # å¯¼å…¥Ultralyticså·¥å…·åŒ…ä¸­çš„åæ ‡è½¬æ¢å‡½æ•°


class HungarianMatcher(nn.Module):
    """
    A module implementing the HungarianMatcher, which is a differentiable module to solve the assignment problem in an
    end-to-end fashion.

    HungarianMatcher performs optimal assignment over the predicted and ground truth bounding boxes using a cost
    function that considers classification scores, bounding box coordinates, and optionally, mask predictions.

    Attributes:
        cost_gain (dict): Dictionary of cost coefficients: 'class', 'bbox', 'giou', 'mask', and 'dice'.
        use_fl (bool): Indicates whether to use Focal Loss for the classification cost calculation.
        with_mask (bool): Indicates whether the model makes mask predictions.
        num_sample_points (int): The number of sample points used in mask cost calculation.
        alpha (float): The alpha factor in Focal Loss calculation.
        gamma (float): The gamma factor in Focal Loss calculation.

    Methods:
        forward(pred_bboxes, pred_scores, gt_bboxes, gt_cls, gt_groups, masks=None, gt_mask=None): Computes the
            assignment between predictions and ground truths for a batch.
        _cost_mask(bs, num_gts, masks=None, gt_mask=None): Computes the mask cost and dice cost if masks are predicted.
    """

    def __init__(self, cost_gain=None, use_fl=True, with_mask=False, num_sample_points=12544, alpha=0.25, gamma=2.0):
        """Initializes HungarianMatcher with cost coefficients, Focal Loss, mask prediction, sample points, and alpha
        gamma factors.
        """
        super().__init__()
        if cost_gain is None:
            cost_gain = {"class": 1, "bbox": 5, "giou": 2, "mask": 1, "dice": 1}
        self.cost_gain = cost_gain  # è®¾ç½®æˆæœ¬ç³»æ•°å­—å…¸ï¼ŒåŒ…æ‹¬'class', 'bbox', 'giou', 'mask', 'dice'
        self.use_fl = use_fl  # æ˜¯å¦ä½¿ç”¨Focal Lossè¿›è¡Œåˆ†ç±»æˆæœ¬è®¡ç®—
        self.with_mask = with_mask  # æ¨¡å‹æ˜¯å¦è¿›è¡Œäº†æ©æ¨¡é¢„æµ‹
        self.num_sample_points = num_sample_points  # æ©æ¨¡æˆæœ¬è®¡ç®—ä¸­ä½¿ç”¨çš„æ ·æœ¬ç‚¹æ•°ç›®
        self.alpha = alpha  # Focal Lossè®¡ç®—ä¸­çš„alphaç³»æ•°
        self.gamma = gamma  # Focal Lossè®¡ç®—ä¸­çš„gammaç³»æ•°

    # This function is for future RT-DETR Segment models
    # def _cost_mask(self, bs, num_gts, masks=None, gt_mask=None):
    #     assert masks is not None and gt_mask is not None, 'Make sure the input has `mask` and `gt_mask`'
    #     # all masks share the same set of points for efficient matching
    #     sample_points = torch.rand([bs, 1, self.num_sample_points, 2])
    #     sample_points = 2.0 * sample_points - 1.0
    #
    #     out_mask = F.grid_sample(masks.detach(), sample_points, align_corners=False).squeeze(-2)
    #     out_mask = out_mask.flatten(0, 1)
    #
    #     tgt_mask = torch.cat(gt_mask).unsqueeze(1)
    #     sample_points = torch.cat([a.repeat(b, 1, 1, 1) for a, b in zip(sample_points, num_gts) if b > 0])
    #     tgt_mask = F.grid_sample(tgt_mask, sample_points, align_corners=False).squeeze([1, 2])
    #
    # ä½¿ç”¨ torch.amp è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œç¦ç”¨ CUDA
    with torch.amp.autocast("cuda", enabled=False):
        # è®¡ç®—äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±
        pos_cost_mask = F.binary_cross_entropy_with_logits(out_mask, torch.ones_like(out_mask), reduction='none')
        neg_cost_mask = F.binary_cross_entropy_with_logits(out_mask, torch.zeros_like(out_mask), reduction='none')
        cost_mask = torch.matmul(pos_cost_mask, tgt_mask.T) + torch.matmul(neg_cost_mask, 1 - tgt_mask.T)
        cost_mask /= self.num_sample_points
        
        # è®¡ç®— Dice æŸå¤±
        out_mask = F.sigmoid(out_mask)
        numerator = 2 * torch.matmul(out_mask, tgt_mask.T)
        denominator = out_mask.sum(-1, keepdim=True) + tgt_mask.sum(-1).unsqueeze(0)
        cost_dice = 1 - (numerator + 1) / (denominator + 1)
        
        # è®¡ç®—æœ€ç»ˆçš„æŸå¤±å‡½æ•° Cï¼Œç»“åˆäºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±å’Œ Dice æŸå¤±ï¼Œæ ¹æ®è®¾å®šçš„æƒé‡
        C = self.cost_gain['mask'] * cost_mask + self.cost_gain['dice'] * cost_dice
    # è¿”å›æœ€ç»ˆçš„æŸå¤± C
    return C
def get_cdn_group(
    batch, num_classes, num_queries, class_embed, num_dn=100, cls_noise_ratio=0.5, box_noise_scale=1.0, training=False
):
    """
    Get contrastive denoising training group. This function creates a contrastive denoising training group with positive
    and negative samples from the ground truths (gt). It applies noise to the class labels and bounding box coordinates,
    and returns the modified labels, bounding boxes, attention mask and meta information.

    Args:
        batch (dict): A dict that includes 'gt_cls' (torch.Tensor with shape [num_gts, ]), 'gt_bboxes'
            (torch.Tensor with shape [num_gts, 4]), 'gt_groups' (List(int)) which is a list of batch size length
            indicating the number of gts of each image.
        num_classes (int): Number of classes.
        num_queries (int): Number of queries.
        class_embed (torch.Tensor): Embedding weights to map class labels to embedding space.
        num_dn (int, optional): Number of denoising. Defaults to 100.
        cls_noise_ratio (float, optional): Noise ratio for class labels. Defaults to 0.5.
        box_noise_scale (float, optional): Noise scale for bounding box coordinates. Defaults to 1.0.
        training (bool, optional): If it's in training mode. Defaults to False.

    Returns:
        (Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Dict]]): The modified class embeddings,
            bounding boxes, attention mask and meta information for denoising. If not in training mode or 'num_dn'
            is less than or equal to 0, the function returns None for all elements in the tuple.
    """

    # å¦‚æœä¸å¤„äºè®­ç»ƒæ¨¡å¼æˆ–è€… num_dn å°äºç­‰äº 0ï¼Œåˆ™è¿”å› None
    if (not training) or num_dn <= 0:
        return None, None, None, None

    # ä» batch ä¸­è·å– gt_groupsï¼Œå³æ¯å¼ å›¾åƒä¸­ gt çš„æ•°é‡åˆ—è¡¨
    gt_groups = batch["gt_groups"]
    # è®¡ç®—æ€»çš„ gt æ•°é‡
    total_num = sum(gt_groups)
    # è·å–ä¸€ä¸ª batch ä¸­æœ€å¤§çš„ gt æ•°é‡
    max_nums = max(gt_groups)
    
    # å¦‚æœæœ€å¤§çš„ gt æ•°é‡ä¸º 0ï¼Œåˆ™è¿”å› None
    if max_nums == 0:
        return None, None, None, None
    
    # è®¡ç®—æ¯ä¸ª group ä¸­çš„æ•°é‡ï¼Œç¡®ä¿è‡³å°‘ä¸º 1
    num_group = num_dn // max_nums
    num_group = 1 if num_group == 0 else num_group
    
    # è·å– batch çš„å¤§å°
    bs = len(gt_groups)
    
    # ä» batch ä¸­è·å– gt_cls å’Œ gt_bbox
    gt_cls = batch["cls"]  # (bs*num, )
    gt_bbox = batch["bboxes"]  # bs*num, 4
    b_idx = batch["batch_idx"]
    
    # æ¯ä¸ª group åŒ…å«æ­£è´Ÿæ ·æœ¬
    dn_cls = gt_cls.repeat(2 * num_group)  # (2*num_group*bs*num, )
    dn_bbox = gt_bbox.repeat(2 * num_group, 1)  # 2*num_group*bs*num, 4
    dn_b_idx = b_idx.repeat(2 * num_group).view(-1)  # (2*num_group*bs*num, )
    
    # åˆ›å»ºè´Ÿæ ·æœ¬çš„ç´¢å¼•
    neg_idx = torch.arange(total_num * num_group, dtype=torch.long, device=gt_bbox.device) + num_group * total_num
    
    # å¦‚æœ cls_noise_ratio å¤§äº 0ï¼Œåˆ™å¯¹ dn_cls åº”ç”¨å™ªå£°
    if cls_noise_ratio > 0:
        # ç”Ÿæˆä¸€ä¸ªæ©ç ï¼Œä»¥åŠæ¦‚ç‡åº”ç”¨äº bbox
        mask = torch.rand(dn_cls.shape) < (cls_noise_ratio * 0.5)
        idx = torch.nonzero(mask).squeeze(-1)
        # éšæœºç”Ÿæˆæ–°çš„æ ‡ç­¾
        new_label = torch.randint_like(idx, 0, num_classes, dtype=dn_cls.dtype, device=dn_cls.device)
        dn_cls[idx] = new_label
    # å¦‚æœç›’å­å™ªå£°æ¯”ä¾‹å¤§äº0ï¼Œåˆ™è¿›è¡Œä»¥ä¸‹æ“ä½œ
    known_bbox = xywh2xyxy(dn_bbox)  # å°†ç›¸å¯¹åæ ‡è½¬æ¢ä¸ºç»å¯¹åæ ‡æ ¼å¼

    # è®¡ç®—éšæœºæ‰°åŠ¨çš„å¤§å°
    diff = (dn_bbox[..., 2:] * 0.5).repeat(1, 2) * box_noise_scale  # 2*num_group*bs*num, 4

    # ç”Ÿæˆéšæœºç¬¦å·å’Œéšæœºéƒ¨åˆ†
    rand_sign = torch.randint_like(dn_bbox, 0, 2) * 2.0 - 1.0
    rand_part = torch.rand_like(dn_bbox)
    rand_part[neg_idx] += 1.0
    rand_part *= rand_sign

    # æ·»åŠ éšæœºæ‰°åŠ¨åˆ°å·²çŸ¥çš„è¾¹ç•Œæ¡†
    known_bbox += rand_part * diff

    # å°†è¾¹ç•Œæ¡†è£å‰ªåˆ°0åˆ°1çš„èŒƒå›´å†…
    known_bbox.clip_(min=0.0, max=1.0)

    # å°†ç»å¯¹åæ ‡æ ¼å¼çš„è¾¹ç•Œæ¡†è½¬æ¢å›ç›¸å¯¹åæ ‡æ ¼å¼
    dn_bbox = xyxy2xywh(known_bbox)

    # å¯¹ç›¸å¯¹åæ ‡è¿›è¡Œé€†sigmoidå˜æ¢
    dn_bbox = torch.logit(dn_bbox, eps=1e-6)

num_dn = int(max_nums * 2 * num_group)  # è®¡ç®—æ€»çš„å»å™ªæŸ¥è¯¢æ•°

# åˆ›å»ºå¡«å……çš„ç±»åˆ«åµŒå…¥å’Œè¾¹ç•Œæ¡†
dn_cls_embed = class_embed[dn_cls]  # bs*num * 2 * num_group, 256
padding_cls = torch.zeros(bs, num_dn, dn_cls_embed.shape[-1], device=gt_cls.device)
padding_bbox = torch.zeros(bs, num_dn, 4, device=gt_bbox.device)

# æ„å»ºæ˜ å°„ç´¢å¼•ç”¨äºå¯¹é½å»å™ªåçš„æŸ¥è¯¢ä¸åŸå§‹æŸ¥è¯¢
map_indices = torch.cat([torch.tensor(range(num), dtype=torch.long) for num in gt_groups])
pos_idx = torch.stack([map_indices + max_nums * i for i in range(num_group)], dim=0)

map_indices = torch.cat([map_indices + max_nums * i for i in range(2 * num_group)])

# å°†ç±»åˆ«åµŒå…¥å’Œè¾¹ç•Œæ¡†å¡«å……åˆ°å¡«å……å¼ é‡ä¸­
padding_cls[(dn_b_idx, map_indices)] = dn_cls_embed
padding_bbox[(dn_b_idx, map_indices)] = dn_bbox

tgt_size = num_dn + num_queries  # è®¡ç®—ç›®æ ‡çš„æ€»å¤§å°
attn_mask = torch.zeros([tgt_size, tgt_size], dtype=torch.bool)  # åˆ›å»ºæ³¨æ„åŠ›æ©ç 

# è®¾å®šæŸ¥è¯¢ä¸é‡æ„ä¹‹é—´çš„åŒ¹é…ä¸èƒ½çœ‹åˆ°
attn_mask[num_dn:, :num_dn] = True

# è®¾å®šé‡æ„ä¹‹é—´ç›¸äº’ä¸èƒ½çœ‹åˆ°
for i in range(num_group):
    if i == 0:
        attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), max_nums * 2 * (i + 1) : num_dn] = True
    if i == num_group - 1:
        attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), : max_nums * i * 2] = True
    else:
        attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), max_nums * 2 * (i + 1) : num_dn] = True
        attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), : max_nums * 2 * i] = True

# æ„å»ºå»å™ªä»»åŠ¡çš„å…ƒä¿¡æ¯å­—å…¸
dn_meta = {
    "dn_pos_idx": [p.reshape(-1) for p in pos_idx.cpu().split(list(gt_groups), dim=1)],
    "dn_num_group": num_group,
    "dn_num_split": [num_dn, num_queries],
}

# è¿”å›ç»“æœ
return (
    padding_cls.to(class_embed.device),
    padding_bbox.to(class_embed.device),
    attn_mask.to(class_embed.device),
    dn_meta,
)
```