# `.\yolov8\ultralytics\utils\benchmarks.py`

```py
# ä» glob æ¨¡å—ä¸­å¯¼å…¥ glob å‡½æ•°ï¼Œç”¨äºæ–‡ä»¶è·¯å¾„çš„æ¨¡ç³ŠåŒ¹é…
import glob
# å¯¼å…¥ os æ¨¡å—ï¼Œæä¾›äº†è®¸å¤šä¸æ“ä½œç³»ç»Ÿäº¤äº’çš„å‡½æ•°
import os
# å¯¼å…¥ platform æ¨¡å—ï¼Œç”¨äºè·å–ç³»ç»Ÿå¹³å°ä¿¡æ¯
import platform
# å¯¼å…¥ re æ¨¡å—ï¼Œæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼æ“ä½œ
import re
# å¯¼å…¥ shutil æ¨¡å—ï¼Œæä¾›äº†é«˜çº§çš„æ–‡ä»¶æ“ä½œåŠŸèƒ½
import shutil
# å¯¼å…¥ time æ¨¡å—ï¼Œæä¾›æ—¶é—´ç›¸å…³çš„åŠŸèƒ½
import time
# ä» pathlib æ¨¡å—ä¸­å¯¼å…¥ Path ç±»ï¼Œç”¨äºæ“ä½œæ–‡ä»¶è·¯å¾„
from pathlib import Path

# å¯¼å…¥ numpy åº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—
import numpy as np
# å¯¼å…¥ torch.cuda æ¨¡å—ï¼Œç”¨äº CUDA ç›¸å…³æ“ä½œ
import torch.cuda
# å¯¼å…¥ yaml åº“ï¼Œç”¨äºå¤„ç† YAML æ ¼å¼çš„æ–‡ä»¶
import yaml

# ä» ultralytics åŒ…ä¸­å¯¼å…¥ YOLO å’Œ YOLOWorld ç±»
from ultralytics import YOLO, YOLOWorld
# ä» ultralytics.cfg æ¨¡å—ä¸­å¯¼å…¥ TASK2DATA å’Œ TASK2METRIC å˜é‡
from ultralytics.cfg import TASK2DATA, TASK2METRIC
# ä» ultralytics.engine.exporter æ¨¡å—ä¸­å¯¼å…¥ export_formats å‡½æ•°
from ultralytics.engine.exporter import export_formats
# ä» ultralytics.utils æ¨¡å—ä¸­å¯¼å…¥ ARM64, ASSETS, IS_JETSON, IS_RASPBERRYPI ç­‰å˜é‡
from ultralytics.utils import ARM64, ASSETS, IS_JETSON, IS_RASPBERRYPI, LINUX, LOGGER, MACOS, TQDM, WEIGHTS_DIR
# ä» ultralytics.utils.checks æ¨¡å—ä¸­å¯¼å…¥ IS_PYTHON_3_12, check_requirements, check_yolo ç­‰å‡½æ•°å’Œå˜é‡
from ultralytics.utils.checks import IS_PYTHON_3_12, check_requirements, check_yolo
# ä» ultralytics.utils.downloads æ¨¡å—ä¸­å¯¼å…¥ safe_download å‡½æ•°
from ultralytics.utils.downloads import safe_download
# ä» ultralytics.utils.files æ¨¡å—ä¸­å¯¼å…¥ file_size å‡½æ•°
from ultralytics.utils.files import file_size
# ä» ultralytics.utils.torch_utils æ¨¡å—ä¸­å¯¼å…¥ select_device å‡½æ•°
from ultralytics.utils.torch_utils import select_device


def benchmark(
    model=WEIGHTS_DIR / "yolov8n.pt", data=None, imgsz=160, half=False, int8=False, device="cpu", verbose=False
):
    """
    Benchmark a YOLO model across different formats for speed and accuracy.

    Args:
        model (str | Path | optional): Path to the model file or directory. Default is
            Path(SETTINGS['weights_dir']) / 'yolov8n.pt'.
        data (str, optional): Dataset to evaluate on, inherited from TASK2DATA if not passed. Default is None.
        imgsz (int, optional): Image size for the benchmark. Default is 160.
        half (bool, optional): Use half-precision for the model if True. Default is False.
        int8 (bool, optional): Use int8-precision for the model if True. Default is False.
        device (str, optional): Device to run the benchmark on, either 'cpu' or 'cuda'. Default is 'cpu'.
        verbose (bool | float | optional): If True or a float, assert benchmarks pass with given metric.
            Default is False.
    """
    # å‡½æ•°ä¸»ä½“ï¼Œç”¨äºè¯„ä¼° YOLO æ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œå‚æ•°è¯¦ç»†è¯´æ˜åœ¨å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å‡º
    pass  # è¿™é‡Œæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä»£ç ä¼šåœ¨æ­¤åŸºç¡€ä¸Šç»§ç»­å¼€å‘
    def benchmark(model='yolov8n.pt', imgsz=640):
        """
        Benchmark function to evaluate model performance.
    
        Args:
            model (str or Path): Path to the model checkpoint.
            imgsz (int): Image size for inference.
    
        Returns:
            df (pandas.DataFrame): A pandas DataFrame with benchmark results for each format, including file size,
                metric, and inference time.
    
        Example:
            ```python
            from ultralytics.utils.benchmarks import benchmark
    
            benchmark(model='yolov8n.pt', imgsz=640)
            ```
        """
        import pandas as pd  # Import pandas library for DataFrame operations
        pd.options.display.max_columns = 10  # Set maximum display columns in pandas DataFrame
        pd.options.display.width = 120  # Set display width for pandas DataFrame
    
        device = select_device(device, verbose=False)  # Select device for model inference
        if isinstance(model, (str, Path)):
            model = YOLO(model)  # Initialize YOLO model if model is given as a string or Path
    
        is_end2end = getattr(model.model.model[-1], "end2end", False)  # Check if model supports end-to-end inference
    
        y = []  # Initialize an empty list to store benchmark results
        t0 = time.time()  # Record current time for benchmarking purposes
    
        check_yolo(device=device)  # Print system information relevant to YOLO
    
        # Create a pandas DataFrame 'df' with columns defined for benchmark results
        df = pd.DataFrame(y, columns=["Format", "Statusâ”", "Size (MB)", key, "Inference time (ms/im)", "FPS"])
    
        name = Path(model.ckpt_path).name  # Extract the name of the model checkpoint file
        # Construct a string 's' summarizing benchmark results and logging information
        s = f"\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({time.time() - t0:.2f}s)\n{df}\n"
        LOGGER.info(s)  # Log 's' to the logger file
    
        with open("benchmarks.log", "a", errors="ignore", encoding="utf-8") as f:
            f.write(s)  # Append string 's' to the 'benchmarks.log' file
    
        if verbose and isinstance(verbose, float):
            metrics = df[key].array  # Extract the 'key' column values from the DataFrame 'df'
            floor = verbose  # Set the minimum metric floor to compare against
            # Assert that all metrics are greater than 'floor' if they are not NaN
            assert all(x > floor for x in metrics if pd.notna(x)), f"Benchmark failure: metric(s) < floor {floor}"
    
        return df  # Return the pandas DataFrame 'df' containing benchmark results
class RF100Benchmark:
    """Benchmark YOLO model performance across formats for speed and accuracy."""

    def __init__(self):
        """Function for initialization of RF100Benchmark."""
        # åˆå§‹åŒ–ç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ•°æ®é›†åç§°
        self.ds_names = []
        # åˆå§‹åŒ–ç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        self.ds_cfg_list = []
        # åˆå§‹åŒ– RF å¯¹è±¡ä¸º None
        self.rf = None
        # å®šä¹‰éªŒè¯æŒ‡æ ‡åˆ—è¡¨
        self.val_metrics = ["class", "images", "targets", "precision", "recall", "map50", "map95"]

    def set_key(self, api_key):
        """
        Set Roboflow API key for processing.

        Args:
            api_key (str): The API key.
        """
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ Roboflow ç›¸å…³çš„ä¾èµ–
        check_requirements("roboflow")
        # å¯¼å…¥ Roboflow æ¨¡å—
        from roboflow import Roboflow
        # åˆ›å»º Roboflow å¯¹è±¡å¹¶è®¾ç½® API å¯†é’¥
        self.rf = Roboflow(api_key=api_key)

    def parse_dataset(self, ds_link_txt="datasets_links.txt"):
        """
        Parse dataset links and downloads datasets.

        Args:
            ds_link_txt (str): Path to dataset_links file.
        """
        # å¦‚æœå­˜åœ¨ rf-100 ç›®å½•ï¼Œåˆ™åˆ é™¤å¹¶é‡æ–°åˆ›å»ºï¼›å¦åˆ™ç›´æ¥åˆ›å»º
        (shutil.rmtree("rf-100"), os.mkdir("rf-100")) if os.path.exists("rf-100") else os.mkdir("rf-100")
        # åˆ‡æ¢å½“å‰å·¥ä½œç›®å½•è‡³ rf-100
        os.chdir("rf-100")
        # åœ¨ rf-100 ç›®å½•ä¸‹åˆ›å»º ultralytics-benchmarks ç›®å½•
        os.mkdir("ultralytics-benchmarks")
        # å®‰å…¨ä¸‹è½½ datasets_links.txt æ–‡ä»¶
        safe_download("https://github.com/ultralytics/assets/releases/download/v0.0.0/datasets_links.txt")

        # æ‰“å¼€æ•°æ®é›†é“¾æ¥æ–‡ä»¶ï¼Œé€è¡Œå¤„ç†
        with open(ds_link_txt, "r") as file:
            for line in file:
                try:
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†æ•°æ®é›†é“¾æ¥
                    _, url, workspace, project, version = re.split("/+", line.strip())
                    # å°†é¡¹ç›®åç§°æ·»åŠ åˆ°æ•°æ®é›†åç§°åˆ—è¡¨
                    self.ds_names.append(project)
                    # ç»„åˆé¡¹ç›®å’Œç‰ˆæœ¬ä¿¡æ¯
                    proj_version = f"{project}-{version}"
                    # å¦‚æœè¯¥ç‰ˆæœ¬æ•°æ®é›†å°šæœªä¸‹è½½ï¼Œåˆ™ä½¿ç”¨ Roboflow å¯¹è±¡ä¸‹è½½åˆ° yolov8 ç›®å½•ä¸‹
                    if not Path(proj_version).exists():
                        self.rf.workspace(workspace).project(project).version(version).download("yolov8")
                    else:
                        print("Dataset already downloaded.")
                    # æ·»åŠ æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„åˆ°åˆ—è¡¨ä¸­
                    self.ds_cfg_list.append(Path.cwd() / proj_version / "data.yaml")
                except Exception:
                    continue

        return self.ds_names, self.ds_cfg_list

    @staticmethod
    def fix_yaml(path):
        """
        Function to fix YAML train and val path.

        Args:
            path (str): YAML file path.
        """
        # ä½¿ç”¨å®‰å…¨åŠ è½½æ–¹å¼è¯»å– YAML æ–‡ä»¶
        with open(path, "r") as file:
            yaml_data = yaml.safe_load(file)
        # ä¿®æ”¹ YAML æ–‡ä»¶ä¸­çš„è®­ç»ƒå’ŒéªŒè¯è·¯å¾„
        yaml_data["train"] = "train/images"
        yaml_data["val"] = "valid/images"
        # ä½¿ç”¨å®‰å…¨å†™å…¥æ–¹å¼å°†ä¿®æ”¹åçš„ YAML æ•°æ®å†™å›æ–‡ä»¶
        with open(path, "w") as file:
            yaml.safe_dump(yaml_data, file)
    def evaluate(self, yaml_path, val_log_file, eval_log_file, list_ind):
        """
        Model evaluation on validation results.

        Args:
            yaml_path (str): YAML file path.
            val_log_file (str): val_log_file path.
            eval_log_file (str): eval_log_file path.
            list_ind (int): Index for current dataset.
        """
        # å®šä¹‰è·³è¿‡çš„ç¬¦å·åˆ—è¡¨ï¼Œè¿™äº›ç¬¦å·å‡ºç°åœ¨æ—¥å¿—è¡Œä¸­æ—¶å°†è¢«è·³è¿‡
        skip_symbols = ["", "âš ï¸", "ğŸ’¡", "âŒ"]
        
        # ä» YAML æ–‡ä»¶ä¸­è¯»å–ç±»åˆ«åç§°åˆ—è¡¨
        with open(yaml_path) as stream:
            class_names = yaml.safe_load(stream)["names"]
        
        # æ‰“å¼€éªŒè¯æ—¥å¿—æ–‡ä»¶ï¼Œè¯»å–å…¶ä¸­çš„æ‰€æœ‰è¡Œ
        with open(val_log_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
            eval_lines = []
            
            # éå†æ¯ä¸€è¡Œæ—¥å¿—
            for line in lines:
                # å¦‚æœæ—¥å¿—è¡ŒåŒ…å«éœ€è¦è·³è¿‡çš„ç¬¦å·ï¼Œåˆ™è·³è¿‡æ­¤è¡Œ
                if any(symbol in line for symbol in skip_symbols):
                    continue
                
                # å°†æ¯è¡Œæ—¥å¿—æŒ‰ç©ºæ ¼åˆ†éš”ä¸ºæ¡ç›®åˆ—è¡¨
                entries = line.split(" ")
                # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å¹¶å»é™¤æ¯ä¸ªæ¡ç›®ç»“å°¾çš„æ¢è¡Œç¬¦
                entries = list(filter(lambda val: val != "", entries))
                entries = [e.strip("\n") for e in entries]
                
                # å°†ç¬¦åˆæ¡ä»¶çš„æ¡ç›®åŠ å…¥åˆ°è¯„ä¼°ç»“æœåˆ—è¡¨ä¸­
                eval_lines.extend(
                    {
                        "class": entries[0],
                        "images": entries[1],
                        "targets": entries[2],
                        "precision": entries[3],
                        "recall": entries[4],
                        "map50": entries[5],
                        "map95": entries[6],
                    }
                    for e in entries
                    if e in class_names or (e == "all" and "(AP)" not in entries and "(AR)" not in entries)
                )
        
        # åˆå§‹åŒ– map_val å˜é‡ä¸º 0.0
        map_val = 0.0
        
        # å¦‚æœè¯„ä¼°ç»“æœåˆ—è¡¨ä¸­æ¡ç›®æ•°é‡å¤§äº 1ï¼Œåˆ™è¿›è¡Œä¸‹åˆ—æ“ä½œ
        if len(eval_lines) > 1:
            print("There's more dicts")
            # éå†è¯„ä¼°ç»“æœåˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªå­—å…¸
            for lst in eval_lines:
                # å¦‚æœå½“å‰å­—å…¸çš„ç±»åˆ«ä¸º "all"ï¼Œåˆ™å°† map_val è®¾ç½®ä¸ºå…¶ map50 å€¼
                if lst["class"] == "all":
                    map_val = lst["map50"]
        else:
            print("There's only one dict res")
            # å¦åˆ™ï¼Œå¦‚æœè¯„ä¼°ç»“æœåˆ—è¡¨ä¸­åªæœ‰ä¸€ä¸ªå­—å…¸ï¼Œåˆ™å°† map_val è®¾ç½®ä¸ºç¬¬ä¸€ä¸ªå­—å…¸çš„ map50 å€¼
            map_val = [res["map50"] for res in eval_lines][0]
        
        # å°†ç»“æœå†™å…¥è¯„ä¼°æ—¥å¿—æ–‡ä»¶ä¸­ï¼Œæ ¼å¼ä¸º "<æ•°æ®é›†åç§°>: <map_val>"
        with open(eval_log_file, "a") as f:
            f.write(f"{self.ds_names[list_ind]}: {map_val}\n")
    """
    ProfileModels class for profiling different models on ONNX and TensorRT.

    This class profiles the performance of different models, returning results such as model speed and FLOPs.

    Attributes:
        paths (list): Paths of the models to profile.
        num_timed_runs (int): Number of timed runs for the profiling. Default is 100.
        num_warmup_runs (int): Number of warmup runs before profiling. Default is 10.
        min_time (float): Minimum number of seconds to profile for. Default is 60.
        imgsz (int): Image size used in the models. Default is 640.
        half (bool): Flag indicating whether to use half-precision floating point for profiling. Default is True.
        trt (bool): Flag indicating whether to use TensorRT for profiling. Default is True.
        device (torch.device): Device used for profiling. Automatically determined if None.

    Methods:
        profile(): Profiles the models and prints the result.

    Example:
        ```py
        from ultralytics.utils.benchmarks import ProfileModels

        ProfileModels(['yolov8n.yaml', 'yolov8s.yaml'], imgsz=640).profile()
        ```
    """

    def __init__(
        self,
        paths: list,
        num_timed_runs=100,
        num_warmup_runs=10,
        min_time=60,
        imgsz=640,
        half=True,
        trt=True,
        device=None,
    ):
        """
        Initialize the ProfileModels class for profiling models.

        Args:
            paths (list): List of paths of the models to be profiled.
            num_timed_runs (int, optional): Number of timed runs for the profiling. Default is 100.
            num_warmup_runs (int, optional): Number of warmup runs before the actual profiling starts. Default is 10.
            min_time (float, optional): Minimum time in seconds for profiling a model. Default is 60.
            imgsz (int, optional): Size of the image used during profiling. Default is 640.
            half (bool, optional): Flag to indicate whether to use half-precision floating point for profiling. Default is True.
            trt (bool, optional): Flag to indicate whether to profile using TensorRT. Default is True.
            device (torch.device, optional): Device used for profiling. If None, it is determined automatically.
        """
        # åˆå§‹åŒ–å„ä¸ªå±æ€§ï¼Œç”¨äºå­˜å‚¨ä¼ å…¥çš„å‚æ•°å’Œè®¾ç½®é»˜è®¤å€¼
        self.paths = paths
        self.num_timed_runs = num_timed_runs
        self.num_warmup_runs = num_warmup_runs
        self.min_time = min_time
        self.imgsz = imgsz
        self.half = half
        self.trt = trt  # æ˜¯å¦è¿è¡Œ TensorRT çš„æ€§èƒ½åˆ†æ
        # å¦‚æœ device ä¸º Noneï¼Œåˆ™è‡ªåŠ¨ç¡®å®šä½¿ç”¨çš„è®¾å¤‡
        self.device = device or torch.device(0 if torch.cuda.is_available() else "cpu")
    def profile(self):
        """
        Logs the benchmarking results of a model, checks metrics against floor and returns the results.
        """
        # è·å–æ‰€æœ‰ç›¸å…³æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        files = self.get_files()

        if not files:
            # è‹¥æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ *.pt æˆ– *.onnx æ–‡ä»¶ï¼Œåˆ™æ‰“å°æ¶ˆæ¯å¹¶è¿”å›
            print("No matching *.pt or *.onnx files found.")
            return

        table_rows = []
        output = []
        for file in files:
            # ç”Ÿæˆå¼•æ“æ–‡ä»¶åï¼ˆåç¼€ä¸º .engineï¼‰
            engine_file = file.with_suffix(".engine")
            if file.suffix in {".pt", ".yaml", ".yml"}:
                # å¦‚æœæ–‡ä»¶åç¼€æ˜¯ .pt, .yaml æˆ– .ymlï¼Œåˆ›å»º YOLO æ¨¡å‹å¯¹è±¡
                model = YOLO(str(file))
                model.fuse()  # æ‰§è¡Œæ¨¡å‹èåˆæ“ä½œï¼Œä»¥è·å–æ­£ç¡®çš„å‚æ•°å’ŒGFLOPsï¼ˆåœ¨ model.info() ä¸­ï¼‰
                model_info = model.info()
                if self.trt and self.device.type != "cpu" and not engine_file.is_file():
                    # å¦‚æœå¯ç”¨ TensorRTï¼ˆself.trtï¼‰ï¼Œä¸”è®¾å¤‡ç±»å‹ä¸æ˜¯ CPUï¼Œå¹¶ä¸”å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™å¯¼å‡ºä¸ºå¼•æ“æ–‡ä»¶
                    engine_file = model.export(
                        format="engine", half=self.half, imgsz=self.imgsz, device=self.device, verbose=False
                    )
                # å¯¼å‡º ONNX æ–‡ä»¶
                onnx_file = model.export(
                    format="onnx", half=self.half, imgsz=self.imgsz, simplify=True, device=self.device, verbose=False
                )
            elif file.suffix == ".onnx":
                # å¦‚æœæ–‡ä»¶åç¼€æ˜¯ .onnxï¼Œè·å– ONNX æ¨¡å‹ä¿¡æ¯
                model_info = self.get_onnx_model_info(file)
                onnx_file = file
            else:
                continue

            # å¯¹ TensorRT æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æ
            t_engine = self.profile_tensorrt_model(str(engine_file))
            # å¯¹ ONNX æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æ
            t_onnx = self.profile_onnx_model(str(onnx_file))
            # ç”Ÿæˆè¡¨æ ¼è¡Œæ•°æ®å¹¶æ·»åŠ åˆ°åˆ—è¡¨
            table_rows.append(self.generate_table_row(file.stem, t_onnx, t_engine, model_info))
            # ç”Ÿæˆç»“æœå­—å…¸å¹¶æ·»åŠ åˆ°è¾“å‡ºåˆ—è¡¨
            output.append(self.generate_results_dict(file.stem, t_onnx, t_engine, model_info))

        # æ‰“å°è¡¨æ ¼
        self.print_table(table_rows)
        # è¿”å›ç»“æœè¾“å‡ºåˆ—è¡¨
        return output

    def get_files(self):
        """
        Returns a list of paths for all relevant model files given by the user.
        """
        # åˆå§‹åŒ–æ–‡ä»¶åˆ—è¡¨
        files = []
        for path in self.paths:
            path = Path(path)
            if path.is_dir():
                # å¦‚æœè·¯å¾„æ˜¯ç›®å½•ï¼Œåˆ™è·å–ç›®å½•ä¸‹æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„
                extensions = ["*.pt", "*.onnx", "*.yaml"]
                files.extend([file for ext in extensions for file in glob.glob(str(path / ext))])
            elif path.suffix in {".pt", ".yaml", ".yml"}:  # add non-existing
                # å¦‚æœè·¯å¾„æ˜¯æ–‡ä»¶ä¸”åç¼€ç¬¦åˆæ¡ä»¶ï¼Œç›´æ¥æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨ä¸­
                files.append(str(path))
            else:
                # å¦åˆ™ï¼Œè·å–è·¯å¾„ä¸‹æ‰€æœ‰æ–‡ä»¶è·¯å¾„å¹¶æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨ä¸­
                files.extend(glob.glob(str(path)))

        # æ‰“å°æ­£åœ¨åˆ†æçš„æ–‡ä»¶åˆ—è¡¨
        print(f"Profiling: {sorted(files)}")
        # è¿”å›è·¯å¾„å¯¹è±¡åˆ—è¡¨
        return [Path(file) for file in sorted(files)]

    def get_onnx_model_info(self, onnx_file: str):
        """
        Retrieves the information including number of layers, parameters, gradients and FLOPs for an ONNX model
        file.
        """
        # æš‚æ—¶è¿”å›é›¶å€¼è¡¨ç¤ºä¿¡æ¯è·å–æœªå®ç°
        return 0.0, 0.0, 0.0, 0.0  # return (num_layers, num_params, num_gradients, num_flops)
    def iterative_sigma_clipping(data, sigma=2, max_iters=3):
        """Applies an iterative sigma clipping algorithm to the given data."""
        # å°†æ•°æ®è½¬æ¢ä¸º NumPy æ•°ç»„
        data = np.array(data)
        # æ‰§è¡Œæœ€å¤§è¿­ä»£æ¬¡æ•°çš„å¾ªç¯
        for _ in range(max_iters):
            # è®¡ç®—æ•°æ®çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
            mean, std = np.mean(data), np.std(data)
            # æ ¹æ®å‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œ sigma å‰ªåˆ‡ï¼Œå¹¶è·å–å‰ªåˆ‡åçš„æ•°æ®
            clipped_data = data[(data > mean - sigma * std) & (data < mean + sigma * std)]
            # å¦‚æœå‰ªåˆ‡åçš„æ•°æ®å’ŒåŸæ•°æ®é•¿åº¦ç›¸åŒï¼Œåˆ™é€€å‡ºå¾ªç¯
            if len(clipped_data) == len(data):
                break
            # æ›´æ–°æ•°æ®ä¸ºå‰ªåˆ‡åçš„æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£
            data = clipped_data
        # è¿”å›æœ€ç»ˆå‰ªåˆ‡åçš„æ•°æ®
        return data

    def profile_tensorrt_model(self, engine_file: str, eps: float = 1e-3):
        """Profiles the TensorRT model, measuring average run time and standard deviation among runs."""
        # å¦‚æœ TensorRT æœªåˆå§‹åŒ–æˆ–è€…å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›é»˜è®¤å€¼
        if not self.trt or not Path(engine_file).is_file():
            return 0.0, 0.0

        # åˆå§‹åŒ–æ¨¡å‹å’Œè¾“å…¥æ•°æ®
        model = YOLO(engine_file)
        input_data = np.random.rand(self.imgsz, self.imgsz, 3).astype(np.float32)  # å¿…é¡»æ˜¯ FP32

        # é¢„çƒ­è¿è¡Œ
        elapsed = 0.0
        for _ in range(3):
            start_time = time.time()
            for _ in range(self.num_warmup_runs):
                model(input_data, imgsz=self.imgsz, verbose=False)
            elapsed = time.time() - start_time

        # è®¡ç®—è¿è¡Œæ¬¡æ•°ï¼Œå–æœ€å¤§å€¼ä½œä¸º min_time æˆ– num_timed_runs çš„å€æ•°
        num_runs = max(round(self.min_time / (elapsed + eps) * self.num_warmup_runs), self.num_timed_runs * 50)

        # è®¡æ—¶è¿è¡Œ
        run_times = []
        for _ in TQDM(range(num_runs), desc=engine_file):
            results = model(input_data, imgsz=self.imgsz, verbose=False)
            # æå–æ¨ç†é€Ÿåº¦å¹¶è½¬æ¢ä¸ºæ¯«ç§’
            run_times.append(results[0].speed["inference"])

        # å¯¹è¿è¡Œæ—¶é—´è¿›è¡Œ sigma å‰ªåˆ‡
        run_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=3)
        # è¿”å›è¿è¡Œæ—¶é—´çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
        return np.mean(run_times), np.std(run_times)
    def profile_onnx_model(self, onnx_file: str, eps: float = 1e-3):
        """Profiles an ONNX model by executing it multiple times and returns the mean and standard deviation of run
        times.
        """
        # æ£€æŸ¥è¿è¡Œç¯å¢ƒæ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Œç¡®ä¿å®‰è£…äº†'onnxruntime'åº“
        check_requirements("onnxruntime")
        import onnxruntime as ort

        # åˆ›å»ºä¼šè¯é€‰é¡¹å¯¹è±¡ï¼Œå¹¶è®¾ç½®å›¾ä¼˜åŒ–çº§åˆ«ä¸ºæœ€å¤§ï¼ŒåŒæ—¶é™åˆ¶çº¿ç¨‹æ•°ä¸º8
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.intra_op_num_threads = 8  # é™åˆ¶å¹¶è¡Œæ‰§è¡Œçš„çº¿ç¨‹æ•°ç›®

        # åˆ›å»º ONNX æ¨ç†ä¼šè¯å¯¹è±¡ï¼ŒæŒ‡å®šä½¿ç”¨CPUæ‰§è¡Œæä¾›è€…
        sess = ort.InferenceSession(onnx_file, sess_options, providers=["CPUExecutionProvider"])

        # è·å–æ¨¡å‹è¾“å…¥å¼ é‡ä¿¡æ¯
        input_tensor = sess.get_inputs()[0]
        input_type = input_tensor.type
        # æ£€æŸ¥è¾“å…¥å¼ é‡æ˜¯å¦å…·æœ‰åŠ¨æ€å½¢çŠ¶
        dynamic = not all(isinstance(dim, int) and dim >= 0 for dim in input_tensor.shape)
        # æ ¹æ®åŠ¨æ€å½¢çŠ¶è®¾ç½®è¾“å…¥å¼ é‡çš„å½¢çŠ¶
        input_shape = (1, 3, self.imgsz, self.imgsz) if dynamic else input_tensor.shape

        # å°†ONNXæ•°æ®ç±»å‹æ˜ å°„åˆ°numpyæ•°æ®ç±»å‹
        if "float16" in input_type:
            input_dtype = np.float16
        elif "float" in input_type:
            input_dtype = np.float32
        elif "double" in input_type:
            input_dtype = np.float64
        elif "int64" in input_type:
            input_dtype = np.int64
        elif "int32" in input_type:
            input_dtype = np.int32
        else:
            raise ValueError(f"Unsupported ONNX datatype {input_type}")

        # ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®ï¼Œä»¥è¾“å…¥å¼ é‡çš„å½¢çŠ¶å’Œæ•°æ®ç±»å‹ä¸ºåŸºç¡€
        input_data = np.random.rand(*input_shape).astype(input_dtype)
        input_name = input_tensor.name
        output_name = sess.get_outputs()[0].name

        # é¢„çƒ­è¿è¡Œï¼Œæ‰§è¡Œè‹¥å¹²æ¬¡ï¼Œè®¡ç®—å¹³å‡æ—¶é—´
        elapsed = 0.0
        for _ in range(3):
            start_time = time.time()
            for _ in range(self.num_warmup_runs):
                sess.run([output_name], {input_name: input_data})
            elapsed = time.time() - start_time

        # è®¡ç®—éœ€è¦è¿è¡Œçš„æ€»æ¬¡æ•°ï¼Œç¡®ä¿æ»¡è¶³æœ€å°æ—¶é—´è¦æ±‚æˆ–æŒ‡å®šçš„è¿è¡Œæ¬¡æ•°
        num_runs = max(round(self.min_time / (elapsed + eps) * self.num_warmup_runs), self.num_timed_runs)

        # æ­£å¼è®¡æ—¶è¿è¡Œ
        run_times = []
        for _ in TQDM(range(num_runs), desc=onnx_file):
            start_time = time.time()
            sess.run([output_name], {input_name: input_data})
            run_times.append((time.time() - start_time) * 1000)  # å°†è¿è¡Œæ—¶é—´è½¬æ¢ä¸ºæ¯«ç§’

        # å¯¹è¿è¡Œæ—¶é—´è¿›è¡Œè¿­ä»£çš„sigmaå‰ªè£
        run_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=5)
        # è¿”å›è¿è¡Œæ—¶é—´çš„å‡å€¼å’Œæ ‡å‡†å·®ä½œä¸ºæ€§èƒ½åˆ†æç»“æœ
        return np.mean(run_times), np.std(run_times)
    # ç”ŸæˆåŒ…å«æ¨¡å‹æ€§èƒ½å’ŒæŒ‡æ ‡è¯¦æƒ…çš„è¡¨æ ¼è¡Œçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    def generate_table_row(self, model_name, t_onnx, t_engine, model_info):
        """Generates a formatted string for a table row that includes model performance and metric details."""
        layers, params, gradients, flops = model_info
        return (
            f"| {model_name:18s} | {self.imgsz} | - | {t_onnx[0]:.2f} Â± {t_onnx[1]:.2f} ms | {t_engine[0]:.2f} Â± "
            f"{t_engine[1]:.2f} ms | {params / 1e6:.1f} | {flops:.1f} |"
        )

    @staticmethod
    # ç”ŸæˆåŒ…å«æ¨¡å‹åç§°ã€å‚æ•°ã€GFLOPSå’Œé€Ÿåº¦æŒ‡æ ‡çš„å­—å…¸
    def generate_results_dict(model_name, t_onnx, t_engine, model_info):
        """Generates a dictionary of model details including name, parameters, GFLOPS and speed metrics."""
        layers, params, gradients, flops = model_info
        return {
            "model/name": model_name,
            "model/parameters": params,
            "model/GFLOPs": round(flops, 3),
            "model/speed_ONNX(ms)": round(t_onnx[0], 3),
            "model/speed_TensorRT(ms)": round(t_engine[0], 3),
        }

    @staticmethod
    # æ ¼å¼åŒ–å¹¶æ‰“å°åŒ…å«ä¸åŒæ¨¡å‹ç»Ÿè®¡å’Œæ€§èƒ½æ•°æ®çš„æ¯”è¾ƒè¡¨æ ¼
    def print_table(table_rows):
        """Formats and prints a comparison table for different models with given statistics and performance data."""
        gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "GPU"
        header = (
            f"| Model | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | "
            f"Speed<br><sup>{gpu} TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |"
        )
        separator = (
            "|-------------|---------------------|--------------------|------------------------------|"
            "-----------------------------------|------------------|-----------------|"
        )

        # æ‰“å°è¡¨æ ¼çš„æ ‡é¢˜å’Œåˆ†éš”çº¿
        print(f"\n\n{header}")
        print(separator)
        # æ‰“å°æ¯è¡Œè¡¨æ ¼å†…å®¹
        for row in table_rows:
            print(row)
```