# `.\yolov8\ultralytics\models\fastsam\predict.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
import torch
from PIL import Image

from ultralytics.models.yolo.segment import SegmentationPredictor  # å¯¼å…¥åˆ†å‰²é¢„æµ‹å™¨ç±»
from ultralytics.utils import DEFAULT_CFG, checks  # å¯¼å…¥é»˜è®¤é…ç½®å’Œæ£€æŸ¥å·¥å…·
from ultralytics.utils.metrics import box_iou  # å¯¼å…¥ IoU è®¡ç®—å·¥å…·
from ultralytics.utils.ops import scale_masks  # å¯¼å…¥ mask ç¼©æ”¾æ“ä½œ

from .utils import adjust_bboxes_to_image_border  # å¯¼å…¥è¾¹ç•Œæ¡†è°ƒæ•´å‡½æ•°


class FastSAMPredictor(SegmentationPredictor):
    """
    FastSAMPredictor is specialized for fast SAM (Segment Anything Model) segmentation prediction tasks in Ultralytics
    YOLO framework.

    This class extends the SegmentationPredictor, customizing the prediction pipeline specifically for fast SAM. It
    adjusts post-processing steps to incorporate mask prediction and non-max suppression while optimizing for single-
    class segmentation.
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ– FastSAMPredictor å¯¹è±¡
        super().__init__(cfg, overrides, _callbacks)
        # åˆå§‹åŒ–æç¤ºä¿¡æ¯ä¸ºç©ºå­—å…¸
        self.prompts = {}

    def postprocess(self, preds, img, orig_imgs):
        """Applies box postprocess for FastSAM predictions."""
        # ä»æç¤ºä¿¡æ¯ä¸­å–å‡ºè¾¹ç•Œæ¡†ã€ç‚¹ã€æ ‡ç­¾å’Œæ–‡æœ¬ä¿¡æ¯
        bboxes = self.prompts.pop("bboxes", None)
        points = self.prompts.pop("points", None)
        labels = self.prompts.pop("labels", None)
        texts = self.prompts.pop("texts", None)
        # è°ƒç”¨çˆ¶ç±»çš„ postprocess æ–¹æ³•è¿›è¡Œé¢„æµ‹ç»“æœåå¤„ç†
        results = super().postprocess(preds, img, orig_imgs)
        # éå†æ¯ä¸ªç»“æœ
        for result in results:
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ•´ä¸ªå›¾åƒè¾¹ç•Œçš„æ¡†
            full_box = torch.tensor(
                [0, 0, result.orig_shape[1], result.orig_shape[0]], device=preds[0].device, dtype=torch.float32
            )
            # è°ƒæ•´ç»“æœä¸­çš„è¾¹ç•Œæ¡†ï¼Œä½¿å…¶é€‚åº”å›¾åƒè¾¹ç•Œ
            boxes = adjust_bboxes_to_image_border(result.boxes.xyxy, result.orig_shape)
            # æ‰¾åˆ°ä¸æ•´ä¸ªå›¾åƒè¾¹ç•Œæ¡† IoU å¤§äº 0.9 çš„è¾¹ç•Œæ¡†ç´¢å¼•
            idx = torch.nonzero(box_iou(full_box[None], boxes) > 0.9).flatten()
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„è¾¹ç•Œæ¡†ç´¢å¼•ï¼Œåˆ™å°†è¿™äº›è¾¹ç•Œæ¡†è®¾ç½®ä¸ºæ•´ä¸ªå›¾åƒè¾¹ç•Œæ¡†
            if idx.numel() != 0:
                result.boxes.xyxy[idx] = full_box

        # è¿”å›å¤„ç†åçš„ç»“æœï¼Œå¹¶å°†åŸå§‹æç¤ºä¿¡æ¯ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå‡½æ•°
        return self.prompt(results, bboxes=bboxes, points=points, labels=labels, texts=texts)
    def _clip_inference(self, images, texts):
        """
        CLIP Inference process.

        Args:
            images (List[PIL.Image]): A list of source images and each of them should be PIL.Image type with RGB channel order.
            texts (List[str]): A list of prompt texts and each of them should be string object.

        Returns:
            (torch.Tensor): The similarity between given images and texts.
        """
        try:
            import clip  # å°è¯•å¯¼å…¥ CLIP åº“
        except ImportError:
            checks.check_requirements("git+https://github.com/ultralytics/CLIP.git")  # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æ£€æŸ¥å¹¶å®‰è£…æ‰€éœ€çš„ä¾èµ–
            import clip  # å†æ¬¡å°è¯•å¯¼å…¥ CLIP åº“

        if (not hasattr(self, "clip_model")) or (not hasattr(self, "clip_preprocess")):
            self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
            # å¦‚æœå¯¹è±¡å®ä¾‹ä¸­æ²¡æœ‰ clip_model æˆ– clip_preprocess å±æ€§ï¼Œåˆ™åŠ è½½ CLIP æ¨¡å‹å’Œé¢„å¤„ç†å™¨

        images = torch.stack([self.clip_preprocess(image).to(self.device) for image in images])
        # å°†è¾“å…¥çš„å›¾åƒåˆ—è¡¨è½¬æ¢ä¸º torch å¼ é‡ï¼Œå¹¶ä½¿ç”¨ clip_preprocess è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç§»åˆ°è®¾å¤‡ä¸Š

        tokenized_text = clip.tokenize(texts).to(self.device)
        # å¯¹è¾“å…¥çš„æ–‡æœ¬åˆ—è¡¨è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶ç§»åˆ°è®¾å¤‡ä¸Š

        image_features = self.clip_model.encode_image(images)
        # ä½¿ç”¨ CLIP æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œç¼–ç ï¼Œå¾—åˆ°å›¾åƒç‰¹å¾

        text_features = self.clip_model.encode_text(tokenized_text)
        # ä½¿ç”¨ CLIP æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°æ–‡æœ¬ç‰¹å¾

        image_features /= image_features.norm(dim=-1, keepdim=True)  # å¯¹å›¾åƒç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
        text_features /= text_features.norm(dim=-1, keepdim=True)  # å¯¹æ–‡æœ¬ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†

        return (image_features * text_features[:, None]).sum(-1)  # è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§
        # è¿”å›å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§å¾—åˆ†ï¼Œå½¢çŠ¶ä¸º (M, N)

    def set_prompts(self, prompts):
        """Set prompts in advance."""
        self.prompts = prompts
        # è®¾ç½®é¢„è®¾æç¤ºä¿¡æ¯ï¼Œå­˜å‚¨åœ¨å¯¹è±¡å®ä¾‹çš„ prompts å±æ€§ä¸­
```