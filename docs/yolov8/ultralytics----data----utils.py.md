# `.\yolov8\ultralytics\data\utils.py`

```
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import contextlib
import hashlib
import json
import os
import random
import subprocess
import time
import zipfile
from multiprocessing.pool import ThreadPool
from pathlib import Path
from tarfile import is_tarfile

import cv2
import numpy as np
from PIL import Image, ImageOps

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—å’Œå‡½æ•°
from ultralytics.nn.autobackend import check_class_names
from ultralytics.utils import (
    DATASETS_DIR,
    LOGGER,
    NUM_THREADS,
    ROOT,
    SETTINGS_YAML,
    TQDM,
    clean_url,
    colorstr,
    emojis,
    is_dir_writeable,
    yaml_load,
    yaml_save,
)
# å¯¼å…¥æ•°æ®æ ¡éªŒå‡½æ•°å’Œä¸‹è½½å‡½æ•°
from ultralytics.utils.checks import check_file, check_font, is_ascii
from ultralytics.utils.downloads import download, safe_download, unzip_file
# å¯¼å…¥æ“ä½œå‡½æ•°
from ultralytics.utils.ops import segments2boxes

# è®¾ç½®å¸®åŠ©é“¾æ¥
HELP_URL = "See https://docs.ultralytics.com/datasets for dataset formatting guidance."
# å®šä¹‰æ”¯æŒçš„å›¾ç‰‡æ ¼å¼å’Œè§†é¢‘æ ¼å¼
IMG_FORMATS = {"bmp", "dng", "jpeg", "jpg", "mpo", "png", "tif", "tiff", "webp"}  # image suffixes
VID_FORMATS = {"asf", "avi", "gif", "m4v", "mkv", "mov", "mp4", "mpeg", "mpg", "ts", "wmv", "webm"}  # video suffixes
# ç¡®å®šæ˜¯å¦å¯ç”¨å†…å­˜å›ºå®šæ ‡è®°ï¼Œæ ¹æ®ç¯å¢ƒå˜é‡PIN_MEMORYçš„å€¼
PIN_MEMORY = str(os.getenv("PIN_MEMORY", True)).lower() == "true"  # global pin_memory for dataloaders
# æ ¼å¼å¸®åŠ©ä¿¡æ¯
FORMATS_HELP_MSG = f"Supported formats are:\nimages: {IMG_FORMATS}\nvideos: {VID_FORMATS}"


def img2label_paths(img_paths):
    """Define label paths as a function of image paths."""
    # å®šä¹‰å›¾ç‰‡è·¯å¾„å’Œæ ‡ç­¾è·¯å¾„çš„è½¬æ¢å…³ç³»
    sa, sb = f"{os.sep}images{os.sep}", f"{os.sep}labels{os.sep}"  # /images/, /labels/ substrings
    return [sb.join(x.rsplit(sa, 1)).rsplit(".", 1)[0] + ".txt" for x in img_paths]


def get_hash(paths):
    """Returns a single hash value of a list of paths (files or dirs)."""
    # è®¡ç®—è·¯å¾„åˆ—è¡¨ä¸­æ–‡ä»¶æˆ–ç›®å½•çš„æ€»å¤§å°
    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes
    # ä½¿ç”¨SHA-256ç®—æ³•è®¡ç®—è·¯å¾„åˆ—è¡¨çš„å“ˆå¸Œå€¼
    h = hashlib.sha256(str(size).encode())  # hash sizes
    h.update("".join(paths).encode())  # hash paths
    return h.hexdigest()  # return hash


def exif_size(img: Image.Image):
    """Returns exif-corrected PIL size."""
    s = img.size  # (width, height)
    if img.format == "JPEG":  # only support JPEG images
        # å°è¯•è·å–å›¾åƒçš„EXIFä¿¡æ¯ï¼Œå¹¶æ ¹æ®EXIFä¿¡æ¯ä¿®æ­£å›¾åƒå°ºå¯¸
        with contextlib.suppress(Exception):
            exif = img.getexif()
            if exif:
                rotation = exif.get(274, None)  # the EXIF key for the orientation tag is 274
                if rotation in {6, 8}:  # rotation 270 or 90
                    s = s[1], s[0]
    return s


def verify_image(args):
    """Verify one image."""
    (im_file, cls), prefix = args
    # åˆå§‹åŒ–è®¡æ•°å™¨å’Œæ¶ˆæ¯å­—ç¬¦ä¸²
    nf, nc, msg = 0, 0, ""
    try:
        # å°è¯•æ‰“å¼€å›¾åƒæ–‡ä»¶
        im = Image.open(im_file)
        # ä½¿ç”¨PILåº“éªŒè¯å›¾åƒæ–‡ä»¶
        im.verify()  # PIL verify
        # è·å–å›¾åƒçš„å°ºå¯¸ä¿¡æ¯
        shape = exif_size(im)  # image size
        # è°ƒæ•´å°ºå¯¸ä¿¡æ¯çš„é¡ºåºä¸ºå®½åº¦åœ¨å‰ï¼Œé«˜åº¦åœ¨å
        shape = (shape[1], shape[0])  # hw
        # æ–­è¨€å›¾åƒçš„å®½åº¦å’Œé«˜åº¦å¤§äº9åƒç´ 
        assert (shape[0] > 9) & (shape[1] > 9), f"image size {shape} <10 pixels"
        # æ–­è¨€å›¾åƒçš„æ ¼å¼åœ¨å…è®¸çš„å›¾åƒæ ¼å¼åˆ—è¡¨ä¸­
        assert im.format.lower() in IMG_FORMATS, f"Invalid image format {im.format}. {FORMATS_HELP_MSG}"
        # å¦‚æœå›¾åƒæ ¼å¼æ˜¯JPEGï¼Œåˆ™è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æŸå
        if im.format.lower() in {"jpg", "jpeg"}:
            # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œå®šä½åˆ°æ–‡ä»¶æœ«å°¾çš„å€’æ•°ç¬¬äºŒä¸ªå­—èŠ‚
            with open(im_file, "rb") as f:
                f.seek(-2, 2)
                # æ£€æŸ¥æ–‡ä»¶æœ«å°¾ä¸¤ä¸ªå­—èŠ‚æ˜¯å¦ä¸ºJPEGæ–‡ä»¶çš„ç»“æŸæ ‡è®°
                if f.read() != b"\xff\xd9":  # corrupt JPEG
                    # ä¿®å¤å¹¶ä¿å­˜æŸåçš„JPEGæ–‡ä»¶
                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, "JPEG", subsampling=0, quality=100)
                    # ç”Ÿæˆè­¦å‘Šä¿¡æ¯
                    msg = f"{prefix}WARNING âš ï¸ {im_file}: corrupt JPEG restored and saved"
        # å¦‚æœæ²¡æœ‰å¼‚å¸¸å‘ç”Ÿï¼Œè®¾ç½®nfä¸º1
        nf = 1
    except Exception as e:
        # æ•è·å¼‚å¸¸ï¼Œå¹¶è®¾ç½®ncä¸º1ï¼Œç”Ÿæˆè­¦å‘Šä¿¡æ¯
        nc = 1
        msg = f"{prefix}WARNING âš ï¸ {im_file}: ignoring corrupt image/label: {e}"
    # è¿”å›ç»“æœå…ƒç»„
    return (im_file, cls), nf, nc, msg
# éªŒè¯å•ä¸ªå›¾åƒ-æ ‡ç­¾å¯¹çš„æœ‰æ•ˆæ€§
def verify_image_label(args):
    # è§£åŒ…å‚æ•°ï¼šå›¾åƒæ–‡ä»¶è·¯å¾„ã€æ ‡ç­¾æ–‡ä»¶è·¯å¾„ã€å‰ç¼€ã€å…³é”®ç‚¹ã€ç±»åˆ«æ•°ã€å…³é”®ç‚¹æ•°ã€ç»´åº¦æ•°
    im_file, lb_file, prefix, keypoint, num_cls, nkpt, ndim = args
    # åˆå§‹åŒ–è®¡æ•°å™¨å’Œæ¶ˆæ¯å˜é‡
    # nm: ç¼ºå¤±çš„æ•°é‡
    # nf: å‘ç°çš„æ•°é‡
    # ne: ç©ºçš„æ•°é‡
    # nc: æŸåçš„æ•°é‡
    # msg: ä¿¡æ¯å­—ç¬¦ä¸²
    # segments: æ®µ
    # keypoints: å…³é”®ç‚¹
    nm, nf, ne, nc, msg, segments, keypoints = 0, 0, 0, 0, "", [], None

    # æ•è·ä»»ä½•å¼‚å¸¸å¹¶è®°å½•ä¸ºæŸåçš„å›¾åƒ/æ ‡ç­¾å¯¹
    except Exception as e:
        # æ ‡è®°ä¸ºæŸåçš„æ•°é‡å¢åŠ 
        nc = 1
        # è®¾ç½®æ¶ˆæ¯å†…å®¹ï¼Œæ ‡è®°æ–‡ä»¶å’Œå…·ä½“çš„é”™è¯¯ä¿¡æ¯
        msg = f"{prefix}WARNING âš ï¸ {im_file}: ignoring corrupt image/label: {e}"
        # è¿”å›ç©ºçš„è®¡æ•°å’Œæ¶ˆæ¯ï¼Œå…¶ä½™å˜é‡ä¸º None
        return [None, None, None, None, None, nm, nf, ne, nc, msg]


def polygon2mask(imgsz, polygons, color=1, downsample_ratio=1):
    """
    å°†å¤šè¾¹å½¢åˆ—è¡¨è½¬æ¢ä¸ºæŒ‡å®šå›¾åƒå°ºå¯¸çš„äºŒè¿›åˆ¶æ©ç ã€‚

    Args:
        imgsz (tuple): å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸º (height, width)ã€‚
        polygons (list[np.ndarray]): å¤šè¾¹å½¢åˆ—è¡¨ã€‚æ¯ä¸ªå¤šè¾¹å½¢æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, M] çš„æ•°ç»„ï¼Œ
                                     å…¶ä¸­ N æ˜¯å¤šè¾¹å½¢çš„æ•°é‡ï¼ŒM æ˜¯ç‚¹çš„æ•°é‡ï¼Œæ»¡è¶³ M % 2 = 0ã€‚
        color (int, optional): åœ¨æ©ç ä¸­å¡«å……å¤šè¾¹å½¢çš„é¢œè‰²å€¼ã€‚é»˜è®¤ä¸º 1ã€‚
        downsample_ratio (int, optional): ç¼©å°æ©ç çš„å› å­ã€‚é»˜è®¤ä¸º 1ã€‚

    Returns:
        (np.ndarray): æŒ‡å®šå›¾åƒå°ºå¯¸çš„äºŒè¿›åˆ¶æ©ç ï¼Œå¡«å……äº†å¤šè¾¹å½¢ã€‚
    """
    # åˆ›å»ºä¸€ä¸ªå…¨é›¶æ•°ç»„ä½œä¸ºæ©ç 
    mask = np.zeros(imgsz, dtype=np.uint8)
    # å°†å¤šè¾¹å½¢åˆ—è¡¨è½¬æ¢ä¸º numpy æ•°ç»„ï¼Œç±»å‹ä¸º int32
    polygons = np.asarray(polygons, dtype=np.int32)
    # é‡æ–°æ•´å½¢å¤šè¾¹å½¢æ•°ç»„ä»¥ä¾¿å¡«å……å¤šè¾¹å½¢çš„é¡¶ç‚¹
    polygons = polygons.reshape((polygons.shape[0], -1, 2))
    # ä½¿ç”¨æŒ‡å®šçš„é¢œè‰²å€¼å¡«å……å¤šè¾¹å½¢åˆ°æ©ç ä¸­
    cv2.fillPoly(mask, polygons, color=color)
    # è®¡ç®—ç¼©å°åçš„æ©ç å°ºå¯¸
    nh, nw = (imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio)
    # è¿”å›ç¼©å°åçš„æ©ç ï¼Œä¿æŒä¸åŸæ©ç ç›¸åŒçš„å¡«å……æ–¹æ³•
    return cv2.resize(mask, (nw, nh))


def polygons2masks(imgsz, polygons, color, downsample_ratio=1):
    """
    å°†å¤šè¾¹å½¢åˆ—è¡¨è½¬æ¢ä¸ºæŒ‡å®šå›¾åƒå°ºå¯¸çš„ä¸€ç»„äºŒè¿›åˆ¶æ©ç ã€‚

    Args:
        imgsz (tuple): å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸º (height, width)ã€‚
        polygons (list[np.ndarray]): å¤šè¾¹å½¢åˆ—è¡¨ã€‚æ¯ä¸ªå¤šè¾¹å½¢æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, M] çš„æ•°ç»„ï¼Œ
                                     å…¶ä¸­ N æ˜¯å¤šè¾¹å½¢çš„æ•°é‡ï¼ŒM æ˜¯ç‚¹çš„æ•°é‡ï¼Œæ»¡è¶³ M % 2 = 0ã€‚
        color (int): åœ¨æ©ç ä¸­å¡«å……å¤šè¾¹å½¢çš„é¢œè‰²å€¼ã€‚
        downsample_ratio (int, optional): ç¼©å°æ¯ä¸ªæ©ç çš„å› å­ã€‚é»˜è®¤ä¸º 1ã€‚

    Returns:
        (np.ndarray): æŒ‡å®šå›¾åƒå°ºå¯¸çš„ä¸€ç»„äºŒè¿›åˆ¶æ©ç ï¼Œå¡«å……äº†å¤šè¾¹å½¢ã€‚
    """
    # å¯¹å¤šè¾¹å½¢åˆ—è¡¨ä¸­çš„æ¯ä¸ªå¤šè¾¹å½¢ï¼Œè°ƒç”¨ polygon2mask å‡½æ•°ç”Ÿæˆæ©ç æ•°ç»„ï¼Œå¹¶è¿”å›ä¸º numpy æ•°ç»„
    return np.array([polygon2mask(imgsz, [x.reshape(-1)], color, downsample_ratio) for x in polygons])


def polygons2masks_overlap(imgsz, segments, downsample_ratio=1):
    """
    è¿”å›ä¸€ä¸ª (640, 640) çš„é‡å æ©ç ã€‚

    Args:
        imgsz (tuple): å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸º (height, width)ã€‚
        segments (list): æ®µåˆ—è¡¨ã€‚
        downsample_ratio (int, optional): ç¼©å°æ©ç çš„å› å­ã€‚é»˜è®¤ä¸º 1ã€‚

    Returns:
        np.ndarray: æŒ‡å®šå›¾åƒå°ºå¯¸çš„é‡å æ©ç ã€‚
    """
    # åˆ›å»ºä¸€ä¸ªå…¨é›¶æ•°ç»„ä½œä¸ºæ©ç ï¼Œå°ºå¯¸æ ¹æ®ç¼©å°å› å­è°ƒæ•´
    masks = np.zeros(
        (imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio),
        dtype=np.int32 if len(segments) > 255 else np.uint8,
    )
    # åˆå§‹åŒ–ä¸€ä¸ªåŒºåŸŸåˆ—è¡¨
    areas = []
    # åˆå§‹åŒ–ä¸€ä¸ªæ®µåˆ—è¡¨
    ms = []
    # å¯¹äºæ¯ä¸ªåˆ†å‰²æ®µè½è¿›è¡Œè¿­ä»£
    for si in range(len(segments)):
        # æ ¹æ®åˆ†å‰²æ®µè½åˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶æ©ç 
        mask = polygon2mask(imgsz, [segments[si].reshape(-1)], downsample_ratio=downsample_ratio, color=1)
        # å°†ç”Ÿæˆçš„æ©ç æ·»åŠ åˆ°æ©ç åˆ—è¡¨ä¸­
        ms.append(mask)
        # è®¡ç®—æ©ç çš„åƒç´ æ€»æ•°ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°é¢ç§¯åˆ—è¡¨ä¸­
        areas.append(mask.sum())
    
    # å°†é¢ç§¯åˆ—è¡¨è½¬æ¢ä¸º NumPy æ•°ç»„
    areas = np.asarray(areas)
    # æŒ‰ç…§é¢ç§¯å¤§å°é™åºæ’åˆ—ç´¢å¼•
    index = np.argsort(-areas)
    # æ ¹æ®æ’åºåçš„ç´¢å¼•é‡æ–°æ’åˆ—æ©ç åˆ—è¡¨
    ms = np.array(ms)[index]
    
    # å¯¹æ¯ä¸ªåˆ†å‰²æ®µè½å†æ¬¡è¿›è¡Œè¿­ä»£
    for i in range(len(segments)):
        # å°†é‡æ–°æ’åºçš„æ©ç ä¹˜ä»¥å½“å‰ç´¢å¼•åŠ ä¸€ï¼Œç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²æ©ç 
        mask = ms[i] * (i + 1)
        # å°†ç”Ÿæˆçš„åˆ†å‰²æ©ç åŠ åˆ°æ€»æ©ç ä¸­
        masks = masks + mask
        # å¯¹æ€»æ©ç è¿›è¡Œæˆªæ–­ï¼Œç¡®ä¿åƒç´ å€¼åœ¨æŒ‡å®šèŒƒå›´å†…
        masks = np.clip(masks, a_min=0, a_max=i + 1)
    
    # è¿”å›æœ€ç»ˆç”Ÿæˆçš„æ€»æ©ç å’Œæ’åºåçš„ç´¢å¼•
    return masks, index
def find_dataset_yaml(path: Path) -> Path:
    """
    Find and return the YAML file associated with a Detect, Segment or Pose dataset.

    This function searches for a YAML file at the root level of the provided directory first, and if not found, it
    performs a recursive search. It prefers YAML files that have the same stem as the provided path. An AssertionError
    is raised if no YAML file is found or if multiple YAML files are found.

    Args:
        path (Path): The directory path to search for the YAML file.

    Returns:
        (Path): The path of the found YAML file.
    """
    # Attempt to find YAML files at the root level first, otherwise perform a recursive search
    files = list(path.glob("*.yaml")) or list(path.rglob("*.yaml"))  # try root level first and then recursive
    
    # Ensure at least one YAML file is found; otherwise, raise an AssertionError
    assert files, f"No YAML file found in '{path.resolve()}'"
    
    # If multiple YAML files are found, filter to prefer those with the same stem as the provided path
    if len(files) > 1:
        files = [f for f in files if f.stem == path.stem]  # prefer *.yaml files that match
    
    # Ensure exactly one YAML file is found; otherwise, raise an AssertionError with details
    assert len(files) == 1, f"Expected 1 YAML file in '{path.resolve()}', but found {len(files)}.\n{files}"
    
    # Return the path of the found YAML file
    return files[0]


def check_det_dataset(dataset, autodownload=True):
    """
    Download, verify, and/or unzip a dataset if not found locally.

    This function checks the availability of a specified dataset, and if not found, it has the option to download and
    unzip the dataset. It then reads and parses the accompanying YAML data, ensuring key requirements are met and also
    resolves paths related to the dataset.

    Args:
        dataset (str): Path to the dataset or dataset descriptor (like a YAML file).
        autodownload (bool, optional): Whether to automatically download the dataset if not found. Defaults to True.

    Returns:
        (dict): Parsed dataset information and paths.
    """

    # Check if the dataset file exists locally and get its path
    file = check_file(dataset)

    # If the dataset file is a ZIP or TAR archive, download and unzip it if necessary
    extract_dir = ""
    if zipfile.is_zipfile(file) or is_tarfile(file):
        new_dir = safe_download(file, dir=DATASETS_DIR, unzip=True, delete=False)
        # Find and return the YAML file within the extracted directory
        file = find_dataset_yaml(DATASETS_DIR / new_dir)
        extract_dir, autodownload = file.parent, False

    # Load YAML data from the specified file, appending the filename to the loaded data
    data = yaml_load(file, append_filename=True)  # dictionary

    # Perform checks on the loaded YAML data
    for k in "train", "val":
        if k not in data:
            if k != "val" or "validation" not in data:
                # Raise a SyntaxError if required keys 'train' and 'val' (or 'validation') are missing
                raise SyntaxError(
                    emojis(f"{dataset} '{k}:' key missing âŒ.\n'train' and 'val' are required in all data YAMLs.")
                )
            # Log a warning and rename 'validation' key to 'val' if necessary
            LOGGER.info("WARNING âš ï¸ renaming data YAML 'validation' key to 'val' to match YOLO format.")
            data["val"] = data.pop("validation")  # replace 'validation' key with 'val' key

    # Ensure 'names' or 'nc' keys are present in the data; otherwise, raise a SyntaxError
    if "names" not in data and "nc" not in data:
        raise SyntaxError(emojis(f"{dataset} key missing âŒ.\n either 'names' or 'nc' are required in all data YAMLs."))

    # Ensure the lengths of 'names' and 'nc' match if both are present
    if "names" in data and "nc" in data and len(data["names"]) != data["nc"]:
        raise SyntaxError(emojis(f"{dataset} 'names' length {len(data['names'])} and 'nc: {data['nc']}' must match."))
    # å¦‚æœæ•°æ®å­—å…¸ä¸­ä¸å­˜åœ¨é”® "names"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåä¸º "names" çš„åˆ—è¡¨ï¼ŒåŒ…å«ä»¥"class_{i}"å‘½åçš„å…ƒç´ ï¼Œå…¶ä¸­iä»0åˆ°data["nc"]-1
    # å¦‚æœæ•°æ®å­—å…¸ä¸­å·²ç»å­˜åœ¨ "names" é”®ï¼Œåˆ™å°† "nc" è®¾ç½®ä¸º "names" åˆ—è¡¨çš„é•¿åº¦
    if "names" not in data:
        data["names"] = [f"class_{i}" for i in range(data["nc"])]
    else:
        data["nc"] = len(data["names"])

    # è°ƒç”¨å‡½æ•° check_class_names()ï¼Œæ£€æŸ¥å¹¶ä¿®æ­£ "names" åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ 
    data["names"] = check_class_names(data["names"])

    # è§£æå’Œè®¾ç½®è·¯å¾„ä¿¡æ¯
    # path å˜é‡æ ¹æ® extract_dirã€data["path"] æˆ–è€… data["yaml_file"] çš„çˆ¶è·¯å¾„åˆ›å»ºï¼Œè¡¨ç¤ºæ•°æ®é›†çš„æ ¹è·¯å¾„
    path = Path(extract_dir or data.get("path") or Path(data.get("yaml_file", "")).parent)  # dataset root
    if not path.is_absolute():
        path = (DATASETS_DIR / path).resolve()  # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œåˆ™åŸºäº DATASETS_DIR è®¾ç½®ç»å¯¹è·¯å¾„

    # è®¾ç½® data["path"] ä¸ºè§£æåçš„è·¯å¾„
    data["path"] = path  # download scripts

    # å¯¹äº "train", "val", "test", "minival" ä¸­çš„æ¯ä¸ªé”®ï¼Œå¦‚æœæ•°æ®å­—å…¸ä¸­å­˜åœ¨è¯¥é”®ï¼Œåˆ™å°†å…¶è·¯å¾„è®¾ç½®ä¸ºç»å¯¹è·¯å¾„
    for k in "train", "val", "test", "minival":
        if data.get(k):  # å¦‚æœè¯¥é”®å­˜åœ¨
            if isinstance(data[k], str):
                # å¦‚æœè·¯å¾„æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œåˆ™åŸºäº path è®¾ç½®ç»å¯¹è·¯å¾„
                x = (path / data[k]).resolve()
                # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ä¸”ä»¥ "../" å¼€å¤´ï¼Œåˆ™ä¿®æ­£è·¯å¾„
                if not x.exists() and data[k].startswith("../"):
                    x = (path / data[k][3:]).resolve()
                data[k] = str(x)
            else:
                # å¦‚æœè·¯å¾„æ˜¯åˆ—è¡¨ï¼Œåˆ™å¯¹åˆ—è¡¨ä¸­æ¯ä¸ªè·¯å¾„åŸºäº path è®¾ç½®ç»å¯¹è·¯å¾„
                data[k] = [str((path / x).resolve()) for x in data[k]]

    # è§£æ YAML æ–‡ä»¶
    val, s = (data.get(x) for x in ("val", "download"))
    if val:
        # å¦‚æœå­˜åœ¨ valï¼Œå°†å…¶è§£æä¸ºç»å¯¹è·¯å¾„åˆ—è¡¨
        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path
        # å¦‚æœå­˜åœ¨æŸä¸ªè·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡º FileNotFoundError
        if not all(x.exists() for x in val):
            name = clean_url(dataset)  # å»é™¤ URL è®¤è¯ä¿¡æ¯åçš„æ•°æ®é›†åç§°
            # æ„å»ºé”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²
            m = f"\nDataset '{name}' images not found âš ï¸, missing path '{[x for x in val if not x.exists()][0]}'"
            if s and autodownload:
                LOGGER.warning(m)
            else:
                m += f"\nNote dataset download directory is '{DATASETS_DIR}'. You can update this in '{SETTINGS_YAML}'"
                raise FileNotFoundError(m)
            t = time.time()
            r = None  # è¡¨ç¤ºæˆåŠŸ
            # å¦‚æœ s æ˜¯ä»¥ "http" å¼€å¤´ä¸”ä»¥ ".zip" ç»“å°¾ï¼Œåˆ™æ‰§è¡Œå®‰å…¨ä¸‹è½½
            if s.startswith("http") and s.endswith(".zip"):  # URL
                safe_download(url=s, dir=DATASETS_DIR, delete=True)
            elif s.startswith("bash "):  # å¦‚æœ s æ˜¯ä»¥ "bash " å¼€å¤´ï¼Œåˆ™è¿è¡Œ bash è„šæœ¬
                LOGGER.info(f"Running {s} ...")
                r = os.system(s)
            else:  # å¦åˆ™ï¼Œæ‰§è¡Œ Python è„šæœ¬
                exec(s, {"yaml": data})
            dt = f"({round(time.time() - t, 1)}s)"
            # æ ¹æ®æ‰§è¡Œç»“æœè®¾ç½®æ—¥å¿—æ¶ˆæ¯
            s = f"success âœ… {dt}, saved to {colorstr('bold', DATASETS_DIR)}" if r in {0, None} else f"failure {dt} âŒ"
            LOGGER.info(f"Dataset download {s}\n")

    # æ£€æŸ¥å¹¶ä¸‹è½½å­—ä½“æ–‡ä»¶ï¼Œæ ¹æ® "names" æ˜¯å¦åªåŒ…å« ASCII å­—ç¬¦é€‰æ‹©ä¸åŒçš„å­—ä½“æ–‡ä»¶è¿›è¡Œä¸‹è½½
    check_font("Arial.ttf" if is_ascii(data["names"]) else "Arial.Unicode.ttf")  # download fonts

    return data  # è¿”å›æ›´æ–°åçš„æ•°æ®å­—å…¸
    # æ£€æŸ¥åˆ†ç±»æ•°æ®é›†ï¼Œå¦‚Imagenetã€‚

    # å¦‚æœ `dataset` ä»¥ "http:/" æˆ– "https:/" å¼€å¤´ï¼Œå°è¯•ä»ç½‘ç»œä¸‹è½½æ•°æ®é›†å¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚
    # å¦‚æœ `dataset` æ˜¯ä»¥ ".zip", ".tar", æˆ– ".gz" ç»“å°¾çš„æ–‡ä»¶è·¯å¾„ï¼Œæ£€æŸ¥æ–‡ä»¶çš„æœ‰æ•ˆæ€§åï¼Œä¸‹è½½å¹¶è§£å‹æ•°æ®é›†åˆ°æŒ‡å®šç›®å½•ã€‚

    # å°† `dataset` è½¬æ¢ä¸º `Path` å¯¹è±¡ï¼Œå¹¶è§£æå…¶ç»å¯¹è·¯å¾„ã€‚
    dataset = Path(dataset)
    data_dir = (dataset if dataset.is_dir() else (DATASETS_DIR / dataset)).resolve()

    # å¦‚æœæŒ‡å®šè·¯å¾„çš„æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°è¯•ä»ç½‘ç»œä¸‹è½½ã€‚
    if not data_dir.is_dir():
        # å¦‚æœ `dataset` æ˜¯ "imagenet"ï¼Œæ‰§è¡Œç‰¹å®šçš„æ•°æ®é›†ä¸‹è½½è„šæœ¬ã€‚
        # å¦åˆ™ï¼Œä» GitHub å‘å¸ƒçš„èµ„æºä¸­ä¸‹è½½æŒ‡å®šçš„æ•°æ®é›†å‹ç¼©æ–‡ä»¶ã€‚
        LOGGER.warning(f"\nDataset not found âš ï¸, missing path {data_dir}, attempting download...")
        t = time.time()
        if str(dataset) == "imagenet":
            subprocess.run(f"bash {ROOT / 'data/scripts/get_imagenet.sh'}", shell=True, check=True)
        else:
            url = f"https://github.com/ultralytics/assets/releases/download/v0.0.0/{dataset}.zip"
            download(url, dir=data_dir.parent)
        s = f"Dataset download success âœ… ({time.time() - t:.1f}s), saved to {colorstr('bold', data_dir)}\n"
        LOGGER.info(s)

    # è®­ç»ƒé›†çš„è·¯å¾„
    train_set = data_dir / "train"

    # éªŒè¯é›†çš„è·¯å¾„ï¼Œä¼˜å…ˆé€‰æ‹© "val" ç›®å½•ï¼Œå…¶æ¬¡é€‰æ‹© "validation" ç›®å½•ï¼Œå¦‚æœéƒ½ä¸å­˜åœ¨åˆ™ä¸º Noneã€‚
    val_set = (
        data_dir / "val"
        if (data_dir / "val").exists()
        else data_dir / "validation"
        if (data_dir / "validation").exists()
        else None
    )  # data/test or data/val

    # æµ‹è¯•é›†çš„è·¯å¾„ï¼Œä¼˜å…ˆé€‰æ‹© "test" ç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸º Noneã€‚
    test_set = data_dir / "test" if (data_dir / "test").exists() else None  # data/val or data/test

    # å¦‚æœ `split` å‚æ•°ä¸º "val"ï¼Œä½†éªŒè¯é›†è·¯å¾„ `val_set` ä¸å­˜åœ¨æ—¶ï¼Œå‘å‡ºè­¦å‘Šå¹¶ä½¿ç”¨æµ‹è¯•é›†è·¯å¾„ä»£æ›¿ã€‚
    if split == "val" and not val_set:
        LOGGER.warning("WARNING âš ï¸ Dataset 'split=val' not found, using 'split=test' instead.")
    
    # å¦‚æœ `split` å‚æ•°ä¸º "test"ï¼Œä½†æµ‹è¯•é›†è·¯å¾„ `test_set` ä¸å­˜åœ¨æ—¶ï¼Œå‘å‡ºè­¦å‘Šå¹¶ä½¿ç”¨éªŒè¯é›†è·¯å¾„ä»£æ›¿ã€‚
    elif split == "test" and not test_set:
        LOGGER.warning("WARNING âš ï¸ Dataset 'split=test' not found, using 'split=val' instead.")

    # è®¡ç®—æ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°ç›®ï¼Œé€šè¿‡ç»Ÿè®¡ `train` ç›®å½•ä¸‹çš„å­ç›®å½•æ•°é‡æ¥å¾—åˆ°ã€‚
    nc = len([x for x in (data_dir / "train").glob("*") if x.is_dir()])  # number of classes

    # è·å–è®­ç»ƒé›†ä¸­çš„ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¹¶æŒ‰å­—æ¯é¡ºåºæ’åºåæ„å»ºæˆå­—å…¸ï¼Œé”®ä¸ºç±»åˆ«ç´¢å¼•ã€‚
    names = [x.name for x in (data_dir / "train").iterdir() if x.is_dir()]  # class names list
    names = dict(enumerate(sorted(names)))

    # æ‰“å°ç»“æœåˆ°æ§åˆ¶å°
    # éå†åŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„å­—å…¸ï¼Œæ¯æ¬¡è¿­ä»£è·å–é”®å€¼å¯¹ï¼ˆkä¸ºé”®ï¼Œvä¸ºå¯¹åº”çš„æ•°æ®é›†ï¼‰
    for k, v in {"train": train_set, "val": val_set, "test": test_set}.items():
        # ä½¿ç”¨f-stringç”Ÿæˆå¸¦é¢œè‰²çš„å‰ç¼€å­—ç¬¦ä¸²ï¼ŒæŒ‡ç¤ºå½“å‰æ•°æ®é›†çš„åç§°å’ŒçŠ¶æ€
        prefix = f'{colorstr(f"{k}:")} {v}...'
        # å¦‚æœå½“å‰æ•°æ®é›†ä¸ºç©ºï¼ˆNoneï¼‰ï¼Œè®°å½•ä¿¡æ¯åˆ°æ—¥å¿—
        if v is None:
            LOGGER.info(prefix)
        else:
            # è·å–å½“å‰æ•°æ®é›†ä¸­æ‰€æœ‰ç¬¦åˆå›¾åƒæ ¼å¼çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            files = [path for path in v.rglob("*.*") if path.suffix[1:].lower() in IMG_FORMATS]
            # è®¡ç®—å½“å‰æ•°æ®é›†ä¸­çš„æ–‡ä»¶æ•°ç›®ï¼ˆnfï¼‰å’Œä¸é‡å¤çˆ¶ç›®å½•æ•°ï¼ˆndï¼‰
            nf = len(files)  # æ–‡ä»¶æ•°ç›®
            nd = len({file.parent for file in files})  # ä¸é‡å¤çˆ¶ç›®å½•æ•°
            # å¦‚æœå½“å‰æ•°æ®é›†ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶
            if nf == 0:
                # å¦‚æœæ˜¯è®­ç»ƒé›†ï¼ŒæŠ›å‡ºæ–‡ä»¶æœªæ‰¾åˆ°çš„é”™è¯¯å¹¶è®°å½•
                if k == "train":
                    raise FileNotFoundError(emojis(f"{dataset} '{k}:' no training images found âŒ "))
                else:
                    # å¦åˆ™è®°å½•è­¦å‘Šä¿¡æ¯ï¼ŒæŒ‡ç¤ºæ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶
                    LOGGER.warning(f"{prefix} found {nf} images in {nd} classes: WARNING âš ï¸ no images found")
            # å¦‚æœå½“å‰æ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°ç›®ä¸æœŸæœ›çš„ç±»åˆ«æ•°ç›®ä¸åŒ¹é…
            elif nd != nc:
                # è®°å½•è­¦å‘Šä¿¡æ¯ï¼ŒæŒ‡ç¤ºç±»åˆ«æ•°ç›®ä¸åŒ¹é…çš„é”™è¯¯
                LOGGER.warning(f"{prefix} found {nf} images in {nd} classes: ERROR âŒï¸ requires {nc} classes, not {nd}")
            else:
                # è®°å½•ä¿¡æ¯ï¼ŒæŒ‡ç¤ºæˆåŠŸæ‰¾åˆ°å›¾åƒæ–‡ä»¶å¹¶ä¸”ç±»åˆ«æ•°ç›®åŒ¹é…
                LOGGER.info(f"{prefix} found {nf} images in {nd} classes âœ… ")

    # è¿”å›åŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ã€ç±»åˆ«æ•°å’Œç±»åˆ«åç§°çš„å­—å…¸
    return {"train": train_set, "val": val_set, "test": test_set, "nc": nc, "names": names}
    """
    A class for generating HUB dataset JSON and `-hub` dataset directory.

    Args:
        path (str): Path to data.yaml or data.zip (with data.yaml inside data.zip). Default is 'coco8.yaml'.
        task (str): Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Default is 'detect'.
        autodownload (bool): Attempt to download dataset if not found locally. Default is False.

    Example:
        Download *.zip files from https://github.com/ultralytics/hub/tree/main/example_datasets
            i.e. https://github.com/ultralytics/hub/raw/main/example_datasets/coco8.zip for coco8.zip.
        ```py
        from ultralytics.data.utils import HUBDatasetStats

        stats = HUBDatasetStats('path/to/coco8.zip', task='detect')  # detect dataset
        stats = HUBDatasetStats('path/to/coco8-seg.zip', task='segment')  # segment dataset
        stats = HUBDatasetStats('path/to/coco8-pose.zip', task='pose')  # pose dataset
        stats = HUBDatasetStats('path/to/dota8.zip', task='obb')  # OBB dataset
        stats = HUBDatasetStats('path/to/imagenet10.zip', task='classify')  # classification dataset

        stats.get_json(save=True)
        stats.process_images()
        ```
    """

    def __init__(self, path="coco8.yaml", task="detect", autodownload=False):
        """Initialize class."""
        # Resolve the given path to its absolute form
        path = Path(path).resolve()
        # Log information message about starting dataset checks
        LOGGER.info(f"Starting HUB dataset checks for {path}....")

        # Initialize class attributes based on arguments
        self.task = task  # detect, segment, pose, classify

        # Depending on the task type, perform different operations
        if self.task == "classify":
            # Unzip the file and check the classification dataset
            unzip_dir = unzip_file(path)
            data = check_cls_dataset(unzip_dir)
            data["path"] = unzip_dir
        else:  # detect, segment, pose
            # Unzip the file, extract data directory and yaml path
            _, data_dir, yaml_path = self._unzip(Path(path))
            try:
                # Load YAML with checks
                data = yaml_load(yaml_path)
                # Strip path since YAML should be in dataset root for all HUB datasets
                data["path"] = ""
                yaml_save(yaml_path, data)
                # Perform dataset checks for detection dataset
                data = check_det_dataset(yaml_path, autodownload)  # dict
                # Set YAML path to data directory (relative) or parent (absolute)
                data["path"] = data_dir
            except Exception as e:
                # Raise an exception with a specific error message
                raise Exception("error/HUB/dataset_stats/init") from e

        # Set attributes for dataset directory and related paths
        self.hub_dir = Path(f'{data["path"]}-hub')
        self.im_dir = self.hub_dir / "images"
        # Create a statistics dictionary based on loaded data
        self.stats = {"nc": len(data["names"]), "names": list(data["names"].values())}
        self.data = data
    # è§£å‹ç¼©æŒ‡å®šè·¯å¾„çš„ ZIP æ–‡ä»¶ï¼Œå¹¶è¿”å›è§£å‹åçš„ç›®å½•è·¯å¾„å’Œæ•°æ®é›† YAML æ–‡ä»¶è·¯å¾„
    def _unzip(path):
        """Unzip data.zip."""
        # å¦‚æœè·¯å¾„ä¸æ˜¯ä»¥ ".zip" ç»“å°¾ï¼Œåˆ™è®¤ä¸ºæ˜¯æ•°æ®æ–‡ä»¶è€Œéå‹ç¼©æ–‡ä»¶ï¼Œç›´æ¥è¿”å› False è¡¨ç¤ºæœªè§£å‹ï¼Œä»¥åŠåŸå§‹è·¯å¾„
        if not str(path).endswith(".zip"):  # path is data.yaml
            return False, None, path
        # è°ƒç”¨ unzip_file å‡½æ•°è§£å‹æŒ‡å®šè·¯å¾„çš„ ZIP æ–‡ä»¶åˆ°å…¶çˆ¶ç›®å½•
        unzip_dir = unzip_file(path, path=path.parent)
        # æ–­è¨€è§£å‹åçš„ç›®å½•å­˜åœ¨ï¼Œå¦åˆ™è¾“å‡ºé”™è¯¯ä¿¡æ¯ï¼Œæç¤ºé¢„æœŸçš„è§£å‹è·¯å¾„
        assert unzip_dir.is_dir(), (
            f"Error unzipping {path}, {unzip_dir} not found. " f"path/to/abc.zip MUST unzip to path/to/abc/"
        )
        # è¿”å› True è¡¨ç¤ºæˆåŠŸè§£å‹ï¼Œè§£å‹åçš„ç›®å½•è·¯å¾„å­—ç¬¦ä¸²ï¼Œä»¥åŠåœ¨è§£å‹ç›®å½•ä¸­æ‰¾åˆ°çš„æ•°æ®é›† YAML æ–‡ä»¶è·¯å¾„
        return True, str(unzip_dir), find_dataset_yaml(unzip_dir)  # zipped, data_dir, yaml_path

    # ä¿å­˜å‹ç¼©åçš„å›¾åƒç”¨äº HUB é¢„è§ˆ
    def _hub_ops(self, f):
        """Saves a compressed image for HUB previews."""
        # è°ƒç”¨ compress_one_image å‡½æ•°ï¼Œå°†æŒ‡å®šæ–‡ä»¶ f å‹ç¼©ä¿å­˜åˆ° self.im_dir ç›®å½•ä¸‹ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºä¿å­˜çš„æ–‡ä»¶å
        compress_one_image(f, self.im_dir / Path(f).name)  # save to dataset-hub

    # å¤„ç†å›¾åƒï¼Œä¸º Ultralytics HUB å‹ç¼©å›¾åƒ
    def process_images(self):
        """Compress images for Ultralytics HUB."""
        from ultralytics.data import YOLODataset  # ClassificationDataset

        # åˆ›å»ºç›®å½• self.im_dirï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œç”¨äºä¿å­˜å‹ç¼©åçš„å›¾åƒæ–‡ä»¶
        self.im_dir.mkdir(parents=True, exist_ok=True)  # makes dataset-hub/images/
        
        # éå† "train", "val", "test" ä¸‰ä¸ªæ•°æ®é›†åˆ†å‰²
        for split in "train", "val", "test":
            # å¦‚æœ self.data ä¸­ä¸å­˜åœ¨å½“å‰åˆ†å‰²çš„æ•°æ®é›†ï¼Œåˆ™è·³è¿‡ç»§ç»­ä¸‹ä¸€ä¸ªåˆ†å‰²
            if self.data.get(split) is None:
                continue
            # åˆ›å»º YOLODataset å¯¹è±¡ï¼ŒæŒ‡å®šå›¾åƒè·¯å¾„ä¸º self.data[split]ï¼Œæ•°æ®ä¸º self.data
            dataset = YOLODataset(img_path=self.data[split], data=self.data)
            # ä½¿ç”¨çº¿ç¨‹æ±  ThreadPoolï¼Œå¹¶å‘å¤„ç†å›¾åƒå‹ç¼©æ“ä½œ
            with ThreadPool(NUM_THREADS) as pool:
                # ä½¿ç”¨ TQDM æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œéå†æ•°æ®é›†ä¸­çš„å›¾åƒæ–‡ä»¶ï¼Œå¯¹æ¯ä¸ªå›¾åƒæ–‡ä»¶è°ƒç”¨ _hub_ops æ–¹æ³•è¿›è¡Œå‹ç¼©ä¿å­˜æ“ä½œ
                for _ in TQDM(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f"{split} images"):
                    pass
        # è¾“å‡ºæ—¥å¿—ä¿¡æ¯ï¼ŒæŒ‡ç¤ºæ‰€æœ‰å›¾åƒä¿å­˜åˆ° self.im_dir ç›®å½•ä¸‹å®Œæˆ
        LOGGER.info(f"Done. All images saved to {self.im_dir}")
        # è¿”å›ä¿å­˜å‹ç¼©å›¾åƒçš„ç›®å½•è·¯å¾„
        return self.im_dir
# è‡ªåŠ¨å°†æ•°æ®é›†åˆ†å‰²æˆè®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°autosplit_*.txtæ–‡ä»¶ä¸­
def autosplit(path=DATASETS_DIR / "coco8/images", weights=(0.9, 0.1, 0.0), annotated_only=False):
    """
    Automatically split a dataset into train/val/test splits and save the resulting splits into autosplit_*.txt files.

    Args:
        path (Path, optional): Path to images directory. Defaults to DATASETS_DIR / 'coco8/images'.
        weights (list | tuple, optional): Train, validation, and test split fractions. Defaults to (0.9, 0.1, 0.0).
        annotated_only (bool, optional): If True, only images with an associated txt file are used. Defaults to False.

    Example:
        ```py
        from ultralytics.data.utils import autosplit

        autosplit()
        ```
    """

    path = Path(path)  # å›¾åƒç›®å½•çš„è·¯å¾„
    # ç­›é€‰å‡ºæ‰€æœ‰ç¬¦åˆå›¾ç‰‡æ ¼å¼çš„æ–‡ä»¶ï¼Œä»¥åˆ—è¡¨å½¢å¼å­˜å‚¨åœ¨filesä¸­
    files = sorted(x for x in path.rglob("*.*") if x.suffix[1:].lower() in IMG_FORMATS)  # åªä¿ç•™å›¾ç‰‡æ–‡ä»¶
    n = len(files)  # æ–‡ä»¶æ€»æ•°
    random.seed(0)  # è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç°ç»“æœ
    # æ ¹æ®æƒé‡éšæœºåˆ†é…æ¯ä¸ªå›¾ç‰‡åˆ°è®­ç»ƒé›†ã€éªŒè¯é›†æˆ–æµ‹è¯•é›†ï¼Œk=nè¡¨ç¤ºç”Ÿæˆnä¸ªéšæœºæ•°
    indices = random.choices([0, 1, 2], weights=weights, k=n)  # å°†æ¯ä¸ªå›¾ç‰‡åˆ†é…åˆ°ç›¸åº”çš„é›†åˆä¸­

    # å®šä¹‰ä¸‰ä¸ªtxtæ–‡ä»¶åï¼Œåˆ†åˆ«ç”¨äºå­˜å‚¨è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ–‡ä»¶åˆ—è¡¨
    txt = ["autosplit_train.txt", "autosplit_val.txt", "autosplit_test.txt"]
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™å…ˆåˆ é™¤
    for x in txt:
        if (path.parent / x).exists():
            (path.parent / x).unlink()  # åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶

    # è¾“å‡ºä¿¡æ¯ï¼ŒæŒ‡ç¤ºæ­£åœ¨å¯¹å›¾åƒè¿›è¡Œè‡ªåŠ¨åˆ†å‰²å¤„ç†ï¼Œå¹¶æ˜¾ç¤ºæ˜¯å¦åªä½¿ç”¨æœ‰æ ‡ç­¾çš„å›¾åƒæ–‡ä»¶
    LOGGER.info(f"Autosplitting images from {path}" + ", using *.txt labeled images only" * annotated_only)
    # ä½¿ç”¨ tqdm è¿­ä»£å¤„ç†ç´¢å¼•å’Œæ–‡ä»¶åˆ—è¡¨ zip(indices, files)ï¼Œæ€»æ•°ä¸º nï¼ŒåŒæ—¶æ˜¾ç¤ºè¿›åº¦æ¡
    for i, img in TQDM(zip(indices, files), total=n):
        # å¦‚æœ annotated_only ä¸º False æˆ–è€…å¯¹åº”å›¾ç‰‡çš„æ ‡ç­¾æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™æ‰§è¡Œä¸‹é¢çš„æ“ä½œ
        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€è·¯å¾„ path.parent / txt[i] å¯¹åº”çš„æ–‡ä»¶ï¼Œå¹¶å†™å…¥å½“å‰å›¾ç‰‡è·¯å¾„
            with open(path.parent / txt[i], "a") as f:
                # å°†å½“å‰å›¾ç‰‡ç›¸å¯¹äº path.parent çš„è·¯å¾„ä½œä¸º POSIX è·¯å¾„æ·»åŠ åˆ°æ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œå¹¶æ¢è¡Œ
                f.write(f"./{img.relative_to(path.parent).as_posix()}" + "\n")
def load_dataset_cache_file(path):
    """Load an Ultralytics *.cache dictionary from path."""
    import gc  # å¯¼å…¥åƒåœ¾å›æ”¶æ¨¡å—

    gc.disable()  # ç¦ç”¨åƒåœ¾å›æ”¶ï¼Œä»¥å‡å°‘ååºåˆ—åŒ–æ—¶é—´ https://github.com/ultralytics/ultralytics/pull/1585
    cache = np.load(str(path), allow_pickle=True).item()  # åŠ è½½å­—å…¸å¯¹è±¡
    gc.enable()  # å¯ç”¨åƒåœ¾å›æ”¶
    return cache  # è¿”å›åŠ è½½çš„ç¼“å­˜æ•°æ®


def save_dataset_cache_file(prefix, path, x, version):
    """Save an Ultralytics dataset *.cache dictionary x to path."""
    x["version"] = version  # æ·»åŠ ç¼“å­˜ç‰ˆæœ¬ä¿¡æ¯
    if is_dir_writeable(path.parent):  # æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦å¯å†™
        if path.exists():
            path.unlink()  # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™åˆ é™¤ *.cache æ–‡ä»¶
        np.save(str(path), x)  # å°†ç¼“å­˜ä¿å­˜åˆ°æ–‡ä»¶ä¸­ä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨
        path.with_suffix(".cache.npy").rename(path)  # ç§»é™¤ .npy åç¼€
        LOGGER.info(f"{prefix}New cache created: {path}")  # è®°å½•æ—¥å¿—ï¼Œæ˜¾ç¤ºåˆ›å»ºäº†æ–°çš„ç¼“å­˜æ–‡ä»¶
    else:
        LOGGER.warning(f"{prefix}WARNING âš ï¸ Cache directory {path.parent} is not writeable, cache not saved.")
        # è®°å½•è­¦å‘Šæ—¥å¿—ï¼Œæ˜¾ç¤ºç¼“å­˜ç›®å½•ä¸å¯å†™ï¼Œæœªä¿å­˜ç¼“å­˜ä¿¡æ¯
```