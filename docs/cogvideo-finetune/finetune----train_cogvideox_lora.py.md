# `.\cogvideo-finetune\finetune\train_cogvideox_lora.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œæ ‡æ˜ä»£ç çš„ç‰ˆæƒæ‰€æœ‰è€…åŠç›¸å…³ä¿¡æ¯
# Copyright 2024 The CogView team, Tsinghua University & ZhipuAI and The HuggingFace Team. All rights reserved.
#
# æŒ‰ç…§ Apache 2.0 è®¸å¯åè®®è¿›è¡Œæˆæƒ
# Licensed under the Apache License, Version 2.0 (the "License");
# ä½ ä¸å¾—åœ¨æœªéµå¾ªè®¸å¯çš„æƒ…å†µä¸‹ä½¿ç”¨æ­¤æ–‡ä»¶
# you may not use this file except in compliance with the License.
# ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰è§„å®šï¼Œè½¯ä»¶æ ¹æ®è®¸å¯è¯åˆ†å‘æ˜¯åŸºäºâ€œæŒ‰ç°çŠ¶â€åŸåˆ™
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# è¯·å‚è§è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€é€‚ç”¨çš„æƒé™å’Œé™åˆ¶
# See the License for the specific language governing permissions and
# limitations under the License.

# å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import argparse
# å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—
import logging
# å¯¼å…¥æ•°å­¦è¿ç®—æ¨¡å—
import math
# å¯¼å…¥æ“ä½œç³»ç»Ÿç›¸å…³çš„æ¨¡å—
import os
# å¯¼å…¥æ–‡ä»¶å’Œç›®å½•æ“ä½œæ¨¡å—
import shutil
# å¯¼å…¥è·¯å¾„å¤„ç†æ¨¡å—
from pathlib import Path
# å¯¼å…¥ç±»å‹æç¤ºç›¸å…³çš„æ¨¡å—
from typing import List, Optional, Tuple, Union

# å¯¼å…¥ PyTorch åº“
import torch
# å¯¼å…¥ Transformers åº“
import transformers
# ä» accelerate åº“å¯¼å…¥åŠ é€Ÿå™¨
from accelerate import Accelerator
# ä» accelerate.logging å¯¼å…¥è·å–æ—¥å¿—è®°å½•å™¨
from accelerate.logging import get_logger
# ä» accelerate.utils å¯¼å…¥åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç›¸å…³çš„å·¥å…·
from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed
# ä» huggingface_hub å¯¼å…¥åˆ›å»ºå’Œä¸Šä¼ æ¨¡å‹çš„å·¥å…·
from huggingface_hub import create_repo, upload_folder
# ä» peft åº“å¯¼å…¥ Lora é…ç½®å’Œæ¨¡å‹çŠ¶æ€å­—å…¸å¤„ç†å·¥å…·
from peft import LoraConfig, get_peft_model_state_dict, set_peft_model_state_dict
# ä» PyTorch çš„å·¥å…·æ•°æ®é›†æ¨¡å—å¯¼å…¥ DataLoader å’Œ Dataset
from torch.utils.data import DataLoader, Dataset
# ä» torchvision å¯¼å…¥æ•°æ®é¢„å¤„ç†å·¥å…·
from torchvision import transforms
# å¯¼å…¥è¿›åº¦æ¡å·¥å…·
from tqdm.auto import tqdm
# ä» Transformers åº“å¯¼å…¥è‡ªåŠ¨æ ‡è®°å™¨å’Œ T5 æ¨¡å‹
from transformers import AutoTokenizer, T5EncoderModel, T5Tokenizer

# å¯¼å…¥ Diffusers åº“åŠå…¶ç›¸å…³æ¨¡å—
import diffusers
# å¯¼å…¥ CogVideoX ç›¸å…³çš„æ¨¡å‹å’Œè°ƒåº¦å™¨
from diffusers import AutoencoderKLCogVideoX, CogVideoXDPMScheduler, CogVideoXPipeline, CogVideoXTransformer3DModel
# ä» Diffusers å¯¼å…¥è·å– 3D æ—‹è½¬ä½ç½®åµŒå…¥çš„å·¥å…·
from diffusers.models.embeddings import get_3d_rotary_pos_embed
# ä» Diffusers å¯¼å…¥è·å–è°ƒåº¦å™¨çš„å·¥å…·
from diffusers.optimization import get_scheduler
# ä» Diffusers çš„ CogVideoX ç®¡é“å¯¼å…¥è°ƒæ•´åŒºåŸŸçš„å·¥å…·
from diffusers.pipelines.cogvideo.pipeline_cogvideox import get_resize_crop_region_for_grid
# ä» Diffusers å¯¼å…¥è®­ç»ƒç›¸å…³çš„å·¥å…·
from diffusers.training_utils import (
    cast_training_params,  # è½¬æ¢è®­ç»ƒå‚æ•°çš„å·¥å…·
    free_memory,           # é‡Šæ”¾å†…å­˜çš„å·¥å…·
)
# ä» Diffusers å¯¼å…¥å·¥å…·é›†
from diffusers.utils import check_min_version, convert_unet_state_dict_to_peft, export_to_video, is_wandb_available
# ä» Diffusers çš„ Hub å·¥å…·å¯¼å…¥æ¨¡å‹å¡åŠ è½½ä¸åˆ›å»ºå·¥å…·
from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card
# ä» Diffusers å¯¼å…¥ä¸ PyTorch ç›¸å…³çš„å·¥å…·
from diffusers.utils.torch_utils import is_compiled_module

# å¦‚æœå¯ç”¨ï¼Œå¯¼å…¥ Weights & Biases åº“
if is_wandb_available():
    import wandb

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æœ€å°ç‰ˆæœ¬çš„ Diffusersï¼Œå¦‚æœæœªå®‰è£…å°†å¼•å‘é”™è¯¯
# Will error if the minimal version of diffusers is not installed. Remove at your own risks.
check_min_version("0.31.0.dev0")

# è·å–æ—¥å¿—è®°å½•å™¨ï¼Œè®°å½•å½“å‰æ¨¡å—çš„æ—¥å¿—ä¿¡æ¯
logger = get_logger(__name__)


# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºä¸€ä¸ªè§£æå™¨ï¼Œæè¿°è®­ç»ƒè„šæœ¬çš„ç®€å•ç¤ºä¾‹
    parser = argparse.ArgumentParser(description="Simple example of a training script for CogVideoX.")

    # æ·»åŠ é¢„è®­ç»ƒæ¨¡å‹ä¿¡æ¯å‚æ•°
    parser.add_argument(
        "--pretrained_model_name_or_path",  # å‚æ•°å
        type=str,                            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        default=None,                        # é»˜è®¤å€¼ä¸º None
        required=True,                       # è¯¥å‚æ•°ä¸ºå¿…éœ€é¡¹
        help="Path to pretrained model or model identifier from huggingface.co/models.",  # å‚æ•°å¸®åŠ©ä¿¡æ¯
    )
    # æ·»åŠ æ¨¡å‹ä¿®è®¢ç‰ˆæœ¬å‚æ•°
    parser.add_argument(
        "--revision",                        # å‚æ•°å
        type=str,                            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        default=None,                        # é»˜è®¤å€¼ä¸º None
        required=False,                      # è¯¥å‚æ•°ä¸ºå¯é€‰é¡¹
        help="Revision of pretrained model identifier from huggingface.co/models.",  # å‚æ•°å¸®åŠ©ä¿¡æ¯
    )
    # æ·»åŠ æ¨¡å‹å˜ä½“å‚æ•°
    parser.add_argument(
        "--variant",                         # å‚æ•°å
        type=str,                            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        default=None,                        # é»˜è®¤å€¼ä¸º None
        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",  # å‚æ•°å¸®åŠ©ä¿¡æ¯
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --cache_dirï¼ŒæŒ‡å®šç¼“å­˜ç›®å½•
        parser.add_argument(
            "--cache_dir",
            type=str,
            default=None,
            help="The directory where the downloaded models and datasets will be stored.",
        )
    
        # æ•°æ®é›†ä¿¡æ¯
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --dataset_nameï¼ŒæŒ‡å®šæ•°æ®é›†åç§°
        parser.add_argument(
            "--dataset_name",
            type=str,
            default=None,
            help=(
                "The name of the Dataset (from the HuggingFace hub) containing the training data of instance images (could be your own, possibly private,"
                " dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,"
                " or to a folder containing files that ğŸ¤— Datasets can understand."
            ),
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --dataset_config_nameï¼ŒæŒ‡å®šæ•°æ®é›†é…ç½®åç§°
        parser.add_argument(
            "--dataset_config_name",
            type=str,
            default=None,
            help="The config of the Dataset, leave as None if there's only one config.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --instance_data_rootï¼ŒæŒ‡å®šè®­ç»ƒæ•°æ®æ ¹ç›®å½•
        parser.add_argument(
            "--instance_data_root",
            type=str,
            default=None,
            help=("A folder containing the training data."),
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --video_columnï¼ŒæŒ‡å®šåŒ…å«è§†é¢‘çš„åˆ—åç§°
        parser.add_argument(
            "--video_column",
            type=str,
            default="video",
            help="The column of the dataset containing videos. Or, the name of the file in `--instance_data_root` folder containing the line-separated path to video data.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --caption_columnï¼ŒæŒ‡å®šåŒ…å«æç¤ºæ–‡æœ¬çš„åˆ—åç§°
        parser.add_argument(
            "--caption_column",
            type=str,
            default="text",
            help="The column of the dataset containing the instance prompt for each video. Or, the name of the file in `--instance_data_root` folder containing the line-separated instance prompts.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --id_tokenï¼ŒæŒ‡å®šæ ‡è¯†ç¬¦ä»¤ç‰Œ
        parser.add_argument(
            "--id_token", type=str, default=None, help="Identifier token appended to the start of each prompt if provided."
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --dataloader_num_workersï¼ŒæŒ‡å®šæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°é‡
        parser.add_argument(
            "--dataloader_num_workers",
            type=int,
            default=0,
            help=(
                "Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process."
            ),
        )
    
        # éªŒè¯
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --validation_promptï¼ŒæŒ‡å®šéªŒè¯æ—¶ä½¿ç”¨çš„æç¤º
        parser.add_argument(
            "--validation_prompt",
            type=str,
            default=None,
            help="One or more prompt(s) that is used during validation to verify that the model is learning. Multiple validation prompts should be separated by the '--validation_prompt_seperator' string.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --validation_prompt_separatorï¼ŒæŒ‡å®šéªŒè¯æç¤ºçš„åˆ†éš”ç¬¦
        parser.add_argument(
            "--validation_prompt_separator",
            type=str,
            default=":::",
            help="String that separates multiple validation prompts",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --num_validation_videosï¼ŒæŒ‡å®šæ¯ä¸ªéªŒè¯æç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
        parser.add_argument(
            "--num_validation_videos",
            type=int,
            default=1,
            help="Number of videos that should be generated during validation per `validation_prompt`.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --validation_epochsï¼ŒæŒ‡å®šæ¯éš”å¤šå°‘ä¸ªå‘¨æœŸè¿›è¡Œä¸€æ¬¡éªŒè¯
        parser.add_argument(
            "--validation_epochs",
            type=int,
            default=50,
            help=(
                "Run validation every X epochs. Validation consists of running the prompt `args.validation_prompt` multiple times: `args.num_validation_videos`."
            ),
        )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæŒ‡å¯¼å°ºåº¦ï¼Œç”¨äºé‡‡æ ·éªŒè¯è§†é¢‘
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=6,
        help="The guidance scale to use while sampling validation videos.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦ä½¿ç”¨åŠ¨æ€é…ç½®æ ‡å¿—
    parser.add_argument(
        "--use_dynamic_cfg",
        action="store_true",
        default=False,
        help="Whether or not to use the default cosine dynamic guidance schedule when sampling validation videos.",
    )

    # è®­ç»ƒä¿¡æ¯
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šéšæœºç§å­ä»¥ç¡®ä¿è®­ç»ƒçš„å¯é‡å¤æ€§
    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šLoRAæ›´æ–°çŸ©é˜µçš„ç»´åº¦
    parser.add_argument(
        "--rank",
        type=int,
        default=128,
        help=("The dimension of the LoRA update matrices."),
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šLoRAæƒé‡æ›´æ–°çš„ç¼©æ”¾å› å­
    parser.add_argument(
        "--lora_alpha",
        type=float,
        default=128,
        help=("The scaling factor to scale LoRA weight update. The actual scaling factor is `lora_alpha / rank`"),
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    parser.add_argument(
        "--mixed_precision",
        type=str,
        default=None,
        choices=["no", "fp16", "bf16"],
        help=(
            "Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
            " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
            " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
        ),
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ¨¡å‹é¢„æµ‹å’Œæ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•
    parser.add_argument(
        "--output_dir",
        type=str,
        default="cogvideox-lora",
        help="The output directory where the model predictions and checkpoints will be written.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘çš„é«˜åº¦
    parser.add_argument(
        "--height",
        type=int,
        default=480,
        help="All input videos are resized to this height.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘çš„å®½åº¦
    parser.add_argument(
        "--width",
        type=int,
        default=720,
        help="All input videos are resized to this width.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘çš„å¸§ç‡
    parser.add_argument("--fps", type=int, default=8, help="All input videos will be used at this FPS.")
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘å°†è¢«æˆªæ–­åˆ°çš„å¸§æ•°
    parser.add_argument(
        "--max_num_frames", type=int, default=49, help="All input videos will be truncated to these many frames."
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šä»æ¯ä¸ªè¾“å…¥è§†é¢‘å¼€å§‹è·³è¿‡çš„å¸§æ•°
    parser.add_argument(
        "--skip_frames_start",
        type=int,
        default=0,
        help="Number of frames to skip from the beginning of each input video. Useful if training data contains intro sequences.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šä»æ¯ä¸ªè¾“å…¥è§†é¢‘æœ«å°¾è·³è¿‡çš„å¸§æ•°
    parser.add_argument(
        "--skip_frames_end",
        type=int,
        default=0,
        help="Number of frames to skip from the end of each input video. Useful if training data contains outro sequences.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦éšæœºæ°´å¹³ç¿»è½¬è§†é¢‘
    parser.add_argument(
        "--random_flip",
        action="store_true",
        help="whether to randomly flip videos horizontally",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šè®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ‰¹é‡å¤§å°ï¼ˆæ¯ä¸ªè®¾å¤‡ï¼‰
    parser.add_argument(
        "--train_batch_size", type=int, default=4, help="Batch size (per device) for the training dataloader."
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šè®­ç»ƒçš„å‘¨æœŸæ•°
    parser.add_argument("--num_train_epochs", type=int, default=1)
    # æ·»åŠ å‚æ•° `--max_train_steps`ï¼Œç”¨äºæŒ‡å®šè®­ç»ƒçš„æœ€å¤§æ­¥æ•°
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Total number of training steps to perform. If provided, overrides `--num_train_epochs`.",
    )
    # æ·»åŠ å‚æ•° `--checkpointing_steps`ï¼Œç”¨äºæŒ‡å®šæ¯ X æ¬¡æ›´æ–°ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    parser.add_argument(
        "--checkpointing_steps",
        type=int,
        default=500,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help=(
            "Save a checkpoint of the training state every X updates. These checkpoints can be used both as final"
            " checkpoints in case they are better than the last checkpoint, and are also suitable for resuming"
            " training using `--resume_from_checkpoint`."
        ),
    )
    # æ·»åŠ å‚æ•° `--checkpoints_total_limit`ï¼Œç”¨äºæŒ‡å®šè¦å­˜å‚¨çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡
    parser.add_argument(
        "--checkpoints_total_limit",
        type=int,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help=("Max number of checkpoints to store."),
    )
    # æ·»åŠ å‚æ•° `--resume_from_checkpoint`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦ä»ä¹‹å‰çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help=(
            "Whether training should be resumed from a previous checkpoint. Use a path saved by"
            ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
        ),
    )
    # æ·»åŠ å‚æ•° `--gradient_accumulation_steps`ï¼Œç”¨äºæŒ‡å®šåœ¨æ‰§è¡Œåå‘ä¼ æ’­å’Œæ›´æ–°ä¹‹å‰ç§¯ç´¯çš„æ›´æ–°æ­¥æ•°
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    # æ·»åŠ å‚æ•° `--gradient_checkpointing`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.",
    )
    # æ·»åŠ å‚æ•° `--learning_rate`ï¼Œç”¨äºæŒ‡å®šåˆå§‹å­¦ä¹ ç‡
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    # æ·»åŠ å‚æ•° `--scale_lr`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦æŒ‰ GPU æ•°é‡ã€æ¢¯åº¦ç§¯ç´¯æ­¥æ•°å’Œæ‰¹é‡å¤§å°ç¼©æ”¾å­¦ä¹ ç‡
    parser.add_argument(
        "--scale_lr",
        action="store_true",
        default=False,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.",
    )
    # æ·»åŠ å‚æ•° `--lr_scheduler`ï¼Œç”¨äºæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„ç±»å‹
    parser.add_argument(
        "--lr_scheduler",
        type=str,
        default="constant",
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„å¯é€‰å€¼
        help=(
            'The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
            ' "constant", "constant_with_warmup"]'
        ),
    )
    # æ·»åŠ å‚æ•° `--lr_warmup_steps`ï¼Œç”¨äºæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„é¢„çƒ­æ­¥æ•°
    parser.add_argument(
        "--lr_warmup_steps", type=int, default=500, 
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Number of steps for the warmup in the lr scheduler."
    )
    # æ·»åŠ å‚æ•° `--lr_num_cycles`ï¼Œç”¨äºæŒ‡å®šåœ¨ `cosine_with_restarts` è°ƒåº¦å™¨ä¸­çš„ç¡¬é‡ç½®æ¬¡æ•°
    parser.add_argument(
        "--lr_num_cycles",
        type=int,
        default=1,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Number of hard resets of the lr in cosine_with_restarts scheduler.",
    )
    # æ·»åŠ å‚æ•° `--lr_power`ï¼Œç”¨äºæŒ‡å®šå¤šé¡¹å¼è°ƒåº¦å™¨çš„å¹‚å› å­
    parser.add_argument("--lr_power", type=float, default=1.0, 
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Power factor of the polynomial scheduler."
    )
    # æ·»åŠ å‚æ•° `--enable_slicing`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦ä½¿ç”¨ VAE åˆ‡ç‰‡ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_slicing",
        action="store_true",
        default=False,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Whether or not to use VAE slicing for saving memory.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºå¯ç”¨æˆ–ç¦ç”¨ VAE ç“¦ç‰‡åŠŸèƒ½ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_tiling",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º False
        default=False,
        help="Whether or not to use VAE tiling for saving memory.",  # å‚æ•°è¯´æ˜
    )

    # ä¼˜åŒ–å™¨é…ç½®
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œé€‰æ‹©ä¼˜åŒ–å™¨ç±»å‹
    parser.add_argument(
        "--optimizer",
        type=lambda s: s.lower(),  # å°†è¾“å…¥è½¬ä¸ºå°å†™
        default="adam",  # é»˜è®¤ä½¿ç”¨ Adam ä¼˜åŒ–å™¨
        choices=["adam", "adamw", "prodigy"],  # å¯é€‰çš„ä¼˜åŒ–å™¨ç±»å‹
        help=("The optimizer type to use."),  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ 8-bit Adam ä¼˜åŒ–å™¨
    parser.add_argument(
        "--use_8bit_adam",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹
        help="Whether or not to use 8-bit Adam from bitsandbytes. Ignored if optimizer is not set to AdamW",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam ä¼˜åŒ–å™¨çš„ beta1 å‚æ•°
    parser.add_argument(
        "--adam_beta1", type=float, default=0.9, help="The beta1 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam ä¼˜åŒ–å™¨çš„ beta2 å‚æ•°
    parser.add_argument(
        "--adam_beta2", type=float, default=0.95, help="The beta2 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Prodigy ä¼˜åŒ–å™¨çš„ beta3 å‚æ•°
    parser.add_argument(
        "--prodigy_beta3",
        type=float,  # å‚æ•°ç±»å‹ä¸ºæµ®ç‚¹æ•°
        default=None,  # é»˜è®¤å€¼ä¸º None
        help="Coefficients for computing the Prodigy optimizer's stepsize using running averages. If set to None, uses the value of square root of beta2.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ AdamW é£æ ¼çš„è§£è€¦æƒé‡è¡°å‡
    parser.add_argument("--prodigy_decouple", action="store_true", help="Use AdamW style decoupled weight decay")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡
    parser.add_argument("--adam_weight_decay", type=float, default=1e-04, help="Weight decay to use for unet params")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ epsilon å€¼
    parser.add_argument(
        "--adam_epsilon",
        type=float,
        default=1e-08,  # é»˜è®¤ epsilon å€¼
        help="Epsilon value for the Adam optimizer and Prodigy optimizers.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®æœ€å¤§æ¢¯åº¦èŒƒæ•°
    parser.add_argument("--max_grad_norm", default=1.0, type=float, help="Max gradient norm.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦å¼€å¯ Adam çš„åå·®ä¿®æ­£
    parser.add_argument("--prodigy_use_bias_correction", action="store_true", help="Turn on Adam's bias correction.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦åœ¨æš–å¯åŠ¨é˜¶æ®µç§»é™¤ lr åœ¨ D ä¼°è®¡çš„åˆ†æ¯ä¸­
    parser.add_argument(
        "--prodigy_safeguard_warmup",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹
        help="Remove lr from the denominator of D estimate to avoid issues during warm-up stage.",  # å‚æ•°è¯´æ˜
    )

    # å…¶ä»–ä¿¡æ¯
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®é¡¹ç›®è¿½è¸ªå™¨åç§°
    parser.add_argument("--tracker_name", type=str, default=None, help="Project tracker name")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° Hub
    parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®æ¨é€åˆ°æ¨¡å‹ Hub æ—¶ä½¿ç”¨çš„ token
    parser.add_argument("--hub_token", type=str, default=None, help="The token to use to push to the Model Hub.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®ä¸æœ¬åœ° output_dir åŒæ­¥çš„ä»“åº“åç§°
    parser.add_argument(
        "--hub_model_id",
        type=str,
        default=None,  # é»˜è®¤å€¼ä¸º None
        help="The name of the repository to keep in sync with the local `output_dir`.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®æ—¥å¿—å­˜å‚¨ç›®å½•
    parser.add_argument(
        "--logging_dir",
        type=str,
        default="logs",  # é»˜è®¤æ—¥å¿—ç›®å½•
        help="Directory where logs are stored.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦å…è®¸åœ¨ Ampere GPU ä¸Šä½¿ç”¨ TF32
    parser.add_argument(
        "--allow_tf32",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹
        help=(
            "Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see"
            " https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices"  # å‚æ•°è¯´æ˜
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° '--report_to' çš„é…ç½®
        parser.add_argument(
            # å‚æ•°åç§°
            "--report_to",
            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
            type=str,
            # é»˜è®¤å€¼ä¸º None
            default=None,
            # å‚æ•°å¸®åŠ©ä¿¡æ¯ï¼Œè§£é‡Šè¯¥å‚æ•°çš„ç”¨é€”
            help=(
                # æä¾›æ”¯æŒçš„å¹³å°å’Œé»˜è®¤å€¼è¯´æ˜
                'The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
                # ç»§ç»­å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜å¯é€‰çš„å¹³å°
                ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
            ),
        )
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ç»“æœ
        return parser.parse_args()
# å®šä¹‰ä¸€ä¸ªè§†é¢‘æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ª Dataset
class VideoDataset(Dataset):
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®æ•°æ®é›†çš„å„ç§å‚æ•°
    def __init__(
        self,
        instance_data_root: Optional[str] = None,  # å®ä¾‹æ•°æ®æ ¹ç›®å½•ï¼Œå¯é€‰
        dataset_name: Optional[str] = None,  # æ•°æ®é›†åç§°ï¼Œå¯é€‰
        dataset_config_name: Optional[str] = None,  # æ•°æ®é›†é…ç½®åç§°ï¼Œå¯é€‰
        caption_column: str = "text",  # æè¿°æ–‡æœ¬æ‰€åœ¨åˆ—åï¼Œé»˜è®¤ä¸º "text"
        video_column: str = "video",  # è§†é¢‘æ•°æ®æ‰€åœ¨åˆ—åï¼Œé»˜è®¤ä¸º "video"
        height: int = 480,  # è§†é¢‘é«˜åº¦ï¼Œé»˜è®¤ä¸º 480
        width: int = 720,  # è§†é¢‘å®½åº¦ï¼Œé»˜è®¤ä¸º 720
        fps: int = 8,  # æ¯ç§’å¸§æ•°ï¼Œé»˜è®¤ä¸º 8
        max_num_frames: int = 49,  # æœ€å¤§å¸§æ•°ï¼Œé»˜è®¤ä¸º 49
        skip_frames_start: int = 0,  # å¼€å§‹è·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
        skip_frames_end: int = 0,  # ç»“æŸè·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
        cache_dir: Optional[str] = None,  # ç¼“å­˜ç›®å½•ï¼Œå¯é€‰
        id_token: Optional[str] = None,  # ID ä»¤ç‰Œï¼Œå¯é€‰
    ) -> None:
        super().__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•

        # å¦‚æœæä¾›äº†å®ä¾‹æ•°æ®æ ¹ç›®å½•ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸º Path å¯¹è±¡
        self.instance_data_root = Path(instance_data_root) if instance_data_root is not None else None
        # è®¾ç½®æ•°æ®é›†åç§°
        self.dataset_name = dataset_name
        # è®¾ç½®æ•°æ®é›†é…ç½®åç§°
        self.dataset_config_name = dataset_config_name
        # è®¾ç½®æè¿°æ–‡æœ¬æ‰€åœ¨åˆ—å
        self.caption_column = caption_column
        # è®¾ç½®è§†é¢‘æ•°æ®æ‰€åœ¨åˆ—å
        self.video_column = video_column
        # è®¾ç½®è§†é¢‘é«˜åº¦
        self.height = height
        # è®¾ç½®è§†é¢‘å®½åº¦
        self.width = width
        # è®¾ç½®æ¯ç§’å¸§æ•°
        self.fps = fps
        # è®¾ç½®æœ€å¤§å¸§æ•°
        self.max_num_frames = max_num_frames
        # è®¾ç½®å¼€å§‹è·³è¿‡çš„å¸§æ•°
        self.skip_frames_start = skip_frames_start
        # è®¾ç½®ç»“æŸè·³è¿‡çš„å¸§æ•°
        self.skip_frames_end = skip_frames_end
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        # è®¾ç½® ID ä»¤ç‰Œï¼Œå¦‚æœæœªæä¾›åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
        self.id_token = id_token or ""

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œåˆ™ä»æ•°æ®é›†ä¸­åŠ è½½å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„
        if dataset_name is not None:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_hub()
        # å¦åˆ™ä»æœ¬åœ°è·¯å¾„åŠ è½½å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„
        else:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_local_path()

        # è®¡ç®—å®ä¾‹è§†é¢‘çš„æ•°é‡
        self.num_instance_videos = len(self.instance_video_paths)
        # æ£€æŸ¥å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„æ•°é‡æ˜¯å¦åŒ¹é…
        if self.num_instance_videos != len(self.instance_prompts):
            raise ValueError(
                # æŠ›å‡ºé”™è¯¯ï¼Œæç¤ºå®ä¾‹æç¤ºå’Œè§†é¢‘æ•°é‡ä¸åŒ¹é…
                f"Expected length of instance prompts and videos to be the same but found {len(self.instance_prompts)=} and {len(self.instance_video_paths)=}. Please ensure that the number of caption prompts and videos match in your dataset."
            )

        # é¢„å¤„ç†æ•°æ®ä»¥è·å–å®ä¾‹è§†é¢‘
        self.instance_videos = self._preprocess_data()

    # è¿”å›æ•°æ®é›†çš„é•¿åº¦
    def __len__(self):
        return self.num_instance_videos

    # æ ¹æ®ç´¢å¼•è·å–å®ä¾‹æ•°æ®
    def __getitem__(self, index):
        return {
            # è¿”å›ç»„åˆåçš„å®ä¾‹æç¤º
            "instance_prompt": self.id_token + self.instance_prompts[index],
            # è¿”å›å¯¹åº”çš„å®ä¾‹è§†é¢‘
            "instance_video": self.instance_videos[index],
        }
    # ä»æ•°æ®é›†ä¸­å¿ƒåŠ è½½æ•°æ®é›†çš„ç§æœ‰æ–¹æ³•
    def _load_dataset_from_hub(self):
        try:
            # å°è¯•å¯¼å…¥ datasets åº“
            from datasets import load_dataset
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡º ImportErrorï¼Œå¹¶æä¾›å®‰è£…æç¤º
            raise ImportError(
                "You are trying to load your data using the datasets library. If you wish to train using custom "
                "captions please install the datasets library: `pip install datasets`. If you wish to load a "
                "local folder containing images only, specify --instance_data_root instead."
            )

        # ä»æ•°æ®é›†ä¸­å¿ƒä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†ï¼Œå…³äºå¦‚ä½•åŠ è½½è‡ªå®šä¹‰å›¾åƒçš„ä¿¡æ¯è§é“¾æ¥
        # https://huggingface.co/docs/datasets/v2.0.0/en/dataset_script
        dataset = load_dataset(
            self.dataset_name,  # æ•°æ®é›†åç§°
            self.dataset_config_name,  # æ•°æ®é›†é…ç½®åç§°
            cache_dir=self.cache_dir,  # ç¼“å­˜ç›®å½•
        )
        # è·å–è®­ç»ƒé›†çš„åˆ—å
        column_names = dataset["train"].column_names

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè§†é¢‘åˆ—
        if self.video_column is None:
            # é»˜è®¤ä½¿ç”¨åˆ—ååˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªåˆ—åä½œä¸ºè§†é¢‘åˆ—
            video_column = column_names[0]
            # è®°å½•ä½¿ç”¨é»˜è®¤è§†é¢‘åˆ—çš„ä¿¡æ¯
            logger.info(f"`video_column` defaulting to {video_column}")
        else:
            # å¦‚æœå·²æŒ‡å®šè§†é¢‘åˆ—ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„åˆ—å
            video_column = self.video_column
            # æ£€æŸ¥æŒ‡å®šçš„è§†é¢‘åˆ—æ˜¯å¦åœ¨åˆ—åä¸­
            if video_column not in column_names:
                # å¦‚æœä¸åœ¨ï¼ŒæŠ›å‡º ValueError
                raise ValueError(
                    f"`--video_column` value '{video_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                )

        # å¦‚æœæ²¡æœ‰æŒ‡å®šå­—å¹•åˆ—
        if self.caption_column is None:
            # é»˜è®¤ä½¿ç”¨åˆ—ååˆ—è¡¨ä¸­çš„ç¬¬äºŒä¸ªåˆ—åä½œä¸ºå­—å¹•åˆ—
            caption_column = column_names[1]
            # è®°å½•ä½¿ç”¨é»˜è®¤å­—å¹•åˆ—çš„ä¿¡æ¯
            logger.info(f"`caption_column` defaulting to {caption_column}")
        else:
            # å¦‚æœå·²æŒ‡å®šå­—å¹•åˆ—ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„åˆ—å
            caption_column = self.caption_column
            # æ£€æŸ¥æŒ‡å®šçš„å­—å¹•åˆ—æ˜¯å¦åœ¨åˆ—åä¸­
            if self.caption_column not in column_names:
                # å¦‚æœä¸åœ¨ï¼ŒæŠ›å‡º ValueError
                raise ValueError(
                    f"`--caption_column` value '{self.caption_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                )

        # ä»è®­ç»ƒé›†ä¸­æå–å®ä¾‹æç¤ºï¼ˆå­—å¹•ï¼‰
        instance_prompts = dataset["train"][caption_column]
        # æ ¹æ®è§†é¢‘åˆ—çš„æ–‡ä»¶è·¯å¾„åˆ›å»ºè§†é¢‘å®ä¾‹åˆ—è¡¨
        instance_videos = [Path(self.instance_data_root, filepath) for filepath in dataset["train"][video_column]]

        # è¿”å›å®ä¾‹æç¤ºå’Œè§†é¢‘å®ä¾‹åˆ—è¡¨
        return instance_prompts, instance_videos
    # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†
        def _load_dataset_from_local_path(self):
            # æ£€æŸ¥å®ä¾‹æ•°æ®æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.instance_data_root.exists():
                # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError("Instance videos root folder does not exist")
    
            # æ„å»ºæç¤ºæ–‡ä»¶è·¯å¾„
            prompt_path = self.instance_data_root.joinpath(self.caption_column)
            # æ„å»ºè§†é¢‘æ–‡ä»¶è·¯å¾„
            video_path = self.instance_data_root.joinpath(self.video_column)
    
            # æ£€æŸ¥æç¤ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ˜¯æ–‡ä»¶
            if not prompt_path.exists() or not prompt_path.is_file():
                # å¦‚æœä¸æ˜¯ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError(
                    "Expected `--caption_column` to be path to a file in `--instance_data_root` containing line-separated text prompts."
                )
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ˜¯æ–‡ä»¶
            if not video_path.exists() or not video_path.is_file():
                # å¦‚æœä¸æ˜¯ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError(
                    "Expected `--video_column` to be path to a file in `--instance_data_root` containing line-separated paths to video data in the same directory."
                )
    
            # æ‰“å¼€æç¤ºæ–‡ä»¶å¹¶è¯»å–æ¯è¡Œï¼Œå»é™¤é¦–å°¾ç©ºç™½ï¼Œå½¢æˆæç¤ºåˆ—è¡¨
            with open(prompt_path, "r", encoding="utf-8") as file:
                instance_prompts = [line.strip() for line in file.readlines() if len(line.strip()) > 0]
            # æ‰“å¼€è§†é¢‘æ–‡ä»¶å¹¶è¯»å–æ¯è¡Œï¼Œå»é™¤é¦–å°¾ç©ºç™½ï¼Œå½¢æˆè§†é¢‘è·¯å¾„åˆ—è¡¨
            with open(video_path, "r", encoding="utf-8") as file:
                instance_videos = [
                    self.instance_data_root.joinpath(line.strip()) for line in file.readlines() if len(line.strip()) > 0
                ]
    
            # æ£€æŸ¥è§†é¢‘è·¯å¾„åˆ—è¡¨ä¸­æ˜¯å¦æœ‰æ— æ•ˆæ–‡ä»¶è·¯å¾„
            if any(not path.is_file() for path in instance_videos):
                # å¦‚æœæœ‰ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError(
                    "Expected '--video_column' to be a path to a file in `--instance_data_root` containing line-separated paths to video data but found atleast one path that is not a valid file."
                )
    
            # è¿”å›å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„åˆ—è¡¨
            return instance_prompts, instance_videos
    # å®šä¹‰æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•
    def _preprocess_data(self):
        # å°è¯•å¯¼å…¥ decord åº“
        try:
            import decord
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æŠ›å‡ºé”™è¯¯ï¼Œæç¤ºéœ€è¦å®‰è£… decord
        except ImportError:
            raise ImportError(
                "The `decord` package is required for loading the video dataset. Install with `pip install decord`"
            )

        # è®¾ç½® decord ä½¿ç”¨ PyTorch ä½œä¸ºåç«¯
        decord.bridge.set_bridge("torch")

        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨è§†é¢‘æ•°æ®
        videos = []
        # å®šä¹‰è®­ç»ƒæ—¶çš„è½¬æ¢æ“ä½œ
        train_transforms = transforms.Compose(
            [
                # å°†åƒç´ å€¼å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
                transforms.Lambda(lambda x: x / 255.0 * 2.0 - 1.0),
            ]
        )

        # éå†æ¯ä¸ªè§†é¢‘æ–‡ä»¶çš„è·¯å¾„
        for filename in self.instance_video_paths:
            # ä½¿ç”¨ decord è¯»å–è§†é¢‘ï¼ŒæŒ‡å®šå®½åº¦å’Œé«˜åº¦
            video_reader = decord.VideoReader(uri=filename.as_posix(), width=self.width, height=self.height)
            # è·å–è§†é¢‘çš„å¸§æ•°
            video_num_frames = len(video_reader)

            # è®¡ç®—å¼€å§‹å¸§å’Œç»“æŸå¸§çš„ç´¢å¼•
            start_frame = min(self.skip_frames_start, video_num_frames)
            end_frame = max(0, video_num_frames - self.skip_frames_end)
            # å¦‚æœç»“æŸå¸§å°äºç­‰äºå¼€å§‹å¸§ï¼Œåªè·å–å¼€å§‹å¸§çš„å¸§æ•°æ®
            if end_frame <= start_frame:
                frames = video_reader.get_batch([start_frame])
            # å¦‚æœè¦è·å–çš„å¸§æ•°é‡åœ¨å…è®¸çš„æœ€å¤§èŒƒå›´å†…
            elif end_frame - start_frame <= self.max_num_frames:
                frames = video_reader.get_batch(list(range(start_frame, end_frame)))
            # å¦‚æœè¦è·å–çš„å¸§æ•°é‡è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œåˆ™æŒ‰æ­¥é•¿è·å–
            else:
                indices = list(range(start_frame, end_frame, (end_frame - start_frame) // self.max_num_frames))
                frames = video_reader.get_batch(indices)

            # ç¡®ä¿å¸§æ•°é‡ä¸è¶…è¿‡æœ€å¤§é™åˆ¶
            frames = frames[: self.max_num_frames]
            # è·å–å½“å‰é€‰æ‹©çš„å¸§æ•°é‡
            selected_num_frames = frames.shape[0]

            # é€‰æ‹©å‰ (4k + 1) å¸§ï¼Œä»¥æ»¡è¶³ VAE çš„éœ€æ±‚
            remainder = (3 + (selected_num_frames % 4)) % 4
            # å¦‚æœå¸§æ•°é‡ä¸æ˜¯ 4 çš„å€æ•°ï¼Œå»æ‰å¤šä½™çš„å¸§
            if remainder != 0:
                frames = frames[:-remainder]
            # æ›´æ–°é€‰æ‹©çš„å¸§æ•°é‡
            selected_num_frames = frames.shape[0]

            # ç¡®ä¿é€‰æ‹©çš„å¸§æ•°é‡å‡ 1 æ˜¯ 4 çš„å€æ•°
            assert (selected_num_frames - 1) % 4 == 0

            # åº”ç”¨è®­ç»ƒè½¬æ¢æ“ä½œ
            frames = frames.float()
            # å°†æ¯ä¸€å¸§åº”ç”¨è½¬æ¢å¹¶å †å æˆä¸€ä¸ªæ–°çš„å¼ é‡
            frames = torch.stack([train_transforms(frame) for frame in frames], dim=0)
            # å°†å¤„ç†åçš„å¸§æŒ‰ç…§ [F, C, H, W] çš„é¡ºåºæ’åˆ—å¹¶å­˜å…¥è§†é¢‘åˆ—è¡¨
            videos.append(frames.permute(0, 3, 1, 2).contiguous())  # [F, C, H, W]

        # è¿”å›å¤„ç†åçš„è§†é¢‘æ•°æ®
        return videos
# ä¿å­˜æ¨¡å‹å¡ç‰‡ä¿¡æ¯
def save_model_card(
    repo_id: str,  # æ¨¡å‹ä»“åº“çš„æ ‡è¯†
    videos=None,  # å¯é€‰çš„è§†é¢‘åˆ—è¡¨
    base_model: str = None,  # åŸºç¡€æ¨¡å‹çš„åç§°ï¼Œé»˜è®¤ä¸º None
    validation_prompt=None,  # éªŒè¯æ—¶ä½¿ç”¨çš„æç¤ºè¯­
    repo_folder=None,  # æ¨¡å‹å­˜å‚¨çš„æ–‡ä»¶å¤¹è·¯å¾„
    fps=8,  # è§†é¢‘å¸§ç‡ï¼Œé»˜è®¤ä¸º 8
):
    widget_dict = []  # åˆå§‹åŒ–å°éƒ¨ä»¶å­—å…¸ï¼Œç”¨äºå­˜å‚¨è§†é¢‘ä¿¡æ¯
    if videos is not None:  # æ£€æŸ¥è§†é¢‘åˆ—è¡¨æ˜¯å¦ä¸ä¸ºç©º
        for i, video in enumerate(videos):  # éå†è§†é¢‘åˆ—è¡¨ï¼Œè·å–ç´¢å¼•å’Œè§†é¢‘å¯¹è±¡
            # å°†è§†é¢‘å¯¼å‡ºåˆ°æŒ‡å®šè·¯å¾„ï¼Œå¹¶è®¾ç½®å¸§ç‡
            export_to_video(video, os.path.join(repo_folder, f"final_video_{i}.mp4", fps=fps))
            # å°†è§†é¢‘ä¿¡æ¯æ·»åŠ åˆ°å°éƒ¨ä»¶å­—å…¸ä¸­
            widget_dict.append(
                {"text": validation_prompt if validation_prompt else " ", "output": {"url": f"video_{i}.mp4"}}
            )

    # å®šä¹‰æ¨¡å‹æè¿°ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹ ID å’ŒåŸºç¡€æ¨¡å‹åç§°
    model_description = f"""
# CogVideoX LoRA - {repo_id}

<Gallery />

## Model description

These are {repo_id} LoRA weights for {base_model}.

The weights were trained using the [CogVideoX Diffusers trainer](https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/train_cogvideox_lora.py).

Was LoRA for the text encoder enabled? No.

## Download model

[Download the *.safetensors LoRA]({repo_id}/tree/main) in the Files & versions tab.

## Use it with the [ğŸ§¨ diffusers library](https://github.com/huggingface/diffusers)


from diffusers import CogVideoXPipeline  # å¯¼å…¥ CogVideoXPipeline ç±»
import torch  # å¯¼å…¥ PyTorch åº“

# ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ç®¡é“ï¼Œè®¾ç½®æ•°æ®ç±»å‹ä¸º bfloat16ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ° CUDA
pipe = CogVideoXPipeline.from_pretrained("THUDM/CogVideoX-5b", torch_dtype=torch.bfloat16).to("cuda")
# åŠ è½½ LoRA æƒé‡ï¼ŒæŒ‡å®šæƒé‡æ–‡ä»¶åå’Œé€‚é…å™¨åç§°
pipe.load_lora_weights("{repo_id}", weight_name="pytorch_lora_weights.safetensors", adapter_name=["cogvideox-lora"])

# LoRA é€‚é…å™¨æƒé‡æ˜¯åŸºäºè®­ç»ƒæ—¶ä½¿ç”¨çš„å‚æ•°ç¡®å®šçš„ã€‚
# åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡è®¾ `--lora_alpha` æ˜¯ 32ï¼Œ`--rank` æ˜¯ 64ã€‚
# å¯ä»¥æ ¹æ®è®­ç»ƒä¸­ä½¿ç”¨çš„å€¼è¿›è¡Œè°ƒæ•´ï¼Œä»¥å‡å°æˆ–æ”¾å¤§ LoRA çš„æ•ˆæœ
# è¶…è¿‡ä¸€å®šçš„å®¹å¿åº¦ï¼Œå¯èƒ½ä¼šæ³¨æ„åˆ°æ²¡æœ‰æ•ˆæœæˆ–æº¢å‡ºã€‚
pipe.set_adapters(["cogvideox-lora"], [32 / 64])

# ä½¿ç”¨ç®¡é“ç”Ÿæˆè§†é¢‘ï¼Œä¼ å…¥éªŒè¯æç¤ºï¼Œè®¾ç½®æŒ‡å¯¼æ¯”ä¾‹ï¼Œå¹¶å¯ç”¨åŠ¨æ€é…ç½®
video = pipe("{validation_prompt}", guidance_scale=6, use_dynamic_cfg=True).frames[0]


# æ›´å¤šç»†èŠ‚ï¼ŒåŒ…æ‹¬æƒé‡ã€åˆå¹¶å’Œèåˆ LoRAï¼Œè¯·æŸ¥çœ‹ [diffusers ä¸­åŠ è½½ LoRA çš„æ–‡æ¡£](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)

## License

è¯·éµå®ˆ [æ­¤å¤„](https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE) å’Œ [æ­¤å¤„](https://huggingface.co/THUDM/CogVideoX-2b/blob/main/LICENSE) ä¸­æè¿°çš„è®¸å¯æ¡æ¬¾ã€‚
"""
    # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡ï¼Œä¼ å…¥å¿…è¦å‚æ•°
    model_card = load_or_create_model_card(
        repo_id_or_path=repo_id,  # æ¨¡å‹ ID æˆ–è·¯å¾„
        from_training=True,  # æŒ‡ç¤ºä»è®­ç»ƒç”Ÿæˆ
        license="other",  # è®¾ç½®è®¸å¯è¯ç±»å‹
        base_model=base_model,  # åŸºç¡€æ¨¡å‹åç§°
        prompt=validation_prompt,  # éªŒè¯æç¤º
        model_description=model_description,  # æ¨¡å‹æè¿°
        widget=widget_dict,  # å°éƒ¨ä»¶ä¿¡æ¯
    )
    # å®šä¹‰æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºæ ‡è¯†æ¨¡å‹ç‰¹æ€§
    tags = [
        "text-to-video",  # æ–‡æœ¬è½¬è§†é¢‘
        "diffusers-training",  # Diffusers è®­ç»ƒ
        "diffusers",  # Diffusers
        "lora",  # LoRA
        "cogvideox",  # CogVideoX
        "cogvideox-diffusers",  # CogVideoX Diffusers
        "template:sd-lora",  # æ¨¡æ¿ç±»å‹
    ]

    # å¡«å……æ¨¡å‹å¡ç‰‡çš„æ ‡ç­¾
    model_card = populate_model_card(model_card, tags=tags)
    # ä¿å­˜æ¨¡å‹å¡ç‰‡åˆ°æŒ‡å®šè·¯å¾„
    model_card.save(os.path.join(repo_folder, "README.md"))


# è®°å½•éªŒè¯ç»“æœ
def log_validation(
    pipe,  # è§†é¢‘ç”Ÿæˆç®¡é“
    args,  # å…¶ä»–å‚æ•°
    accelerator,  # åŠ é€Ÿå™¨å®ä¾‹
    pipeline_args,  # ç®¡é“å‚æ•°
    epoch,  # å½“å‰è®­ç»ƒçš„è½®æ¬¡
    is_final_validation: bool = False,  # æ˜¯å¦ä¸ºæœ€ç»ˆéªŒè¯
):
    # è®°å½•æ­£åœ¨è¿è¡ŒéªŒè¯çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”Ÿæˆè§†é¢‘çš„æ•°é‡å’Œæç¤ºå†…å®¹
        logger.info(
            f"Running validation... \n Generating {args.num_validation_videos} videos with prompt: {pipeline_args['prompt']}."
        )
        # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨è°ƒåº¦å™¨çš„å‚æ•°
        scheduler_args = {}
    
        # æ£€æŸ¥è°ƒåº¦å™¨é…ç½®ä¸­æ˜¯å¦åŒ…å«æ–¹å·®ç±»å‹
        if "variance_type" in pipe.scheduler.config:
            # è·å–æ–¹å·®ç±»å‹
            variance_type = pipe.scheduler.config.variance_type
    
            # å¦‚æœæ–¹å·®ç±»å‹æ˜¯â€œlearnedâ€æˆ–â€œlearned_rangeâ€ï¼Œåˆ™å°†å…¶æ›´æ”¹ä¸ºâ€œfixed_smallâ€
            if variance_type in ["learned", "learned_range"]:
                variance_type = "fixed_small"
    
            # å°†æ–¹å·®ç±»å‹æ·»åŠ åˆ°è°ƒåº¦å™¨å‚æ•°ä¸­
            scheduler_args["variance_type"] = variance_type
    
        # æ ¹æ®è°ƒåº¦å™¨é…ç½®å’Œå‚æ•°åˆ›å»ºæ–°çš„è°ƒåº¦å™¨
        pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, **scheduler_args)
        # å°†ç®¡é“ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š
        pipe = pipe.to(accelerator.device)
        # å…³é—­è¿›åº¦æ¡é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼‰
        # pipe.set_progress_bar_config(disable=True)
    
        # è¿è¡Œæ¨ç†ï¼Œåˆ›å»ºéšæœºæ•°ç”Ÿæˆå™¨ï¼Œè®¾ç½®ç§å­
        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
    
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ä»¥å­˜å‚¨ç”Ÿæˆçš„è§†é¢‘
        videos = []
        # æ ¹æ®éœ€è¦ç”ŸæˆæŒ‡å®šæ•°é‡çš„è§†é¢‘
        for _ in range(args.num_validation_videos):
            # è°ƒç”¨ç®¡é“ç”Ÿæˆè§†é¢‘ï¼Œè·å–ç¬¬ä¸€å¸§
            video = pipe(**pipeline_args, generator=generator, output_type="np").frames[0]
            # å°†ç”Ÿæˆçš„è§†é¢‘æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            videos.append(video)
    
        # éå†æ‰€æœ‰è·Ÿè¸ªå™¨
        for tracker in accelerator.trackers:
            # æ ¹æ®æ˜¯å¦ä¸ºæœ€ç»ˆéªŒè¯é€‰æ‹©é˜¶æ®µåç§°
            phase_name = "test" if is_final_validation else "validation"
            # æ£€æŸ¥è·Ÿè¸ªå™¨åç§°æ˜¯å¦ä¸ºâ€œwandbâ€
            if tracker.name == "wandb":
                # åˆå§‹åŒ–è§†é¢‘æ–‡ä»¶ååˆ—è¡¨
                video_filenames = []
                # éå†ç”Ÿæˆçš„è§†é¢‘åˆ—è¡¨
                for i, video in enumerate(videos):
                    # å¤„ç†æç¤ºæ–‡æœ¬ä»¥åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
                    prompt = (
                        pipeline_args["prompt"][:25]
                        .replace(" ", "_")
                        .replace(" ", "_")
                        .replace("'", "_")
                        .replace('"', "_")
                        .replace("/", "_")
                    )
                    # åˆ›å»ºè§†é¢‘æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                    filename = os.path.join(args.output_dir, f"{phase_name}_video_{i}_{prompt}.mp4")
                    # å°†è§†é¢‘å¯¼å‡ºä¸ºæ–‡ä»¶
                    export_to_video(video, filename, fps=8)
                    # å°†æ–‡ä»¶åæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                    video_filenames.append(filename)
    
                # è®°å½•è§†é¢‘åˆ° wandb
                tracker.log(
                    {
                        phase_name: [
                            wandb.Video(filename, caption=f"{i}: {pipeline_args['prompt']}")
                            for i, filename in enumerate(video_filenames)
                        ]
                    }
                )
    
        # é‡Šæ”¾å†…å­˜
        free_memory()
    
        # è¿”å›ç”Ÿæˆçš„è§†é¢‘åˆ—è¡¨
        return videos
# è·å– T5 æ¨¡å‹çš„æç¤ºåµŒå…¥
def _get_t5_prompt_embeds(
    # å®šä¹‰ T5 ä»¤ç‰ŒåŒ–å™¨
    tokenizer: T5Tokenizer,
    # å®šä¹‰ T5 ç¼–ç å™¨æ¨¡å‹
    text_encoder: T5EncoderModel,
    # æç¤ºæ–‡æœ¬ï¼Œå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
    prompt: Union[str, List[str]],
    # æ¯ä¸ªæç¤ºç”Ÿæˆè§†é¢‘çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 1
    num_videos_per_prompt: int = 1,
    # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸º 226
    max_sequence_length: int = 226,
    # æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ï¼Œå¯é€‰
    device: Optional[torch.device] = None,
    # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚ float32ï¼‰ï¼Œå¯é€‰
    dtype: Optional[torch.dtype] = None,
    # é¢„å…ˆæä¾›çš„æ–‡æœ¬è¾“å…¥ IDï¼Œå¯é€‰
    text_input_ids=None,
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è·å–æç¤ºçš„æ‰¹å¤„ç†å¤§å°
    batch_size = len(prompt)

    # å¦‚æœæä¾›äº†ä»¤ç‰ŒåŒ–å™¨
    if tokenizer is not None:
        # ä½¿ç”¨ä»¤ç‰ŒåŒ–å™¨å¯¹æç¤ºè¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå¼ é‡
        text_inputs = tokenizer(
            prompt,
            padding="max_length",  # å¡«å……åˆ°æœ€å¤§é•¿åº¦
            max_length=max_sequence_length,  # æœ€å¤§é•¿åº¦
            truncation=True,  # è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æˆªæ–­
            add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šä»¤ç‰Œ
            return_tensors="pt",  # è¿”å› PyTorch å¼ é‡
        )
        # è·å–æ–‡æœ¬è¾“å…¥ ID
        text_input_ids = text_inputs.input_ids
    else:
        # å¦‚æœæ²¡æœ‰ä»¤ç‰ŒåŒ–å™¨ä¸”æœªæä¾›æ–‡æœ¬è¾“å…¥ IDï¼ŒæŠ›å‡ºé”™è¯¯
        if text_input_ids is None:
            raise ValueError("`text_input_ids` must be provided when the tokenizer is not specified.")

    # å°†æ–‡æœ¬è¾“å…¥ ID è¾“å…¥ç¼–ç å™¨ä»¥è·å–æç¤ºåµŒå…¥
    prompt_embeds = text_encoder(text_input_ids.to(device))[0]
    # å°†åµŒå…¥è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
    prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)

    # ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆçš„æ¯ä¸ªè§†é¢‘å¤åˆ¶æ–‡æœ¬åµŒå…¥ï¼Œä½¿ç”¨é€‚åˆ MPS çš„æ–¹æ³•
    _, seq_len, _ = prompt_embeds.shape  # è·å–åµŒå…¥çš„å½¢çŠ¶
    # é‡å¤åµŒå…¥ä»¥åŒ¹é…æ¯ä¸ªæç¤ºçš„è§†é¢‘æ•°é‡
    prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)
    # å°†åµŒå…¥è°ƒæ•´ä¸ºæ–°çš„å½¢çŠ¶
    prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)

    # è¿”å›æœ€ç»ˆçš„æç¤ºåµŒå…¥
    return prompt_embeds


# ç¼–ç æç¤ºï¼Œç”Ÿæˆå…¶åµŒå…¥
def encode_prompt(
    # å®šä¹‰ T5 ä»¤ç‰ŒåŒ–å™¨
    tokenizer: T5Tokenizer,
    # å®šä¹‰ T5 ç¼–ç å™¨æ¨¡å‹
    text_encoder: T5EncoderModel,
    # æç¤ºæ–‡æœ¬ï¼Œå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
    prompt: Union[str, List[str]],
    # æ¯ä¸ªæç¤ºç”Ÿæˆè§†é¢‘çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 1
    num_videos_per_prompt: int = 1,
    # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸º 226
    max_sequence_length: int = 226,
    # æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ï¼Œå¯é€‰
    device: Optional[torch.device] = None,
    # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚ float32ï¼‰ï¼Œå¯é€‰
    dtype: Optional[torch.dtype] = None,
    # é¢„å…ˆæä¾›çš„æ–‡æœ¬è¾“å…¥ IDï¼Œå¯é€‰
    text_input_ids=None,
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è°ƒç”¨å†…éƒ¨å‡½æ•°è·å–æç¤ºåµŒå…¥
    prompt_embeds = _get_t5_prompt_embeds(
        tokenizer,
        text_encoder,
        prompt=prompt,
        num_videos_per_prompt=num_videos_per_prompt,
        max_sequence_length=max_sequence_length,
        device=device,
        dtype=dtype,
        text_input_ids=text_input_ids,
    )
    # è¿”å›æç¤ºåµŒå…¥
    return prompt_embeds


# è®¡ç®—æç¤ºçš„åµŒå…¥
def compute_prompt_embeddings(
    # å®šä¹‰ T5 ä»¤ç‰ŒåŒ–å™¨
    tokenizer, 
    # å®šä¹‰ T5 ç¼–ç å™¨æ¨¡å‹
    text_encoder, 
    # æç¤ºæ–‡æœ¬
    prompt, 
    # æœ€å¤§åºåˆ—é•¿åº¦
    max_sequence_length, 
    # æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰
    device, 
    # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚ float32ï¼‰
    dtype, 
    # æ˜¯å¦éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œé»˜è®¤ä¸º False
    requires_grad: bool = False
):
    # å¦‚æœéœ€è¦è®¡ç®—æ¢¯åº¦
    if requires_grad:
        # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
        prompt_embeds = encode_prompt(
            tokenizer,
            text_encoder,
            prompt,
            num_videos_per_prompt=1,  # é»˜è®¤ä¸º 1
            max_sequence_length=max_sequence_length,
            device=device,
            dtype=dtype,
        )
    else:
        # å¦‚æœä¸éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œä½¿ç”¨ no_grad ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        with torch.no_grad():
            # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
            prompt_embeds = encode_prompt(
                tokenizer,
                text_encoder,
                prompt,
                num_videos_per_prompt=1,  # é»˜è®¤ä¸º 1
                max_sequence_length=max_sequence_length,
                device=device,
                dtype=dtype,
            )
    # è¿”å›è®¡ç®—å¾—åˆ°çš„æç¤ºåµŒå…¥
    return prompt_embeds


# å‡†å¤‡æ—‹è½¬ä½ç½®åµŒå…¥
def prepare_rotary_positional_embeddings(
    # åµŒå…¥çš„é«˜åº¦
    height: int,
    # åµŒå…¥çš„å®½åº¦
    width: int,
    # å¸§çš„æ•°é‡
    num_frames: int,
    # ç©ºé—´ VAE ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 8
    vae_scale_factor_spatial: int = 8,
    # è´´ç‰‡å¤§å°ï¼Œé»˜è®¤ä¸º 2
    patch_size: int = 2,
    # æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 64
    attention_head_dim: int = 64,
    # å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šè®¾å¤‡ç±»å‹ï¼ˆå¦‚ CPU æˆ– GPUï¼‰ï¼Œé»˜è®¤ä¸º None
    device: Optional[torch.device] = None,
    # åŸºç¡€é«˜åº¦ï¼Œé»˜è®¤ä¸º 480 åƒç´ 
    base_height: int = 480,
    # åŸºç¡€å®½åº¦ï¼Œé»˜è®¤ä¸º 720 åƒç´ 
    base_width: int = 720,
# å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„
) -> Tuple[torch.Tensor, torch.Tensor]:
    # è®¡ç®—ç½‘æ ¼çš„é«˜åº¦
    grid_height = height // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—ç½‘æ ¼çš„å®½åº¦
    grid_width = width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€å®½åº¦
    base_size_width = base_width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€é«˜åº¦
    base_size_height = base_height // (vae_scale_factor_spatial * patch_size)

    # è·å–ç½‘æ ¼çš„è£å‰ªåŒºåŸŸåæ ‡
    grid_crops_coords = get_resize_crop_region_for_grid((grid_height, grid_width), base_size_width, base_size_height)
    # è®¡ç®—3Dæ—‹è½¬ä½ç½®åµŒå…¥çš„ä½™å¼¦å’Œæ­£å¼¦é¢‘ç‡
    freqs_cos, freqs_sin = get_3d_rotary_pos_embed(
        embed_dim=attention_head_dim,
        crops_coords=grid_crops_coords,
        grid_size=(grid_height, grid_width),
        temporal_size=num_frames,
    )

    # å°†ä½™å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_cos = freqs_cos.to(device=device)
    # å°†æ­£å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_sin = freqs_sin.to(device=device)
    # è¿”å›ä½™å¼¦å’Œæ­£å¼¦é¢‘ç‡å¼ é‡
    return freqs_cos, freqs_sin


# åˆ›å»ºä¼˜åŒ–å™¨çš„å‡½æ•°ï¼Œæ¥å—å‚æ•°å’Œä¼˜åŒ–å‚æ•°
def get_optimizer(args, params_to_optimize, use_deepspeed: bool = False):
    # ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    if use_deepspeed:
        from accelerate.utils import DummyOptim

        # è¿”å›ä¸€ä¸ªè™šæ‹Ÿä¼˜åŒ–å™¨ä»¥ä¾›ä½¿ç”¨
        return DummyOptim(
            params_to_optimize,
            lr=args.learning_rate,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )

    # ä¼˜åŒ–å™¨åˆ›å»ºéƒ¨åˆ†
    supported_optimizers = ["adam", "adamw", "prodigy"]
    # æ£€æŸ¥æ‰€é€‰ä¼˜åŒ–å™¨æ˜¯å¦å—æ”¯æŒ
    if args.optimizer not in supported_optimizers:
        # è®°å½•ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨è­¦å‘Š
        logger.warning(
            f"Unsupported choice of optimizer: {args.optimizer}. Supported optimizers include {supported_optimizers}. Defaulting to AdamW"
        )
        # é»˜è®¤ä¼˜åŒ–å™¨è®¾ç½®ä¸º AdamW
        args.optimizer = "adamw"

    # æ£€æŸ¥8ä½Adamçš„ä½¿ç”¨æ¡ä»¶
    if args.use_8bit_adam and not (args.optimizer.lower() not in ["adam", "adamw"]):
        logger.warning(
            f"use_8bit_adam is ignored when optimizer is not set to 'Adam' or 'AdamW'. Optimizer was "
            f"set to {args.optimizer.lower()}"
        )

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨8ä½Adamä¼˜åŒ–å™¨
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            # å¦‚æœæœªå®‰è£…bitsandbytesï¼ŒæŠ›å‡ºå¯¼å…¥é”™è¯¯
            raise ImportError(
                "To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`."
            )

    # åˆ›å»ºAdamWä¼˜åŒ–å™¨
    if args.optimizer.lower() == "adamw":
        optimizer_class = bnb.optim.AdamW8bit if args.use_8bit_adam else torch.optim.AdamW

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # åˆ›å»ºAdamä¼˜åŒ–å™¨
    elif args.optimizer.lower() == "adam":
        optimizer_class = bnb.optim.Adam8bit if args.use_8bit_adam else torch.optim.Adam

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # æ£€æŸ¥ä¼˜åŒ–å™¨å‚æ•°æ˜¯å¦ä¸º "prodigy"ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    elif args.optimizer.lower() == "prodigy":
        # å°è¯•å¯¼å…¥ prodigyopt åº“
        try:
            import prodigyopt
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡º ImportError å¹¶æç¤ºå®‰è£…å‘½ä»¤
        except ImportError:
            raise ImportError("To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`")

        # è®¾ç½®ä¼˜åŒ–å™¨ç±»ä¸º Prodigy
        optimizer_class = prodigyopt.Prodigy

        # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡ä½ï¼Œå¹¶å‘å‡ºè­¦å‘Š
        if args.learning_rate <= 0.1:
            logger.warning(
                "Learning rate is too low. When using prodigy, it's generally better to set learning rate around 1.0"
            )

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å¯¹è±¡ï¼Œä¼ å…¥æ‰€éœ€å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            lr=args.learning_rate,
            betas=(args.adam_beta1, args.adam_beta2),
            beta3=args.prodigy_beta3,
            weight_decay=args.adam_weight_decay,
            eps=args.adam_epsilon,
            decouple=args.prodigy_decouple,
            use_bias_correction=args.prodigy_use_bias_correction,
            safeguard_warmup=args.prodigy_safeguard_warmup,
        )

    # è¿”å›åˆ›å»ºçš„ä¼˜åŒ–å™¨å¯¹è±¡
    return optimizer
# ä¸»å‡½æ•°ï¼Œæ¥æ”¶å‘½ä»¤è¡Œå‚æ•°
def main(args):
    # å¦‚æœæŠ¥å‘Šç›®æ ‡æ˜¯ "wandb" ä¸”æä¾›äº† hub_tokenï¼Œåˆ™æŠ›å‡ºé”™è¯¯
    if args.report_to == "wandb" and args.hub_token is not None:
        raise ValueError(
            "You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token."
            " Please use `huggingface-cli login` to authenticate with the Hub."
        )

    # æ£€æŸ¥ MPS åç«¯æ˜¯å¦å¯ç”¨ï¼Œå¹¶ä¸”æ··åˆç²¾åº¦è®¾ç½®ä¸º bf16ï¼Œè‹¥æ˜¯åˆ™æŠ›å‡ºé”™è¯¯
    if torch.backends.mps.is_available() and args.mixed_precision == "bf16":
        # ç”±äº pytorch#99272ï¼ŒMPS ç›®å‰ä¸æ”¯æŒ bfloat16ã€‚
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )

    # åˆ›å»ºæ—¥å¿—ç›®å½•çš„è·¯å¾„
    logging_dir = Path(args.output_dir, args.logging_dir)

    # åˆ›å»ºé¡¹ç›®é…ç½®ï¼ŒåŒ…æ‹¬é¡¹ç›®ç›®å½•å’Œæ—¥å¿—ç›®å½•
    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
    # é…ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œçš„å‚æ•°
    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    # åˆ›å»ºåŠ é€Ÿå™¨å®ä¾‹ï¼Œé…ç½®å…¶å‚æ•°
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
        kwargs_handlers=[kwargs],
    )

    # ç¦ç”¨ MPS çš„è‡ªåŠ¨æ··åˆç²¾åº¦
    if torch.backends.mps.is_available():
        accelerator.native_amp = False

    # å¦‚æœæŠ¥å‘Šç›®æ ‡æ˜¯ "wandb"ï¼Œæ£€æŸ¥å…¶æ˜¯å¦å¯ç”¨
    if args.report_to == "wandb":
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # é…ç½®æ—¥å¿—ï¼Œç¡®ä¿æ¯ä¸ªè¿›ç¨‹éƒ½èƒ½è®°å½•è°ƒè¯•ä¿¡æ¯
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    # è®°å½•åŠ é€Ÿå™¨çš„çŠ¶æ€ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½èƒ½çœ‹åˆ°
    logger.info(accelerator.state, main_process_only=False)
    # è®¾ç½®ä¸»è¿›ç¨‹çš„æ—¥å¿—çº§åˆ«
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # å¦‚æœæä¾›äº†ç§å­ï¼Œåˆ™è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_seed(args.seed)

    # å¤„ç†ä»“åº“çš„åˆ›å»º
    if accelerator.is_main_process:
        # å¦‚æœè¾“å‡ºç›®å½•ä¸ä¸ºç©ºï¼Œåˆ›å»ºè¾“å‡ºç›®å½•
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        # å¦‚æœéœ€è¦æ¨é€åˆ° hubï¼Œåˆ›å»ºä»“åº“
        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
            ).repo_id

    # å‡†å¤‡æ¨¡å‹å’Œè°ƒåº¦å™¨
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision
    )

    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½æ–‡æœ¬ç¼–ç å™¨
    text_encoder = T5EncoderModel.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision
    )

    # CogVideoX-2b æƒé‡å­˜å‚¨ä¸º float16
    # CogVideoX-5b å’Œ CogVideoX-5b-I2V æƒé‡å­˜å‚¨ä¸º bfloat16
    # æ ¹æ®é¢„è®­ç»ƒæ¨¡å‹åç§°é€‰æ‹©åŠ è½½çš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒ bfloat16 æˆ– float16
    load_dtype = torch.bfloat16 if "5b" in args.pretrained_model_name_or_path.lower() else torch.float16
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ 3D å˜æ¢å™¨æ¨¡å‹
    transformer = CogVideoXTransformer3DModel.from_pretrained(
        args.pretrained_model_name_or_path,  # é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        subfolder="transformer",  # æŒ‡å®šå­æ–‡ä»¶å¤¹
        torch_dtype=load_dtype,  # è®¾ç½®æ•°æ®ç±»å‹
        revision=args.revision,  # æŒ‡å®šç‰ˆæœ¬
        variant=args.variant,  # æŒ‡å®šå˜ä½“
    )

    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ VAE æ¨¡å‹
    vae = AutoencoderKLCogVideoX.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision, variant=args.variant
    )

    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½è°ƒåº¦å™¨
    scheduler = CogVideoXDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")

    # å¦‚æœå¯ç”¨äº†åˆ‡ç‰‡åŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„åˆ‡ç‰‡
    if args.enable_slicing:
        vae.enable_slicing()
    # å¦‚æœå¯ç”¨äº†å¹³é“ºåŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„å¹³é“º
    if args.enable_tiling:
        vae.enable_tiling()

    # åªè®­ç»ƒé¢å¤–çš„é€‚é…å™¨ LoRA å±‚
    text_encoder.requires_grad_(False)  # ç¦ç”¨æ–‡æœ¬ç¼–ç å™¨çš„æ¢¯åº¦è®¡ç®—
    transformer.requires_grad_(False)  # ç¦ç”¨å˜æ¢å™¨çš„æ¢¯åº¦è®¡ç®—
    vae.requires_grad_(False)  # ç¦ç”¨ VAE çš„æ¢¯åº¦è®¡ç®—

    # å¯¹äºæ··åˆç²¾åº¦è®­ç»ƒï¼Œå°†æ‰€æœ‰éå¯è®­ç»ƒæƒé‡ï¼ˆVAEã€æ–‡æœ¬ç¼–ç å™¨å’Œå˜æ¢å™¨ï¼‰è½¬æ¢ä¸ºåŠç²¾åº¦
    # å› ä¸ºè¿™äº›æƒé‡ä»…ç”¨äºæ¨ç†ï¼Œå› æ­¤ä¸éœ€è¦ä¿æŒå…¨ç²¾åº¦
    weight_dtype = torch.float32  # é»˜è®¤æƒé‡æ•°æ®ç±»å‹ä¸º float32
    if accelerator.state.deepspeed_plugin:  # å¦‚æœä½¿ç”¨ DeepSpeed
        # DeepSpeed å¤„ç†ç²¾åº¦ï¼Œä½¿ç”¨ DeepSpeed é…ç½®ä¸­çš„è®¾ç½®
        if (
            "fp16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["fp16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¦‚æœå¯ç”¨ fp16ï¼Œåˆ™è®¾ç½®æƒé‡ä¸º float16
        if (
            "bf16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["bf16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¦‚æœå¯ç”¨ bf16ï¼Œåˆ™è®¾ç½®æƒé‡ä¸º float16
    else:  # å¦‚æœä¸ä½¿ç”¨ DeepSpeed
        if accelerator.mixed_precision == "fp16":  # å¦‚æœæ··åˆç²¾åº¦ä¸º fp16
            weight_dtype = torch.float16  # è®¾ç½®æƒé‡ä¸º float16
        elif accelerator.mixed_precision == "bf16":  # å¦‚æœæ··åˆç²¾åº¦ä¸º bf16
            weight_dtype = torch.bfloat16  # è®¾ç½®æƒé‡ä¸º bfloat16

    # å¦‚æœ MPS å¯ç”¨ä¸”æƒé‡ç±»å‹ä¸º bfloat16ï¼ŒæŠ›å‡ºé”™è¯¯
    if torch.backends.mps.is_available() and weight_dtype == torch.bfloat16:
        # ç”±äº pytorch#99272ï¼ŒMPS ç›®å‰ä¸æ”¯æŒ bfloat16ã€‚
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )

    # å°†æ–‡æœ¬ç¼–ç å™¨ã€å˜æ¢å™¨å’Œ VAE è½¬ç§»åˆ°åŠ é€Ÿå™¨è®¾å¤‡ï¼Œå¹¶è®¾ç½®æƒé‡æ•°æ®ç±»å‹
    text_encoder.to(accelerator.device, dtype=weight_dtype)
    transformer.to(accelerator.device, dtype=weight_dtype)
    vae.to(accelerator.device, dtype=weight_dtype)

    # å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™å¯ç”¨å˜æ¢å™¨çš„æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()

    # ç°åœ¨å°†æ–°çš„ LoRA æƒé‡æ·»åŠ åˆ°æ³¨æ„åŠ›å±‚
    transformer_lora_config = LoraConfig(
        r=args.rank,  # LoRA çš„ç§©
        lora_alpha=args.lora_alpha,  # LoRA çš„ alpha å€¼
        init_lora_weights=True,  # åˆå§‹åŒ– LoRA æƒé‡
        target_modules=["to_k", "to_q", "to_v", "to_out.0"],  # ç›®æ ‡æ¨¡å—
    )
    # å°† LoRA é€‚é…å™¨æ·»åŠ åˆ°å˜æ¢å™¨ä¸­
    transformer.add_adapter(transformer_lora_config)

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè§£åŒ…æ¨¡å‹
    def unwrap_model(model):
        # è§£åŒ…åŠ é€Ÿå™¨ä¸­çš„æ¨¡å‹
        model = accelerator.unwrap_model(model)
        # å¦‚æœæ˜¯ç¼–è¯‘æ¨¡å—ï¼Œåˆ™è¿”å›å…¶åŸå§‹æ¨¡å—
        model = model._orig_mod if is_compiled_module(model) else model
        return model  # è¿”å›è§£åŒ…åçš„æ¨¡å‹
    # åˆ›å»ºè‡ªå®šä¹‰çš„ä¿å­˜å’ŒåŠ è½½é’©å­ï¼Œä»¥ä¾¿ `accelerator.save_state(...)` å¯ä»¥åºåˆ—åŒ–ä¸ºè‰¯å¥½çš„æ ¼å¼
    def save_model_hook(models, weights, output_dir):
        # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
        if accelerator.is_main_process:
            # åˆå§‹åŒ–è¦ä¿å­˜çš„å˜æ¢å™¨ LoRA å±‚å˜é‡
            transformer_lora_layers_to_save = None
    
            # éå†æ‰€æœ‰æ¨¡å‹
            for model in models:
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ˜¯å˜æ¢å™¨çš„å®ä¾‹
                if isinstance(model, type(unwrap_model(transformer))):
                    # è·å–å˜æ¢å™¨æ¨¡å‹çš„çŠ¶æ€å­—å…¸
                    transformer_lora_layers_to_save = get_peft_model_state_dict(model)
                else:
                    # å¦‚æœæ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise ValueError(f"unexpected save model: {model.__class__}")
    
                # ç¡®ä¿ä»æƒé‡ä¸­ç§»é™¤ç›¸åº”çš„æƒé‡ï¼Œä»¥é¿å…é‡å¤ä¿å­˜
                weights.pop()
    
            # ä¿å­˜ LoRA æƒé‡åˆ°æŒ‡å®šè¾“å‡ºç›®å½•
            CogVideoXPipeline.save_lora_weights(
                output_dir,
                transformer_lora_layers=transformer_lora_layers_to_save,
            )
    
        # å®šä¹‰åŠ è½½æ¨¡å‹çš„é’©å­
        def load_model_hook(models, input_dir):
            # åˆå§‹åŒ–å˜æ¢å™¨å˜é‡
            transformer_ = None
    
            # å½“æ¨¡å‹åˆ—è¡¨éç©ºæ—¶æŒç»­æ‰§è¡Œ
            while len(models) > 0:
                # å¼¹å‡ºæ¨¡å‹
                model = models.pop()
    
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ˜¯å˜æ¢å™¨çš„å®ä¾‹
                if isinstance(model, type(unwrap_model(transformer))):
                    transformer_ = model
                else:
                    # å¦‚æœæ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise ValueError(f"Unexpected save model: {model.__class__}")
    
            # ä»æŒ‡å®šè¾“å…¥ç›®å½•è·å– LoRA çŠ¶æ€å­—å…¸
            lora_state_dict = CogVideoXPipeline.lora_state_dict(input_dir)
    
            # åˆ›å»ºå˜æ¢å™¨çŠ¶æ€å­—å…¸ï¼Œä»…ä¿ç•™ä»¥ "transformer." å¼€å¤´çš„é”®
            transformer_state_dict = {
                f'{k.replace("transformer.", "")}': v for k, v in lora_state_dict.items() if k.startswith("transformer.")
            }
            # è½¬æ¢ UNet çŠ¶æ€å­—å…¸ä¸º PEFT æ ¼å¼
            transformer_state_dict = convert_unet_state_dict_to_peft(transformer_state_dict)
            # è®¾ç½® PEFT æ¨¡å‹çŠ¶æ€å­—å…¸å¹¶è·å–ä¸å…¼å®¹çš„é”®
            incompatible_keys = set_peft_model_state_dict(transformer_, transformer_state_dict, adapter_name="default")
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ„å¤–çš„é”®
            if incompatible_keys is not None:
                # è·å–æ„å¤–çš„é”®
                unexpected_keys = getattr(incompatible_keys, "unexpected_keys", None)
                if unexpected_keys:
                    # è®°å½•è­¦å‘Šæ—¥å¿—
                    logger.warning(
                        f"Loading adapter weights from state_dict led to unexpected keys not found in the model: "
                        f" {unexpected_keys}. "
                    )
    
            # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
            if args.mixed_precision == "fp16":
                # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰æå‡ä¸º fp32
                cast_training_params([transformer_])
    
        # æ³¨å†Œä¿å­˜çŠ¶æ€å‰é’©å­
        accelerator.register_save_state_pre_hook(save_model_hook)
        # æ³¨å†ŒåŠ è½½çŠ¶æ€å‰é’©å­
        accelerator.register_load_state_pre_hook(load_model_hook)
    
        # å¦‚æœå…è®¸ä½¿ç”¨ TF32ï¼Œåˆ™åœ¨ Ampere GPU ä¸Šå¯ç”¨æ›´å¿«çš„è®­ç»ƒ
        if args.allow_tf32 and torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = True
    
        # å¦‚æœéœ€è¦ç¼©æ”¾å­¦ä¹ ç‡ï¼Œåˆ™è¿›è¡Œç›¸åº”è°ƒæ•´
        if args.scale_lr:
            args.learning_rate = (
                args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes
            )
    # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
    if args.mixed_precision == "fp16":
        # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰ä¸Šå‡ä¸º fp32 ç±»å‹
        cast_training_params([transformer], dtype=torch.float32)

    # è¿‡æ»¤å‡ºéœ€è¦æ¢¯åº¦æ›´æ–°çš„å‚æ•°
    transformer_lora_parameters = list(filter(lambda p: p.requires_grad, transformer.parameters()))

    # ä¼˜åŒ–å™¨çš„å‚æ•°å­—å…¸
    transformer_parameters_with_lr = {"params": transformer_lora_parameters, "lr": args.learning_rate}
    # å°†ä¼˜åŒ–å‚æ•°æ”¾å…¥åˆ—è¡¨ä¸­
    params_to_optimize = [transformer_parameters_with_lr]

    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    use_deepspeed_optimizer = (
        accelerator.state.deepspeed_plugin is not None
        and "optimizer" in accelerator.state.deepspeed_plugin.deepspeed_config
    )
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨
    use_deepspeed_scheduler = (
        accelerator.state.deepspeed_plugin is not None
        and "scheduler" not in accelerator.state.deepspeed_plugin.deepspeed_config
    )

    # è·å–ä¼˜åŒ–å™¨å®ä¾‹
    optimizer = get_optimizer(args, params_to_optimize, use_deepspeed=use_deepspeed_optimizer)

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = VideoDataset(
        instance_data_root=args.instance_data_root,  # å®ä¾‹æ•°æ®æ ¹ç›®å½•
        dataset_name=args.dataset_name,  # æ•°æ®é›†åç§°
        dataset_config_name=args.dataset_config_name,  # æ•°æ®é›†é…ç½®åç§°
        caption_column=args.caption_column,  # æè¿°åˆ—åç§°
        video_column=args.video_column,  # è§†é¢‘åˆ—åç§°
        height=args.height,  # è§†é¢‘é«˜åº¦
        width=args.width,  # è§†é¢‘å®½åº¦
        fps=args.fps,  # å¸§ç‡
        max_num_frames=args.max_num_frames,  # æœ€å¤§å¸§æ•°
        skip_frames_start=args.skip_frames_start,  # å¼€å§‹è·³è¿‡çš„å¸§æ•°
        skip_frames_end=args.skip_frames_end,  # ç»“æŸè·³è¿‡çš„å¸§æ•°
        cache_dir=args.cache_dir,  # ç¼“å­˜ç›®å½•
        id_token=args.id_token,  # ID ä»¤ç‰Œ
    )

    # å®šä¹‰ç¼–ç è§†é¢‘çš„å‡½æ•°
    def encode_video(video):
        # å°†è§†é¢‘è½¬ç§»åˆ°è®¾å¤‡å¹¶å¢åŠ ç»´åº¦
        video = video.to(accelerator.device, dtype=vae.dtype).unsqueeze(0)
        # è°ƒæ•´è§†é¢‘ç»´åº¦é¡ºåº
        video = video.permute(0, 2, 1, 3, 4)  # [B, C, F, H, W]
        # ä½¿ç”¨ VAE ç¼–ç è§†é¢‘å¹¶è·å–æ½œåœ¨åˆ†å¸ƒ
        latent_dist = vae.encode(video).latent_dist
        return latent_dist

    # å¯¹æ•°æ®é›†ä¸­æ¯ä¸ªå®ä¾‹è§†é¢‘è¿›è¡Œç¼–ç 
    train_dataset.instance_videos = [encode_video(video) for video in train_dataset.instance_videos]

    # å®šä¹‰æ•´ç†å‡½æ•°ä»¥ç»„åˆæ•°æ®
    def collate_fn(examples):
        # æå–è§†é¢‘æ ·æœ¬å¹¶è¿›è¡Œç¼©æ”¾
        videos = [example["instance_video"].sample() * vae.config.scaling_factor for example in examples]
        # æå–å¯¹åº”çš„æç¤ºæ–‡æœ¬
        prompts = [example["instance_prompt"] for example in examples]

        # å°†è§†é¢‘å¼ é‡åˆå¹¶
        videos = torch.cat(videos)
        # ç¡®ä¿è§†é¢‘å¼ é‡è¿ç»­å¹¶è½¬æ¢ä¸º float ç±»å‹
        videos = videos.to(memory_format=torch.contiguous_format).float()

        return {
            "videos": videos,  # è¿”å›è§†é¢‘å¼ é‡
            "prompts": prompts,  # è¿”å›æç¤ºæ–‡æœ¬
        }

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(
        train_dataset,  # ä½¿ç”¨çš„æ•°æ®é›†
        batch_size=args.train_batch_size,  # æ¯æ‰¹æ¬¡çš„å¤§å°
        shuffle=True,  # æ˜¯å¦æ‰“ä¹±æ•°æ®
        collate_fn=collate_fn,  # è‡ªå®šä¹‰æ•´ç†å‡½æ•°
        num_workers=args.dataloader_num_workers,  # ä½¿ç”¨çš„å·¥ä½œçº¿ç¨‹æ•°
    )

    # è°ƒåº¦å™¨å’Œè®­ç»ƒæ­¥éª¤çš„æ•°å­¦è®¡ç®—
    overrode_max_train_steps = False  # æ ‡è®°æ˜¯å¦è¦†ç›–æœ€å¤§è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)  # æ¯ä¸ª epoch çš„æ›´æ–°æ­¥éª¤æ•°
    if args.max_train_steps is None:
        # å¦‚æœæœªæŒ‡å®šæœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œåˆ™æ ¹æ®è®­ç»ƒå‘¨æœŸè®¡ç®—
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True  # è®¾ç½®æ ‡è®°ä¸º True
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨
    if use_deepspeed_scheduler:
        # ä» accelerate.utils å¯¼å…¥ DummyScheduler ç±»
        from accelerate.utils import DummyScheduler

        # åˆ›å»ºä¸€ä¸ª DummyScheduler å®ä¾‹ï¼Œç”¨äºå­¦ä¹ ç‡è°ƒåº¦
        lr_scheduler = DummyScheduler(
            # è®¾ç½®è°ƒåº¦å™¨åç§°
            name=args.lr_scheduler,
            # ä¼ å…¥ä¼˜åŒ–å™¨
            optimizer=optimizer,
            # è®¾ç½®æ€»è®­ç»ƒæ­¥æ•°
            total_num_steps=args.max_train_steps * accelerator.num_processes,
            # è®¾ç½®é¢„çƒ­æ­¥æ•°
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
        )
    else:
        # å¦‚æœä¸ä½¿ç”¨ DeepSpeedï¼Œè°ƒç”¨ get_scheduler å‡½æ•°è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = get_scheduler(
            # ä¼ å…¥è°ƒåº¦å™¨åç§°
            args.lr_scheduler,
            # ä¼ å…¥ä¼˜åŒ–å™¨
            optimizer=optimizer,
            # è®¾ç½®é¢„çƒ­æ­¥æ•°
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
            # è®¾ç½®æ€»è®­ç»ƒæ­¥æ•°
            num_training_steps=args.max_train_steps * accelerator.num_processes,
            # è®¾ç½®å­¦ä¹ ç‡å¾ªç¯æ¬¡æ•°
            num_cycles=args.lr_num_cycles,
            # è®¾ç½®å­¦ä¹ ç‡è¡°å‡çš„å¹‚
            power=args.lr_power,
        )

    # ä½¿ç”¨ accelerator å‡†å¤‡æ‰€æœ‰ç»„ä»¶
    transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        # å‡†å¤‡å˜æ¢å™¨ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒæ•°æ®åŠ è½½å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        transformer, optimizer, train_dataloader, lr_scheduler
    )

    # éœ€è¦é‡æ–°è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®åŠ è½½å™¨çš„å¤§å°å¯èƒ½å·²ç»æ”¹å˜
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    # å¦‚æœè¦†ç›–äº†æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if overrode_max_train_steps:
        # æ ¹æ®è®­ç»ƒå‘¨æœŸå’Œæ›´æ–°æ­¥éª¤è®¡ç®—æœ€å¤§è®­ç»ƒæ­¥æ•°
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    # éšåé‡æ–°è®¡ç®—è®­ç»ƒå‘¨æœŸæ•°
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # åˆå§‹åŒ–è·Ÿè¸ªå™¨å¹¶å­˜å‚¨é…ç½®
    # è·Ÿè¸ªå™¨åœ¨ä¸»è¿›ç¨‹ä¸­è‡ªåŠ¨åˆå§‹åŒ–
    if accelerator.is_main_process:
        # è·å–è·Ÿè¸ªå™¨åç§°ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤åç§°
        tracker_name = args.tracker_name or "cogvideox-lora"
        # åˆå§‹åŒ–è·Ÿè¸ªå™¨ï¼Œå¹¶ä¼ å…¥é…ç½®
        accelerator.init_trackers(tracker_name, config=vars(args))

    # å¼€å§‹è®­ç»ƒï¼
    # è®¡ç®—æ€»æ‰¹é‡å¤§å°
    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
    # è®¡ç®—å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
    num_trainable_parameters = sum(param.numel() for model in params_to_optimize for param in model["params"])

    # è®°å½•è®­ç»ƒå¼€å§‹çš„ä¿¡æ¯
    logger.info("***** Running training *****")
    # è®°å½•å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
    logger.info(f"  Num trainable parameters = {num_trainable_parameters}")
    # è®°å½•æ ·æœ¬æ•°é‡
    logger.info(f"  Num examples = {len(train_dataset)}")
    # è®°å½•æ¯ä¸ªå‘¨æœŸçš„æ‰¹æ¬¡æ•°
    logger.info(f"  Num batches each epoch = {len(train_dataloader)}")
    # è®°å½•è®­ç»ƒå‘¨æœŸæ•°
    logger.info(f"  Num epochs = {args.num_train_epochs}")
    # è®°å½•æ¯ä¸ªè®¾å¤‡çš„ç¬æ—¶æ‰¹é‡å¤§å°
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")
    # è®°å½•æ€»æ‰¹é‡å¤§å°ï¼ˆåŒ…æ‹¬å¹¶è¡Œã€åˆ†å¸ƒå¼å’Œç§¯ç´¯ï¼‰
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    # è®°å½•æ¢¯åº¦ç§¯ç´¯æ­¥æ•°
    logger.info(f"  Gradient accumulation steps = {args.gradient_accumulation_steps}")
    # è®°å½•æ€»ä¼˜åŒ–æ­¥éª¤
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    # åˆå§‹åŒ–å…¨å±€æ­¥æ•°
    global_step = 0
    # åˆå§‹åŒ–é¦–ä¸ªå‘¨æœŸ
    first_epoch = 0

    # å¯èƒ½ä»ä¹‹å‰çš„ä¿å­˜ä¸­åŠ è½½æƒé‡å’ŒçŠ¶æ€
    if not args.resume_from_checkpoint:
        # å¦‚æœæœªä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œè®¾ç½®åˆå§‹å…¨å±€æ­¥æ•°ä¸º0
        initial_global_step = 0
    else:  # å¦‚æœå‰é¢çš„æ¡ä»¶ä¸æ»¡è¶³ï¼Œæ‰§è¡Œä»¥ä¸‹ä»£ç 
        if args.resume_from_checkpoint != "latest":  # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†éæœ€æ–°çš„æ£€æŸ¥ç‚¹
            path = os.path.basename(args.resume_from_checkpoint)  # è·å–æŒ‡å®šæ£€æŸ¥ç‚¹çš„åŸºæœ¬æ–‡ä»¶å
        else:  # å¦‚æœæ²¡æœ‰æŒ‡å®šéæœ€æ–°æ£€æŸ¥ç‚¹
            # è·å–æœ€è¿‘çš„æ£€æŸ¥ç‚¹
            dirs = os.listdir(args.output_dir)  # åˆ—å‡ºè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œç›®å½•
            dirs = [d for d in dirs if d.startswith("checkpoint")]  # è¿‡æ»¤å‡ºä»¥ "checkpoint" å¼€å¤´çš„ç›®å½•
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))  # æ ¹æ®æ£€æŸ¥ç‚¹çš„æ•°å­—éƒ¨åˆ†æ’åº
            path = dirs[-1] if len(dirs) > 0 else None  # å¦‚æœæœ‰æ£€æŸ¥ç‚¹ï¼Œå–æœ€æ–°çš„ä¸€ä¸ªï¼Œå¦åˆ™ä¸º None

        if path is None:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹
            accelerator.print(  # è¾“å‡ºä¿¡æ¯
                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."  # æç¤ºæ£€æŸ¥ç‚¹ä¸å­˜åœ¨ï¼Œå¼€å§‹æ–°çš„è®­ç»ƒ
            )
            args.resume_from_checkpoint = None  # å°†æ¢å¤æ£€æŸ¥ç‚¹å‚æ•°è®¾ç½®ä¸º None
            initial_global_step = 0  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤ä¸º 0
        else:  # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹
            accelerator.print(f"Resuming from checkpoint {path}")  # è¾“å‡ºæ¢å¤æ£€æŸ¥ç‚¹çš„ä¿¡æ¯
            accelerator.load_state(os.path.join(args.output_dir, path))  # åŠ è½½æŒ‡å®šæ£€æŸ¥ç‚¹çš„çŠ¶æ€
            global_step = int(path.split("-")[1])  # ä»æ£€æŸ¥ç‚¹çš„æ–‡ä»¶åä¸­æå–å…¨å±€æ­¥éª¤

            initial_global_step = global_step  # å°†åˆå§‹å…¨å±€æ­¥éª¤è®¾ç½®ä¸ºæå–çš„å€¼
            first_epoch = global_step // num_update_steps_per_epoch  # è®¡ç®—å½“å‰æ˜¯ç¬¬å‡ ä¸ª epoch

    progress_bar = tqdm(  # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
        range(0, args.max_train_steps),  # è®¾ç½®è¿›åº¦æ¡çš„èŒƒå›´
        initial=initial_global_step,  # è®¾ç½®è¿›åº¦æ¡çš„åˆå§‹å€¼
        desc="Steps",  # è®¾ç½®è¿›åº¦æ¡çš„æè¿°
        # ä»…åœ¨æ¯å°æœºå™¨ä¸Šæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡ã€‚
        disable=not accelerator.is_local_main_process,  # å¦‚æœä¸æ˜¯æœ¬åœ°ä¸»è¿›ç¨‹ï¼Œåˆ™ç¦ç”¨è¿›åº¦æ¡
    )
    vae_scale_factor_spatial = 2 ** (len(vae.config.block_out_channels) - 1)  # è®¡ç®— VAE çš„ç©ºé—´ç¼©æ”¾å› å­

    # ç”¨äº DeepSpeed è®­ç»ƒ
    model_config = transformer.module.config if hasattr(transformer, "module") else transformer.config  # è·å–æ¨¡å‹é…ç½®ï¼Œè€ƒè™‘æ¨¡å—å±æ€§

    # ä¿å­˜ LoRA å±‚
    accelerator.wait_for_everyone()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
        if accelerator.is_main_process:
            # è§£åŒ…æ¨¡å‹ä»¥è·å–ä¸»æ¨¡å‹
            transformer = unwrap_model(transformer)
            # æ ¹æ®æ··åˆç²¾åº¦è®¾ç½®é€‰æ‹©æ•°æ®ç±»å‹
            dtype = (
                torch.float16
                if args.mixed_precision == "fp16"
                else torch.bfloat16
                if args.mixed_precision == "bf16"
                else torch.float32
            )
            # å°†æ¨¡å‹è½¬æ¢ä¸ºæ‰€é€‰çš„æ•°æ®ç±»å‹
            transformer = transformer.to(dtype)
            # è·å–æ¨¡å‹çš„ LoRA å±‚çŠ¶æ€å­—å…¸
            transformer_lora_layers = get_peft_model_state_dict(transformer)
    
            # ä¿å­˜ LoRA æƒé‡åˆ°æŒ‡å®šç›®å½•
            CogVideoXPipeline.save_lora_weights(
                save_directory=args.output_dir,
                transformer_lora_layers=transformer_lora_layers,
            )
    
            # æœ€ç»ˆæµ‹è¯•æ¨ç†
            pipe = CogVideoXPipeline.from_pretrained(
                args.pretrained_model_name_or_path,
                revision=args.revision,
                variant=args.variant,
                torch_dtype=weight_dtype,
            )
            # ä½¿ç”¨é…ç½®åˆ›å»ºè°ƒåº¦å™¨
            pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config)
    
            # å¦‚æœå¯ç”¨åˆ‡ç‰‡åŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„åˆ‡ç‰‡
            if args.enable_slicing:
                pipe.vae.enable_slicing()
            # å¦‚æœå¯ç”¨å¹³é“ºåŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„å¹³é“º
            if args.enable_tiling:
                pipe.vae.enable_tiling()
    
            # åŠ è½½ LoRA æƒé‡
            lora_scaling = args.lora_alpha / args.rank
            # ä»è¾“å‡ºç›®å½•åŠ è½½ LoRA æƒé‡
            pipe.load_lora_weights(args.output_dir, adapter_name="cogvideox-lora")
            # è®¾ç½®é€‚é…å™¨åŠå…¶ç¼©æ”¾å› å­
            pipe.set_adapters(["cogvideox-lora"], [lora_scaling])
    
            # è¿è¡Œæ¨ç†å¹¶è¿›è¡ŒéªŒè¯
            validation_outputs = []
            # å¦‚æœæœ‰éªŒè¯æç¤ºä¸”æ•°é‡å¤§äºé›¶ï¼Œåˆ™è¿›è¡ŒéªŒè¯
            if args.validation_prompt and args.num_validation_videos > 0:
                validation_prompts = args.validation_prompt.split(args.validation_prompt_separator)
                # éå†æ¯ä¸ªéªŒè¯æç¤º
                for validation_prompt in validation_prompts:
                    # å‡†å¤‡æ¨ç†å‚æ•°
                    pipeline_args = {
                        "prompt": validation_prompt,
                        "guidance_scale": args.guidance_scale,
                        "use_dynamic_cfg": args.use_dynamic_cfg,
                        "height": args.height,
                        "width": args.width,
                    }
    
                    # è®°å½•éªŒè¯è¾“å‡º
                    video = log_validation(
                        pipe=pipe,
                        args=args,
                        accelerator=accelerator,
                        pipeline_args=pipeline_args,
                        epoch=epoch,
                        is_final_validation=True,
                    )
                    # æ‰©å±•éªŒè¯è¾“å‡ºåˆ—è¡¨
                    validation_outputs.extend(video)
    
            # å¦‚æœéœ€è¦ä¸Šä¼ åˆ°ä¸­å¿ƒ
            if args.push_to_hub:
                # ä¿å­˜æ¨¡å‹å¡ä¿¡æ¯åˆ°æŒ‡å®šçš„åº“
                save_model_card(
                    repo_id,
                    videos=validation_outputs,
                    base_model=args.pretrained_model_name_or_path,
                    validation_prompt=args.validation_prompt,
                    repo_folder=args.output_dir,
                    fps=args.fps,
                )
                # ä¸Šä¼ è¾“å‡ºç›®å½•åˆ°æŒ‡å®šçš„åº“
                upload_folder(
                    repo_id=repo_id,
                    folder_path=args.output_dir,
                    commit_message="End of training",
                    ignore_patterns=["step_*", "epoch_*"],
                )
    
        # ç»“æŸè®­ç»ƒè¿‡ç¨‹
        accelerator.end_training()
# å¦‚æœè¯¥è„šæœ¬æ˜¯ä¸»ç¨‹åºï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()
    # è°ƒç”¨ä¸»å‡½æ•°ï¼Œå¹¶å°†å‚æ•°ä¼ é€’ç»™å®ƒ
    main(args)
```