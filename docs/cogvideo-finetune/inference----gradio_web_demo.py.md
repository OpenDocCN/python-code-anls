# `.\cogvideo-finetune\inference\gradio_web_demo.py`

```
"""
# ä¸»æ–‡ä»¶ç”¨äº Gradio ç½‘ç»œæ¼”ç¤ºï¼Œä½¿ç”¨ CogVideoX-2B æ¨¡å‹ç”Ÿæˆè§†é¢‘
# è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY ä½¿ç”¨ OpenAI API å¢å¼ºæç¤º

# æ­¤æ¼”ç¤ºä»…æ”¯æŒæ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆæ¨¡å‹ã€‚
# å¦‚æœå¸Œæœ›ä½¿ç”¨å›¾åƒåˆ°è§†é¢‘æˆ–è§†é¢‘åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œ
# è¯·ä½¿ç”¨ gradio_composite_demo å®ç°å®Œæ•´çš„ GUI åŠŸèƒ½ã€‚

# ä½¿ç”¨æ–¹æ³•ï¼š
# OpenAI_API_KEY=your_openai_api_key OpenAI_BASE_URL=https://api.openai.com/v1 python inference/gradio_web_demo.py
"""

# å¯¼å…¥æ“ä½œç³»ç»Ÿç›¸å…³åŠŸèƒ½
import os
# å¯¼å…¥å¤šçº¿ç¨‹åŠŸèƒ½
import threading
# å¯¼å…¥æ—¶é—´åŠŸèƒ½
import time

# å¯¼å…¥ Gradio åº“ä»¥æ„å»º Web åº”ç”¨
import gradio as gr
# å¯¼å…¥ PyTorch åº“è¿›è¡Œæ·±åº¦å­¦ä¹ 
import torch
# å¯¼å…¥ CogVideoXPipeline æ¨¡å‹
from diffusers import CogVideoXPipeline
# å¯¼å…¥å¯¼å‡ºè§†é¢‘åŠŸèƒ½
from diffusers.utils import export_to_video
# å¯¼å…¥æ—¥æœŸæ—¶é—´å¤„ç†åŠŸèƒ½
from datetime import datetime, timedelta
# å¯¼å…¥ OpenAI åº“ä»¥ä½¿ç”¨å…¶ API
from openai import OpenAI
# å¯¼å…¥ MoviePy åº“è¿›è¡Œè§†é¢‘ç¼–è¾‘
import moviepy.editor as mp

# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ CogVideoXPipelineï¼ŒæŒ‡å®šæ•°æ®ç±»å‹ä¸º bfloat16ï¼Œå¹¶ç§»åŠ¨åˆ° GPU
pipe = CogVideoXPipeline.from_pretrained("THUDM/CogVideoX-5b", torch_dtype=torch.bfloat16).to("cuda")

# å¯ç”¨ VAE çš„åˆ‡ç‰‡åŠŸèƒ½
pipe.vae.enable_slicing()
# å¯ç”¨ VAE çš„å¹³é“ºåŠŸèƒ½
pipe.vae.enable_tiling()

# åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
os.makedirs("./output", exist_ok=True)
# åˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
os.makedirs("./gradio_tmp", exist_ok=True)

# å®šä¹‰ç³»ç»Ÿæç¤ºï¼ŒæŒ‡å¯¼è§†é¢‘ç”Ÿæˆçš„æè¿°
sys_prompt = """You are part of a team of bots that creates videos. You work with an assistant bot that will draw anything you say in square brackets.

For example , outputting " a beautiful morning in the woods with the sun peaking through the trees " will trigger your partner bot to output an video of a forest morning , as described. You will be prompted by people looking to create detailed , amazing videos. The way to accomplish this is to take their short prompts and make them extremely detailed and descriptive.
There are a few rules to follow:

You will only ever output a single video description per user request.

When modifications are requested , you should not simply make the description longer . You should refactor the entire description to integrate the suggestions.
Other times the user will not want modifications , but instead want a new image . In this case , you should ignore your previous conversation with the user.

Video descriptions must have the same num of words as examples below. Extra words will be ignored.
"""

# å®šä¹‰è½¬æ¢æç¤ºçš„å‡½æ•°ï¼Œæ¥å—æç¤ºå’Œé‡è¯•æ¬¡æ•°ä½œä¸ºå‚æ•°
def convert_prompt(prompt: str, retry_times: int = 3) -> str:
    # å¦‚æœæ²¡æœ‰è®¾ç½® OpenAI API å¯†é’¥ï¼Œè¿”å›åŸå§‹æç¤º
    if not os.environ.get("OPENAI_API_KEY"):
        return prompt

    # åˆ›å»º OpenAI å®¢æˆ·ç«¯
    client = OpenAI()
    # å»é™¤æç¤ºä¸¤ç«¯çš„ç©ºç™½
    text = prompt.strip()

    # è¿”å›åŸå§‹æç¤º
    return prompt

# å®šä¹‰æ¨æ–­å‡½æ•°ï¼Œæ¥å—æç¤ºã€æ¨æ–­æ­¥éª¤å’Œå¼•å¯¼å°ºåº¦
def infer(prompt: str, num_inference_steps: int, guidance_scale: float, progress=gr.Progress(track_tqdm=True)):
    # æ¸…ç©º GPU ç¼“å­˜
    torch.cuda.empty_cache()
    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè§†é¢‘ï¼ŒæŒ‡å®šç›¸å…³å‚æ•°
    video = pipe(
        prompt=prompt,
        num_videos_per_prompt=1,
        num_inference_steps=num_inference_steps,
        num_frames=49,
        guidance_scale=guidance_scale,
    ).frames[0]

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘
    return video

# å®šä¹‰ä¿å­˜è§†é¢‘çš„å‡½æ•°ï¼Œæ¥å—å¼ é‡ä½œä¸ºå‚æ•°
def save_video(tensor):
    # è·å–å½“å‰æ—¶é—´æˆ³ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # å®šä¹‰è§†é¢‘ä¿å­˜è·¯å¾„
    video_path = f"./output/{timestamp}.mp4"
    # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
    os.makedirs(os.path.dirname(video_path), exist_ok=True)
    # å°†å¼ é‡å¯¼å‡ºä¸ºè§†é¢‘æ–‡ä»¶
    export_to_video(tensor, video_path)
    # è¿”å›è§†é¢‘æ–‡ä»¶è·¯å¾„
    return video_path

# å®šä¹‰å°†è§†é¢‘è½¬æ¢ä¸º GIF çš„å‡½æ•°ï¼Œæ¥å—è§†é¢‘è·¯å¾„ä½œä¸ºå‚æ•°
def convert_to_gif(video_path):
    # ä½¿ç”¨ MoviePy åŠ è½½è§†é¢‘æ–‡ä»¶
    clip = mp.VideoFileClip(video_path)
    # è®¾ç½®è§†é¢‘çš„å¸§ç‡ä¸º 8
    clip = clip.set_fps(8)
    # è°ƒæ•´å‰ªè¾‘çš„é«˜åº¦ä¸º 240 åƒç´ ï¼Œä¿æŒå®½é«˜æ¯”
        clip = clip.resize(height=240)
        # å°†è§†é¢‘è·¯å¾„ä¸­çš„ ".mp4" åç¼€æ›¿æ¢ä¸º ".gif" åç¼€ï¼Œç”Ÿæˆ GIF æ–‡ä»¶è·¯å¾„
        gif_path = video_path.replace(".mp4", ".gif")
        # å°†å‰ªè¾‘å†™å…¥ GIF æ–‡ä»¶ï¼Œè®¾ç½®æ¯ç§’å¸§æ•°ä¸º 8
        clip.write_gif(gif_path, fps=8)
        # è¿”å›ç”Ÿæˆçš„ GIF æ–‡ä»¶è·¯å¾„
        return gif_path
# å®šä¹‰åˆ é™¤æ—§æ–‡ä»¶çš„å‡½æ•°
def delete_old_files():
    # æ— é™å¾ªç¯ï¼ŒæŒç»­æ‰§è¡Œåˆ é™¤æ—§æ–‡ä»¶çš„ä»»åŠ¡
    while True:
        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        # è®¡ç®—10åˆ†é’Ÿå‰çš„æ—¶é—´ï¼Œç”¨äºåˆ¤æ–­æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
        cutoff = now - timedelta(minutes=10)
        # å®šä¹‰éœ€è¦æ¸…ç†çš„ç›®å½•åˆ—è¡¨
        directories = ["./output", "./gradio_tmp"]

        # éå†æ¯ä¸ªç›®å½•
        for directory in directories:
            # éå†ç›®å½•ä¸­çš„æ¯ä¸ªæ–‡ä»¶
            for filename in os.listdir(directory):
                # æ„å»ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                file_path = os.path.join(directory, filename)
                # æ£€æŸ¥è¯¥è·¯å¾„æ˜¯å¦ä¸ºæ–‡ä»¶
                if os.path.isfile(file_path):
                    # è·å–æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦æ—©äºæˆªæ­¢æ—¶é—´
                    if file_mtime < cutoff:
                        # åˆ é™¤è¯¥æ–‡ä»¶
                        os.remove(file_path)
        # æ¯600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰æš‚åœä¸€æ¬¡
        time.sleep(600)

# å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ¥æ‰§è¡Œåˆ é™¤æ—§æ–‡ä»¶çš„å‡½æ•°ï¼Œè®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
threading.Thread(target=delete_old_files, daemon=True).start()

# ä½¿ç”¨ Gradio åˆ›å»ºç”¨æˆ·ç•Œé¢
with gr.Blocks() as demo:
    # åˆ›å»º Markdown ç»„ä»¶ï¼Œæ˜¾ç¤ºæ ‡é¢˜
    gr.Markdown("""
           <div style="text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;">
               CogVideoX Gradio Simple SpaceğŸ¤—
            """)

    # åˆ›å»ºä¸€è¡Œå¸ƒå±€
    with gr.Row():
        # åˆ›å»ºä¸€åˆ—å¸ƒå±€
        with gr.Column():
            # åˆ›å»ºæ–‡æœ¬æ¡†ç”¨äºè¾“å…¥æç¤º
            prompt = gr.Textbox(label="Prompt (Less than 200 Words)", placeholder="Enter your prompt here", lines=5)

            # åˆ›å»ºä¸€ä¸ªè¡Œå¸ƒå±€
            with gr.Row():
                # åˆ›å»º Markdown ç»„ä»¶ï¼Œè¯´æ˜å¢å¼ºæç¤ºæŒ‰é’®çš„åŠŸèƒ½
                gr.Markdown(
                    "âœ¨Upon pressing the enhanced prompt button, we will use [GLM-4 Model](https://github.com/THUDM/GLM-4) to polish the prompt and overwrite the original one."
                )
                # åˆ›å»ºå¢å¼ºæç¤ºçš„æŒ‰é’®
                enhance_button = gr.Button("âœ¨ Enhance Prompt(Optional)")

            # åˆ›å»ºå¦ä¸€åˆ—å¸ƒå±€
            with gr.Column():
                # åˆ›å»º Markdown ç»„ä»¶ï¼Œæè¿°å¯é€‰å‚æ•°
                gr.Markdown(
                    "**Optional Parameters** (default values are recommended)<br>"
                    "Increasing the number of inference steps will produce more detailed videos, but it will slow down the process.<br>"
                    "50 steps are recommended for most cases.<br>"
                )
                # åˆ›å»ºä¸€è¡Œå¸ƒå±€ï¼ŒåŒ…å«æ¨ç†æ­¥æ•°å’Œå¼•å¯¼æ¯”ä¾‹è¾“å…¥æ¡†
                with gr.Row():
                    num_inference_steps = gr.Number(label="Inference Steps", value=50)
                    guidance_scale = gr.Number(label="Guidance Scale", value=6.0)
                # åˆ›å»ºç”Ÿæˆè§†é¢‘çš„æŒ‰é’®
                generate_button = gr.Button("ğŸ¬ Generate Video")

        # åˆ›å»ºå¦ä¸€åˆ—å¸ƒå±€
        with gr.Column():
            # åˆ›å»ºè§†é¢‘è¾“å‡ºç»„ä»¶
            video_output = gr.Video(label="CogVideoX Generate Video", width=720, height=480)
            # åˆ›å»ºä¸€è¡Œå¸ƒå±€ï¼ŒåŒ…å«ä¸‹è½½æŒ‰é’®
            with gr.Row():
                download_video_button = gr.File(label="ğŸ“¥ Download Video", visible=False)
                download_gif_button = gr.File(label="ğŸ“¥ Download GIF", visible=False)

    # å®šä¹‰ç”Ÿæˆè§†é¢‘çš„å‡½æ•°
    def generate(prompt, num_inference_steps, guidance_scale, model_choice, progress=gr.Progress(track_tqdm=True)):
        # è°ƒç”¨æ¨ç†å‡½æ•°ç”Ÿæˆå¼ é‡
        tensor = infer(prompt, num_inference_steps, guidance_scale, progress=progress)
        # ä¿å­˜ç”Ÿæˆçš„è§†é¢‘å¹¶è·å–å…¶è·¯å¾„
        video_path = save_video(tensor)
        # æ›´æ–°è§†é¢‘è¾“å‡ºç»„ä»¶ä¸ºå¯è§ï¼Œå¹¶è®¾ç½®è§†é¢‘è·¯å¾„
        video_update = gr.update(visible=True, value=video_path)
        # å°†è§†é¢‘è½¬æ¢ä¸º GIF å¹¶è·å–å…¶è·¯å¾„
        gif_path = convert_to_gif(video_path)
        # æ›´æ–° GIF ä¸‹è½½æŒ‰é’®ä¸ºå¯è§ï¼Œå¹¶è®¾ç½® GIF è·¯å¾„
        gif_update = gr.update(visible=True, value=gif_path)

        # è¿”å›è§†é¢‘è·¯å¾„å’Œæ›´æ–°ä¿¡æ¯
        return video_path, video_update, gif_update

    # å®šä¹‰å¢å¼ºæç¤ºçš„å‡½æ•°
    def enhance_prompt_func(prompt):
        # è½¬æ¢æç¤ºå¹¶å…è®¸é‡è¯•ä¸€æ¬¡
        return convert_prompt(prompt, retry_times=1)
    # ä¸ºç”ŸæˆæŒ‰é’®æ·»åŠ ç‚¹å‡»äº‹ä»¶ï¼Œè§¦å‘ç”Ÿæˆå‡½æ•°
        generate_button.click(
            # ç»‘å®šç”Ÿæˆå‡½æ•°åˆ°ç‚¹å‡»äº‹ä»¶
            generate,
            # å®šä¹‰è¾“å…¥ç»„ä»¶ï¼ŒåŒ…æ‹¬æç¤ºæ–‡æœ¬ã€æ¨ç†æ­¥éª¤æ•°å’Œå¼•å¯¼å°ºåº¦
            inputs=[prompt, num_inference_steps, guidance_scale],
            # å®šä¹‰è¾“å‡ºç»„ä»¶ï¼ŒåŒ…æ‹¬è§†é¢‘è¾“å‡ºå’Œä¸‹è½½æŒ‰é’®
            outputs=[video_output, download_video_button, download_gif_button],
        )
    
    # ä¸ºå¢å¼ºæŒ‰é’®æ·»åŠ ç‚¹å‡»äº‹ä»¶ï¼Œè§¦å‘å¢å¼ºæç¤ºå‡½æ•°
        enhance_button.click(enhance_prompt_func, 
            # å®šä¹‰è¾“å…¥ç»„ä»¶ï¼ŒåŒ…æ‹¬æç¤ºæ–‡æœ¬
            inputs=[prompt], 
            # å®šä¹‰è¾“å‡ºç»„ä»¶ï¼Œæ›´æ–°æç¤ºæ–‡æœ¬
            outputs=[prompt]
        )
# æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦ä¸ºä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # è°ƒç”¨ demo å¯¹è±¡çš„ launch æ–¹æ³•
    demo.launch()
```