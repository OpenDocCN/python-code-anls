# `.\cogvideo-finetune\finetune\train_cogvideox_image_to_video_lora.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œæ ‡æ˜ç‰ˆæƒå½’å±äºCogViewå›¢é˜Ÿã€æ¸…åå¤§å­¦ã€ZhipuAIå’ŒHuggingFaceå›¢é˜Ÿï¼Œæ‰€æœ‰æƒåˆ©ä¿ç•™ã€‚
# 
# æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›
# é™¤ééµå¾ªè¯¥è®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œè½¯ä»¶åœ¨è®¸å¯è¯ä¸‹ä»¥â€œæŒ‰åŸæ ·â€æ–¹å¼åˆ†å‘ï¼Œ
# ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚
# æœ‰å…³è®¸å¯è¯ä¸‹æƒé™å’Œé™åˆ¶çš„å…·ä½“ä¿¡æ¯ï¼Œè¯·å‚è§è®¸å¯è¯ã€‚

# å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£æåº“
import argparse
# å¯¼å…¥æ—¥å¿—è®°å½•åº“
import logging
# å¯¼å…¥æ•°å­¦åº“
import math
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£åº“
import os
# å¯¼å…¥éšæœºæ•°ç”Ÿæˆåº“
import random
# å¯¼å…¥æ–‡ä»¶å’Œç›®å½•æ“ä½œåº“
import shutil
# å¯¼å…¥æ—¶é—´å¤„ç†åº“
from datetime import timedelta
# å¯¼å…¥è·¯å¾„æ“ä½œåº“
from pathlib import Path
# å¯¼å…¥ç±»å‹æ³¨è§£ç›¸å…³çš„åº“
from typing import List, Optional, Tuple, Union

# å¯¼å…¥PyTorchåº“
import torch
# å¯¼å…¥transformersåº“ï¼Œç”¨äºå¤„ç†é¢„è®­ç»ƒæ¨¡å‹
import transformers
# ä»accelerateåº“å¯¼å…¥åŠ é€Ÿå™¨ç±»
from accelerate import Accelerator
# ä»accelerateåº“å¯¼å…¥æ—¥å¿—è®°å½•å‡½æ•°
from accelerate.logging import get_logger
# ä»accelerateåº“å¯¼å…¥åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç›¸å…³å‚æ•°
from accelerate.utils import DistributedDataParallelKwargs, InitProcessGroupKwargs, ProjectConfiguration, set_seed
# ä»huggingface_hubåº“å¯¼å…¥åˆ›å»ºå’Œä¸Šä¼ æ¨¡å‹åº“çš„å‡½æ•°
from huggingface_hub import create_repo, upload_folder
# ä»peftåº“å¯¼å…¥Loraé…ç½®åŠæ¨¡å‹çŠ¶æ€å­—å…¸ç›¸å…³å‡½æ•°
from peft import LoraConfig, get_peft_model_state_dict, set_peft_model_state_dict
# ä»torch.utils.dataåº“å¯¼å…¥æ•°æ®åŠ è½½å™¨å’Œæ•°æ®é›†ç±»
from torch.utils.data import DataLoader, Dataset
# ä»torchvisionåº“å¯¼å…¥å›¾åƒå˜æ¢å‡½æ•°
from torchvision import transforms
# ä»tqdmåº“å¯¼å…¥è¿›åº¦æ¡æ˜¾ç¤º
from tqdm.auto import tqdm
# ä»transformersåº“å¯¼å…¥è‡ªåŠ¨æ ‡è®°å™¨å’Œæ¨¡å‹
from transformers import AutoTokenizer, T5EncoderModel, T5Tokenizer

# å¯¼å…¥diffusersåº“
import diffusers
# ä»diffusersåº“å¯¼å…¥ä¸åŒæ¨¡å‹å’Œè°ƒåº¦å™¨
from diffusers import (
    AutoencoderKLCogVideoX,
    CogVideoXDPMScheduler,
    CogVideoXImageToVideoPipeline,
    CogVideoXTransformer3DModel,
)
# ä»diffusers.models.embeddingsåº“å¯¼å…¥3Dæ—‹è½¬ä½ç½®åµŒå…¥å‡½æ•°
from diffusers.models.embeddings import get_3d_rotary_pos_embed
# ä»diffusers.optimizationåº“å¯¼å…¥è°ƒåº¦å™¨è·å–å‡½æ•°
from diffusers.optimization import get_scheduler
# ä»diffusers.pipelines.cogvideoåº“å¯¼å…¥å›¾åƒç¼©æ”¾è£å‰ªåŒºåŸŸè·å–å‡½æ•°
from diffusers.pipelines.cogvideo.pipeline_cogvideox import get_resize_crop_region_for_grid
# ä»diffusers.training_utilsåº“å¯¼å…¥è®­ç»ƒå‚æ•°å¤„ç†å’Œå†…å­˜é‡Šæ”¾å‡½æ•°
from diffusers.training_utils import cast_training_params, free_memory
# ä»diffusers.utilsåº“å¯¼å…¥å¤šç§å·¥å…·å‡½æ•°
from diffusers.utils import (
    check_min_version,
    convert_unet_state_dict_to_peft,
    export_to_video,
    is_wandb_available,
    load_image,
)
# ä»diffusers.utils.hub_utilsåº“å¯¼å…¥æ¨¡å‹å¡åŠ è½½å’Œå¡«å……å‡½æ•°
from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card
# ä»diffusers.utils.torch_utilsåº“å¯¼å…¥æ£€æŸ¥æ¨¡å—ç¼–è¯‘çŠ¶æ€çš„å‡½æ•°
from diffusers.utils.torch_utils import is_compiled_module
# ä»torchvision.transforms.functionalåº“å¯¼å…¥ä¸­å¿ƒè£å‰ªå’Œè°ƒæ•´å¤§å°å‡½æ•°
from torchvision.transforms.functional import center_crop, resize
# ä»torchvision.transformsåº“å¯¼å…¥æ’å€¼æ¨¡å¼
from torchvision.transforms import InterpolationMode
# å¯¼å…¥torchvision.transformsåº“
import torchvision.transforms as TT
# å¯¼å…¥NumPyåº“
import numpy as np
# ä»diffusers.image_processoråº“å¯¼å…¥å›¾åƒå¤„ç†å™¨
from diffusers.image_processor import VaeImageProcessor

# å¦‚æœWandBåº“å¯ç”¨ï¼Œåˆ™å¯¼å…¥WandB
if is_wandb_available():
    import wandb

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æœ€å°ç‰ˆæœ¬çš„diffusersåº“ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°†ä¼šæŠ¥é”™ã€‚é£é™©è‡ªè´Ÿã€‚
check_min_version("0.31.0.dev0")

# è·å–æ—¥å¿—è®°å½•å™¨å®ä¾‹ï¼Œä½¿ç”¨å½“å‰æ¨¡å—çš„åç§°
logger = get_logger(__name__)

# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºå‚æ•°è§£æå™¨ï¼Œæè¿°ä¸ºCogVideoXçš„è®­ç»ƒè„šæœ¬ç¤ºä¾‹
    parser = argparse.ArgumentParser(description="Simple example of a training script for CogVideoX.")

    # æ·»åŠ æ¨¡å‹ä¿¡æ¯çš„å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        default=None,
        required=True,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„æˆ–Hugging Faceæ¨¡å‹æ ‡è¯†ç¬¦
        help="Path to pretrained model or model identifier from huggingface.co/models.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹çš„ä¿®è®¢ç‰ˆ
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        required=False,
        # å¸®åŠ©ä¿¡æ¯ï¼šæ¥è‡ª huggingface.co/models çš„é¢„è®­ç»ƒæ¨¡å‹æ ‡è¯†ç¬¦çš„ä¿®è®¢ç‰ˆ
        help="Revision of pretrained model identifier from huggingface.co/models.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹çš„å˜ä½“
    parser.add_argument(
        "--variant",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šé¢„è®­ç»ƒæ¨¡å‹æ ‡è¯†ç¬¦çš„æ¨¡å‹æ–‡ä»¶çš„å˜ä½“ï¼Œä¾‹å¦‚ fp16
        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šç¼“å­˜ç›®å½•
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šä¸‹è½½çš„æ¨¡å‹å’Œæ•°æ®é›†å°†å­˜å‚¨çš„ç›®å½•
        help="The directory where the downloaded models and datasets will be stored.",
    )

    # æ•°æ®é›†ä¿¡æ¯
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ•°æ®é›†åç§°
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šåŒ…å«å®ä¾‹å›¾åƒè®­ç»ƒæ•°æ®çš„æ•°æ®é›†åç§°ï¼Œå¯ä»¥æ˜¯æœ¬åœ°æ•°æ®é›†è·¯å¾„
        help=(
            "The name of the Dataset (from the HuggingFace hub) containing the training data of instance images (could be your own, possibly private,"
            " dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,"
            " or to a folder containing files that ğŸ¤— Datasets can understand."
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ•°æ®é›†é…ç½®åç§°
    parser.add_argument(
        "--dataset_config_name",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šæ•°æ®é›†çš„é…ç½®ï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªé…ç½®åˆ™ä¿ç•™ä¸º None
        help="The config of the Dataset, leave as None if there's only one config.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šå®ä¾‹æ•°æ®æ ¹ç›®å½•
    parser.add_argument(
        "--instance_data_root",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šåŒ…å«è®­ç»ƒæ•°æ®çš„æ–‡ä»¶å¤¹
        help=("A folder containing the training data."),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šè§†é¢‘åˆ—å
    parser.add_argument(
        "--video_column",
        type=str,
        default="video",
        # å¸®åŠ©ä¿¡æ¯ï¼šæ•°æ®é›†ä¸­åŒ…å«è§†é¢‘çš„åˆ—åï¼Œæˆ–åŒ…å«è§†é¢‘æ•°æ®è·¯å¾„çš„æ–‡ä»¶å
        help="The column of the dataset containing videos. Or, the name of the file in `--instance_data_root` folder containing the line-separated path to video data.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæç¤ºæ–‡æœ¬åˆ—å
    parser.add_argument(
        "--caption_column",
        type=str,
        default="text",
        # å¸®åŠ©ä¿¡æ¯ï¼šæ•°æ®é›†ä¸­æ¯ä¸ªè§†é¢‘çš„å®ä¾‹æç¤ºçš„åˆ—åï¼Œæˆ–åŒ…å«å®ä¾‹æç¤ºçš„æ–‡ä»¶å
        help="The column of the dataset containing the instance prompt for each video. Or, the name of the file in `--instance_data_root` folder containing the line-separated instance prompts.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ ‡è¯†ç¬¦ä»¤ç‰Œ
    parser.add_argument(
        "--id_token", type=str, default=None, 
        # å¸®åŠ©ä¿¡æ¯ï¼šå¦‚æœæä¾›ï¼Œå°†é™„åŠ åˆ°æ¯ä¸ªæç¤ºçš„å¼€å¤´çš„æ ‡è¯†ç¬¦ä»¤ç‰Œ
        help="Identifier token appended to the start of each prompt if provided."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ•°æ®åŠ è½½å™¨çš„å·¥ä½œè¿›ç¨‹æ•°
    parser.add_argument(
        "--dataloader_num_workers",
        type=int,
        default=0,
        # å¸®åŠ©ä¿¡æ¯ï¼šç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°é‡ã€‚0 è¡¨ç¤ºåœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ•°æ®
        help=(
            "Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process."
        ),
    )

    # éªŒè¯
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šéªŒè¯æç¤º
    parser.add_argument(
        "--validation_prompt",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šåœ¨éªŒè¯æœŸé—´ä½¿ç”¨çš„ä¸€ä¸ªæˆ–å¤šä¸ªæç¤ºï¼Œä»¥éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨å­¦ä¹ 
        help="One or more prompt(s) that is used during validation to verify that the model is learning. Multiple validation prompts should be separated by the '--validation_prompt_seperator' string.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šéªŒè¯å›¾åƒè·¯å¾„
    parser.add_argument(
        "--validation_images",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º None
        default=None,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="One or more image path(s) that is used during validation to verify that the model is learning. Multiple validation paths should be separated by the '--validation_prompt_seperator' string. These should correspond to the order of the validation prompts.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šéªŒè¯æç¤ºåˆ†éš”ç¬¦
    parser.add_argument(
        "--validation_prompt_separator",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º ':::'
        default=":::",
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="String that separates multiple validation prompts",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šç”ŸæˆéªŒè¯è§†é¢‘çš„æ•°é‡
    parser.add_argument(
        "--num_validation_videos",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 1
        default=1,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="Number of videos that should be generated during validation per `validation_prompt`.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šæ¯ X ä¸ªè®­ç»ƒå‘¨æœŸè¿›è¡ŒéªŒè¯
    parser.add_argument(
        "--validation_epochs",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 50
        default=50,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=(
            "Run validation every X epochs. Validation consists of running the prompt `args.validation_prompt` multiple times: `args.num_validation_videos`."
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šå¼•å¯¼å°ºåº¦
    parser.add_argument(
        "--guidance_scale",
        # å‚æ•°ç±»å‹ä¸ºæµ®ç‚¹æ•°
        type=float,
        # é»˜è®¤å€¼ä¸º 6
        default=6,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="The guidance scale to use while sampling validation videos.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šæ˜¯å¦ä½¿ç”¨åŠ¨æ€é…ç½®
    parser.add_argument(
        "--use_dynamic_cfg",
        # å‚æ•°ç±»å‹ä¸ºå¸ƒå°”å€¼ï¼Œè®¾ç½®ä¸ºçœŸæ—¶å¯ç”¨åŠ¨æ€é…ç½®
        action="store_true",
        # é»˜è®¤å€¼ä¸º False
        default=False,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="Whether or not to use the default cosine dynamic guidance schedule when sampling validation videos.",
    )

    # è®­ç»ƒä¿¡æ¯
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šéšæœºç§å­
    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®š LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦
    parser.add_argument(
        "--rank",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 128
        default=128,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=("The dimension of the LoRA update matrices."),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®š LoRA çš„ç¼©æ”¾å› å­
    parser.add_argument(
        "--lora_alpha",
        # å‚æ•°ç±»å‹ä¸ºæµ®ç‚¹æ•°
        type=float,
        # é»˜è®¤å€¼ä¸º 128
        default=128,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=("The scaling factor to scale LoRA weight update. The actual scaling factor is `lora_alpha / rank`"),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šæ··åˆç²¾åº¦è®¾ç½®
    parser.add_argument(
        "--mixed_precision",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º None
        default=None,
        # å¯é€‰å€¼åŒ…æ‹¬ "no", "fp16", "bf16"
        choices=["no", "fp16", "bf16"],
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=(
            "Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
            " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
            " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šè¾“å‡ºç›®å½•
    parser.add_argument(
        "--output_dir",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º 'cogvideox-i2v-lora'
        default="cogvideox-i2v-lora",
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="The output directory where the model predictions and checkpoints will be written.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šè¾“å…¥è§†é¢‘çš„é«˜åº¦
    parser.add_argument(
        "--height",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 480
        default=480,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="All input videos are resized to this height.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šè¾“å…¥è§†é¢‘çš„å®½åº¦
    parser.add_argument(
        "--width",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 720
        default=720,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="All input videos are resized to this width.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è§†é¢‘é‡å¡‘æ¨¡å¼ï¼Œæ¥å—çš„å€¼æœ‰ ['center', 'random', 'none']
    parser.add_argument(
        "--video_reshape_mode",
        type=str,
        default="center",
        help="All input videos are reshaped to this mode. Choose between ['center', 'random', 'none']",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è¾“å…¥è§†é¢‘çš„å¸§ç‡ï¼Œé»˜è®¤ä¸º 8
    parser.add_argument("--fps", type=int, default=8, help="All input videos will be used at this FPS.")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è¾“å…¥è§†é¢‘çš„æœ€å¤§å¸§æ•°ï¼Œé»˜è®¤ä¸º 49
    parser.add_argument(
        "--max_num_frames", type=int, default=49, help="All input videos will be truncated to these many frames."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®ä»æ¯ä¸ªè¾“å…¥è§†é¢‘å¼€å§‹è·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
    parser.add_argument(
        "--skip_frames_start",
        type=int,
        default=0,
        help="Number of frames to skip from the beginning of each input video. Useful if training data contains intro sequences.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®ä»æ¯ä¸ªè¾“å…¥è§†é¢‘ç»“æŸè·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
    parser.add_argument(
        "--skip_frames_end",
        type=int,
        default=0,
        help="Number of frames to skip from the end of each input video. Useful if training data contains outro sequences.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ˜¯å¦éšæœºæ°´å¹³ç¿»è½¬è§†é¢‘
    parser.add_argument(
        "--random_flip",
        action="store_true",
        help="whether to randomly flip videos horizontally",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º 4
    parser.add_argument(
        "--train_batch_size", type=int, default=4, help="Batch size (per device) for the training dataloader."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è®­ç»ƒçš„æ€»å‘¨æœŸæ•°ï¼Œé»˜è®¤ä¸º 1
    parser.add_argument("--num_train_epochs", type=int, default=1)
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ€»è®­ç»ƒæ­¥éª¤æ•°ï¼Œé»˜è®¤ä¸º Noneï¼Œè¦†ç›–å‘¨æœŸè®¾ç½®
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="Total number of training steps to perform. If provided, overrides `--num_train_epochs`.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ¯ X æ¬¡æ›´æ–°ä¿å­˜è®­ç»ƒçŠ¶æ€æ£€æŸ¥ç‚¹çš„æ­¥æ•°ï¼Œé»˜è®¤ä¸º 500
    parser.add_argument(
        "--checkpointing_steps",
        type=int,
        default=500,
        help=(
            "Save a checkpoint of the training state every X updates. These checkpoints can be used both as final"
            " checkpoints in case they are better than the last checkpoint, and are also suitable for resuming"
            " training using `--resume_from_checkpoint`."
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®å­˜å‚¨çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡ï¼Œé»˜è®¤ä¸º None
    parser.add_argument(
        "--checkpoints_total_limit",
        type=int,
        default=None,
        help=("Max number of checkpoints to store."),
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ˜¯å¦ä»å…ˆå‰çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œé»˜è®¤ä¸º None
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help=(
            "Whether training should be resumed from a previous checkpoint. Use a path saved by"
            ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®åœ¨æ‰§è¡Œåå‘ä¼ æ’­/æ›´æ–°å‰è¦ç´¯ç§¯çš„æ›´æ–°æ­¥éª¤æ•°ï¼Œé»˜è®¤ä¸º 1
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ¥èŠ‚çœå†…å­˜ï¼Œé»˜è®¤ä¸º False
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        help="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®åˆå§‹å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º 1e-4
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --scale_lrï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œé»˜è®¤å€¼ä¸º False
    parser.add_argument(
        "--scale_lr",
        action="store_true",
        default=False,
        help="Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_schedulerï¼ŒæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º "constant"
    parser.add_argument(
        "--lr_scheduler",
        type=str,
        default="constant",
        help=(
            'The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
            ' "constant", "constant_with_warmup"]'
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_warmup_stepsï¼ŒæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„é¢„çƒ­æ­¥éª¤æ•°ï¼Œé»˜è®¤å€¼ä¸º 500
    parser.add_argument(
        "--lr_warmup_steps", type=int, default=500, help="Number of steps for the warmup in the lr scheduler."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_num_cyclesï¼ŒæŒ‡å®šåœ¨ cosine_with_restarts è°ƒåº¦å™¨ä¸­å­¦ä¹ ç‡çš„ç¡¬é‡ç½®æ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º 1
    parser.add_argument(
        "--lr_num_cycles",
        type=int,
        default=1,
        help="Number of hard resets of the lr in cosine_with_restarts scheduler.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_powerï¼ŒæŒ‡å®šå¤šé¡¹å¼è°ƒåº¦å™¨çš„å¹‚å› å­ï¼Œé»˜è®¤å€¼ä¸º 1.0
    parser.add_argument("--lr_power", type=float, default=1.0, help="Power factor of the polynomial scheduler.")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --enable_slicingï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œé»˜è®¤å€¼ä¸º Falseï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ VAE åˆ‡ç‰‡ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_slicing",
        action="store_true",
        default=False,
        help="Whether or not to use VAE slicing for saving memory.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --enable_tilingï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œé»˜è®¤å€¼ä¸º Falseï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ VAE ç“·ç –ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_tiling",
        action="store_true",
        default=False,
        help="Whether or not to use VAE tiling for saving memory.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --noised_image_dropoutï¼ŒæŒ‡å®šå›¾åƒæ¡ä»¶çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œé»˜è®¤å€¼ä¸º 0.05
    parser.add_argument(
        "--noised_image_dropout",
        type=float,
        default=0.05,
        help="Image condition dropout probability.",
    )

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --optimizerï¼ŒæŒ‡å®šä¼˜åŒ–å™¨ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º "adam"
    parser.add_argument(
        "--optimizer",
        type=lambda s: s.lower(),
        default="adam",
        choices=["adam", "adamw", "prodigy"],
        help=("The optimizer type to use."),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --use_8bit_adamï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ bitsandbytes çš„ 8 ä½ Adam
    parser.add_argument(
        "--use_8bit_adam",
        action="store_true",
        help="Whether or not to use 8-bit Adam from bitsandbytes. Ignored if optimizer is not set to AdamW",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_beta1ï¼ŒæŒ‡å®š Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ beta1 å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 0.9
    parser.add_argument(
        "--adam_beta1", type=float, default=0.9, help="The beta1 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_beta2ï¼ŒæŒ‡å®š Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ beta2 å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 0.95
    parser.add_argument(
        "--adam_beta2", type=float, default=0.95, help="The beta2 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --prodigy_beta3ï¼ŒæŒ‡å®š Prodigy ä¼˜åŒ–å™¨çš„æ­¥é•¿ç³»æ•°ï¼Œé»˜è®¤å€¼ä¸º None
    parser.add_argument(
        "--prodigy_beta3",
        type=float,
        default=None,
        help="Coefficients for computing the Prodigy optimizer's stepsize using running averages. If set to None, uses the value of square root of beta2.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --prodigy_decoupleï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ AdamW é£æ ¼çš„è§£è€¦æƒé‡è¡°å‡
    parser.add_argument("--prodigy_decouple", action="store_true", help="Use AdamW style decoupled weight decay")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_weight_decayï¼ŒæŒ‡å®š UNet å‚æ•°ä½¿ç”¨çš„æƒé‡è¡°å‡ï¼Œé»˜è®¤å€¼ä¸º 1e-04
    parser.add_argument("--adam_weight_decay", type=float, default=1e-04, help="Weight decay to use for unet params")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_epsilonï¼ŒæŒ‡å®š Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ epsilon å€¼ï¼Œé»˜è®¤å€¼ä¸º 1e-08
    parser.add_argument(
        "--adam_epsilon",
        type=float,
        default=1e-08,
        help="Epsilon value for the Adam optimizer and Prodigy optimizers.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --max_grad_normï¼ŒæŒ‡å®šæœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼Œé»˜è®¤å€¼ä¸º 1.0
    parser.add_argument("--max_grad_norm", default=1.0, type=float, help="Max gradient norm.")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºå¼€å¯ Adam çš„åå·®ä¿®æ­£
        parser.add_argument("--prodigy_use_bias_correction", action="store_true", help="Turn on Adam's bias correction.")
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºåœ¨çƒ­èº«é˜¶æ®µç§»é™¤å­¦ä¹ ç‡ï¼Œä»¥é¿å… D ä¼°è®¡çš„é—®é¢˜
        parser.add_argument(
            "--prodigy_safeguard_warmup",
            action="store_true",
            help="Remove lr from the denominator of D estimate to avoid issues during warm-up stage.",
        )
    
        # æ·»åŠ é¡¹ç›®è¿½è¸ªå™¨åç§°çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
        parser.add_argument("--tracker_name", type=str, default=None, help="Project tracker name")
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° Hub
        parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
        # æ·»åŠ  Hub è®¿é—®ä»¤ç‰Œçš„å‘½ä»¤è¡Œå‚æ•°ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
        parser.add_argument("--hub_token", type=str, default=None, help="The token to use to push to the Model Hub.")
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šè¦ä¸æœ¬åœ°è¾“å‡ºç›®å½•åŒæ­¥çš„å­˜å‚¨åº“åç§°
        parser.add_argument(
            "--hub_model_id",
            type=str,
            default=None,
            help="The name of the repository to keep in sync with the local `output_dir`.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ—¥å¿—æ–‡ä»¶å­˜å‚¨ç›®å½•ï¼Œé»˜è®¤ä¸º "logs"
        parser.add_argument(
            "--logging_dir",
            type=str,
            default="logs",
            help="Directory where logs are stored.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦å…è®¸åœ¨ Ampere GPU ä¸Šä½¿ç”¨ TF32ï¼Œä»¥åŠ é€Ÿè®­ç»ƒ
        parser.add_argument(
            "--allow_tf32",
            action="store_true",
            help=(
                "Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see"
                " https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices"
            ),
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šå°†ç»“æœå’Œæ—¥å¿—æŠ¥å‘Šåˆ°çš„é›†æˆå¹³å°
        parser.add_argument(
            "--report_to",
            type=str,
            default=None,
            help=(
                'The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
                ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
            ),
        )
        # æ·»åŠ  NCCL åç«¯è¶…æ—¶çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œå•ä½ä¸ºç§’ï¼Œé»˜è®¤ä¸º 600
        parser.add_argument("--nccl_timeout", type=int, default=600, help="NCCL backend timeout in seconds.")
    
        # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ç»“æœ
        return parser.parse_args()
# å®šä¹‰ä¸€ä¸ªè§†é¢‘æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ª Dataset åŸºç±»
class VideoDataset(Dataset):
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—å¤šä¸ªå‚æ•°ä»¥é…ç½®æ•°æ®é›†
    def __init__(
        # æ•°æ®æ ¹ç›®å½•ï¼Œå¯é€‰
        self,
        instance_data_root: Optional[str] = None,
        # æ•°æ®é›†åç§°ï¼Œå¯é€‰
        dataset_name: Optional[str] = None,
        # æ•°æ®é›†é…ç½®åç§°ï¼Œå¯é€‰
        dataset_config_name: Optional[str] = None,
        # ç”¨äºæ–‡æœ¬æè¿°çš„åˆ—å
        caption_column: str = "text",
        # è§†é¢‘åˆ—å
        video_column: str = "video",
        # è§†é¢‘é«˜åº¦ï¼Œé»˜è®¤ 480
        height: int = 480,
        # è§†é¢‘å®½åº¦ï¼Œé»˜è®¤ 720
        width: int = 720,
        # è§†é¢‘é‡å¡‘æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨ä¸­å¿ƒæ¨¡å¼
        video_reshape_mode: str = "center",
        # å¸§ç‡ï¼Œé»˜è®¤ 8 å¸§æ¯ç§’
        fps: int = 8,
        # æœ€å¤§å¸§æ•°ï¼Œé»˜è®¤ 49
        max_num_frames: int = 49,
        # å¼€å§‹è·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ 0
        skip_frames_start: int = 0,
        # ç»“æŸè·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ 0
        skip_frames_end: int = 0,
        # ç¼“å­˜ç›®å½•ï¼Œå¯é€‰
        cache_dir: Optional[str] = None,
        # ID ä»¤ç‰Œï¼Œå¯é€‰
        id_token: Optional[str] = None,
    ) -> None:
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()

        # å°†æ•°æ®æ ¹ç›®å½•è½¬æ¢ä¸º Path å¯¹è±¡ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä¸º None
        self.instance_data_root = Path(instance_data_root) if instance_data_root is not None else None
        # è®¾ç½®æ•°æ®é›†åç§°
        self.dataset_name = dataset_name
        # è®¾ç½®æ•°æ®é›†é…ç½®åç§°
        self.dataset_config_name = dataset_config_name
        # è®¾ç½®æ–‡æœ¬æè¿°åˆ—å
        self.caption_column = caption_column
        # è®¾ç½®è§†é¢‘åˆ—å
        self.video_column = video_column
        # è®¾ç½®è§†é¢‘é«˜åº¦
        self.height = height
        # è®¾ç½®è§†é¢‘å®½åº¦
        self.width = width
        # è®¾ç½®è§†é¢‘é‡å¡‘æ¨¡å¼
        self.video_reshape_mode = video_reshape_mode
        # è®¾ç½®å¸§ç‡
        self.fps = fps
        # è®¾ç½®æœ€å¤§å¸§æ•°
        self.max_num_frames = max_num_frames
        # è®¾ç½®å¼€å§‹è·³è¿‡çš„å¸§æ•°
        self.skip_frames_start = skip_frames_start
        # è®¾ç½®ç»“æŸè·³è¿‡çš„å¸§æ•°
        self.skip_frames_end = skip_frames_end
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        # è®¾ç½® ID ä»¤ç‰Œï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
        self.id_token = id_token or ""

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œåˆ™ä» hub åŠ è½½æ•°æ®é›†
        if dataset_name is not None:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_hub()
        # å¦åˆ™ï¼Œä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†
        else:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_local_path()

        # å°† ID ä»¤ç‰Œæ·»åŠ åˆ°æ¯ä¸ªæç¤ºå‰
        self.instance_prompts = [self.id_token + prompt for prompt in self.instance_prompts]

        # è®¡ç®—å®ä¾‹è§†é¢‘çš„æ•°é‡
        self.num_instance_videos = len(self.instance_video_paths)
        # ç¡®ä¿è§†é¢‘å’Œæç¤ºæ•°é‡åŒ¹é…ï¼Œä¸åŒ¹é…åˆ™å¼•å‘é”™è¯¯
        if self.num_instance_videos != len(self.instance_prompts):
            raise ValueError(
                f"Expected length of instance prompts and videos to be the same but found {len(self.instance_prompts)=} and {len(self.instance_video_paths)=}. Please ensure that the number of caption prompts and videos match in your dataset."
            )

        # é¢„å¤„ç†æ•°æ®å¹¶å­˜å‚¨å¤„ç†åçš„å®ä¾‹è§†é¢‘
        self.instance_videos = self._preprocess_data()

    # è¿”å›æ•°æ®é›†ä¸­çš„å®ä¾‹æ•°é‡
    def __len__(self):
        return self.num_instance_videos

    # æ ¹æ®ç´¢å¼•è·å–æ•°æ®é¡¹
    def __getitem__(self, index):
        return {
            # è¿”å›å¯¹åº”çš„å®ä¾‹æç¤º
            "instance_prompt": self.instance_prompts[index],
            # è¿”å›å¯¹åº”çš„å®ä¾‹è§†é¢‘
            "instance_video": self.instance_videos[index],
        }
    # ä»æ•°æ®é›†ä¸­åŠ è½½æ•°æ®çš„ç§æœ‰æ–¹æ³•
        def _load_dataset_from_hub(self):
            # å°è¯•å¯¼å…¥ datasets åº“ä»¥åŠ è½½æ•°æ®é›†
            try:
                from datasets import load_dataset
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æŠ›å‡º ImportError
            except ImportError:
                raise ImportError(
                    "You are trying to load your data using the datasets library. If you wish to train using custom "
                    "captions please install the datasets library: `pip install datasets`. If you wish to load a "
                    "local folder containing images only, specify --instance_data_root instead."
                )
    
            # ä»æ•°æ®é›†ä¸­å¿ƒä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è§æ–‡æ¡£é“¾æ¥
            dataset = load_dataset(
                self.dataset_name,  # æ•°æ®é›†åç§°
                self.dataset_config_name,  # æ•°æ®é›†é…ç½®åç§°
                cache_dir=self.cache_dir,  # ç¼“å­˜ç›®å½•
            )
            # è·å–è®­ç»ƒé›†çš„åˆ—å
            column_names = dataset["train"].column_names
    
            # å¦‚æœæœªæŒ‡å®šè§†é¢‘åˆ—ï¼Œåˆ™é»˜è®¤ä¸ºåˆ—åçš„ç¬¬ä¸€ä¸ª
            if self.video_column is None:
                video_column = column_names[0]
                logger.info(f"`video_column` defaulting to {video_column}")
            else:
                video_column = self.video_column
                # æ£€æŸ¥æŒ‡å®šçš„è§†é¢‘åˆ—æ˜¯å¦å­˜åœ¨äºåˆ—åä¸­
                if video_column not in column_names:
                    raise ValueError(
                        f"`--video_column` value '{video_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                    )
    
            # å¦‚æœæœªæŒ‡å®šå­—å¹•åˆ—ï¼Œåˆ™é»˜è®¤ä¸ºåˆ—åçš„ç¬¬äºŒä¸ª
            if self.caption_column is None:
                caption_column = column_names[1]
                logger.info(f"`caption_column` defaulting to {caption_column}")
            else:
                caption_column = self.caption_column
                # æ£€æŸ¥æŒ‡å®šçš„å­—å¹•åˆ—æ˜¯å¦å­˜åœ¨äºåˆ—åä¸­
                if self.caption_column not in column_names:
                    raise ValueError(
                        f"`--caption_column` value '{self.caption_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                    )
    
            # è·å–è®­ç»ƒé›†ä¸­çš„å®ä¾‹æç¤ºï¼ˆå­—å¹•ï¼‰
            instance_prompts = dataset["train"][caption_column]
            # è·å–è®­ç»ƒé›†ä¸­è§†é¢‘æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨
            instance_videos = [Path(self.instance_data_root, filepath) for filepath in dataset["train"][video_column]]
    
            # è¿”å›å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„
            return instance_prompts, instance_videos
    # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†
        def _load_dataset_from_local_path(self):
            # æ£€æŸ¥å®ä¾‹æ•°æ®æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.instance_data_root.exists():
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜æ ¹æ–‡ä»¶å¤¹ä¸å­˜åœ¨
                raise ValueError("Instance videos root folder does not exist")
    
            # æ„å»ºæç¤ºæ–‡æœ¬æ–‡ä»¶è·¯å¾„
            prompt_path = self.instance_data_root.joinpath(self.caption_column)
            # æ„å»ºè§†é¢‘æ–‡ä»¶è·¯å¾„
            video_path = self.instance_data_root.joinpath(self.video_column)
    
            # æ£€æŸ¥æç¤ºæ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæ–‡ä»¶
            if not prompt_path.exists() or not prompt_path.is_file():
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜æç¤ºæ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®
                raise ValueError(
                    "Expected `--caption_column` to be path to a file in `--instance_data_root` containing line-separated text prompts."
                )
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæ–‡ä»¶
            if not video_path.exists() or not video_path.is_file():
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®
                raise ValueError(
                    "Expected `--video_column` to be path to a file in `--instance_data_root` containing line-separated paths to video data in the same directory."
                )
    
            # è¯»å–æç¤ºæ–‡æœ¬æ–‡ä»¶ï¼ŒæŒ‰è¡Œå»é™¤ç©ºç™½å¹¶è¿”å›åˆ—è¡¨
            with open(prompt_path, "r", encoding="utf-8") as file:
                instance_prompts = [line.strip() for line in file.readlines() if len(line.strip()) > 0]
            # è¯»å–è§†é¢‘æ–‡ä»¶ï¼ŒæŒ‰è¡Œå»é™¤ç©ºç™½å¹¶æ„å»ºè§†é¢‘è·¯å¾„åˆ—è¡¨
            with open(video_path, "r", encoding="utf-8") as file:
                instance_videos = [
                    self.instance_data_root.joinpath(line.strip()) for line in file.readlines() if len(line.strip()) > 0
                ]
    
            # æ£€æŸ¥è§†é¢‘è·¯å¾„åˆ—è¡¨ä¸­æ˜¯å¦å­˜åœ¨æ— æ•ˆæ–‡ä»¶è·¯å¾„
            if any(not path.is_file() for path in instance_videos):
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜è‡³å°‘ä¸€ä¸ªè·¯å¾„ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶
                raise ValueError(
                    "Expected '--video_column' to be a path to a file in `--instance_data_root` containing line-separated paths to video data but found atleast one path that is not a valid file."
                )
    
            # è¿”å›æç¤ºæ–‡æœ¬å’Œè§†é¢‘è·¯å¾„åˆ—è¡¨
            return instance_prompts, instance_videos
    
        # æ ¹æ®é•¿å®½è°ƒæ•´æ•°ç»„ä»¥é€‚åº”çŸ©å½¢è£å‰ª
        def _resize_for_rectangle_crop(self, arr):
            # è·å–ç›®æ ‡å›¾åƒå°ºå¯¸
            image_size = self.height, self.width
            # è·å–é‡å¡‘æ¨¡å¼
            reshape_mode = self.video_reshape_mode
            # æ£€æŸ¥æ•°ç»„å®½é«˜æ¯”ä¸ç›®æ ‡å›¾åƒå®½é«˜æ¯”
            if arr.shape[3] / arr.shape[2] > image_size[1] / image_size[0]:
                # è°ƒæ•´æ•°ç»„å°ºå¯¸ä»¥åŒ¹é…ç›®æ ‡å›¾åƒå®½åº¦
                arr = resize(
                    arr,
                    size=[image_size[0], int(arr.shape[3] * image_size[0] / arr.shape[2])],
                    interpolation=InterpolationMode.BICUBIC,
                )
            else:
                # è°ƒæ•´æ•°ç»„å°ºå¯¸ä»¥åŒ¹é…ç›®æ ‡å›¾åƒé«˜åº¦
                arr = resize(
                    arr,
                    size=[int(arr.shape[2] * image_size[1] / arr.shape[3]), image_size[1]],
                    interpolation=InterpolationMode.BICUBIC,
                )
    
            # è·å–è°ƒæ•´åæ•°ç»„çš„é«˜åº¦å’Œå®½åº¦
            h, w = arr.shape[2], arr.shape[3]
            # å»æ‰æ•°ç»„çš„ç¬¬ä¸€ç»´
            arr = arr.squeeze(0)
    
            # è®¡ç®—é«˜åº¦å’Œå®½åº¦çš„å·®å€¼
            delta_h = h - image_size[0]
            delta_w = w - image_size[1]
    
            # æ ¹æ®é‡å¡‘æ¨¡å¼è®¡ç®—è£å‰ªçš„èµ·å§‹ç‚¹
            if reshape_mode == "random" or reshape_mode == "none":
                # éšæœºç”Ÿæˆè£å‰ªèµ·å§‹ç‚¹
                top = np.random.randint(0, delta_h + 1)
                left = np.random.randint(0, delta_w + 1)
            elif reshape_mode == "center":
                # è®¡ç®—ä¸­å¿ƒè£å‰ªèµ·å§‹ç‚¹
                top, left = delta_h // 2, delta_w // 2
            else:
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜é‡å¡‘æ¨¡å¼æœªå®ç°
                raise NotImplementedError
            # è£å‰ªæ•°ç»„åˆ°æŒ‡å®šçš„é«˜åº¦å’Œå®½åº¦
            arr = TT.functional.crop(arr, top=top, left=left, height=image_size[0], width=image_size[1])
            # è¿”å›è£å‰ªåçš„æ•°ç»„
            return arr
    # æ•°æ®é¢„å¤„ç†å‡½æ•°
    def _preprocess_data(self):
        # å°è¯•å¯¼å…¥ decord åº“
        try:
            import decord
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æŠ›å‡º ImportError å¼‚å¸¸ï¼Œå¹¶æç¤ºç”¨æˆ·å®‰è£… decord
        except ImportError:
            raise ImportError(
                "The `decord` package is required for loading the video dataset. Install with `pip install decord`"
            )

        # è®¾ç½® decord ä½¿ç”¨ PyTorch ä½œä¸ºæ¡¥æ¥åº“
        decord.bridge.set_bridge("torch")

        # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºè§†é¢‘åŠ è½½ã€è°ƒæ•´å¤§å°å’Œè£å‰ªçš„è¿›åº¦
        progress_dataset_bar = tqdm(
            range(0, len(self.instance_video_paths)),
            desc="Loading progress resize and crop videos",
        )

        # åˆå§‹åŒ–è§†é¢‘åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å¤„ç†åçš„è§†é¢‘å¸§
        videos = []

        # éå†æ¯ä¸ªè§†é¢‘æ–‡ä»¶çš„è·¯å¾„
        for filename in self.instance_video_paths:
            # ä½¿ç”¨ decord.VideoReader è¯»å–è§†é¢‘æ–‡ä»¶
            video_reader = decord.VideoReader(uri=filename.as_posix())
            # è·å–è§†é¢‘å¸§çš„æ•°é‡
            video_num_frames = len(video_reader)

            # ç¡®å®šå¼€å§‹å’Œç»“æŸå¸§çš„ç´¢å¼•
            start_frame = min(self.skip_frames_start, video_num_frames)
            end_frame = max(0, video_num_frames - self.skip_frames_end)
            # å¦‚æœç»“æŸå¸§ç´¢å¼•å°äºç­‰äºå¼€å§‹å¸§ç´¢å¼•ï¼Œåˆ™åªè·å–å¼€å§‹å¸§
            if end_frame <= start_frame:
                frames = video_reader.get_batch([start_frame])
            # å¦‚æœå¸§æ•°åœ¨å¼€å§‹å’Œç»“æŸå¸§ä¹‹é—´å°äºç­‰äºæœ€å¤§å¸§æ•°ï¼Œåˆ™è·å–å…¨éƒ¨å¸§
            elif end_frame - start_frame <= self.max_num_frames:
                frames = video_reader.get_batch(list(range(start_frame, end_frame)))
            # å¦åˆ™ï¼Œå‡åŒ€é€‰æ‹©å¸§çš„ç´¢å¼•
            else:
                indices = list(range(start_frame, end_frame, (end_frame - start_frame) // self.max_num_frames))
                frames = video_reader.get_batch(indices)

            # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§å¸§æ•°é™åˆ¶
            frames = frames[: self.max_num_frames]
            # è·å–é€‰ä¸­çš„å¸§æ•°
            selected_num_frames = frames.shape[0]

            # é€‰æ‹©å‰ (4k + 1) å¸§ï¼Œç¡®ä¿å¸§æ•°æ»¡è¶³ VAE çš„è¦æ±‚
            remainder = (3 + (selected_num_frames % 4)) % 4
            # å¦‚æœæœ‰å¤šä½™å¸§ï¼Œå»æ‰è¿™äº›å¸§
            if remainder != 0:
                frames = frames[:-remainder]
            # æ›´æ–°é€‰ä¸­çš„å¸§æ•°
            selected_num_frames = frames.shape[0]

            # æ–­è¨€é€‰ä¸­çš„å¸§æ•°å‡å» 1 èƒ½è¢« 4 æ•´é™¤
            assert (selected_num_frames - 1) % 4 == 0

            # è¿›è¡Œè®­ç»ƒå˜æ¢ï¼Œå°†å¸§å€¼å½’ä¸€åŒ–åˆ° [-1, 1]
            frames = (frames - 127.5) / 127.5
            # è°ƒæ•´å¸§çš„ç»´åº¦é¡ºåºä¸º [F, C, H, W]
            frames = frames.permute(0, 3, 1, 2) # [F, C, H, W]
            # æ›´æ–°è¿›åº¦æ¡æè¿°ï¼Œæ˜¾ç¤ºå½“å‰è§†é¢‘çš„å°ºå¯¸
            progress_dataset_bar.set_description(
                f"Loading progress Resizing video from {frames.shape[2]}x{frames.shape[3]} to {self.height}x{self.width}"
            )
            # è°ƒæ•´å¸§çš„å°ºå¯¸ä»¥é€‚åº”çŸ©å½¢è£å‰ª
            frames = self._resize_for_rectangle_crop(frames)
            # å°†å¤„ç†åçš„å¸§æ·»åŠ åˆ°è§†é¢‘åˆ—è¡¨ä¸­
            videos.append(frames.contiguous())  # [F, C, H, W]
            # æ›´æ–°è¿›åº¦æ¡
            progress_dataset_bar.update(1)

        # å…³é—­è¿›åº¦æ¡
        progress_dataset_bar.close()

        # è¿”å›å¤„ç†åçš„æ‰€æœ‰è§†é¢‘å¸§
        return videos
# ä¿å­˜æ¨¡å‹å¡ç‰‡ï¼ŒåŒ…å«æ¨¡å‹ä¿¡æ¯å’Œè§†é¢‘éªŒè¯
def save_model_card(
    # ä»“åº“æ ‡è¯†
    repo_id: str,
    # è§†é¢‘åˆ—è¡¨ï¼Œé»˜è®¤å€¼ä¸º None
    videos=None,
    # åŸºç¡€æ¨¡å‹åç§°ï¼Œé»˜è®¤å€¼ä¸º None
    base_model: str = None,
    # éªŒè¯æç¤ºï¼Œé»˜è®¤å€¼ä¸º None
    validation_prompt=None,
    # ä»“åº“æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤å€¼ä¸º None
    repo_folder=None,
    # å¸§ç‡ï¼Œé»˜è®¤å€¼ä¸º 8
    fps=8,
):
    # åˆå§‹åŒ–å°éƒ¨ä»¶å­—å…¸
    widget_dict = []
    # æ£€æŸ¥æ˜¯å¦æä¾›è§†é¢‘
    if videos is not None:
        # éå†è§†é¢‘åˆ—è¡¨åŠå…¶ç´¢å¼•
        for i, video in enumerate(videos):
            # ä¸ºæ¯ä¸ªè§†é¢‘ç”Ÿæˆæ–‡ä»¶å
            video_path = f"final_video_{i}.mp4"
            # å¯¼å‡ºè§†é¢‘åˆ°æŒ‡å®šè·¯å¾„
            export_to_video(video, os.path.join(repo_folder, video_path, fps=fps))
            # å°†è§†é¢‘ä¿¡æ¯æ·»åŠ åˆ°å°éƒ¨ä»¶å­—å…¸ä¸­
            widget_dict.append(
                {"text": validation_prompt if validation_prompt else " ", "output": {"url": video_path}},
            )

    # å®šä¹‰æ¨¡å‹æè¿°æ–‡æœ¬
    model_description = f"""
# CogVideoX LoRA - {repo_id}

<Gallery />

## Model description

These are {repo_id} LoRA weights for {base_model}.

The weights were trained using the [CogVideoX Diffusers trainer](https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/train_cogvideox_image_to_video_lora.py).

Was LoRA for the text encoder enabled? No.

## Download model

[Download the *.safetensors LoRA]({repo_id}/tree/main) in the Files & versions tab.

## Use it with the [ğŸ§¨ diffusers library](https://github.com/huggingface/diffusers)


import torch
from diffusers import CogVideoXImageToVideoPipeline
from diffusers.utils import load_image, export_to_video

pipe = CogVideoXImageToVideoPipeline.from_pretrained("THUDM/CogVideoX-5b", torch_dtype=torch.bfloat16).to("cuda")
pipe.load_lora_weights("{repo_id}", weight_name="pytorch_lora_weights.safetensors", adapter_name=["cogvideox-i2v-lora"])

# The LoRA adapter weights are determined by what was used for training.
# In this case, we assume `--lora_alpha` is 32 and `--rank` is 64.
# It can be made lower or higher from what was used in training to decrease or amplify the effect
# of the LoRA upto a tolerance, beyond which one might notice no effect at all or overflows.
pipe.set_adapters(["cogvideox-i2v-lora"], [32 / 64])

image = load_image("/path/to/image")
video = pipe(image=image, "{validation_prompt}", guidance_scale=6, use_dynamic_cfg=True).frames[0]
export_to_video(video, "output.mp4", fps=8)


For more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)

## License

Please adhere to the licensing terms as described [here](https://huggingface.co/THUDM/CogVideoX-5b-I2V/blob/main/LICENSE).
"""
    # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡
    model_card = load_or_create_model_card(
        # ä»“åº“ ID æˆ–è·¯å¾„
        repo_id_or_path=repo_id,
        # æŒ‡ç¤ºæ˜¯å¦ä»è®­ç»ƒä¸­åˆ›å»º
        from_training=True,
        # è®¸å¯è¯ç±»å‹
        license="other",
        # åŸºç¡€æ¨¡å‹åç§°
        base_model=base_model,
        # éªŒè¯æç¤º
        prompt=validation_prompt,
        # æ¨¡å‹æè¿°
        model_description=model_description,
        # å°éƒ¨ä»¶ä¿¡æ¯
        widget=widget_dict,
    )
    # å®šä¹‰æ ‡ç­¾åˆ—è¡¨
    tags = [
        "image-to-video",
        "diffusers-training",
        "diffusers",
        "lora",
        "cogvideox",
        "cogvideox-diffusers",
        "template:sd-lora",
    ]

    # å¡«å……æ¨¡å‹å¡ç‰‡çš„æ ‡ç­¾
    model_card = populate_model_card(model_card, tags=tags)
    # ä¿å­˜æ¨¡å‹å¡ç‰‡åˆ°æŒ‡å®šè·¯å¾„
    model_card.save(os.path.join(repo_folder, "README.md"))


# è®°å½•éªŒè¯è¿‡ç¨‹
def log_validation(
    # ç®¡é“å¯¹è±¡
    pipe,
    # å‚æ•°
    args,
    # åŠ é€Ÿå™¨å¯¹è±¡
    accelerator,
    # ç®¡é“å‚æ•°ï¼Œç”¨äºé…ç½®å’Œç®¡ç†æ•°æ®å¤„ç†æµç¨‹
        pipeline_args,
        # å½“å‰è®­ç»ƒçš„è½®æ¬¡ï¼Œé€šå¸¸ç”¨äºæ§åˆ¶è®­ç»ƒè¿‡ç¨‹
        epoch,
        # æŒ‡ç¤ºæ˜¯å¦è¿›è¡Œæœ€ç»ˆéªŒè¯çš„å¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸º False
        is_final_validation: bool = False,
# æ—¥å¿—è®°å½•å½“å‰éªŒè¯è¿è¡Œçš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”Ÿæˆè§†é¢‘çš„æ•°é‡å’Œæç¤ºå†…å®¹
):
    logger.info(
        f"Running validation... \n Generating {args.num_validation_videos} videos with prompt: {pipeline_args['prompt']}."
    )
    # åˆå§‹åŒ–è°ƒåº¦å™¨å‚æ•°å­—å…¸
    scheduler_args = {}

    # æ£€æŸ¥è°ƒåº¦å™¨é…ç½®ä¸­æ˜¯å¦åŒ…å«æ–¹å·®ç±»å‹
    if "variance_type" in pipe.scheduler.config:
        # è·å–æ–¹å·®ç±»å‹
        variance_type = pipe.scheduler.config.variance_type

        # å¦‚æœæ–¹å·®ç±»å‹æ˜¯å·²å­¦ä¹ çš„ç±»å‹ï¼Œè®¾ç½®ä¸ºå›ºå®šå°
        if variance_type in ["learned", "learned_range"]:
            variance_type = "fixed_small"

        # æ›´æ–°è°ƒåº¦å™¨å‚æ•°å­—å…¸ä¸­çš„æ–¹å·®ç±»å‹
        scheduler_args["variance_type"] = variance_type

    # ä½¿ç”¨è°ƒåº¦å™¨é…ç½®å’Œå‚æ•°åˆå§‹åŒ–è°ƒåº¦å™¨
    pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, **scheduler_args)
    # å°†ç®¡é“ç§»åŠ¨åˆ°æŒ‡å®šçš„åŠ é€Ÿå™¨è®¾å¤‡ä¸Š
    pipe = pipe.to(accelerator.device)
    # å…³é—­è¿›åº¦æ¡é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨è¿›åº¦æ¡ï¼‰

    # è¿è¡Œæ¨ç†
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶è®¾ç½®éšæœºç§å­ï¼Œå¦‚æœæœªæŒ‡å®šç§å­ï¼Œåˆ™ä¸º None
    generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None

    # åˆå§‹åŒ–è§†é¢‘åˆ—è¡¨
    videos = []
    # å¾ªç¯ç”ŸæˆæŒ‡å®šæ•°é‡çš„è§†é¢‘
    for _ in range(args.num_validation_videos):
        # é€šè¿‡ç®¡é“ç”Ÿæˆè§†é¢‘å¸§
        pt_images = pipe(**pipeline_args, generator=generator, output_type="pt").frames[0]
        # å°†ç”Ÿæˆçš„å¸§å †å æˆä¸€ä¸ªå¼ é‡
        pt_images = torch.stack([pt_images[i] for i in range(pt_images.shape[0])])

        # å°† PyTorch å›¾åƒè½¬æ¢ä¸º NumPy æ•°ç»„
        image_np = VaeImageProcessor.pt_to_numpy(pt_images)
        # å°† NumPy æ•°ç»„è½¬æ¢ä¸º PIL å›¾åƒ
        image_pil = VaeImageProcessor.numpy_to_pil(image_np)

        # å°†ç”Ÿæˆçš„ PIL å›¾åƒæ·»åŠ åˆ°è§†é¢‘åˆ—è¡¨ä¸­
        videos.append(image_pil)

    # éå†åŠ é€Ÿå™¨çš„è·Ÿè¸ªå™¨
    for tracker in accelerator.trackers:
        # ç¡®å®šå½“å‰é˜¶æ®µåç§°
        phase_name = "test" if is_final_validation else "validation"
        # æ£€æŸ¥æ˜¯å¦ä¸º WandB è·Ÿè¸ªå™¨
        if tracker.name == "wandb":
            # åˆå§‹åŒ–è§†é¢‘æ–‡ä»¶ååˆ—è¡¨
            video_filenames = []
            # éå†ç”Ÿæˆçš„è§†é¢‘
            for i, video in enumerate(videos):
                # æ ¼å¼åŒ–æç¤ºå†…å®¹å¹¶æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
                prompt = (
                    pipeline_args["prompt"][:25]
                    .replace(" ", "_")
                    .replace(" ", "_")
                    .replace("'", "_")
                    .replace('"', "_")
                    .replace("/", "_")
                )
                # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
                filename = os.path.join(args.output_dir, f"{phase_name}_video_{i}_{prompt}.mp4")
                # å°†è§†é¢‘å¯¼å‡ºä¸ºæ–‡ä»¶
                export_to_video(video, filename, fps=8)
                # å°†æ–‡ä»¶åæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                video_filenames.append(filename)

            # è®°å½•è§†é¢‘åˆ°è·Ÿè¸ªå™¨
            tracker.log(
                {
                    phase_name: [
                        wandb.Video(filename, caption=f"{i}: {pipeline_args['prompt']}")
                        for i, filename in enumerate(video_filenames)
                    ]
                }
            )

    # åˆ é™¤ç®¡é“å¯¹è±¡ä»¥é‡Šæ”¾å†…å­˜
    del pipe
    # é‡Šæ”¾å†…å­˜èµ„æº
    free_memory()

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘åˆ—è¡¨
    return videos


# å®šä¹‰è·å– T5 æç¤ºåµŒå…¥çš„å‡½æ•°
def _get_t5_prompt_embeds(
    tokenizer: T5Tokenizer,
    text_encoder: T5EncoderModel,
    prompt: Union[str, List[str]],
    num_videos_per_prompt: int = 1,
    max_sequence_length: int = 226,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = None,
    text_input_ids=None,
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è·å–æ‰¹å¤„ç†å¤§å°ï¼Œå³æç¤ºçš„æ•°é‡
    batch_size = len(prompt)
    # æ£€æŸ¥ tokenizer æ˜¯å¦è¢«æŒ‡å®š
        if tokenizer is not None:
            # ä½¿ç”¨ tokenizer å¯¹æç¤ºæ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå¼ é‡å½¢å¼çš„è¾“å…¥
            text_inputs = tokenizer(
                prompt,
                padding="max_length",  # å¡«å……è‡³æœ€å¤§é•¿åº¦
                max_length=max_sequence_length,  # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
                truncation=True,  # å…è®¸æˆªæ–­è¶…å‡ºæœ€å¤§é•¿åº¦çš„è¾“å…¥
                add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šæ ‡è®°
                return_tensors="pt",  # è¿”å› PyTorch å¼ é‡
            )
            # æå–ç¼–ç åçš„è¾“å…¥ ID
            text_input_ids = text_inputs.input_ids
        else:
            # å¦‚æœæœªæä¾› tokenizerï¼Œæ£€æŸ¥è¾“å…¥ ID æ˜¯å¦ä¸º None
            if text_input_ids is None:
                # å¼•å‘é”™è¯¯ï¼Œæç¤ºå¿…é¡»æä¾› text_input_ids
                raise ValueError("`text_input_ids` must be provided when the tokenizer is not specified.")
    
        # ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæç¤ºçš„åµŒå…¥
        prompt_embeds = text_encoder(text_input_ids.to(device))[0]
        # å°†åµŒå…¥è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
    
        # ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆé‡å¤çš„æ–‡æœ¬åµŒå…¥ï¼Œä½¿ç”¨å…¼å®¹ MPS çš„æ–¹æ³•
        _, seq_len, _ = prompt_embeds.shape  # è·å–åµŒå…¥çš„å½¢çŠ¶
        # é‡å¤åµŒå…¥ä»¥åŒ¹é…ç”Ÿæˆæ•°é‡
        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)
        # è°ƒæ•´åµŒå…¥çš„å½¢çŠ¶ä»¥é€‚åº”æ‰¹å¤„ç†
        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)
    
        # è¿”å›å¤„ç†åçš„æ–‡æœ¬åµŒå…¥
        return prompt_embeds
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºç¼–ç æç¤ºæ–‡æœ¬ï¼Œå‚æ•°åŒ…æ‹¬åˆ†è¯å™¨ã€æ–‡æœ¬ç¼–ç å™¨ã€æç¤ºå†…å®¹ç­‰
def encode_prompt(
    tokenizer: T5Tokenizer,  # ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼çš„åˆ†è¯å™¨
    text_encoder: T5EncoderModel,  # æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹
    prompt: Union[str, List[str]],  # æç¤ºæ–‡æœ¬ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
    num_videos_per_prompt: int = 1,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡ï¼Œé»˜è®¤ä¸º1
    max_sequence_length: int = 226,  # è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ä¸º226
    device: Optional[torch.device] = None,  # æŒ‡å®šè¿è¡Œè®¾å¤‡ï¼ˆå¦‚GPUï¼‰ï¼Œé»˜è®¤ä¸ºNone
    dtype: Optional[torch.dtype] = None,  # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚float32ï¼‰ï¼Œé»˜è®¤ä¸ºNone
    text_input_ids=None,  # é¢„å…ˆæä¾›çš„æ–‡æœ¬è¾“å…¥IDï¼Œé»˜è®¤ä¸ºNone
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºå•å…ƒç´ åˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è·å–æç¤ºçš„åµŒå…¥è¡¨ç¤ºï¼Œè°ƒç”¨è‡ªå®šä¹‰å‡½æ•°
    prompt_embeds = _get_t5_prompt_embeds(
        tokenizer,  # åˆ†è¯å™¨
        text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
        prompt=prompt,  # æç¤ºæ–‡æœ¬
        num_videos_per_prompt=num_videos_per_prompt,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
        max_sequence_length=max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
        device=device,  # è¿è¡Œè®¾å¤‡
        dtype=dtype,  # æ•°æ®ç±»å‹
        text_input_ids=text_input_ids,  # æ–‡æœ¬è¾“å…¥ID
    )
    # è¿”å›æç¤ºåµŒå…¥è¡¨ç¤º
    return prompt_embeds


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºè®¡ç®—æç¤ºçš„åµŒå…¥è¡¨ç¤ºï¼Œæ¥å—å¤šä¸ªå‚æ•°
def compute_prompt_embeddings(
    tokenizer,  # åˆ†è¯å™¨
    text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
    prompt,  # æç¤ºæ–‡æœ¬
    max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
    device,  # è¿è¡Œè®¾å¤‡
    dtype,  # æ•°æ®ç±»å‹
    requires_grad: bool = False  # æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œé»˜è®¤ä¸ºFalse
):
    # å¦‚æœéœ€è¦è®¡ç®—æ¢¯åº¦
    if requires_grad:
        # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
        prompt_embeds = encode_prompt(
            tokenizer,  # åˆ†è¯å™¨
            text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
            prompt,  # æç¤ºæ–‡æœ¬
            num_videos_per_prompt=1,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
            max_sequence_length=max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
            device=device,  # è¿è¡Œè®¾å¤‡
            dtype=dtype,  # æ•°æ®ç±»å‹
        )
    else:
        # å¦‚æœä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¦æ­¢æ¢¯åº¦è®¡ç®—
        with torch.no_grad():
            # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
            prompt_embeds = encode_prompt(
                tokenizer,  # åˆ†è¯å™¨
                text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
                prompt,  # æç¤ºæ–‡æœ¬
                num_videos_per_prompt=1,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
                max_sequence_length=max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
                device=device,  # è¿è¡Œè®¾å¤‡
                dtype=dtype,  # æ•°æ®ç±»å‹
            )
    # è¿”å›è®¡ç®—å¾—åˆ°çš„æç¤ºåµŒå…¥
    return prompt_embeds


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºå‡†å¤‡æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œæ¥å—å¤šä¸ªå‚æ•°
def prepare_rotary_positional_embeddings(
    height: int,  # è¾“å…¥å›¾åƒçš„é«˜åº¦
    width: int,  # è¾“å…¥å›¾åƒçš„å®½åº¦
    num_frames: int,  # å¸§æ•°
    vae_scale_factor_spatial: int = 8,  # VAEç©ºé—´ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º8
    patch_size: int = 2,  # æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼Œé»˜è®¤ä¸º2
    attention_head_dim: int = 64,  # æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º64
    device: Optional[torch.device] = None,  # æŒ‡å®šè¿è¡Œè®¾å¤‡ï¼ˆå¦‚GPUï¼‰ï¼Œé»˜è®¤ä¸ºNone
    base_height: int = 480,  # åŸºç¡€é«˜åº¦ï¼Œé»˜è®¤ä¸º480
    base_width: int = 720,  # åŸºç¡€å®½åº¦ï¼Œé»˜è®¤ä¸º720
) -> Tuple[torch.Tensor, torch.Tensor]:  # è¿”å›ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„
    # è®¡ç®—ç½‘æ ¼é«˜åº¦
    grid_height = height // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—ç½‘æ ¼å®½åº¦
    grid_width = width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€å®½åº¦
    base_size_width = base_width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€é«˜åº¦
    base_size_height = base_height // (vae_scale_factor_spatial * patch_size)

    # è·å–ç½‘æ ¼çš„è£å‰ªåŒºåŸŸåæ ‡
    grid_crops_coords = get_resize_crop_region_for_grid((grid_height, grid_width), base_size_width, base_size_height)
    # è·å–æ—‹è½¬ä½ç½®åµŒå…¥çš„æ­£å¼¦å’Œä½™å¼¦é¢‘ç‡
    freqs_cos, freqs_sin = get_3d_rotary_pos_embed(
        embed_dim=attention_head_dim,  # åµŒå…¥ç»´åº¦
        crops_coords=grid_crops_coords,  # ç½‘æ ¼è£å‰ªåæ ‡
        grid_size=(grid_height, grid_width),  # ç½‘æ ¼å¤§å°
        temporal_size=num_frames,  # æ—¶é—´ç»´åº¦å¤§å°
    )

    # å°†ä½™å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_cos = freqs_cos.to(device=device)
    # å°†æ­£å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_sin = freqs_sin.to(device=device)
    # è¿”å›ä½™å¼¦å’Œæ­£å¼¦é¢‘ç‡å¼ é‡
    return freqs_cos, freqs_sin


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºè·å–ä¼˜åŒ–å™¨ï¼Œæ¥å—å¤šä¸ªå‚æ•°
def get_optimizer(args, params_to_optimize, use_deepspeed: bool = False):
    # å¦‚æœä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    if use_deepspeed:
        # ä» accelerate åº“å¯¼å…¥ DummyOptim
        from accelerate.utils import DummyOptim

        # è¿”å› DeepSpeed ä¼˜åŒ–å™¨çš„å®ä¾‹
        return DummyOptim(
            params_to_optimize,  # å¾…ä¼˜åŒ–çš„å‚æ•°
            lr=args.learning_rate,  # å­¦ä¹ ç‡
            betas=(args.adam_beta1, args.adam_beta2),  # Adamä¼˜åŒ–å™¨çš„åŠ¨é‡å‚æ•°
            eps=args.adam_epsilon,  # Adamä¼˜åŒ–å™¨çš„ epsilon
            weight_decay=args.adam_weight_decay,  # æƒé‡è¡°å‡
        )

    # ä¼˜åŒ–å™¨åˆ›å»º
    # å®šä¹‰æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹åˆ—è¡¨
    supported_optimizers = ["adam", "adamw", "prodigy"]
    # æ£€æŸ¥ç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯å¦åœ¨æ”¯æŒçš„åˆ—è¡¨ä¸­
    if args.optimizer not in supported_optimizers:
        # è®°å½•ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨è­¦å‘Šä¿¡æ¯
        logger.warning(
            f"Unsupported choice of optimizer: {args.optimizer}. Supported optimizers include {supported_optimizers}. Defaulting to AdamW"
        )
        # å°†ä¼˜åŒ–å™¨é»˜è®¤è®¾ç½®ä¸º "adamw"
        args.optimizer = "adamw"

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ 8 ä½ Adam ä¼˜åŒ–å™¨ï¼Œå¹¶ä¸”å½“å‰ä¼˜åŒ–å™¨ä¸æ˜¯ Adam æˆ– AdamW
    if args.use_8bit_adam and args.optimizer.lower() not in ["adam", "adamw"]:
        # è®°å½•è­¦å‘Šï¼Œè¯´æ˜ä½¿ç”¨ 8 ä½ Adam æ—¶ä¼˜åŒ–å™¨å¿…é¡»ä¸º Adam æˆ– AdamW
        logger.warning(
            f"use_8bit_adam is ignored when optimizer is not set to 'Adam' or 'AdamW'. Optimizer was "
            f"set to {args.optimizer.lower()}"
        )

    # å¦‚æœç”¨æˆ·é€‰æ‹©ä½¿ç”¨ 8 ä½ Adam ä¼˜åŒ–å™¨
    if args.use_8bit_adam:
        try:
            # å°è¯•å¯¼å…¥ bitsandbytes åº“
            import bitsandbytes as bnb
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯æç¤ºç”¨æˆ·å®‰è£… bitsandbytes åº“
            raise ImportError(
                "To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`."
            )

    # å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯ AdamW
    if args.optimizer.lower() == "adamw":
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨ 8 ä½ Adam é€‰æ‹©ç›¸åº”çš„ä¼˜åŒ–å™¨ç±»
        optimizer_class = bnb.optim.AdamW8bit if args.use_8bit_adam else torch.optim.AdamW

        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¼ å…¥ä¼˜åŒ–å‚æ•°å’Œç›¸å…³è¶…å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯ Adam
    elif args.optimizer.lower() == "adam":
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨ 8 ä½ Adam é€‰æ‹©ç›¸åº”çš„ä¼˜åŒ–å™¨ç±»
        optimizer_class = bnb.optim.Adam8bit if args.use_8bit_adam else torch.optim.Adam

        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¼ å…¥ä¼˜åŒ–å‚æ•°å’Œç›¸å…³è¶…å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯ Prodigy
    elif args.optimizer.lower() == "prodigy":
        try:
            # å°è¯•å¯¼å…¥ prodigyopt åº“
            import prodigyopt
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯æç¤ºç”¨æˆ·å®‰è£… prodigyopt åº“
            raise ImportError("To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`")

        # è®¾ç½® Prodigy ä¼˜åŒ–å™¨ç±»
        optimizer_class = prodigyopt.Prodigy

        # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡ä½ï¼Œå¹¶è®°å½•è­¦å‘Š
        if args.learning_rate <= 0.1:
            logger.warning(
                "Learning rate is too low. When using prodigy, it's generally better to set learning rate around 1.0"
            )

        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¼ å…¥ä¼˜åŒ–å‚æ•°å’Œç›¸å…³è¶…å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            lr=args.learning_rate,
            betas=(args.adam_beta1, args.adam_beta2),
            beta3=args.prodigy_beta3,
            weight_decay=args.adam_weight_decay,
            eps=args.adam_epsilon,
            decouple=args.prodigy_decouple,
            use_bias_correction=args.prodigy_use_bias_correction,
            safeguard_warmup=args.prodigy_safeguard_warmup,
        )

    # è¿”å›åˆ›å»ºçš„ä¼˜åŒ–å™¨å®ä¾‹
    return optimizer
# ä¸»å‡½æ•°ï¼Œæ¥æ”¶å‘½ä»¤è¡Œå‚æ•°
def main(args):
    # æ£€æŸ¥æ˜¯å¦åŒæ—¶ä½¿ç”¨ wandb å’Œ hub_tokenï¼Œè‹¥æ˜¯åˆ™æŠ›å‡ºé”™è¯¯
    if args.report_to == "wandb" and args.hub_token is not None:
        raise ValueError(
            "You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token."
            " Please use `huggingface-cli login` to authenticate with the Hub."
        )

    # æ£€æŸ¥ MPS æ˜¯å¦å¯ç”¨ä¸”æ··åˆç²¾åº¦ä¸º bf16ï¼Œè‹¥æ˜¯åˆ™æŠ›å‡ºé”™è¯¯
    if torch.backends.mps.is_available() and args.mixed_precision == "bf16":
        # due to pytorch#99272, MPS does not yet support bfloat16.
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )

    # ç”Ÿæˆæ—¥å¿—ç›®å½•çš„è·¯å¾„
    logging_dir = Path(args.output_dir, args.logging_dir)

    # åˆå§‹åŒ–é¡¹ç›®é…ç½®
    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
    # è®¾ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œçš„å‚æ•°
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    # åˆå§‹åŒ–è¿›ç¨‹ç»„çš„å‚æ•°
    init_kwargs = InitProcessGroupKwargs(backend="nccl", timeout=timedelta(seconds=args.nccl_timeout))
    # åˆ›å»ºåŠ é€Ÿå™¨å®ä¾‹
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
        kwargs_handlers=[ddp_kwargs, init_kwargs],
    )

    # å¦‚æœ MPS å¯ç”¨ï¼Œç¦ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
    if torch.backends.mps.is_available():
        accelerator.native_amp = False

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ wandb è¿›è¡ŒæŠ¥å‘Š
    if args.report_to == "wandb":
        # å¦‚æœ wandb ä¸å¯ç”¨ï¼Œåˆ™æŠ›å‡ºå¯¼å…¥é”™è¯¯
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # é…ç½®æ—¥å¿—è®°å½•ä»¥ä¾¿äºè°ƒè¯•
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    # è®°å½•åŠ é€Ÿå™¨çš„çŠ¶æ€ä¿¡æ¯
    logger.info(accelerator.state, main_process_only=False)
    # å¦‚æœæ˜¯æœ¬åœ°ä¸»è¿›ç¨‹ï¼Œè®¾ç½®ä¸åŒçš„æ—¥å¿—è¯¦ç»†çº§åˆ«
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # å¦‚æœæä¾›äº†ç§å­ï¼Œåˆ™è®¾ç½®è®­ç»ƒç§å­
    if args.seed is not None:
        set_seed(args.seed)

    # å¤„ç†ä»“åº“åˆ›å»º
    if accelerator.is_main_process:
        # å¦‚æœè¾“å‡ºç›®å½•ä¸ä¸ºç©ºï¼Œåˆ›å»ºè¯¥ç›®å½•
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        # å¦‚æœéœ€è¦æ¨é€åˆ° Hubï¼Œåˆ›å»ºä»“åº“
        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
            ).repo_id

    # å‡†å¤‡æ¨¡å‹å’Œè°ƒåº¦å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision
    )

    text_encoder = T5EncoderModel.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision
    )

    # CogVideoX-2b æƒé‡ä»¥ float16 å­˜å‚¨
    # CogVideoX-5b å’Œ CogVideoX-5b-I2V çš„æƒé‡ä»¥ bfloat16 å­˜å‚¨
    load_dtype = torch.bfloat16 if "5b" in args.pretrained_model_name_or_path.lower() else torch.float16
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½ 3D Transformer æ¨¡å‹
    transformer = CogVideoXTransformer3DModel.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="transformer",  # æŒ‡å®šå­æ–‡ä»¶å¤¹ä¸º transformer
        torch_dtype=load_dtype,  # è®¾ç½®åŠ è½½çš„æƒé‡æ•°æ®ç±»å‹
        revision=args.revision,  # ä½¿ç”¨æŒ‡å®šçš„ä¿®è®¢ç‰ˆæœ¬
        variant=args.variant,  # ä½¿ç”¨æŒ‡å®šçš„å˜ä½“
    )
    
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½ VAE æ¨¡å‹
    vae = AutoencoderKLCogVideoX.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision, variant=args.variant
    )
    
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½è°ƒåº¦å™¨
    scheduler = CogVideoXDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
    
    # å¦‚æœå¯ç”¨äº†åˆ‡ç‰‡ï¼Œåˆ™å¯ç”¨ VAE çš„åˆ‡ç‰‡åŠŸèƒ½
    if args.enable_slicing:
        vae.enable_slicing()
    # å¦‚æœå¯ç”¨äº†å¹³é“ºï¼Œåˆ™å¯ç”¨ VAE çš„å¹³é“ºåŠŸèƒ½
    if args.enable_tiling:
        vae.enable_tiling()
    
    # ä»…è®­ç»ƒé™„åŠ çš„é€‚é…å™¨ LoRA å±‚
    text_encoder.requires_grad_(False)  # ç¦ç”¨æ–‡æœ¬ç¼–ç å™¨çš„æ¢¯åº¦è®¡ç®—
    transformer.requires_grad_(False)  # ç¦ç”¨ Transformer çš„æ¢¯åº¦è®¡ç®—
    vae.requires_grad_(False)  # ç¦ç”¨ VAE çš„æ¢¯åº¦è®¡ç®—
    
    # å¯¹äºæ··åˆç²¾åº¦è®­ç»ƒï¼Œå°†æ‰€æœ‰ä¸å¯è®­ç»ƒæƒé‡ï¼ˆvaeã€text_encoder å’Œ transformerï¼‰è½¬æ¢ä¸ºåŠç²¾åº¦
    weight_dtype = torch.float32  # é»˜è®¤æƒé‡æ•°æ®ç±»å‹ä¸º float32
    if accelerator.state.deepspeed_plugin:
        # DeepSpeed å¤„ç†ç²¾åº¦ï¼Œä½¿ç”¨ DeepSpeed é…ç½®ä¸­çš„è®¾ç½®
        if (
            "fp16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["fp16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¯ç”¨ fp16 æ—¶è®¾ç½®æƒé‡æ•°æ®ç±»å‹ä¸º float16
        if (
            "bf16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["bf16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¯ç”¨ bf16 æ—¶ä¹Ÿè®¾ç½®ä¸º float16
    else:
        if accelerator.mixed_precision == "fp16":
            weight_dtype = torch.float16  # å¦‚æœä½¿ç”¨ fp16ï¼Œè®¾ç½®æƒé‡æ•°æ®ç±»å‹ä¸º float16
        elif accelerator.mixed_precision == "bf16":
            weight_dtype = torch.bfloat16  # å¦‚æœä½¿ç”¨ bf16ï¼Œè®¾ç½®ä¸º bfloat16
    
    # æ£€æŸ¥ MPS æ˜¯å¦å¯ç”¨ï¼Œä¸”æƒé‡æ•°æ®ç±»å‹ä¸º bfloat16
    if torch.backends.mps.is_available() and weight_dtype == torch.bfloat16:
        # ç”±äº pytorch#99272ï¼ŒMPS å°šä¸æ”¯æŒ bfloat16ã€‚
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )
    
    # å°†æ–‡æœ¬ç¼–ç å™¨ã€Transformer å’Œ VAE è½¬ç§»åˆ°åŠ é€Ÿå™¨è®¾å¤‡ï¼ŒæŒ‡å®šæ•°æ®ç±»å‹
    text_encoder.to(accelerator.device, dtype=weight_dtype)
    transformer.to(accelerator.device, dtype=weight_dtype)
    vae.to(accelerator.device, dtype=weight_dtype)
    
    # å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™å¯ç”¨ Transformer çš„æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()
    
    # ç°åœ¨æˆ‘ä»¬å°†æ–° LoRA æƒé‡æ·»åŠ åˆ°æ³¨æ„åŠ›å±‚
    transformer_lora_config = LoraConfig(
        r=args.rank,  # è®¾ç½® LoRA çš„ç§©
        lora_alpha=args.lora_alpha,  # è®¾ç½® LoRA çš„ alpha å€¼
        init_lora_weights=True,  # åˆå§‹åŒ– LoRA æƒé‡
        target_modules=["to_k", "to_q", "to_v", "to_out.0"],  # ç›®æ ‡æ¨¡å—åˆ—è¡¨
    )
    # å°† LoRA é€‚é…å™¨æ·»åŠ åˆ° Transformer
    transformer.add_adapter(transformer_lora_config)
    # è§£åŒ…æ¨¡å‹ï¼Œä»¥ä¾¿äºå¤„ç†
        def unwrap_model(model):
            # ä½¿ç”¨åŠ é€Ÿå™¨è§£åŒ…æ¨¡å‹
            model = accelerator.unwrap_model(model)
            # å¦‚æœæ˜¯ç¼–è¯‘çš„æ¨¡å—ï¼Œè·å–åŸå§‹æ¨¡å‹ï¼Œå¦åˆ™è¿”å›å½“å‰æ¨¡å‹
            model = model._orig_mod if is_compiled_module(model) else model
            # è¿”å›å¤„ç†åçš„æ¨¡å‹
            return model
    
        # åˆ›å»ºè‡ªå®šä¹‰ä¿å­˜å’ŒåŠ è½½é’©å­ï¼Œä»¥ä¾¿åŠ é€Ÿå™¨ä»¥è‰¯å¥½æ ¼å¼åºåˆ—åŒ–çŠ¶æ€
        def save_model_hook(models, weights, output_dir):
            # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
            if accelerator.is_main_process:
                # åˆå§‹åŒ–å¾…ä¿å­˜çš„å±‚ä¸º None
                transformer_lora_layers_to_save = None
    
                # éå†æ¨¡å‹åˆ—è¡¨
                for model in models:
                    # æ£€æŸ¥æ¨¡å‹ç±»å‹æ˜¯å¦ä¸è§£åŒ…åçš„ transformer ç›¸åŒ
                    if isinstance(model, type(unwrap_model(transformer))):
                        # è·å–æ¨¡å‹çš„çŠ¶æ€å­—å…¸
                        transformer_lora_layers_to_save = get_peft_model_state_dict(model)
                    else:
                        # æŠ›å‡ºå¼‚å¸¸ä»¥å¤„ç†æ„å¤–æ¨¡å‹ç±»å‹
                        raise ValueError(f"unexpected save model: {model.__class__}")
    
                    # ç¡®ä¿ä»æƒé‡ä¸­ç§»é™¤å·²å¤„ç†çš„æ¨¡å‹
                    weights.pop()
    
                # ä¿å­˜ LoRA æƒé‡
                CogVideoXImageToVideoPipeline.save_lora_weights(
                    output_dir,
                    transformer_lora_layers=transformer_lora_layers_to_save,
                )
    
        # åˆ›å»ºåŠ è½½æ¨¡å‹çš„é’©å­
        def load_model_hook(models, input_dir):
            # åˆå§‹åŒ– transformer ä¸º None
            transformer_ = None
    
            # å½“æ¨¡å‹åˆ—è¡¨ä¸ä¸ºç©ºæ—¶
            while len(models) > 0:
                # ä»æ¨¡å‹åˆ—è¡¨ä¸­å¼¹å‡ºæ¨¡å‹
                model = models.pop()
    
                # æ£€æŸ¥æ¨¡å‹ç±»å‹
                if isinstance(model, type(unwrap_model(transformer))):
                    # å°† transformer è®¾ç½®ä¸ºå½“å‰æ¨¡å‹
                    transformer_ = model
                else:
                    # æŠ›å‡ºå¼‚å¸¸ä»¥å¤„ç†æ„å¤–æ¨¡å‹ç±»å‹
                    raise ValueError(f"Unexpected save model: {model.__class__}")
    
            # ä»è¾“å…¥ç›®å½•è·å– LoRA çŠ¶æ€å­—å…¸
            lora_state_dict = CogVideoXImageToVideoPipeline.lora_state_dict(input_dir)
    
            # åˆ›å»ºè½¬æ¢åçš„ transformer çŠ¶æ€å­—å…¸
            transformer_state_dict = {
                f'{k.replace("transformer.", "")}': v for k, v in lora_state_dict.items() if k.startswith("transformer.")
            }
            # å°†çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºé€‚åˆ PEFT çš„æ ¼å¼
            transformer_state_dict = convert_unet_state_dict_to_peft(transformer_state_dict)
            # è®¾ç½® PEFT æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå¹¶è·å–ä¸å…¼å®¹çš„é”®
            incompatible_keys = set_peft_model_state_dict(transformer_, transformer_state_dict, adapter_name="default")
            # å¦‚æœå­˜åœ¨ä¸å…¼å®¹çš„é”®ï¼Œæ£€æŸ¥æ„å¤–çš„é”®
            if incompatible_keys is not None:
                # è·å–æ„å¤–çš„é”®
                unexpected_keys = getattr(incompatible_keys, "unexpected_keys", None)
                if unexpected_keys:
                    # è®°å½•è­¦å‘Šä¿¡æ¯
                    logger.warning(
                        f"Loading adapter weights from state_dict led to unexpected keys not found in the model: "
                        f" {unexpected_keys}. "
                    )
    
            # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
            if args.mixed_precision == "fp16":
                # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰è½¬ä¸º fp32 ç±»å‹
                cast_training_params([transformer_])
    
        # æ³¨å†Œä¿å­˜çŠ¶æ€å‰é’©å­
        accelerator.register_save_state_pre_hook(save_model_hook)
        # æ³¨å†ŒåŠ è½½çŠ¶æ€å‰é’©å­
        accelerator.register_load_state_pre_hook(load_model_hook)
    
        # å¯ç”¨ TF32 ä»¥åŠ é€Ÿ Ampere GPU çš„è®­ç»ƒ
        if args.allow_tf32 and torch.cuda.is_available():
            # å…è®¸ä½¿ç”¨ TF32
            torch.backends.cuda.matmul.allow_tf32 = True
    # å¦‚æœæŒ‡å®šäº†ç¼©æ”¾å­¦ä¹ ç‡çš„æ ‡å¿—
    if args.scale_lr:
        # æ ¹æ®æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ã€è®­ç»ƒæ‰¹é‡å¤§å°å’Œè¿›ç¨‹æ•°ç¼©æ”¾å­¦ä¹ ç‡
        args.learning_rate = (
            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes
        )

    # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
    if args.mixed_precision == "fp16":
        # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰æå‡ä¸º fp32 ç±»å‹
        cast_training_params([transformer], dtype=torch.float32)

    # è·å–æ‰€æœ‰å¯è®­ç»ƒçš„ LoRA å‚æ•°
    transformer_lora_parameters = list(filter(lambda p: p.requires_grad, transformer.parameters()))

    # ä¼˜åŒ–å‚æ•°
    transformer_parameters_with_lr = {"params": transformer_lora_parameters, "lr": args.learning_rate}
    # å°†å‚æ•°æ”¾å…¥å¾…ä¼˜åŒ–çš„åˆ—è¡¨ä¸­
    params_to_optimize = [transformer_parameters_with_lr]

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    use_deepspeed_optimizer = (
        accelerator.state.deepspeed_plugin is not None
        and "optimizer" in accelerator.state.deepspeed_plugin.deepspeed_config
    )
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨
    use_deepspeed_scheduler = (
        accelerator.state.deepspeed_plugin is not None
        and "scheduler" not in accelerator.state.deepspeed_plugin.deepspeed_config
    )

    # è·å–ä¼˜åŒ–å™¨
    optimizer = get_optimizer(args, params_to_optimize, use_deepspeed=use_deepspeed_optimizer)

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = VideoDataset(
        # å®ä¾‹æ•°æ®çš„æ ¹ç›®å½•
        instance_data_root=args.instance_data_root,
        # æ•°æ®é›†åç§°
        dataset_name=args.dataset_name,
        # æ•°æ®é›†é…ç½®åç§°
        dataset_config_name=args.dataset_config_name,
        # æè¿°æ€§æ–‡æœ¬åˆ—åç§°
        caption_column=args.caption_column,
        # è§†é¢‘åˆ—åç§°
        video_column=args.video_column,
        # è§†é¢‘é«˜åº¦
        height=args.height,
        # è§†é¢‘å®½åº¦
        width=args.width,
        # è§†é¢‘é‡å¡‘æ¨¡å¼
        video_reshape_mode=args.video_reshape_mode,
        # å¸§ç‡
        fps=args.fps,
        # æœ€å¤§å¸§æ•°
        max_num_frames=args.max_num_frames,
        # å¼€å§‹è·³è¿‡çš„å¸§æ•°
        skip_frames_start=args.skip_frames_start,
        # ç»“æŸè·³è¿‡çš„å¸§æ•°
        skip_frames_end=args.skip_frames_end,
        # ç¼“å­˜ç›®å½•
        cache_dir=args.cache_dir,
        # èº«ä»½ä»¤ç‰Œ
        id_token=args.id_token,
    )

    # å®šä¹‰ç¼–ç è§†é¢‘çš„å‡½æ•°
    def encode_video(video, bar):
        # æ›´æ–°è¿›åº¦æ¡
        bar.update(1)
        # å°†è§†é¢‘è½¬æ¢ä¸ºæŒ‡å®šè®¾å¤‡å¹¶å¢åŠ ä¸€ä¸ªç»´åº¦
        video = video.to(accelerator.device, dtype=vae.dtype).unsqueeze(0)
        # è°ƒæ•´è§†é¢‘ç»´åº¦é¡ºåºä¸º [B, C, F, H, W]
        video = video.permute(0, 2, 1, 3, 4)  # [B, C, F, H, W]
        # å…‹éš†ç¬¬ä¸€ä¸ªå¸§ä½œä¸ºå›¾åƒ
        image = video[:, :, :1].clone()

        # ç¼–ç è§†é¢‘ä»¥è·å–æ½œåœ¨åˆ†å¸ƒ
        latent_dist = vae.encode(video).latent_dist

        # ç”Ÿæˆå›¾åƒå™ªå£°æ ‡å‡†å·®
        image_noise_sigma = torch.normal(mean=-3.0, std=0.5, size=(1,), device=image.device)
        # å–æŒ‡æ•°å¹¶è½¬æ¢ä¸ºå›¾åƒæ•°æ®ç±»å‹
        image_noise_sigma = torch.exp(image_noise_sigma).to(dtype=image.dtype)
        # ç”Ÿæˆä¸å›¾åƒå¤§å°ç›¸åŒçš„å™ªå£°å›¾åƒ
        noisy_image = torch.randn_like(image) * image_noise_sigma[:, None, None, None, None]
        # å¯¹å™ªå£°å›¾åƒè¿›è¡Œç¼–ç ä»¥è·å–æ½œåœ¨åˆ†å¸ƒ
        image_latent_dist = vae.encode(noisy_image).latent_dist

        # è¿”å›æ½œåœ¨åˆ†å¸ƒ
        return latent_dist, image_latent_dist

    # è®¡ç®—å®ä¾‹æç¤ºçš„åµŒå…¥
    train_dataset.instance_prompts = [
        compute_prompt_embeddings(
            tokenizer,
            text_encoder,
            [prompt],
            transformer.config.max_text_seq_length,
            accelerator.device,
            weight_dtype,
            requires_grad=False,
        )
        for prompt in train_dataset.instance_prompts
    ]

    # åˆ›å»ºè¿›åº¦æ¡ä»¥æ˜¾ç¤ºç¼–ç è§†é¢‘çš„åŠ è½½è¿›åº¦
    progress_encode_bar = tqdm(
        range(0, len(train_dataset.instance_videos)),
        desc="Loading Encode videos",
    )
    # å¯¹è®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸ªå®ä¾‹è§†é¢‘è¿›è¡Œç¼–ç ï¼Œå¹¶æ›´æ–°æ•°æ®é›†çš„å®ä¾‹è§†é¢‘åˆ—è¡¨
    train_dataset.instance_videos = [encode_video(video, progress_encode_bar) for video in train_dataset.instance_videos]
    # å…³é—­è¿›åº¦ç¼–ç æ¡
    progress_encode_bar.close()

    # å®šä¹‰ç”¨äºåˆå¹¶æ ·æœ¬çš„å‡½æ•°
    def collate_fn(examples):
        # åˆå§‹åŒ–è§†é¢‘å’Œå›¾åƒçš„åˆ—è¡¨
        videos = []
        images = []
        # éå†æ‰€æœ‰ç¤ºä¾‹
        for example in examples:
            # è·å–å®ä¾‹è§†é¢‘çš„æ½œåœ¨åˆ†å¸ƒå’Œå›¾åƒæ½œåœ¨åˆ†å¸ƒ
            latent_dist, image_latent_dist = example["instance_video"]

            # ä»æ½œåœ¨åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œå¹¶åº”ç”¨ç¼©æ”¾å› å­
            video_latents = latent_dist.sample() * vae.config.scaling_factor
            image_latents = image_latent_dist.sample() * vae.config.scaling_factor
            # è°ƒæ•´è§†é¢‘æ½œåœ¨è¡¨ç¤ºçš„ç»´åº¦é¡ºåº
            video_latents = video_latents.permute(0, 2, 1, 3, 4)
            # è°ƒæ•´å›¾åƒæ½œåœ¨è¡¨ç¤ºçš„ç»´åº¦é¡ºåº
            image_latents = image_latents.permute(0, 2, 1, 3, 4)

            # è®¡ç®—å¡«å……çš„å½¢çŠ¶ï¼Œä»¥ä¾¿ä¸ºè§†é¢‘æ½œåœ¨è¡¨ç¤ºä¿ç•™æ—¶é—´æ­¥é•¿
            padding_shape = (video_latents.shape[0], video_latents.shape[1] - 1, *video_latents.shape[2:])
            # åˆ›å»ºæ–°çš„é›¶å¡«å……å¼ é‡
            latent_padding = image_latents.new_zeros(padding_shape)
            # å°†å¡«å……å¼ é‡é™„åŠ åˆ°å›¾åƒæ½œåœ¨è¡¨ç¤º
            image_latents = torch.cat([image_latents, latent_padding], dim=1)

            # æ ¹æ®éšæœºå€¼å†³å®šæ˜¯å¦å°†å›¾åƒæ½œåœ¨è¡¨ç¤ºç½®ä¸ºé›¶ï¼ˆæ·»åŠ å™ªå£°ï¼‰
            if random.random() < args.noised_image_dropout:
                image_latents = torch.zeros_like(image_latents)

            # å°†è§†é¢‘å’Œå›¾åƒæ½œåœ¨è¡¨ç¤ºæ·»åŠ åˆ°åˆ—è¡¨ä¸­
            videos.append(video_latents)
            images.append(image_latents)

        # å°†è§†é¢‘å’Œå›¾åƒåˆ—è¡¨åˆå¹¶æˆå•ä¸€å¼ é‡
        videos = torch.cat(videos)
        images = torch.cat(images)
        # å°†å¼ é‡è½¬æ¢ä¸ºè¿ç»­æ ¼å¼å¹¶è½¬ä¸ºæµ®ç‚¹å‹
        videos = videos.to(memory_format=torch.contiguous_format).float()
        images = images.to(memory_format=torch.contiguous_format).float()

        # æå–æ¯ä¸ªç¤ºä¾‹çš„æç¤ºä¿¡æ¯
        prompts = [example["instance_prompt"] for example in examples]
        # å°†æç¤ºä¿¡æ¯åˆå¹¶ä¸ºä¸€ä¸ªå¼ é‡
        prompts = torch.cat(prompts)

        # è¿”å›åŒ…å«è§†é¢‘ã€å›¾åƒå’Œæç¤ºçš„å­—å…¸
        return {
            "videos": (videos, images),
            "prompts": prompts,
        }

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ä»¥ä¾¿äºæ‰¹é‡åŠ è½½è®­ç»ƒæ•°æ®
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.train_batch_size,  # è®¾ç½®æ‰¹é‡å¤§å°
        shuffle=True,  # æ‰“ä¹±æ•°æ®
        collate_fn=collate_fn,  # ä½¿ç”¨è‡ªå®šä¹‰çš„åˆå¹¶å‡½æ•°
        num_workers=args.dataloader_num_workers,  # è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
    )

    # è®¡ç®—è®­ç»ƒæ­¥éª¤æ•°çš„è°ƒåº¦å™¨åŠç›¸å…³æ•°å­¦
    overrode_max_train_steps = False  # åˆå§‹åŒ–æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦è¦†ç›–æœ€å¤§è®­ç»ƒæ­¥éª¤
    # è®¡ç®—æ¯ä¸ªè®­ç»ƒå‘¨æœŸçš„æ›´æ–°æ­¥éª¤æ•°
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    # å¦‚æœæ²¡æœ‰è®¾ç½®æœ€å¤§è®­ç»ƒæ­¥éª¤ï¼Œåˆ™æ ¹æ®è®­ç»ƒå‘¨æœŸå’Œæ›´æ–°æ­¥éª¤è®¡ç®—æœ€å¤§è®­ç»ƒæ­¥éª¤
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨é€‰æ‹©ç›¸åº”çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    if use_deepspeed_scheduler:
        from accelerate.utils import DummyScheduler

        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè°ƒåº¦å™¨
        lr_scheduler = DummyScheduler(
            name=args.lr_scheduler,  # å­¦ä¹ ç‡è°ƒåº¦å™¨åç§°
            optimizer=optimizer,  # å…³è”çš„ä¼˜åŒ–å™¨
            total_num_steps=args.max_train_steps * accelerator.num_processes,  # æ€»è®­ç»ƒæ­¥éª¤
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,  # é¢„çƒ­æ­¥éª¤æ•°
        )
    else:
        # åˆ›å»ºæ ‡å‡†å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = get_scheduler(
            args.lr_scheduler,  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
            optimizer=optimizer,  # å…³è”çš„ä¼˜åŒ–å™¨
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,  # é¢„çƒ­æ­¥éª¤æ•°
            num_training_steps=args.max_train_steps * accelerator.num_processes,  # æ€»è®­ç»ƒæ­¥éª¤
            num_cycles=args.lr_num_cycles,  # å¾ªç¯æ¬¡æ•°
            power=args.lr_power,  # å­¦ä¹ ç‡è°ƒæ•´çš„æŒ‡æ•°
        )

    # ä½¿ç”¨åŠ é€Ÿå™¨å‡†å¤‡æ‰€æœ‰ç»„ä»¶
    transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        transformer,  # è½¬æ¢å™¨æ¨¡å‹
        optimizer,  # ä¼˜åŒ–å™¨
        train_dataloader,  # æ•°æ®åŠ è½½å™¨
        lr_scheduler  # å­¦ä¹ ç‡è°ƒåº¦å™¨
    )
    # ç”±äºè®­ç»ƒæ•°æ®åŠ è½½å™¨çš„å¤§å°å¯èƒ½å·²ç»æ”¹å˜ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®—æ€»çš„è®­ç»ƒæ­¥éª¤
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)  
    # å¦‚æœè¦†ç›–äº†æœ€å¤§è®­ç»ƒæ­¥éª¤ï¼Œåˆ™æ›´æ–°æœ€å¤§è®­ç»ƒæ­¥éª¤ä¸ºè®­ç»ƒè½®æ•°ä¹˜ä»¥æ¯è½®çš„æ›´æ–°æ­¥éª¤
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch  
    # ä¹‹åæˆ‘ä»¬é‡æ–°è®¡ç®—è®­ç»ƒè½®æ•°
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)  

    # æˆ‘ä»¬éœ€è¦åˆå§‹åŒ–è¿½è¸ªå™¨ï¼Œå¹¶å­˜å‚¨æˆ‘ä»¬çš„é…ç½®
    # è¿½è¸ªå™¨ä¼šåœ¨ä¸»è¿›ç¨‹ä¸­è‡ªåŠ¨åˆå§‹åŒ–
    if accelerator.is_main_process:
        # è·å–è¿½è¸ªå™¨åç§°ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤åç§°
        tracker_name = args.tracker_name or "cogvideox-i2v-lora"  
        # åˆå§‹åŒ–è¿½è¸ªå™¨ï¼Œå¹¶ä¼ å…¥é…ç½®å‚æ•°
        accelerator.init_trackers(tracker_name, config=vars(args))  

    # å¼€å§‹è®­ç»ƒï¼
    # è®¡ç®—æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ€»æ‰¹é‡å¤§å°
    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps  
    # è®¡ç®—å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°
    num_trainable_parameters = sum(param.numel() for model in params_to_optimize for param in model["params"])  

    # è®°å½•è®­ç»ƒä¿¡æ¯
    logger.info("***** Running training *****")  
    logger.info(f"  Num trainable parameters = {num_trainable_parameters}")  # è®°å½•å¯è®­ç»ƒå‚æ•°æ•°é‡
    logger.info(f"  Num examples = {len(train_dataset)}")  # è®°å½•è®­ç»ƒæ ·æœ¬æ•°é‡
    logger.info(f"  Num batches each epoch = {len(train_dataloader)}")  # è®°å½•æ¯è½®çš„æ‰¹æ¬¡æ•°
    logger.info(f"  Num epochs = {args.num_train_epochs}")  # è®°å½•æ€»è®­ç»ƒè½®æ•°
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")  # è®°å½•æ¯ä¸ªè®¾å¤‡çš„å³æ—¶æ‰¹é‡å¤§å°
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")  # è®°å½•æ€»æ‰¹é‡å¤§å°
    logger.info(f"  Gradient accumulation steps = {args.gradient_accumulation_steps}")  # è®°å½•æ¢¯åº¦ç´¯ç§¯æ­¥éª¤æ•°
    logger.info(f"  Total optimization steps = {args.max_train_steps}")  # è®°å½•æ€»ä¼˜åŒ–æ­¥éª¤æ•°
    global_step = 0  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤
    first_epoch = 0  # åˆå§‹åŒ–ç¬¬ä¸€è½®

    # å¯èƒ½åŠ è½½æ¥è‡ªä¹‹å‰ä¿å­˜çš„æƒé‡å’ŒçŠ¶æ€
    if not args.resume_from_checkpoint:
        initial_global_step = 0  # å¦‚æœä¸ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œåˆå§‹å…¨å±€æ­¥éª¤è®¾ä¸º0
    else:
        # å¦‚æœæŒ‡å®šçš„æ£€æŸ¥ç‚¹ä¸æ˜¯"latest"ï¼Œåˆ™æå–è·¯å¾„
        if args.resume_from_checkpoint != "latest":
            path = os.path.basename(args.resume_from_checkpoint)  
        else:
            # è·å–æœ€è¿‘çš„æ£€æŸ¥ç‚¹
            dirs = os.listdir(args.output_dir)  # åˆ—å‡ºè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
            dirs = [d for d in dirs if d.startswith("checkpoint")]  # è¿‡æ»¤å‡ºæ£€æŸ¥ç‚¹æ–‡ä»¶
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))  # æŒ‰æ£€æŸ¥ç‚¹ç¼–å·æ’åº
            path = dirs[-1] if len(dirs) > 0 else None  # è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è®¾ä¸ºNone

        # æ£€æŸ¥ç‚¹è·¯å¾„ä¸ºç©ºï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯å¹¶å¼€å§‹æ–°çš„è®­ç»ƒ
        if path is None:
            accelerator.print(
                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
            )  
            args.resume_from_checkpoint = None  # å°†æ¢å¤æ£€æŸ¥ç‚¹è®¾ä¸ºNone
            initial_global_step = 0  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤ä¸º0
        else:
            # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
            accelerator.print(f"Resuming from checkpoint {path}")  
            # åŠ è½½æ£€æŸ¥ç‚¹çŠ¶æ€
            accelerator.load_state(os.path.join(args.output_dir, path))  
            # æå–å…¨å±€æ­¥éª¤æ•°
            global_step = int(path.split("-")[1])  

            initial_global_step = global_step  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤ä¸ºå½“å‰æ­¥éª¤
            first_epoch = global_step // num_update_steps_per_epoch  # è®¡ç®—ç¬¬ä¸€è½®
    # åˆ›å»ºè¿›åº¦æ¡ï¼ŒèŒƒå›´ä¸ºæœ€å¤§è®­ç»ƒæ­¥éª¤ï¼Œåˆå§‹å€¼ä¸ºå…¨å±€æ­¥æ•°
        progress_bar = tqdm(
            range(0, args.max_train_steps),
            initial=initial_global_step,
            desc="Steps",
            # ä»…åœ¨æ¯å°æœºå™¨ä¸Šæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡
            disable=not accelerator.is_local_main_process,
        )
        # è®¡ç®— VAE ç©ºé—´ç¼©æ”¾å› å­ï¼Œæ ¹æ®å—è¾“å‡ºé€šé“çš„æ•°é‡
        vae_scale_factor_spatial = 2 ** (len(vae.config.block_out_channels) - 1)
    
        # è·å–æ¨¡å‹é…ç½®ï¼Œæ”¯æŒ DeepSpeed è®­ç»ƒ
        model_config = transformer.module.config if hasattr(transformer, "module") else transformer.config
    
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å‡†å¤‡å®Œæ¯•
        accelerator.wait_for_everyone()
        # ç»“æŸè®­ç»ƒ
        accelerator.end_training()
# åˆ¤æ–­å½“å‰æ¨¡å—æ˜¯å¦æ˜¯ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()
    # è°ƒç”¨ä¸»å‡½æ•°å¹¶ä¼ é€’å‚æ•°
    main(args)
```