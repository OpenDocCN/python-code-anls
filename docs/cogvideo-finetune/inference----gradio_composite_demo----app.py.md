# `.\cogvideo-finetune\inference\gradio_composite_demo\app.py`

```py
# è¿™æ˜¯ Gradio ç½‘é¡µæ¼”ç¤ºçš„ä¸»æ–‡ä»¶ï¼Œä½¿ç”¨ CogVideoX-5B æ¨¡å‹ç”Ÿæˆè§†é¢‘å¹¶è¿›è¡Œå¢å¼º
"""
THis is the main file for the gradio web demo. It uses the CogVideoX-5B model to generate videos gradio web demo.
set environment variable OPENAI_API_KEY to use the OpenAI API to enhance the prompt.

Usage:
    OpenAI_API_KEY=your_openai_api_key OPENAI_BASE_URL=https://api.openai.com/v1 python inference/gradio_web_demo.py
"""

# å¯¼å…¥æ•°å­¦åº“
import math
# å¯¼å…¥æ“ä½œç³»ç»Ÿç›¸å…³çš„åº“
import os
# å¯¼å…¥éšæœºæ•°ç”Ÿæˆåº“
import random
# å¯¼å…¥å¤šçº¿ç¨‹åº“
import threading
# å¯¼å…¥æ—¶é—´åº“
import time

# å¯¼å…¥è®¡ç®—æœºè§†è§‰åº“
import cv2
# å¯¼å…¥ä¸´æ—¶æ–‡ä»¶å¤„ç†åº“
import tempfile
# å¯¼å…¥è§†é¢‘å¤„ç†åº“
import imageio_ffmpeg
# å¯¼å…¥ Gradio åº“ç”¨äºæ„å»ºç•Œé¢
import gradio as gr
# å¯¼å…¥ PyTorch åº“
import torch
# å¯¼å…¥å›¾åƒå¤„ç†åº“
from PIL import Image
# å¯¼å…¥ Diffusers åº“ä¸­çš„æ¨¡å‹å’Œè°ƒåº¦å™¨
from diffusers import (
    CogVideoXPipeline,
    CogVideoXDPMScheduler,
    CogVideoXVideoToVideoPipeline,
    CogVideoXImageToVideoPipeline,
    CogVideoXTransformer3DModel,
)
# å¯¼å…¥è§†é¢‘å’Œå›¾åƒåŠ è½½å·¥å…·
from diffusers.utils import load_video, load_image
# å¯¼å…¥æ—¥æœŸå’Œæ—¶é—´å¤„ç†åº“
from datetime import datetime, timedelta

# å¯¼å…¥å›¾åƒå¤„ç†å™¨
from diffusers.image_processor import VaeImageProcessor
# å¯¼å…¥ OpenAI åº“
from openai import OpenAI
# å¯¼å…¥è§†é¢‘ç¼–è¾‘åº“
import moviepy.editor as mp
# å¯¼å…¥å®ç”¨å·¥å…·æ¨¡å—
import utils
# å¯¼å…¥ RIFE æ¨¡å‹çš„åŠ è½½å’Œæ¨æ–­å·¥å…·
from rife_model import load_rife_model, rife_inference_with_latents
# å¯¼å…¥ Hugging Face Hub çš„ä¸‹è½½å·¥å…·
from huggingface_hub import hf_hub_download, snapshot_download

# æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ GPUï¼Œå¹¶è®¾ç½®è®¾å¤‡ç±»å‹
device = "cuda" if torch.cuda.is_available() else "cpu"

# å®šä¹‰æ¨¡å‹çš„åç§°
MODEL = "THUDM/CogVideoX-5b"

# ä» Hugging Face Hub ä¸‹è½½ Real-ESRGAN æ¨¡å‹æƒé‡
hf_hub_download(repo_id="ai-forever/Real-ESRGAN", filename="RealESRGAN_x4.pth", local_dir="model_real_esran")
# ä» Hugging Face Hub ä¸‹è½½ RIFE æ¨¡å‹å¿«ç…§
snapshot_download(repo_id="AlexWortega/RIFE", local_dir="model_rife")

# ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½è§†é¢‘ç”Ÿæˆç®¡é“ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°æŒ‡å®šè®¾å¤‡
pipe = CogVideoXPipeline.from_pretrained(MODEL, torch_dtype=torch.bfloat16).to(device)
# è®¾ç½®è°ƒåº¦å™¨ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing="trailing")
# åˆ›å»ºè§†é¢‘åˆ°è§†é¢‘çš„ç”Ÿæˆç®¡é“
pipe_video = CogVideoXVideoToVideoPipeline.from_pretrained(
    MODEL,
    transformer=pipe.transformer,
    vae=pipe.vae,
    scheduler=pipe.scheduler,
    tokenizer=pipe.tokenizer,
    text_encoder=pipe.text_encoder,
    torch_dtype=torch.bfloat16,
).to(device)

# åˆ›å»ºå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆç®¡é“
pipe_image = CogVideoXImageToVideoPipeline.from_pretrained(
    MODEL,
    transformer=CogVideoXTransformer3DModel.from_pretrained(
        MODEL, subfolder="transformer", torch_dtype=torch.bfloat16
    ),
    vae=pipe.vae,
    scheduler=pipe.scheduler,
    tokenizer=pipe.tokenizer,
    text_encoder=pipe.text_encoder,
    torch_dtype=torch.bfloat16,
).to(device)

# ä¸‹é¢çš„è¡Œè¢«æ³¨é‡Šæ‰ï¼Œç”¨äºå†…å­˜ä¼˜åŒ–
# pipe.transformer.to(memory_format=torch.channels_last)
# pipe.transformer = torch.compile(pipe.transformer, mode="max-autotune", fullgraph=True)
# pipe_image.transformer.to(memory_format=torch.channels_last)
# pipe_image.transformer = torch.compile(pipe_image.transformer, mode="max-autotune", fullgraph=True)

# åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
os.makedirs("./output", exist_ok=True)
# åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹ç”¨äº Gradio
os.makedirs("./gradio_tmp", exist_ok=True)

# åŠ è½½è¶…åˆ†è¾¨ç‡æ¨¡å‹
upscale_model = utils.load_sd_upscale("model_real_esran/RealESRGAN_x4.pth", device)
# åŠ è½½å¸§æ’å€¼æ¨¡å‹
frame_interpolation_model = load_rife_model("model_rife")

# ç³»ç»Ÿæç¤ºï¼Œç”¨äºæŒ‡å¯¼ç”Ÿæˆè§†é¢‘çš„åŠ©æ‰‹
sys_prompt = """You are part of a team of bots that creates videos. You work with an assistant bot that will draw anything you say in square brackets.
```  
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
```py 
``` 
``` 
# æ£€æŸ¥è¾“å…¥è§†é¢‘çš„å°ºå¯¸æ˜¯å¦é€‚åˆè¦æ±‚ï¼Œè‹¥ä¸é€‚åˆåˆ™è¿›è¡Œå¤„ç†
def resize_if_unfit(input_video, progress=gr.Progress(track_tqdm=True)):
    # è·å–è¾“å…¥è§†é¢‘çš„å®½åº¦å’Œé«˜åº¦
    width, height = get_video_dimensions(input_video)

    # å¦‚æœè§†é¢‘å°ºå¯¸ä¸º720x480ï¼Œç›´æ¥ä½¿ç”¨åŸè§†é¢‘
    if width == 720 and height == 480:
        processed_video = input_video
    # å¦åˆ™è¿›è¡Œä¸­å¿ƒè£å‰ªå’Œè°ƒæ•´å¤§å°
    else:
        processed_video = center_crop_resize(input_video)
    # è¿”å›å¤„ç†åçš„è§†é¢‘
    return processed_video


# è·å–è¾“å…¥è§†é¢‘çš„å°ºå¯¸ä¿¡æ¯
def get_video_dimensions(input_video_path):
    # è¯»å–è§†é¢‘å¸§
    reader = imageio_ffmpeg.read_frames(input_video_path)
    # è·å–è§†é¢‘å…ƒæ•°æ®
    metadata = next(reader)
    # è¿”å›è§†é¢‘å°ºå¯¸
    return metadata["size"]


# å¯¹è§†é¢‘è¿›è¡Œä¸­å¿ƒè£å‰ªå’Œè°ƒæ•´å¤§å°
def center_crop_resize(input_video_path, target_width=720, target_height=480):
    # æ‰“å¼€è¾“å…¥è§†é¢‘
    cap = cv2.VideoCapture(input_video_path)

    # è·å–åŸè§†é¢‘çš„å®½åº¦ã€é«˜åº¦å’Œå¸§ç‡
    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    orig_fps = cap.get(cv2.CAP_PROP_FPS)
    # è·å–è§†é¢‘çš„æ€»å¸§æ•°
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # è®¡ç®—å®½åº¦å’Œé«˜åº¦çš„ç¼©æ”¾å› å­
    width_factor = target_width / orig_width
    height_factor = target_height / orig_height
    # é€‰æ‹©è¾ƒå¤§çš„ç¼©æ”¾å› å­è¿›è¡Œè°ƒæ•´
    resize_factor = max(width_factor, height_factor)

    # è®¡ç®—ä¸­é—´å®½åº¦å’Œé«˜åº¦
    inter_width = int(orig_width * resize_factor)
    inter_height = int(orig_height * resize_factor)

    # è®¾ç½®ç›®æ ‡å¸§ç‡
    target_fps = 8
    # è®¡ç®—ç†æƒ³è·³è¿‡çš„å¸§æ•°
    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)
    # é™åˆ¶è·³è¿‡çš„å¸§æ•°æœ€å¤§ä¸º5
    skip = min(5, ideal_skip)  # Cap at 5

    # è°ƒæ•´è·³è¿‡çš„å¸§æ•°ï¼Œä»¥ç¡®ä¿è¶³å¤Ÿçš„å¸§æ•°
    while (total_frames / (skip + 1)) < 49 and skip > 0:
        skip -= 1

    processed_frames = []  # å­˜å‚¨å¤„ç†åçš„å¸§
    frame_count = 0  # è®°å½•å·²å¤„ç†å¸§æ•°
    total_read = 0  # è®°å½•å·²è¯»å–å¸§æ•°

    # è¯»å–å¸§å¹¶è¿›è¡Œå¤„ç†ï¼Œç›´åˆ°å¤„ç†49å¸§æˆ–è¯»å–å®Œæˆ
    while frame_count < 49 and total_read < total_frames:
        ret, frame = cap.read()  # è¯»å–ä¸€å¸§
        if not ret:  # å¦‚æœæœªæˆåŠŸè¯»å–ï¼Œé€€å‡ºå¾ªç¯
            break

        # åªå¤„ç†æŒ‡å®šé—´éš”çš„å¸§
        if total_read % (skip + 1) == 0:
            # è°ƒæ•´å¸§çš„å¤§å°
            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)

            # è®¡ç®—è£å‰ªåŒºåŸŸçš„èµ·å§‹ä½ç½®
            start_x = (inter_width - target_width) // 2
            start_y = (inter_height - target_height) // 2
            # è£å‰ªå¸§
            cropped = resized[start_y : start_y + target_height, start_x : start_x + target_width]

            processed_frames.append(cropped)  # å°†è£å‰ªåçš„å¸§æ·»åŠ åˆ°åˆ—è¡¨
            frame_count += 1  # æ›´æ–°å¤„ç†å¸§æ•°

        total_read += 1  # æ›´æ–°å·²è¯»å–å¸§æ•°

    cap.release()  # é‡Šæ”¾è§†é¢‘æ•è·å¯¹è±¡
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶åˆ›å»ºä¸€ä¸ªåç¼€ä¸º .mp4 çš„æ–‡ä»¶ï¼Œä¸ä¼šåœ¨å…³é—­æ—¶åˆ é™¤
        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
            # è·å–ä¸´æ—¶è§†é¢‘æ–‡ä»¶çš„è·¯å¾„
            temp_video_path = temp_file.name
            # æŒ‡å®šè§†é¢‘ç¼–ç æ ¼å¼ä¸º mp4v
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            # åˆå§‹åŒ–è§†é¢‘å†™å…¥å¯¹è±¡ï¼Œè®¾ç½®è¾“å‡ºè·¯å¾„ã€ç¼–ç æ ¼å¼ã€å¸§ç‡å’Œå¸§å¤§å°
            out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))
    
            # éå†å¤„ç†è¿‡çš„å¸§
            for frame in processed_frames:
                # å°†æ¯ä¸€å¸§å†™å…¥è§†é¢‘æ–‡ä»¶
                out.write(frame)
    
            # é‡Šæ”¾è§†é¢‘å†™å…¥å¯¹è±¡ï¼Œå®Œæˆæ–‡ä»¶å†™å…¥
            out.release()
    
        # è¿”å›ä¸´æ—¶è§†é¢‘æ–‡ä»¶çš„è·¯å¾„
        return temp_video_path
# å®šä¹‰ä¸€ä¸ªè½¬æ¢æç¤ºçš„å‡½æ•°ï¼Œæ¥å—æç¤ºå­—ç¬¦ä¸²å’Œé‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä¸º3ï¼‰
def convert_prompt(prompt: str, retry_times: int = 3) -> str:
    # æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦å­˜åœ¨ OPENAI_API_KEY
    if not os.environ.get("OPENAI_API_KEY"):
        # å¦‚æœæ²¡æœ‰ API å¯†é’¥ï¼Œè¿”å›åŸå§‹æç¤º
        return prompt
    # åˆ›å»º OpenAI å®¢æˆ·ç«¯
    client = OpenAI()
    # å»æ‰æç¤ºå­—ç¬¦ä¸²ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦
    text = prompt.strip()

    # è¿”å›å¤„ç†åçš„æç¤ºå­—ç¬¦ä¸²
    return prompt


# å®šä¹‰ä¸€ä¸ªæ¨æ–­å‡½æ•°ï¼Œæ¥å—å¤šç§å‚æ•°
def infer(
    prompt: str,
    image_input: str,
    video_input: str,
    video_strenght: float,
    num_inference_steps: int,
    guidance_scale: float,
    seed: int = -1,
    progress=gr.Progress(track_tqdm=True),
):
    # å¦‚æœç§å­ä¸º -1ï¼Œéšæœºç”Ÿæˆä¸€ä¸ªç§å­
    if seed == -1:
        seed = random.randint(0, 2**8 - 1)

    # å¦‚æœæœ‰è§†é¢‘è¾“å…¥
    if video_input is not None:
        # åŠ è½½è§†é¢‘å¹¶é™åˆ¶ä¸º49å¸§
        video = load_video(video_input)[:49]  # Limit to 49 frames
        # é€šè¿‡ç®¡é“å¤„ç†è§†é¢‘
        video_pt = pipe_video(
            video=video,
            prompt=prompt,
            num_inference_steps=num_inference_steps,
            num_videos_per_prompt=1,
            strength=video_strenght,
            use_dynamic_cfg=True,
            output_type="pt",
            guidance_scale=guidance_scale,
            generator=torch.Generator(device="cpu").manual_seed(seed),
        ).frames
    # å¦‚æœæœ‰å›¾åƒè¾“å…¥
    elif image_input is not None:
        # å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºPILæ ¼å¼å¹¶è°ƒæ•´å¤§å°
        image_input = Image.fromarray(image_input).resize(size=(720, 480))  # Convert to PIL
        # åŠ è½½å›¾åƒ
        image = load_image(image_input)
        # é€šè¿‡ç®¡é“å¤„ç†å›¾åƒ
        video_pt = pipe_image(
            image=image,
            prompt=prompt,
            num_inference_steps=num_inference_steps,
            num_videos_per_prompt=1,
            use_dynamic_cfg=True,
            output_type="pt",
            guidance_scale=guidance_scale,
            generator=torch.Generator(device="cpu").manual_seed(seed),
        ).frames
    # å¦‚æœæ²¡æœ‰å›¾åƒæˆ–è§†é¢‘è¾“å…¥
    else:
        # é€šè¿‡ç®¡é“ç›´æ¥å¤„ç†æç¤ºç”Ÿæˆè§†é¢‘
        video_pt = pipe(
            prompt=prompt,
            num_videos_per_prompt=1,
            num_inference_steps=num_inference_steps,
            num_frames=49,
            use_dynamic_cfg=True,
            output_type="pt",
            guidance_scale=guidance_scale,
            generator=torch.Generator(device="cpu").manual_seed(seed),
        ).frames

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘å’Œç§å­
    return (video_pt, seed)


# å®šä¹‰ä¸€ä¸ªå°†è§†é¢‘è½¬æ¢ä¸ºGIFçš„å‡½æ•°
def convert_to_gif(video_path):
    # åŠ è½½è§†é¢‘æ–‡ä»¶
    clip = mp.VideoFileClip(video_path)
    # è®¾ç½®è§†é¢‘å¸§ç‡ä¸º8
    clip = clip.set_fps(8)
    # è°ƒæ•´è§†é¢‘é«˜åº¦ä¸º240
    clip = clip.resize(height=240)
    # åˆ›å»ºGIFæ–‡ä»¶çš„è·¯å¾„
    gif_path = video_path.replace(".mp4", ".gif")
    # å°†è§†é¢‘å†™å…¥GIFæ–‡ä»¶
    clip.write_gif(gif_path, fps=8)
    # è¿”å›ç”Ÿæˆçš„GIFæ–‡ä»¶è·¯å¾„
    return gif_path


# å®šä¹‰ä¸€ä¸ªåˆ é™¤æ—§æ–‡ä»¶çš„å‡½æ•°
def delete_old_files():
    # æ— é™å¾ªç¯ä»¥æŒç»­æ£€æŸ¥æ—§æ–‡ä»¶
    while True:
        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        # è®¡ç®—10åˆ†é’Ÿå‰çš„æ—¶é—´
        cutoff = now - timedelta(minutes=10)
        # å®šä¹‰è¦æ£€æŸ¥çš„ç›®å½•
        directories = ["./output", "./gradio_tmp"]

        # éå†æ¯ä¸ªç›®å½•
        for directory in directories:
            # éå†ç›®å½•ä¸­çš„æ¯ä¸ªæ–‡ä»¶
            for filename in os.listdir(directory):
                # ç”Ÿæˆæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                file_path = os.path.join(directory, filename)
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶
                if os.path.isfile(file_path):
                    # è·å–æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    # å¦‚æœæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æ—©äºæˆªæ­¢æ—¶é—´ï¼Œåˆ™åˆ é™¤æ–‡ä»¶
                    if file_mtime < cutoff:
                        os.remove(file_path)
        # æ¯600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰ä¼‘çœ ä¸€æ¬¡
        time.sleep(600)


# å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ¥æ‰§è¡Œåˆ é™¤æ—§æ–‡ä»¶çš„å‡½æ•°ï¼Œè®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
threading.Thread(target=delete_old_files, daemon=True).start()
# å®šä¹‰ç¤ºä¾‹è§†é¢‘åˆ—è¡¨
examples_videos = [["example_videos/horse.mp4"], ["example_videos/kitten.mp4"], ["example_videos/train_running.mp4"]]
# åˆ›å»ºä¸€ä¸ªåŒ…å«ç¤ºä¾‹å›¾ç‰‡è·¯å¾„çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨åŒ…å«ä¸€ä¸ªå›¾ç‰‡è·¯å¾„
examples_images = [["example_images/beach.png"], ["example_images/street.png"], ["example_images/camping.png"]]

# ä½¿ç”¨ Gradio åº“åˆ›å»ºä¸€ä¸ªå—ç»“æ„çš„ç•Œé¢
with gr.Blocks() as demo:
    # æ·»åŠ ä¸€ä¸ª Markdown ç»„ä»¶ï¼Œç”¨äºæ˜¾ç¤ºæ ‡é¢˜å’Œé“¾æ¥
    gr.Markdown("""
           <div style="text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;">
               # åœ¨é¡µé¢ä¸­å±…ä¸­æ˜¾ç¤ºçš„æ ‡é¢˜ï¼Œå­—ä½“å¤§å°ä¸º 32pxï¼Œç²—ä½“ï¼Œå¹¶æœ‰åº•éƒ¨é—´è·
               CogVideoX-5B Huggingface SpaceğŸ¤—
           </div>
           <div style="text-align: center;">
               # æä¾›å¤šä¸ªé“¾æ¥ï¼ŒæŒ‡å‘ Huggingface æ¨¡å‹åº“å’Œç›¸å…³èµ„æº
               <a href="https://huggingface.co/THUDM/CogVideoX-5B">ğŸ¤— 5B(T2V) Model Hub</a> |
               <a href="https://huggingface.co/THUDM/CogVideoX-5B-I2V">ğŸ¤— 5B(I2V) Model Hub</a> |
               <a href="https://github.com/THUDM/CogVideo">ğŸŒ Github</a> |
               <a href="https://arxiv.org/pdf/2408.06072">ğŸ“œ arxiv </a>
           </div>
           <div style="text-align: center;display: flex;justify-content: center;align-items: center;margin-top: 1em;margin-bottom: .5em;">
              # æç¤ºç”¨æˆ·å¦‚æœç©ºé—´å¤ªå¿™ï¼Œå¯ä»¥å¤åˆ¶ä½¿ç”¨
              <span>If the Space is too busy, duplicate it to use privately</span>
              # æä¾›ä¸€ä¸ªæŒ‰é’®å›¾æ ‡ï¼Œé“¾æ¥åˆ°å¤åˆ¶ç©ºé—´çš„åœ°å€
              <a href="https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space?duplicate=true"><img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/duplicate-this-space-lg.svg" width="160" style="
                margin-left: .75em;
            "></a>
           </div>
           <div style="text-align: center; font-size: 15px; font-weight: bold; color: red; margin-bottom: 20px;">
            # æ˜¾ç¤ºä¸€æ¡è­¦å‘Šä¿¡æ¯ï¼Œè¡¨æ˜æ­¤æ¼”ç¤ºä»…ç”¨äºå­¦æœ¯ç ”ç©¶å’Œå®éªŒä½¿ç”¨
            âš ï¸ This demo is for academic research and experimental use only. 
            </div>
           """)
    # åˆ›å»ºä¸€ä¸ªè¡Œå®¹å™¨ï¼Œç”¨äºæ’åˆ—å­ç»„ä»¶
    with gr.Row():
        # åˆ›å»ºä¸€ä¸ªåˆ—å®¹å™¨ï¼Œç”¨äºæ’åˆ—å›¾åƒå’Œè§†é¢‘è¾“å…¥ç»„ä»¶
        with gr.Column():
            # åˆ›å»ºä¸€ä¸ªæŠ˜å ç»„ä»¶ï¼Œç”¨äºå›¾åƒè¾“å…¥ï¼Œåˆå§‹çŠ¶æ€ä¸ºå…³é—­
            with gr.Accordion("I2V: Image Input (cannot be used simultaneously with video input)", open=False):
                # åˆ›å»ºå›¾åƒè¾“å…¥ç»„ä»¶ï¼Œå¹¶è®¾ç½®æ ‡ç­¾
                image_input = gr.Image(label="Input Image (will be cropped to 720 * 480)")
                # åˆ›å»ºç¤ºä¾‹ç»„ä»¶ï¼Œä¾›ç”¨æˆ·é€‰æ‹©é¢„è®¾çš„å›¾åƒç¤ºä¾‹
                examples_component_images = gr.Examples(examples_images, inputs=[image_input], cache_examples=False)
            # åˆ›å»ºä¸€ä¸ªæŠ˜å ç»„ä»¶ï¼Œç”¨äºè§†é¢‘è¾“å…¥ï¼Œåˆå§‹çŠ¶æ€ä¸ºå…³é—­
            with gr.Accordion("V2V: Video Input (cannot be used simultaneously with image input)", open=False):
                # åˆ›å»ºè§†é¢‘è¾“å…¥ç»„ä»¶ï¼Œå¹¶è®¾ç½®æ ‡ç­¾
                video_input = gr.Video(label="Input Video (will be cropped to 49 frames, 6 seconds at 8fps)")
                # åˆ›å»ºæ»‘å—ç»„ä»¶ï¼Œç”¨äºè°ƒæ•´å¼ºåº¦ï¼ŒèŒƒå›´ä»0.1åˆ°1.0ï¼Œé»˜è®¤å€¼ä¸º0.8
                strength = gr.Slider(0.1, 1.0, value=0.8, step=0.01, label="Strength")
                # åˆ›å»ºç¤ºä¾‹ç»„ä»¶ï¼Œä¾›ç”¨æˆ·é€‰æ‹©é¢„è®¾çš„è§†é¢‘ç¤ºä¾‹
                examples_component_videos = gr.Examples(examples_videos, inputs=[video_input], cache_examples=False)
            # åˆ›å»ºæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥æç¤ºï¼Œé™åˆ¶ä¸º200ä¸ªå•è¯
            prompt = gr.Textbox(label="Prompt (Less than 200 Words)", placeholder="Enter your prompt here", lines=5)

            # åˆ›å»ºä¸€ä¸ªè¡Œå®¹å™¨ï¼Œç”¨äºæ’åˆ—æŒ‰é’®å’Œè¯´æ˜æ–‡æœ¬
            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªMarkdownç»„ä»¶ï¼Œæ˜¾ç¤ºå…³äºå¢å¼ºæç¤ºæŒ‰é’®çš„è¯´æ˜
                gr.Markdown(
                    "âœ¨Upon pressing the enhanced prompt button, we will use [GLM-4 Model](https://github.com/THUDM/GLM-4) to polish the prompt and overwrite the original one."
                )
                # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œç”¨äºå¢å¼ºæç¤ºï¼Œæ ‡è®°ä¸ºå¯é€‰
                enhance_button = gr.Button("âœ¨ Enhance Prompt(Optional)")
            # åˆ›å»ºä¸€ä¸ªç»„å®¹å™¨ï¼Œç”¨äºæ’åˆ—ç”Ÿæˆç›¸å…³çš„å‚æ•°è®¾ç½®
            with gr.Group():
                # åˆ›å»ºä¸€ä¸ªåˆ—å®¹å™¨ï¼Œç”¨äºæ’åˆ—ç”Ÿæˆå‚æ•°
                with gr.Column():
                    # åˆ›å»ºä¸€ä¸ªè¡Œå®¹å™¨ï¼Œç”¨äºæ’åˆ—éšæœºç§å­è¾“å…¥
                    with gr.Row():
                        # åˆ›å»ºä¸€ä¸ªæ•°å­—è¾“å…¥ç»„ä»¶ï¼Œç”¨äºè¾“å…¥æ¨ç†ç§å­ï¼Œ-1è¡¨ç¤ºéšæœº
                        seed_param = gr.Number(
                            label="Inference Seed (Enter a positive number, -1 for random)", value=-1
                        )
                    # åˆ›å»ºä¸€ä¸ªè¡Œå®¹å™¨ï¼Œç”¨äºæ’åˆ—å¤é€‰æ¡†
                    with gr.Row():
                        # åˆ›å»ºå¤é€‰æ¡†ç»„ä»¶ï¼Œè¡¨ç¤ºå¯ç”¨è¶…åˆ†è¾¨ç‡åŠŸèƒ½
                        enable_scale = gr.Checkbox(label="Super-Resolution (720 Ã— 480 -> 2880 Ã— 1920)", value=False)
                        # åˆ›å»ºå¤é€‰æ¡†ç»„ä»¶ï¼Œè¡¨ç¤ºå¯ç”¨å¸§æ’å€¼åŠŸèƒ½
                        enable_rife = gr.Checkbox(label="Frame Interpolation (8fps -> 16fps)", value=False)
                    # åˆ›å»ºä¸€ä¸ªMarkdownç»„ä»¶ï¼Œæ˜¾ç¤ºå…³äºä½¿ç”¨çš„æŠ€æœ¯å’Œå·¥å…·çš„è¯´æ˜
                    gr.Markdown(
                        "âœ¨In this demo, we use [RIFE](https://github.com/hzwer/ECCV2022-RIFE) for frame interpolation and [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) for upscaling(Super-Resolution).<br>&nbsp;&nbsp;&nbsp;&nbsp;The entire process is based on open-source solutions."
                    )

            # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œç”¨äºç”Ÿæˆè§†é¢‘
            generate_button = gr.Button("ğŸ¬ Generate Video")

        # åˆ›å»ºä¸€ä¸ªåˆ—å®¹å™¨ï¼Œç”¨äºæ˜¾ç¤ºç”Ÿæˆçš„è§†é¢‘è¾“å‡º
        with gr.Column():
            # åˆ›å»ºè§†é¢‘è¾“å‡ºç»„ä»¶ï¼Œç”¨äºæ˜¾ç¤ºç”Ÿæˆçš„è§†é¢‘
            video_output = gr.Video(label="CogVideoX Generate Video", width=720, height=480)
            # åˆ›å»ºä¸€ä¸ªè¡Œå®¹å™¨ï¼Œç”¨äºæ’åˆ—ä¸‹è½½æŒ‰é’®å’Œç§å­æ˜¾ç¤º
            with gr.Row():
                # åˆ›å»ºæ–‡ä»¶ä¸‹è½½æŒ‰é’®ï¼Œç”¨äºä¸‹è½½ç”Ÿæˆçš„è§†é¢‘ï¼Œåˆå§‹çŠ¶æ€ä¸ºä¸å¯è§
                download_video_button = gr.File(label="ğŸ“¥ Download Video", visible=False)
                # åˆ›å»ºæ–‡ä»¶ä¸‹è½½æŒ‰é’®ï¼Œç”¨äºä¸‹è½½ç”Ÿæˆçš„GIFï¼Œåˆå§‹çŠ¶æ€ä¸ºä¸å¯è§
                download_gif_button = gr.File(label="ğŸ“¥ Download GIF", visible=False)
                # åˆ›å»ºæ•°å­—è¾“å…¥ç»„ä»¶ï¼Œç”¨äºæ˜¾ç¤ºç”¨äºè§†é¢‘ç”Ÿæˆçš„ç§å­ï¼Œåˆå§‹çŠ¶æ€ä¸ºä¸å¯è§
                seed_text = gr.Number(label="Seed Used for Video Generation", visible=False)

    # åˆ›å»ºä¸€ä¸ªMarkdownç»„ä»¶ï¼Œæ˜¾ç¤ºè¡¨æ ¼çš„ç»“æŸæ ‡ç­¾
    gr.Markdown("""
    </table>
        """)

    # å®šä¹‰ç”Ÿæˆè§†é¢‘çš„å‡½æ•°ï¼Œæ¥æ”¶å¤šä¸ªå‚æ•°
    def generate(
        prompt,
        image_input,
        video_input,
        video_strength,
        seed_value,
        scale_status,
        rife_status,
        progress=gr.Progress(track_tqdm=True)
    ):
        # è°ƒç”¨ infer å‡½æ•°è·å–æ½œåœ¨è¡¨ç¤ºå’Œéšæœºç§å­
        latents, seed = infer(
            # è¾“å…¥çš„æç¤ºæ–‡æœ¬
            prompt,
            # è¾“å…¥çš„å›¾åƒæ•°æ®
            image_input,
            # è¾“å…¥çš„è§†é¢‘æ•°æ®
            video_input,
            # è§†é¢‘å¼ºåº¦å‚æ•°
            video_strength,
            # è®¾ç½®æ¨ç†æ­¥æ•°ä¸º 50
            num_inference_steps=50,  # NOT Changed
            # è®¾ç½®å¼•å¯¼æ¯”ä¾‹ä¸º 7.0
            guidance_scale=7.0,  # NOT Changed
            # ä½¿ç”¨ç»™å®šçš„ç§å­å€¼
            seed=seed_value,
            # è¿›åº¦æ˜¾ç¤ºå‚æ•°
            progress=progress,
        )
        # å¦‚æœç¼©æ”¾çŠ¶æ€ä¸ºçœŸï¼Œè¿›è¡Œæ‰¹é‡æ”¾å¤§å’Œæ‹¼æ¥
        if scale_status:
            latents = utils.upscale_batch_and_concatenate(upscale_model, latents, device)
        # å¦‚æœ RIFE çŠ¶æ€ä¸ºçœŸï¼Œä½¿ç”¨æ½œåœ¨è¡¨ç¤ºè¿›è¡Œæ’å¸§æ¨ç†
        if rife_status:
            latents = rife_inference_with_latents(frame_interpolation_model, latents)

        # è·å–æ½œåœ¨è¡¨ç¤ºçš„æ‰¹é‡å¤§å°
        batch_size = latents.shape[0]
        # åˆå§‹åŒ–å­˜å‚¨è§†é¢‘å¸§çš„åˆ—è¡¨
        batch_video_frames = []
        # éå†æ¯ä¸ªæ‰¹æ¬¡çš„ç´¢å¼•
        for batch_idx in range(batch_size):
            # è·å–å½“å‰æ‰¹æ¬¡çš„æ½œåœ¨å›¾åƒ
            pt_image = latents[batch_idx]
            # å°†å½“å‰å›¾åƒçš„æ¯ä¸ªé€šé“å †å æˆä¸€ä¸ªå¼ é‡
            pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])

            # å°† PyTorch å›¾åƒè½¬æ¢ä¸º NumPy æ ¼å¼
            image_np = VaeImageProcessor.pt_to_numpy(pt_image)
            # å°† NumPy å›¾åƒè½¬æ¢ä¸º PIL æ ¼å¼
            image_pil = VaeImageProcessor.numpy_to_pil(image_np)
            # å°†è½¬æ¢åçš„å›¾åƒæ·»åŠ åˆ°è§†é¢‘å¸§åˆ—è¡¨ä¸­
            batch_video_frames.append(image_pil)

        # ä¿å­˜è§†é¢‘ï¼Œå¹¶è®¡ç®—æ¯ç§’å¸§æ•°
        video_path = utils.save_video(batch_video_frames[0], fps=math.ceil((len(batch_video_frames[0]) - 1) / 6))
        # æ›´æ–°è§†é¢‘æ˜¾ç¤ºçŠ¶æ€
        video_update = gr.update(visible=True, value=video_path)
        # å°†è§†é¢‘è½¬æ¢ä¸º GIF æ ¼å¼
        gif_path = convert_to_gif(video_path)
        # æ›´æ–° GIF æ˜¾ç¤ºçŠ¶æ€
        gif_update = gr.update(visible=True, value=gif_path)
        # æ›´æ–°ç§å­æ˜¾ç¤ºçŠ¶æ€
        seed_update = gr.update(visible=True, value=seed)

        # è¿”å›è§†é¢‘è·¯å¾„å’Œæ›´æ–°çŠ¶æ€
        return video_path, video_update, gif_update, seed_update

    # å®šä¹‰å¢å¼ºæç¤ºåŠŸèƒ½çš„å‡½æ•°
    def enhance_prompt_func(prompt):
        # è½¬æ¢æç¤ºæ–‡æœ¬ï¼Œå¹¶è®¾ç½®é‡è¯•æ¬¡æ•°ä¸º 1
        return convert_prompt(prompt, retry_times=1)

    # ä¸ºç”ŸæˆæŒ‰é’®ç‚¹å‡»äº‹ä»¶ç»‘å®šç”Ÿæˆå‡½æ•°
    generate_button.click(
        # è°ƒç”¨ç”Ÿæˆå‡½æ•°
        generate,
        # è¾“å…¥å‚æ•°åˆ—è¡¨
        inputs=[prompt, image_input, video_input, strength, seed_param, enable_scale, enable_rife],
        # è¾“å‡ºå‚æ•°åˆ—è¡¨
        outputs=[video_output, download_video_button, download_gif_button, seed_text],
    )

    # ä¸ºå¢å¼ºæŒ‰é’®ç‚¹å‡»äº‹ä»¶ç»‘å®šå¢å¼ºæç¤ºå‡½æ•°
    enhance_button.click(enhance_prompt_func, inputs=[prompt], outputs=[prompt])
    # å¤„ç†è§†é¢‘è¾“å…¥çš„ä¸Šä¼ äº‹ä»¶ï¼Œè°ƒæ•´å¤§å°ä»¥é€‚åº”
    video_input.upload(resize_if_unfit, inputs=[video_input], outputs=[video_input])
# åˆ¤æ–­å½“å‰æ¨¡å—æ˜¯å¦ä¸ºä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # åˆå§‹åŒ–é˜Ÿåˆ—ï¼Œè®¾ç½®æœ€å¤§å¤§å°ä¸º 15
    demo.queue(max_size=15)
    # å¯åŠ¨ demo ç¨‹åº
    demo.launch()
```