# `.\cogvideo-finetune\tools\convert_weight_sat2hf.py`

```
"""
# æ­¤è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä»æ–‡æœ¬æç¤ºè½¬æ¢å’Œç”Ÿæˆè§†é¢‘
# ä½¿ç”¨ CogVideoX å’Œ ğŸ¤—Huggingface Diffusers Pipelineã€‚
# æ­¤è„šæœ¬éœ€è¦å®‰è£… `diffusers>=0.30.2` åº“ã€‚

# å‡½æ•°åˆ—è¡¨ï¼š
#     - reassign_query_key_value_inplace: å°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢ã€é”®å’Œå€¼çš„æƒé‡ã€‚
#     - reassign_query_key_layernorm_inplace: å°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢å’Œé”®çš„å±‚å½’ä¸€åŒ–ã€‚
#     - reassign_adaln_norm_inplace: å°±åœ°é‡æ–°åˆ†é…è‡ªé€‚åº”å±‚å½’ä¸€åŒ–ã€‚
#     - remove_keys_inplace: å°±åœ°ç§»é™¤çŠ¶æ€å­—å…¸ä¸­æŒ‡å®šçš„é”®ã€‚
#     - replace_up_keys_inplace: å°±åœ°æ›¿æ¢â€œupâ€å—ä¸­çš„é”®ã€‚
#     - get_state_dict: ä»ä¿å­˜çš„æ£€æŸ¥ç‚¹ä¸­æå–çŠ¶æ€å­—å…¸ã€‚
#     - update_state_dict_inplace: å°±åœ°æ›´æ–°çŠ¶æ€å­—å…¸ä»¥è¿›è¡Œæ–°çš„é”®åˆ†é…ã€‚
#     - convert_transformer: å°†å˜æ¢å™¨æ£€æŸ¥ç‚¹è½¬æ¢ä¸º CogVideoX æ ¼å¼ã€‚
#     - convert_vae: å°† VAE æ£€æŸ¥ç‚¹è½¬æ¢ä¸º CogVideoX æ ¼å¼ã€‚
#     - get_args: è§£æè„šæœ¬çš„å‘½ä»¤è¡Œå‚æ•°ã€‚
#     - generate_video: ä½¿ç”¨ CogVideoX ç®¡é“ä»æ–‡æœ¬æç¤ºç”Ÿæˆè§†é¢‘ã€‚
"""

# å¯¼å…¥ argparse æ¨¡å—ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import argparse
# ä» typing å¯¼å…¥ Any å’Œ Dict ç±»å‹
from typing import Any, Dict

# å¯¼å…¥ PyTorch åº“
import torch
# ä» transformers åº“å¯¼å…¥ T5EncoderModel å’Œ T5Tokenizer
from transformers import T5EncoderModel, T5Tokenizer

# ä» diffusers åº“å¯¼å…¥å¤šä¸ªç±»
from diffusers import (
    AutoencoderKLCogVideoX,  # è‡ªåŠ¨ç¼–ç å™¨ç±»
    CogVideoXDDIMScheduler,   # è°ƒåº¦å™¨ç±»
    CogVideoXImageToVideoPipeline,  # å›¾åƒåˆ°è§†é¢‘çš„ç®¡é“ç±»
    CogVideoXPipeline,        # ä¸»ç®¡é“ç±»
    CogVideoXTransformer3DModel,  # 3D å˜æ¢å™¨æ¨¡å‹ç±»
)

# å‡½æ•°ï¼šå°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢ã€é”®å’Œå€¼çš„æƒé‡
def reassign_query_key_value_inplace(key: str, state_dict: Dict[str, Any]):
    # æ ¹æ®åŸå§‹é”®ç”Ÿæˆæ–°çš„é”®ï¼Œæ›¿æ¢æŸ¥è¯¢é”®å€¼
    to_q_key = key.replace("query_key_value", "to_q")
    to_k_key = key.replace("query_key_value", "to_k")
    to_v_key = key.replace("query_key_value", "to_v")
    # å°†çŠ¶æ€å­—å…¸ä¸­è¯¥é”®çš„å€¼åˆ†å‰²æˆä¸‰éƒ¨åˆ†ï¼ˆæŸ¥è¯¢ã€é”®å’Œå€¼ï¼‰
    to_q, to_k, to_v = torch.chunk(state_dict[key], chunks=3, dim=0)
    # å°†åˆ†å‰²åçš„æŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
    state_dict[to_q_key] = to_q
    state_dict[to_k_key] = to_k
    state_dict[to_v_key] = to_v
    # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤åŸå§‹é”®
    state_dict.pop(key)

# å‡½æ•°ï¼šå°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢å’Œé”®çš„å±‚å½’ä¸€åŒ–
def reassign_query_key_layernorm_inplace(key: str, state_dict: Dict[str, Any]):
    # ä»é”®ä¸­æå–å±‚ ID å’Œæƒé‡æˆ–åå·®ç±»å‹
    layer_id, weight_or_bias = key.split(".")[-2:]

    # æ ¹æ®é”®åç¡®å®šæ–°é”®å
    if "query" in key:
        new_key = f"transformer_blocks.{layer_id}.attn1.norm_q.{weight_or_bias}"
    elif "key" in key:
        new_key = f"transformer_blocks.{layer_id}.attn1.norm_k.{weight_or_bias}"

    # å°†çŠ¶æ€å­—å…¸ä¸­åŸé”®çš„å€¼ç§»åˆ°æ–°é”®ä¸­
    state_dict[new_key] = state_dict.pop(key)

# å‡½æ•°ï¼šå°±åœ°é‡æ–°åˆ†é…è‡ªé€‚åº”å±‚å½’ä¸€åŒ–
def reassign_adaln_norm_inplace(key: str, state_dict: Dict[str, Any]):
    # ä»é”®ä¸­æå–å±‚ ID å’Œæƒé‡æˆ–åå·®ç±»å‹
    layer_id, _, weight_or_bias = key.split(".")[-3:]

    # å°†çŠ¶æ€å­—å…¸ä¸­è¯¥é”®çš„å€¼åˆ†å‰²ä¸º 12 éƒ¨åˆ†
    weights_or_biases = state_dict[key].chunk(12, dim=0)
    # åˆå¹¶ç‰¹å®šéƒ¨åˆ†å½¢æˆæ–°çš„æƒé‡æˆ–åå·®
    norm1_weights_or_biases = torch.cat(weights_or_biases[0:3] + weights_or_biases[6:9])
    norm2_weights_or_biases = torch.cat(weights_or_biases[3:6] + weights_or_biases[9:12])

    # æ„å»ºæ–°é”®åå¹¶æ›´æ–°çŠ¶æ€å­—å…¸
    norm1_key = f"transformer_blocks.{layer_id}.norm1.linear.{weight_or_bias}"
    state_dict[norm1_key] = norm1_weights_or_biases

    norm2_key = f"transformer_blocks.{layer_id}.norm2.linear.{weight_or_bias}"
    state_dict[norm2_key] = norm2_weights_or_biases

    # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤åŸå§‹é”®
    state_dict.pop(key)

# å‡½æ•°ï¼šå°±åœ°ç§»é™¤çŠ¶æ€å­—å…¸ä¸­çš„æŒ‡å®šé”®
def remove_keys_inplace(key: str, state_dict: Dict[str, Any]):
    # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤æŒ‡å®šçš„é”®
    state_dict.pop(key)
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ›¿æ¢çŠ¶æ€å­—å…¸ä¸­çš„ç‰¹å®šé”®ï¼Œç›´æ¥åœ¨å­—å…¸ä¸­ä¿®æ”¹
def replace_up_keys_inplace(key: str, state_dict: Dict[str, Any]):
    # å°†é”®å­—ç¬¦ä¸²æŒ‰ç‚¹åˆ†å‰²æˆåˆ—è¡¨
    key_split = key.split(".")
    # è·å–æŒ‡å®šå±‚çš„ç´¢å¼•ï¼Œå‡è®¾ç´¢å¼•åœ¨ç¬¬ä¸‰ä¸ªä½ç½®
    layer_index = int(key_split[2])
    # è®¡ç®—æ›¿æ¢åçš„å±‚ç´¢å¼•
    replace_layer_index = 4 - 1 - layer_index

    # å°†åˆ†å‰²åçš„é”®æ›´æ–°ä¸º "up_blocks" ä½œä¸ºæ–°çš„ç¬¬äºŒå±‚
    key_split[1] = "up_blocks"
    # æ›´æ–°å±‚ç´¢å¼•ä¸ºè®¡ç®—åçš„æ–°ç´¢å¼•
    key_split[2] = str(replace_layer_index)
    # å°†åˆ†å‰²çš„é”®é‡æ–°æ‹¼æ¥ä¸ºå­—ç¬¦ä¸²
    new_key = ".".join(key_split)

    # åœ¨çŠ¶æ€å­—å…¸ä¸­ç”¨æ–°é”®æ›¿æ¢æ—§é”®å¯¹åº”çš„å€¼
    state_dict[new_key] = state_dict.pop(key)


# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºé‡å‘½å Transformer æ¨¡å‹çš„é”®
TRANSFORMER_KEYS_RENAME_DICT = {
    # é‡å‘½å final_layernorm é”®ä¸º norm_final
    "transformer.final_layernorm": "norm_final",
    # å°† transformer é”®é‡å‘½åä¸º transformer_blocks
    "transformer": "transformer_blocks",
    # é‡å‘½åæ³¨æ„åŠ›å±‚çš„é”®
    "attention": "attn1",
    # é‡å‘½å MLP å±‚çš„é”®
    "mlp": "ff.net",
    # é‡å‘½åå¯†é›†å±‚çš„é”®
    "dense_h_to_4h": "0.proj",
    "dense_4h_to_h": "2",
    # å¤„ç† layers é”®çš„é‡å‘½å
    ".layers": "",
    # å°† dense é”®é‡å‘½åä¸º to_out.0
    "dense": "to_out.0",
    # å¤„ç†è¾“å…¥å±‚å½’ä¸€åŒ–çš„é‡å‘½å
    "input_layernorm": "norm1.norm",
    # å¤„ç†åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–çš„é‡å‘½å
    "post_attn1_layernorm": "norm2.norm",
    # é‡å‘½åæ—¶é—´åµŒå…¥çš„å±‚
    "time_embed.0": "time_embedding.linear_1",
    "time_embed.2": "time_embedding.linear_2",
    # å¤„ç† Patch åµŒå…¥çš„é‡å‘½å
    "mixins.patch_embed": "patch_embed",
    # å¤„ç†æœ€ç»ˆå±‚çš„é‡å‘½å
    "mixins.final_layer.norm_final": "norm_out.norm",
    "mixins.final_layer.linear": "proj_out",
    # å¤„ç† ADA LN è°ƒåˆ¶å±‚çš„é‡å‘½å
    "mixins.final_layer.adaLN_modulation.1": "norm_out.linear",
    # å¤„ç†ç‰¹å®šäº CogVideoX-5b-I2V çš„é‡å‘½å
    "mixins.pos_embed.pos_embedding": "patch_embed.pos_embedding",  # Specific to CogVideoX-5b-I2V
}

# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºç‰¹æ®Šé”®çš„é‡æ˜ å°„
TRANSFORMER_SPECIAL_KEYS_REMAP = {
    # æ˜ å°„ç‰¹å®šçš„æŸ¥è¯¢é”®å€¼å¤„ç†å‡½æ•°
    "query_key_value": reassign_query_key_value_inplace,
    # æ˜ å°„æŸ¥è¯¢å±‚å½’ä¸€åŒ–åˆ—è¡¨çš„å¤„ç†å‡½æ•°
    "query_layernorm_list": reassign_query_key_layernorm_inplace,
    # æ˜ å°„é”®å±‚å½’ä¸€åŒ–åˆ—è¡¨çš„å¤„ç†å‡½æ•°
    "key_layernorm_list": reassign_query_key_layernorm_inplace,
    # æ˜ å°„ ADA LN è°ƒåˆ¶å±‚çš„å¤„ç†å‡½æ•°
    "adaln_layer.adaLN_modulations": reassign_adaln_norm_inplace,
    # æ˜ å°„åµŒå…¥ä»¤ç‰Œçš„å¤„ç†å‡½æ•°
    "embed_tokens": remove_keys_inplace,
    # æ˜ å°„é¢‘ç‡æ­£å¼¦çš„å¤„ç†å‡½æ•°
    "freqs_sin": remove_keys_inplace,
    # æ˜ å°„é¢‘ç‡ä½™å¼¦çš„å¤„ç†å‡½æ•°
    "freqs_cos": remove_keys_inplace,
    # æ˜ å°„ä½ç½®åµŒå…¥çš„å¤„ç†å‡½æ•°
    "position_embedding": remove_keys_inplace,
}

# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºé‡å‘½å VAE æ¨¡å‹çš„é”®
VAE_KEYS_RENAME_DICT = {
    # å°†å—çš„é”®é‡å‘½åä¸º resnets. 
    "block.": "resnets.",
    # å°† down çš„é”®é‡å‘½åä¸º down_blocks.
    "down.": "down_blocks.",
    # å°† downsample çš„é”®é‡å‘½åä¸º downsamplers.0
    "downsample": "downsamplers.0",
    # å°† upsample çš„é”®é‡å‘½åä¸º upsamplers.0
    "upsample": "upsamplers.0",
    # å°† nin_shortcut çš„é”®é‡å‘½åä¸º conv_shortcut
    "nin_shortcut": "conv_shortcut",
    # å°†ç¼–ç å™¨çš„å—é‡å‘½å
    "encoder.mid.block_1": "encoder.mid_block.resnets.0",
    "encoder.mid.block_2": "encoder.mid_block.resnets.1",
    # å°†è§£ç å™¨çš„å—é‡å‘½å
    "decoder.mid.block_1": "decoder.mid_block.resnets.0",
    "decoder.mid.block_2": "decoder.mid_block.resnets.1",
}

# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºç‰¹æ®Šé”®çš„é‡æ˜ å°„ï¼Œé€‚ç”¨äº VAE
VAE_SPECIAL_KEYS_REMAP = {
    # æ˜ å°„æŸå¤±çš„å¤„ç†å‡½æ•°
    "loss": remove_keys_inplace,
    # æ˜ å°„ up çš„å¤„ç†å‡½æ•°
    "up.": replace_up_keys_inplace,
}

# å®šä¹‰ä¸€ä¸ªå¸¸é‡ï¼Œè¡¨ç¤ºæ ‡è®°å™¨çš„æœ€å¤§é•¿åº¦
TOKENIZER_MAX_LENGTH = 226


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»ä¿å­˜çš„å­—å…¸ä¸­è·å–çŠ¶æ€å­—å…¸
def get_state_dict(saved_dict: Dict[str, Any]) -> Dict[str, Any]:
    # é»˜è®¤çŠ¶æ€å­—å…¸ä¸ºä¿å­˜çš„å­—å…¸
    state_dict = saved_dict
    # å¦‚æœä¿å­˜çš„å­—å…¸ä¸­åŒ…å« "model" é”®ï¼Œåˆ™æå–æ¨¡å‹éƒ¨åˆ†
    if "model" in saved_dict.keys():
        state_dict = state_dict["model"]
    # å¦‚æœä¿å­˜çš„å­—å…¸ä¸­åŒ…å« "module" é”®ï¼Œåˆ™æå–æ¨¡å—éƒ¨åˆ†
    if "module" in saved_dict.keys():
        state_dict = state_dict["module"]
    # å¦‚æœä¿å­˜çš„å­—å…¸ä¸­åŒ…å« "state_dict" é”®ï¼Œåˆ™æå–çŠ¶æ€å­—å…¸
    if "state_dict" in saved_dict.keys():
        state_dict = state_dict["state_dict"]
    # è¿”å›æœ€ç»ˆæå–çš„çŠ¶æ€å­—å…¸
    return state_dict


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç›´æ¥åœ¨çŠ¶æ€å­—å…¸ä¸­æ›´æ–°é”®
def update_state_dict_inplace(state_dict: Dict[str, Any], old_key: str, new_key: str) -> Dict[str, Any]:
    # ç”¨æ–°é”®æ›¿æ¢æ—§é”®åœ¨å­—å…¸ä¸­çš„å€¼
    state_dict[new_key] = state_dict.pop(old_key)


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè½¬æ¢ Transformer æ¨¡å‹
def convert_transformer(
    ckpt_path: str,
    num_layers: int,
    num_attention_heads: int,
    use_rotary_positional_embeddings: bool,
    i2v: bool,
    dtype: torch.dtype,
):
    # å®šä¹‰ä¸€ä¸ªå‰ç¼€é”®ï¼Œè¡¨ç¤ºæ¨¡å‹çš„å‰ç¼€éƒ¨åˆ†
    PREFIX_KEY = "model.diffusion_model."

    # ä»æŒ‡å®šè·¯å¾„åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸ï¼Œè®¾ç½® map_location ä¸º "cpu" å’Œ mmap ä¸º True
    original_state_dict = get_state_dict(torch.load(ckpt_path, map_location="cpu", mmap=True))
    # åˆ›å»ºä¸€ä¸ª CogVideoXTransformer3DModel å®ä¾‹ï¼Œè®¾ç½®è¾“å…¥é€šé“ã€å±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰å‚æ•°
    transformer = CogVideoXTransformer3DModel(
        # æ ¹æ® i2v çš„å€¼å†³å®šè¾“å…¥é€šé“æ•°
        in_channels=32 if i2v else 16,
        # è®¾ç½®æ¨¡å‹çš„å±‚æ•°
        num_layers=num_layers,
        # è®¾ç½®æ³¨æ„åŠ›å¤´çš„æ•°é‡
        num_attention_heads=num_attention_heads,
        # æ˜¯å¦ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥
        use_rotary_positional_embeddings=use_rotary_positional_embeddings,
        # æ˜¯å¦ä½¿ç”¨å­¦ä¹ åˆ°çš„ä½ç½®åµŒå…¥
        use_learned_positional_embeddings=i2v,
    ).to(dtype=dtype)  # å°†æ¨¡å‹è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹

    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®åˆ—è¡¨
    for key in list(original_state_dict.keys()):
        # ä»é”®ä¸­å»æ‰å‰ç¼€ï¼Œä»¥è·å¾—æ–°çš„é”®å
        new_key = key[len(PREFIX_KEY) :]
        # éå†é‡å‘½åå­—å…¸ï¼Œæ›¿æ¢é”®åä¸­çš„ç‰¹å®šéƒ¨åˆ†
        for replace_key, rename_key in TRANSFORMER_KEYS_RENAME_DICT.items():
            new_key = new_key.replace(replace_key, rename_key)
        # æ›´æ–°åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„é”®å€¼å¯¹
        update_state_dict_inplace(original_state_dict, key, new_key)

    # å†æ¬¡éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®åˆ—è¡¨
    for key in list(original_state_dict.keys()):
        # éå†ç‰¹æ®Šé”®çš„æ˜ å°„å­—å…¸
        for special_key, handler_fn_inplace in TRANSFORMER_SPECIAL_KEYS_REMAP.items():
            # å¦‚æœç‰¹æ®Šé”®ä¸åœ¨å½“å‰é”®ä¸­ï¼Œåˆ™ç»§ç»­ä¸‹ä¸€ä¸ªé”®
            if special_key not in key:
                continue
            # è°ƒç”¨å¤„ç†å‡½æ•°ä»¥æ›´æ–°çŠ¶æ€å­—å…¸
            handler_fn_inplace(key, original_state_dict)
    
    # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸åˆ° transformer ä¸­ï¼Œä¸¥æ ¼åŒ¹é…é”®
    transformer.load_state_dict(original_state_dict, strict=True)
    # è¿”å› transformer å®ä¾‹
    return transformer
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°† VAE æ¨¡å‹ä»æ£€æŸ¥ç‚¹è·¯å¾„è½¬æ¢
def convert_vae(ckpt_path: str, scaling_factor: float, dtype: torch.dtype):
    # ä»æŒ‡å®šè·¯å¾„åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸ï¼Œä½¿ç”¨ CPU æ˜ å°„
    original_state_dict = get_state_dict(torch.load(ckpt_path, map_location="cpu", mmap=True))
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ VAE å¯¹è±¡ï¼Œå¹¶å°†å…¶æ•°æ®ç±»å‹è®¾ç½®ä¸ºæŒ‡å®šçš„ dtype
    vae = AutoencoderKLCogVideoX(scaling_factor=scaling_factor).to(dtype=dtype)

    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„æ‰€æœ‰é”®
    for key in list(original_state_dict.keys()):
        # å¤åˆ¶å½“å‰é”®ä»¥ä¾¿ä¿®æ”¹
        new_key = key[:]
        # éå†é‡å‘½åå­—å…¸ï¼Œå°†æ—§é”®æ›¿æ¢ä¸ºæ–°é”®
        for replace_key, rename_key in VAE_KEYS_RENAME_DICT.items():
            new_key = new_key.replace(replace_key, rename_key)
        # æ›´æ–°åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„é”®
        update_state_dict_inplace(original_state_dict, key, new_key)

    # å†æ¬¡éå†åŸå§‹çŠ¶æ€å­—å…¸çš„æ‰€æœ‰é”®
    for key in list(original_state_dict.keys()):
        # éå†ç‰¹æ®Šé”®æ˜ å°„å­—å…¸
        for special_key, handler_fn_inplace in VAE_SPECIAL_KEYS_REMAP.items():
            # å¦‚æœç‰¹æ®Šé”®ä¸åœ¨å½“å‰é”®ä¸­ï¼Œåˆ™è·³è¿‡
            if special_key not in key:
                continue
            # ä½¿ç”¨å¤„ç†å‡½æ•°å¤„ç†åŸå§‹çŠ¶æ€å­—å…¸
            handler_fn_inplace(key, original_state_dict)

    # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸åˆ° VAE æ¨¡å‹ä¸­ï¼Œä¸¥æ ¼åŒ¹é…
    vae.load_state_dict(original_state_dict, strict=True)
    # è¿”å›è½¬æ¢åçš„ VAE å¯¹è±¡
    return vae


# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    # æ·»åŠ åŸå§‹å˜æ¢å™¨æ£€æŸ¥ç‚¹è·¯å¾„å‚æ•°
    parser.add_argument(
        "--transformer_ckpt_path", type=str, default=None, help="Path to original transformer checkpoint")
    # æ·»åŠ åŸå§‹ VAE æ£€æŸ¥ç‚¹è·¯å¾„å‚æ•°
    parser.add_argument("--vae_ckpt_path", type=str, default=None, help="Path to original vae checkpoint")
    # æ·»åŠ è¾“å‡ºè·¯å¾„å‚æ•°ï¼Œä½œä¸ºå¿…éœ€å‚æ•°
    parser.add_argument("--output_path", type=str, required=True, help="Path where converted model should be saved")
    # æ·»åŠ æ˜¯å¦ä»¥ fp16 æ ¼å¼ä¿å­˜æ¨¡å‹æƒé‡çš„å¸ƒå°”å‚æ•°
    parser.add_argument("--fp16", action="store_true", default=False, help="Whether to save the model weights in fp16")
    # æ·»åŠ æ˜¯å¦ä»¥ bf16 æ ¼å¼ä¿å­˜æ¨¡å‹æƒé‡çš„å¸ƒå°”å‚æ•°
    parser.add_argument("--bf16", action="store_true", default=False, help="Whether to save the model weights in bf16")
    # æ·»åŠ æ˜¯å¦åœ¨ä¿å­˜åæ¨é€åˆ° HF Hub çš„å¸ƒå°”å‚æ•°
    parser.add_argument(
        "--push_to_hub", action="store_true", default=False, help="Whether to push to HF Hub after saving"
    )
    # æ·»åŠ æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜ç›®å½•è·¯å¾„å‚æ•°
    parser.add_argument(
        "--text_encoder_cache_dir", type=str, default=None, help="Path to text encoder cache directory"
    )
    # æ·»åŠ å˜æ¢å™¨å—æ•°é‡å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 30
    parser.add_argument("--num_layers", type=int, default=30, help="Number of transformer blocks")
    # æ·»åŠ æ³¨æ„åŠ›å¤´æ•°é‡å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 30
    parser.add_argument("--num_attention_heads", type=int, default=30, help="Number of attention heads")
    # æ·»åŠ æ˜¯å¦ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥çš„å¸ƒå°”å‚æ•°
    parser.add_argument(
        "--use_rotary_positional_embeddings", action="store_true", default=False, help="Whether to use RoPE or not"
    )
    # æ·»åŠ  VAE çš„ç¼©æ”¾å› å­å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 1.15258426
    parser.add_argument("--scaling_factor", type=float, default=1.15258426, help="Scaling factor in the VAE")
    # æ·»åŠ  SNR åç§»æ¯”ä¾‹å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 3.0
    parser.add_argument("--snr_shift_scale", type=float, default=3.0, help="Scaling factor in the VAE")
    # æ·»åŠ æ˜¯å¦ä»¥ fp16 æ ¼å¼ä¿å­˜æ¨¡å‹æƒé‡çš„å¸ƒå°”å‚æ•°
    parser.add_argument("--i2v", action="store_true", default=False, help="Whether to save the model weights in fp16")
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›
    return parser.parse_args()


# å¦‚æœè„šæœ¬ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()

    # åˆå§‹åŒ– transformer å’Œ vae ä¸º None
    transformer = None
    vae = None
    # æ£€æŸ¥æ˜¯å¦åŒæ—¶ä¼ é€’äº† --fp16 å’Œ --bf16 å‚æ•°
    if args.fp16 and args.bf16:
        # å¦‚æœåŒæ—¶å­˜åœ¨åˆ™æŠ›å‡ºå€¼é”™è¯¯
        raise ValueError("You cannot pass both --fp16 and --bf16 at the same time.")

    # æ ¹æ®è¾“å…¥å‚æ•°é€‰æ‹©æ•°æ®ç±»å‹
    dtype = torch.float16 if args.fp16 else torch.bfloat16 if args.bf16 else torch.float32

    # å¦‚æœæä¾›äº†å˜æ¢å™¨æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œåˆ™è½¬æ¢å˜æ¢å™¨
    if args.transformer_ckpt_path is not None:
        transformer = convert_transformer(
            # ä¼ é€’å˜æ¢å™¨æ£€æŸ¥ç‚¹è·¯å¾„åŠç›¸å…³å‚æ•°
            args.transformer_ckpt_path,
            args.num_layers,
            args.num_attention_heads,
            args.use_rotary_positional_embeddings,
            args.i2v,
            dtype,
        )
    # å¦‚æœæä¾›äº† VAE æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œåˆ™è½¬æ¢ VAE
    if args.vae_ckpt_path is not None:
        vae = convert_vae(args.vae_ckpt_path, args.scaling_factor, dtype)

    # è®¾ç½®æ–‡æœ¬ç¼–ç å™¨çš„æ¨¡å‹ ID
    text_encoder_id = "google/t5-v1_1-xxl"
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½åˆ†è¯å™¨
    tokenizer = T5Tokenizer.from_pretrained(text_encoder_id, model_max_length=TOKENIZER_MAX_LENGTH)
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½æ–‡æœ¬ç¼–ç å™¨
    text_encoder = T5EncoderModel.from_pretrained(text_encoder_id, cache_dir=args.text_encoder_cache_dir)
    # å¤„ç†å‚æ•°ä»¥ç¡®ä¿æ•°æ®è¿ç»­æ€§
    for param in text_encoder.parameters():
        # ä½¿å‚æ•°æ•°æ®è¿ç»­
        param.data = param.data.contiguous()

    # ä»é…ç½®ä¸­åˆ›å»ºè°ƒåº¦å™¨
    scheduler = CogVideoXDDIMScheduler.from_config(
        {
            # è®¾ç½®è°ƒåº¦å™¨çš„è¶…å‚æ•°
            "snr_shift_scale": args.snr_shift_scale,
            "beta_end": 0.012,
            "beta_schedule": "scaled_linear",
            "beta_start": 0.00085,
            "clip_sample": False,
            "num_train_timesteps": 1000,
            "prediction_type": "v_prediction",
            "rescale_betas_zero_snr": True,
            "set_alpha_to_one": True,
            "timestep_spacing": "trailing",
        }
    )
    # æ ¹æ® i2v å‚æ•°é€‰æ‹©ç®¡é“ç±»
    if args.i2v:
        pipeline_cls = CogVideoXImageToVideoPipeline
    else:
        pipeline_cls = CogVideoXPipeline

    # å®ä¾‹åŒ–ç®¡é“
    pipe = pipeline_cls(
        # ä¼ é€’æ‰€éœ€çš„ç»„ä»¶
        tokenizer=tokenizer,
        text_encoder=text_encoder,
        vae=vae,
        transformer=transformer,
        scheduler=scheduler,
    )

    # å¦‚æœé€‰æ‹© fp16 åˆ™å°†ç®¡é“è½¬ä¸º fp16
    if args.fp16:
        pipe = pipe.to(dtype=torch.float16)
    # å¦‚æœé€‰æ‹© bf16 åˆ™å°†ç®¡é“è½¬ä¸º bf16
    if args.bf16:
        pipe = pipe.to(dtype=torch.bfloat16)

    # ä¿å­˜é¢„è®­ç»ƒçš„ç®¡é“åˆ°æŒ‡å®šè·¯å¾„
    pipe.save_pretrained(args.output_path, safe_serialization=True, push_to_hub=args.push_to_hub)
```