# `D:\src\scipysrc\pandas\pandas\tests\io\test_common.py`

```
"""
Tests for the pandas.io.common functionalities
"""

# å¯¼å…¥æ‰€éœ€æ¨¡å—å’Œåº“
import codecs  # æä¾›ç¼–è§£ç å™¨å’Œæ–‡ä»¶å¯¹è±¡æ¥å£çš„å®ç”¨åŠŸèƒ½
import errno  # å®šä¹‰å¸¸è§çš„é”™è¯¯ç 
from functools import partial  # åˆ›å»ºå¯è°ƒç”¨å¯¹è±¡çš„é«˜çº§å·¥å…·
from io import (  # æä¾›å¯¹æµå’Œç¼“å†²åŒºæ¥å£çš„æ ¸å¿ƒå·¥å…·
    BytesIO,  # ç”¨äºæ“ä½œå­—èŠ‚æ•°æ®çš„æµ
    StringIO,  # ç”¨äºæ“ä½œå­—ç¬¦ä¸²æ•°æ®çš„æµ
    UnsupportedOperation,  # å½“å¯¹æµæ‰§è¡Œä¸æ”¯æŒçš„æ“ä½œæ—¶å¼•å‘çš„å¼‚å¸¸
)
import mmap  # æä¾›åœ¨æ–‡ä»¶ä¸Šæ‰§è¡Œå†…å­˜æ˜ å°„çš„æ”¯æŒ
import os  # æä¾›ä¸æ“ä½œç³»ç»Ÿäº¤äº’çš„åŠŸèƒ½
from pathlib import Path  # æä¾›å¤„ç†è·¯å¾„çš„å¯¹è±¡
import pickle  # ç”¨äºåºåˆ—åŒ–å’Œååºåˆ—åŒ–Pythonå¯¹è±¡
import tempfile  # æä¾›åˆ›å»ºä¸´æ—¶æ–‡ä»¶å’Œç›®å½•çš„åŠŸèƒ½

import numpy as np  # æ•°å€¼è®¡ç®—åº“
import pytest  # Pythonçš„å•å…ƒæµ‹è¯•æ¡†æ¶

from pandas.compat import (  # pandasçš„å…¼å®¹æ€§æ¨¡å—
    WASM,  # WebAssemblyçš„å…¼å®¹æ€§æ£€æŸ¥
    is_platform_windows,  # æ£€æŸ¥æ“ä½œç³»ç»Ÿæ˜¯å¦ä¸ºWindows
)

import pandas as pd  # æä¾›æ•°æ®åˆ†æåŠŸèƒ½çš„åº“
import pandas._testing as tm  # pandasæµ‹è¯•å·¥å…·

import pandas.io.common as icom  # pandasçš„I/Oå…¬å…±åŠŸèƒ½

# å¿½ç•¥ç‰¹å®šçš„è­¦å‘Šä¿¡æ¯
pytestmark = pytest.mark.filterwarnings(
    "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
)


class CustomFSPath:
    """For testing fspath on unknown objects"""

    def __init__(self, path) -> None:
        self.path = path

    def __fspath__(self):
        return self.path


HERE = os.path.abspath(os.path.dirname(__file__))  # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„


# https://github.com/cython/cython/issues/1720
class TestCommonIOCapabilities:
    data1 = """index,A,B,C,D
foo,2,3,4,5
bar,7,8,9,10
baz,12,13,14,15
qux,12,13,14,15
foo2,12,13,14,15
bar2,12,13,14,15
"""

    def test_expand_user(self):
        filename = "~/sometest"
        expanded_name = icom._expand_user(filename)  # æ‰§è¡Œç”¨æˆ·ç›®å½•æ‰©å±•

        assert expanded_name != filename  # æ–­è¨€æ‰©å±•åçš„æ–‡ä»¶åä¸åŸå§‹æ–‡ä»¶åä¸åŒ
        assert os.path.isabs(expanded_name)  # æ–­è¨€æ‰©å±•åçš„æ–‡ä»¶åæ˜¯ç»å¯¹è·¯å¾„
        assert os.path.expanduser(filename) == expanded_name  # æ–­è¨€ä½¿ç”¨os.pathæ‰©å±•ç”¨æˆ·è·¯å¾„åä¸icm._expand_userçš„ç»“æœç›¸åŒ

    def test_expand_user_normal_path(self):
        filename = "/somefolder/sometest"
        expanded_name = icom._expand_user(filename)  # æ‰§è¡Œç”¨æˆ·ç›®å½•æ‰©å±•

        assert expanded_name == filename  # æ–­è¨€æœªæ›´æ”¹è·¯å¾„
        assert os.path.expanduser(filename) == expanded_name  # æ–­è¨€ä½¿ç”¨os.pathæ‰©å±•ç”¨æˆ·è·¯å¾„åä¸icm._expand_userçš„ç»“æœç›¸åŒ

    def test_stringify_path_pathlib(self):
        rel_path = icom.stringify_path(Path("."))  # å°†Pathå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„
        assert rel_path == "."  # æ–­è¨€ç»“æœä¸ºå½“å‰ç›®å½•è·¯å¾„
        redundant_path = icom.stringify_path(Path("foo//bar"))  # å°†Pathå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„ï¼Œå¤„ç†å¤šä½™çš„åˆ†éš”ç¬¦
        assert redundant_path == os.path.join("foo", "bar")  # æ–­è¨€ç»“æœä¸é¢„æœŸè·¯å¾„æ‹¼æ¥ç›¸åŒ

    def test_stringify_path_fspath(self):
        p = CustomFSPath("foo/bar.csv")
        result = icom.stringify_path(p)  # å°†è‡ªå®šä¹‰å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„
        assert result == "foo/bar.csv"  # æ–­è¨€ç»“æœä¸é¢„æœŸè·¯å¾„ç›¸åŒ

    def test_stringify_file_and_path_like(self):
        # GH 38125: do not stringify file objects that are also path-like
        fsspec = pytest.importorskip("fsspec")  # å¯¼å…¥å¹¶æ£€æŸ¥fsspecåº“æ˜¯å¦å¯ç”¨
        with tm.ensure_clean() as path:  # ç¡®ä¿åœ¨ä¸´æ—¶è·¯å¾„ä¸­è¿›è¡Œæ“ä½œ
            with fsspec.open(f"file://{path}", mode="wb") as fsspec_obj:
                assert fsspec_obj == icom.stringify_path(fsspec_obj)  # æ–­è¨€ä¸å¯¹åŒæ—¶æ˜¯æ–‡ä»¶å¯¹è±¡å’Œè·¯å¾„å¯¹è±¡çš„å¯¹è±¡è¿›è¡Œå­—ç¬¦ä¸²åŒ–å¤„ç†

    @pytest.mark.parametrize("path_type", [str, CustomFSPath, Path])
    def test_infer_compression_from_path(self, compression_format, path_type):
        extension, expected = compression_format
        path = path_type("foo/bar.csv" + extension)
        compression = icom.infer_compression(path, compression="infer")  # æ¨æ–­è·¯å¾„çš„å‹ç¼©æ ¼å¼
        assert compression == expected  # æ–­è¨€æ¨æ–­çš„å‹ç¼©æ ¼å¼ä¸é¢„æœŸç›¸åŒ

    @pytest.mark.parametrize("path_type", [str, CustomFSPath, Path])
    # å®šä¹‰æµ‹è¯•å‡½æ•°ï¼Œæµ‹è¯•å¸¦æœ‰è·¯å¾„å‚æ•°çš„get_handleæ–¹æ³•
    def test_get_handle_with_path(self, path_type):
        # åœ¨ä¸´æ—¶ç›®å½•ä¸­åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹ï¼Œå¹¶ç”Ÿæˆæ–‡ä»¶å
        with tempfile.TemporaryDirectory(dir=Path.home()) as tmp:
            filename = path_type("~/" + Path(tmp).name + "/sometest")
            # è°ƒç”¨icomæ¨¡å—çš„get_handleæ–¹æ³•ï¼Œä»¥å†™å…¥æ¨¡å¼æ‰“å¼€æ–‡ä»¶
            with icom.get_handle(filename, "w") as handles:
                # æ–­è¨€æ–‡ä»¶å¥æŸ„çš„è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
                assert Path(handles.handle.name).is_absolute()
                # æ–­è¨€å±•å¼€ç”¨æˆ·è·¯å¾„åä¸å¥æŸ„çš„æ–‡ä»¶åç›¸åŒ¹é…
                assert os.path.expanduser(filename) == handles.handle.name

    # å®šä¹‰æµ‹è¯•å‡½æ•°ï¼Œæµ‹è¯•å¸¦æœ‰ç¼“å†²åŒºå‚æ•°çš„get_handleæ–¹æ³•
    def test_get_handle_with_buffer(self):
        # ä½¿ç”¨StringIOåˆ›å»ºè¾“å…¥ç¼“å†²åŒº
        with StringIO() as input_buffer:
            # è°ƒç”¨icomæ¨¡å—çš„get_handleæ–¹æ³•ï¼Œä»¥è¯»å–æ¨¡å¼æ‰“å¼€ç¼“å†²åŒº
            with icom.get_handle(input_buffer, "r") as handles:
                # æ–­è¨€å¥æŸ„ä¸è¾“å…¥ç¼“å†²åŒºå¯¹è±¡ç›¸ç­‰
                assert handles.handle == input_buffer
            # æ–­è¨€è¾“å…¥ç¼“å†²åŒºæ²¡æœ‰è¢«å…³é—­
            assert not input_buffer.closed
        # æ–­è¨€è¾“å…¥ç¼“å†²åŒºå·²ç»å…³é—­
        assert input_buffer.closed

    # æµ‹è¯•BytesIOWrapper(get_handle)æ–¹æ³•è¿”å›æ­£ç¡®æ•°é‡çš„å­—èŠ‚
    def test_bytesiowrapper_returns_correct_bytes(self):
        # æµ‹è¯•åŒ…å«æ‹‰ä¸å­—æ¯ã€ucs-2å’Œucs-4å­—ç¬¦çš„æ•°æ®
        data = """a,b,c
# ä½¿ç”¨ icom æ¨¡å—çš„ get_handle æ–¹æ³•æ‰“å¼€æ•°æ®æµï¼Œå¹¶ä»¥äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
with icom.get_handle(StringIO(data), "rb", is_text=False) as handles:
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ç»“æœå­—èŠ‚ä¸²
    result = b""
    # æŒ‡å®šæ¯æ¬¡è¯»å–çš„å—å¤§å°ä¸º 5 å­—èŠ‚
    chunksize = 5
    # å¾ªç¯è¯»å–æ•°æ®æµä¸­çš„å†…å®¹
    while True:
        # ä»å¤„ç†å¥æŸ„ä¸­è¯»å–æŒ‡å®šå¤§å°çš„æ•°æ®å—
        chunk = handles.handle.read(chunksize)
        # ç¡®ä¿æ¯ä¸ªè¯»å–çš„å—å¤§å°ä¸è¶…è¿‡æŒ‡å®šçš„ chunksize
        assert len(chunk) <= chunksize
        # å¦‚æœå®é™…è¯»å–çš„å—å¤§å°å°äº chunksizeï¼Œåˆ™å¯èƒ½å·²ç»åˆ°è¾¾æ–‡ä»¶æœ«å°¾
        if len(chunk) < chunksize:
            # åœ¨è¯»å–åˆ°æ–‡ä»¶æœ«å°¾æ—¶ï¼Œç¡®ä¿å†æ¬¡è¯»å–è¿”å›ç©ºå†…å®¹
            assert len(handles.handle.read()) == 0
            # å°†æœ€åä¸€ä¸ªå—æ·»åŠ åˆ°ç»“æœä¸­
            result += chunk
            break
        # å°†è¯»å–çš„å—æ·»åŠ åˆ°ç»“æœä¸­
        result += chunk
    # æ–­è¨€æœ€ç»ˆçš„ç»“æœä¸åŸå§‹æ•°æ®ç¼–ç æˆ utf-8 åç›¸åŒ
    assert result == data.encode("utf-8")

# æµ‹è¯• pyarrow æ˜¯å¦èƒ½å¤Ÿå¤„ç†é€šè¿‡ get_handle æ‰“å¼€çš„æ–‡ä»¶
def test_get_handle_pyarrow_compat(self):
    # ä½¿ç”¨ pytest çš„ importorskip æ–¹æ³•å¯¼å…¥ pyarrow.csv æ¨¡å—ï¼Œå¦‚æœå¯¼å…¥å¤±è´¥åˆ™è·³è¿‡æµ‹è¯•
    pa_csv = pytest.importorskip("pyarrow.csv")
    
    # å®šä¹‰åŒ…å«ä¸åŒå­—ç¬¦é›†çš„æµ‹è¯•æ•°æ®
    data = """a,b,c
1,2,3
Â©,Â®,Â®
Look,a snake,ğŸ"""
    
    # å®šä¹‰é¢„æœŸçš„ Pandas DataFrame ç»“æœ
    expected = pd.DataFrame(
        {"a": ["1", "Â©", "Look"], "b": ["2", "Â®", "a snake"], "c": ["3", "Â®", "ğŸ"]}
    )
    
    # ä½¿ç”¨ StringIO åˆ›å»ºä¸€ä¸ªæ•°æ®æµ
    s = StringIO(data)
    # ä½¿ç”¨ icom æ¨¡å—çš„ get_handle æ–¹æ³•æ‰“å¼€æ•°æ®æµï¼Œå¹¶ä»¥äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
    with icom.get_handle(s, "rb", is_text=False) as handles:
        # ä½¿ç”¨ pyarrow.csv æ¨¡å—è¯»å–å¤„ç†å¥æŸ„ä¸­çš„å†…å®¹ï¼Œå¹¶è½¬æ¢ä¸º Pandas DataFrame
        df = pa_csv.read_csv(handles.handle).to_pandas()
        # æ–­è¨€è¯»å–çš„ DataFrame ä¸é¢„æœŸçš„ DataFrame ç›¸ç­‰
        tm.assert_frame_equal(df, expected)
        # æ–­è¨€æ•°æ®æµæ²¡æœ‰è¢«å…³é—­
        assert not s.closed

# æµ‹è¯•è¿­ä»£å™¨åŠŸèƒ½
def test_iterator(self):
    # ä½¿ç”¨ pandas çš„ read_csv æ–¹æ³•è¯»å–æ•°æ®æµ self.data1ï¼Œå¹¶æŒ‡å®š chunksize ä¸º 1
    with pd.read_csv(StringIO(self.data1), chunksize=1) as reader:
        # å°†æ‰€æœ‰ chunk åˆå¹¶æˆä¸€ä¸ª DataFrameï¼Œå¿½ç•¥ç´¢å¼•
        result = pd.concat(reader, ignore_index=True)
    # ä½¿ç”¨ pandas è¯»å–æ•°æ®æµ self.data1 ä½œä¸ºé¢„æœŸç»“æœ
    expected = pd.read_csv(StringIO(self.data1))
    # æ–­è¨€åˆå¹¶åçš„ç»“æœ DataFrame ä¸é¢„æœŸçš„ DataFrame ç›¸ç­‰
    tm.assert_frame_equal(result, expected)
    
    # GH12153
    # ä½¿ç”¨ pandas çš„ read_csv æ–¹æ³•è¯»å–æ•°æ®æµ self.data1ï¼Œå¹¶æŒ‡å®š chunksize ä¸º 1
    with pd.read_csv(StringIO(self.data1), chunksize=1) as it:
        # è¯»å–ç¬¬ä¸€ä¸ª chunkï¼Œå¹¶ä¸é¢„æœŸçš„ç¬¬ä¸€è¡Œ DataFrame ç›¸æ¯”è¾ƒ
        first = next(it)
        tm.assert_frame_equal(first, expected.iloc[[0]])
        # åˆå¹¶å‰©ä½™çš„ chunkï¼Œå¹¶ä¸é¢„æœŸçš„ç¬¬äºŒè¡Œè‡³æœ«å°¾ DataFrame ç›¸æ¯”è¾ƒ
        tm.assert_frame_equal(pd.concat(it), expected.iloc[1:])

# å‚æ•°åŒ–æµ‹è¯•ï¼ŒéªŒè¯ä¸åŒçš„è¯»å–æ–¹æ³•åœ¨æ–‡ä»¶ä¸å­˜åœ¨æ—¶çš„å¼‚å¸¸å¤„ç†
@pytest.mark.skipif(WASM, reason="limited file system access on WASM")
@pytest.mark.parametrize(
    "reader, module, error_class, fn_ext",
    [
        (pd.read_csv, "os", FileNotFoundError, "csv"),
        (pd.read_fwf, "os", FileNotFoundError, "txt"),
        (pd.read_excel, "xlrd", FileNotFoundError, "xlsx"),
        (pd.read_feather, "pyarrow", OSError, "feather"),
        (pd.read_hdf, "tables", FileNotFoundError, "h5"),
        (pd.read_stata, "os", FileNotFoundError, "dta"),
        (pd.read_sas, "os", FileNotFoundError, "sas7bdat"),
        (pd.read_json, "os", FileNotFoundError, "json"),
        (pd.read_pickle, "os", FileNotFoundError, "pickle"),
    ],
)
    # å®šä¹‰æµ‹è¯•æ–¹æ³•test_read_non_existentï¼Œç”¨äºæµ‹è¯•è¯»å–ä¸å­˜åœ¨æ–‡ä»¶æ—¶çš„æƒ…å†µï¼ŒåŒ…æ‹¬è¯»å–å™¨ã€æ¨¡å—ã€é”™è¯¯ç±»å’Œæ–‡ä»¶æ‰©å±•åå‚æ•°
    def test_read_non_existent(self, reader, module, error_class, fn_ext):
        # ä½¿ç”¨pytestçš„importorskipè£…é¥°å™¨ï¼Œå¦‚æœæ¨¡å—ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
        pytest.importorskip(module)

        # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼ŒæŒ‡å‘ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰ç›®å½•HEREä¸‹çš„"data"å­ç›®å½•ï¼Œæ–‡ä»¶ååŒ…å«ç»™å®šçš„æ–‡ä»¶æ‰©å±•åfn_ext
        path = os.path.join(HERE, "data", "does_not_exist." + fn_ext)

        # å®šä¹‰å¤šä¸ªæœŸæœ›çš„é”™è¯¯æ¶ˆæ¯ï¼Œç”¨äºåŒ¹é…å¼‚å¸¸ä¿¡æ¯ä¸­çš„å¤šç§å¯èƒ½æƒ…å†µ
        msg1 = rf"File (b')?.+does_not_exist\.{fn_ext}'? does not exist"
        msg2 = rf"\[Errno 2\] No such file or directory: '.+does_not_exist\.{fn_ext}'"
        msg3 = "Expected object or value"
        msg4 = "path_or_buf needs to be a string file path or file-like"
        msg5 = (
            rf"\[Errno 2\] File .+does_not_exist\.{fn_ext} does not exist: "
            rf"'.+does_not_exist\.{fn_ext}'"
        )
        msg6 = rf"\[Errno 2\] æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•: '.+does_not_exist\.{fn_ext}'"
        msg7 = (
            rf"\[Errno 2\] File o directory non esistente: '.+does_not_exist\.{fn_ext}'"
        )
        msg8 = rf"Failed to open local file.+does_not_exist\.{fn_ext}"

        # ä½¿ç”¨pytestçš„raisesæ–­è¨€æ¥éªŒè¯reader(path)æ“ä½œæŠ›å‡ºerror_classç±»å‹çš„å¼‚å¸¸ï¼Œå¹¶åŒ¹é…é¢„æœŸçš„é”™è¯¯æ¶ˆæ¯
        with pytest.raises(
            error_class,
            match=rf"({msg1}|{msg2}|{msg3}|{msg4}|{msg5}|{msg6}|{msg7}|{msg8})",
        ):
            reader(path)

    # ä½¿ç”¨pytestçš„parametrizeè£…é¥°å™¨å®šä¹‰å¤šç»„å‚æ•°åŒ–æµ‹è¯•
    @pytest.mark.parametrize(
        "method, module, error_class, fn_ext",
        [
            (pd.DataFrame.to_csv, "os", OSError, "csv"),
            (pd.DataFrame.to_html, "os", OSError, "html"),
            (pd.DataFrame.to_excel, "xlrd", OSError, "xlsx"),
            (pd.DataFrame.to_feather, "pyarrow", OSError, "feather"),
            (pd.DataFrame.to_parquet, "pyarrow", OSError, "parquet"),
            (pd.DataFrame.to_stata, "os", OSError, "dta"),
            (pd.DataFrame.to_json, "os", OSError, "json"),
            (pd.DataFrame.to_pickle, "os", OSError, "pickle"),
        ],
    )
    # NOTE: Missing parent directory for pd.DataFrame.to_hdf is handled by PyTables
    # å®šä¹‰æµ‹è¯•æ–¹æ³•test_write_missing_parent_directoryï¼Œç”¨äºæµ‹è¯•å†™å…¥æ—¶ç¼ºå¤±çˆ¶ç›®å½•çš„æƒ…å†µï¼ŒåŒ…æ‹¬æ–¹æ³•ã€æ¨¡å—ã€é”™è¯¯ç±»å’Œæ–‡ä»¶æ‰©å±•åå‚æ•°
    def test_write_missing_parent_directory(self, method, module, error_class, fn_ext):
        # ä½¿ç”¨pytestçš„importorskipè£…é¥°å™¨ï¼Œå¦‚æœæ¨¡å—ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
        pytest.importorskip(module)

        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„DataFrameå¯¹è±¡dummy_frameï¼Œç”¨äºæµ‹è¯•å†™å…¥æ“ä½œ
        dummy_frame = pd.DataFrame({"a": [1, 2, 3], "b": [2, 3, 4], "c": [3, 4, 5]})

        # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼ŒæŒ‡å‘ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹"missing_folder"ä¸‹çš„æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰ç›®å½•HEREï¼Œæ–‡ä»¶ååŒ…å«ç»™å®šçš„æ–‡ä»¶æ‰©å±•åfn_ext
        path = os.path.join(HERE, "data", "missing_folder", "does_not_exist." + fn_ext)

        # ä½¿ç”¨pytestçš„raisesæ–­è¨€æ¥éªŒè¯method(dummy_frame, path)æ“ä½œæŠ›å‡ºerror_classç±»å‹çš„å¼‚å¸¸ï¼Œå¹¶åŒ¹é…é¢„æœŸçš„é”™è¯¯æ¶ˆæ¯
        with pytest.raises(
            error_class,
            match=r"Cannot save file into a non-existent directory: .*missing_folder",
        ):
            method(dummy_frame, path)

    # ä½¿ç”¨pytestçš„mark.skipifè£…é¥°å™¨ï¼Œå¦‚æœWASMä¸ºTrueï¼Œåˆ™è·³è¿‡æ­¤æµ‹è¯•ï¼ŒåŸå› æ˜¯WASMç¯å¢ƒä¸‹æœ‰é™çš„æ–‡ä»¶ç³»ç»Ÿè®¿é—®æƒé™
    @pytest.mark.skipif(WASM, reason="limited file system access on WASM")
    # ä½¿ç”¨ pytest çš„ parametrize è£…é¥°å™¨ï¼Œä¸ºæµ‹è¯•æ–¹æ³•å‚æ•°åŒ–ï¼Œä»¥ä¾¿å¤šæ¬¡è¿è¡Œæµ‹è¯•ç”¨ä¾‹
    @pytest.mark.parametrize(
        "reader, module, error_class, fn_ext",
        [
            # å‚æ•°åŒ–æµ‹è¯•æ•°æ®ï¼ŒåŒ…æ‹¬è¯»å–å‡½æ•°ã€æ‰€éœ€æ¨¡å—ã€é¢„æœŸé”™è¯¯ç±»å‹ã€æ–‡ä»¶æ‰©å±•å
            (pd.read_csv, "os", FileNotFoundError, "csv"),
            (pd.read_table, "os", FileNotFoundError, "csv"),
            (pd.read_fwf, "os", FileNotFoundError, "txt"),
            (pd.read_excel, "xlrd", FileNotFoundError, "xlsx"),
            (pd.read_feather, "pyarrow", OSError, "feather"),
            (pd.read_hdf, "tables", FileNotFoundError, "h5"),
            (pd.read_stata, "os", FileNotFoundError, "dta"),
            (pd.read_sas, "os", FileNotFoundError, "sas7bdat"),
            (pd.read_json, "os", FileNotFoundError, "json"),
            (pd.read_pickle, "os", FileNotFoundError, "pickle"),
        ],
    )
    # å®šä¹‰æµ‹è¯•æ–¹æ³•ï¼Œç”¨äºéªŒè¯æ–‡ä»¶è¯»å–å‡½æ•°åœ¨ç”¨æˆ·ä¸»ç›®å½•æ‰©å±•åæ˜¯å¦èƒ½æ­£ç¡®å¤„ç†å¼‚å¸¸æƒ…å†µ
    def test_read_expands_user_home_dir(
        self, reader, module, error_class, fn_ext, monkeypatch
    ):
        # å¦‚æœæ‰€éœ€æ¨¡å—ä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•
        pytest.importorskip(module)

        # æ„é€ æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«ç”¨æˆ·ä¸»ç›®å½•çš„æ‰©å±•ï¼Œå¹¶è®¾ç½® monkeypatch ä»¥æ¨¡æ‹Ÿç”¨æˆ·ä¸»ç›®å½•çš„è·¯å¾„
        path = os.path.join("~", "does_not_exist." + fn_ext)
        monkeypatch.setattr(icom, "_expand_user", lambda x: os.path.join("foo", x))

        # å®šä¹‰åŒ¹é…é”™è¯¯æ¶ˆæ¯çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œç”¨äºæ–­è¨€ç‰¹å®šå¼‚å¸¸è¢«æ­£ç¡®æŠ›å‡º
        msg1 = rf"File (b')?.+does_not_exist\.{fn_ext}'? does not exist"
        msg2 = rf"\[Errno 2\] No such file or directory: '.+does_not_exist\.{fn_ext}'"
        msg3 = "Unexpected character found when decoding 'false'"
        msg4 = "path_or_buf needs to be a string file path or file-like"
        msg5 = (
            rf"\[Errno 2\] File .+does_not_exist\.{fn_ext} does not exist: "
            rf"'.+does_not_exist\.{fn_ext}'"
        )
        msg6 = rf"\[Errno 2\] æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•: '.+does_not_exist\.{fn_ext}'"
        msg7 = (
            rf"\[Errno 2\] File o directory non esistente: '.+does_not_exist\.{fn_ext}'"
        )
        msg8 = rf"Failed to open local file.+does_not_exist\.{fn_ext}"

        # ä½¿ç”¨ pytest.raises æ–­è¨€ç‰¹å®šçš„å¼‚å¸¸è¢«æŠ›å‡ºï¼Œå¹¶åŒ¹é…å…¶ä¸­ä»»ä½•ä¸€ä¸ªé¢„å®šä¹‰çš„é”™è¯¯æ¶ˆæ¯æ¨¡å¼
        with pytest.raises(
            error_class,
            match=rf"({msg1}|{msg2}|{msg3}|{msg4}|{msg5}|{msg6}|{msg7}|{msg8})",
        ):
            # è°ƒç”¨è¢«æµ‹è¯•çš„æ–‡ä»¶è¯»å–å‡½æ•°ï¼Œä¼ å…¥æ„é€ çš„æ–‡ä»¶è·¯å¾„ï¼ŒéªŒè¯å¼‚å¸¸æƒ…å†µ
            reader(path)
    # ä½¿ç”¨ pytest çš„å‚æ•°åŒ–è£…é¥°å™¨æ¥å®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è¯»å–å‡½æ•°ã€ä¾èµ–æ¨¡å—å’Œæ–‡ä»¶è·¯å¾„
    @pytest.mark.parametrize(
        "reader, module, path",
        [
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– CSV æ–‡ä»¶
            (pd.read_csv, "os", ("io", "data", "csv", "iris.csv")),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å–æ–‡æœ¬æ–‡ä»¶ï¼ˆé€šç”¨ï¼‰
            (pd.read_table, "os", ("io", "data", "csv", "iris.csv")),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å–å›ºå®šå®½åº¦æ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶
            (
                pd.read_fwf,
                "os",
                ("io", "data", "fixed_width", "fixed_width_format.txt"),
            ),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– Excel æ–‡ä»¶
            (pd.read_excel, "xlrd", ("io", "data", "excel", "test1.xlsx")),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– Feather æ–‡ä»¶
            (
                pd.read_feather,
                "pyarrow",
                ("io", "data", "feather", "feather-0_3_1.feather"),
            ),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– HDF5 æ–‡ä»¶
            (
                pd.read_hdf,
                "tables",
                ("io", "data", "legacy_hdf", "pytables_native2.h5"),
            ),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– Stata æ–‡ä»¶
            (pd.read_stata, "os", ("io", "data", "stata", "stata10_115.dta")),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– SAS æ–‡ä»¶
            (pd.read_sas, "os", ("io", "sas", "data", "test1.sas7bdat")),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– JSON æ–‡ä»¶
            (pd.read_json, "os", ("io", "json", "data", "tsframe_v012.json")),
            # æµ‹è¯•ç”¨ä¾‹ï¼šè¯»å– Pickle æ–‡ä»¶
            (
                pd.read_pickle,
                "os",
                ("io", "data", "pickle", "categorical.0.25.0.pickle"),
            ),
        ],
    )
    # å®šä¹‰æµ‹è¯•æ–¹æ³•ï¼šæµ‹è¯•è¯»å–ä¸åŒæ–‡ä»¶è·¯å¾„ä¸‹çš„æ•°æ®ï¼Œå¹¶è¿›è¡Œæ¯”è¾ƒ
    def test_read_fspath_all(self, reader, module, path, datapath):
        # ä½¿ç”¨ pytest çš„ importorskip å‡½æ•°å¯¼å…¥å¿…è¦çš„æ¨¡å—æˆ–è·³è¿‡æµ‹è¯•
        pytest.importorskip(module)
        # è°ƒç”¨ datapath å‡½æ•°è·å–æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        path = datapath(*path)

        # åˆ›å»º CustomFSPath å¯¹è±¡
        mypath = CustomFSPath(path)
        # ä½¿ç”¨æŒ‡å®šçš„è¯»å–å‡½æ•°è¯»å–æ•°æ®
        result = reader(mypath)
        # è¯»å–åŸå§‹æ–‡ä»¶æ•°æ®
        expected = reader(path)

        # æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ–‡ä»¶ç±»å‹ï¼Œé€‰æ‹©ä¸åŒçš„æ¯”è¾ƒæ–¹æ³•
        if path.endswith(".pickle"):
            # å¦‚æœæ˜¯ Pickle æ–‡ä»¶ï¼Œä½¿ç”¨ assert_categorical_equal æ–¹æ³•æ¯”è¾ƒç»“æœ
            # è¿™é‡Œå‡è®¾æ˜¯æ¯”è¾ƒåˆ†ç±»æ•°æ®
            tm.assert_categorical_equal(result, expected)
        else:
            # å¦åˆ™ä½¿ç”¨ assert_frame_equal æ–¹æ³•æ¯”è¾ƒç»“æœï¼Œå‡è®¾æ˜¯æ¯”è¾ƒæ•°æ®æ¡†æ¶
            tm.assert_frame_equal(result, expected)
    # å®šä¹‰ä¸€ä¸ªæµ‹è¯•å‡½æ•°ï¼Œç”¨äºæµ‹è¯•ä»¥æŒ‡å®šçš„å†™å…¥å™¨åç§°å’Œå‚æ•°å†™å…¥æ•°æ®åˆ°æ–‡ä»¶ç³»ç»Ÿè·¯å¾„çš„åŠŸèƒ½
    def test_write_fspath_all(self, writer_name, writer_kwargs, module):
        # å¦‚æœå†™å…¥å™¨åç§°åœ¨ ["to_latex"] ä¸­ï¼Œéœ€è¦ä½¿ç”¨ Styler å®ç°ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
        if writer_name in ["to_latex"]:  # uses Styler implementation
            pytest.importorskip("jinja2")
        
        # ç¡®ä¿å­—ç¬¦ä¸²å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„å‚æ•°çš„æœ‰æ•ˆæ€§å¹¶è¿›è¡Œæ¸…ç†
        p1 = tm.ensure_clean("string")
        p2 = tm.ensure_clean("fspath")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ DataFrame
        df = pd.DataFrame({"A": [1, 2]})

        # ä½¿ç”¨ä¸¤ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ†åˆ«æ‰“å¼€å­—ç¬¦ä¸²å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„
        with p1 as string, p2 as fspath:
            # æ ¹æ®æŒ‡å®šçš„æ¨¡å—å¯¼å…¥å¿…è¦çš„ä¾èµ–
            pytest.importorskip(module)
            
            # ä½¿ç”¨è‡ªå®šä¹‰çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„å¯¹è±¡åˆ›å»ºè·¯å¾„
            mypath = CustomFSPath(fspath)
            
            # è·å– DataFrame çš„æŒ‡å®šå†™å…¥å™¨æ–¹æ³•
            writer = getattr(df, writer_name)

            # å°† DataFrame å†…å®¹åˆ†åˆ«å†™å…¥å­—ç¬¦ä¸²å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„
            writer(string, **writer_kwargs)
            writer(mypath, **writer_kwargs)
            
            # æ‰“å¼€å­—ç¬¦ä¸²å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„å¯¹åº”çš„æ–‡ä»¶è¿›è¡Œè¯»å–æ¯”è¾ƒ
            with open(string, "rb") as f_str, open(fspath, "rb") as f_path:
                if writer_name == "to_excel":
                    # å¦‚æœæ˜¯å†™å…¥ Excel æ ¼å¼ï¼Œè¯»å–ç»“æœè¿›è¡Œ DataFrame æ¯”è¾ƒ
                    # Excel æ–‡ä»¶åŒ…å«æ—¶é—´åˆ›å»ºæ•°æ®ï¼Œå¯èƒ½å¯¼è‡´æŒç»­é›†æˆå¤±è´¥ï¼Œå› æ­¤ç‰¹æ®Šå¤„ç†
                    result = pd.read_excel(f_str, **writer_kwargs)
                    expected = pd.read_excel(f_path, **writer_kwargs)
                    tm.assert_frame_equal(result, expected)
                else:
                    # å¦åˆ™ï¼Œç›´æ¥è¯»å–å­—ç¬¦ä¸²å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„ä¸­çš„æ•°æ®å¹¶è¿›è¡Œæ¯”è¾ƒ
                    result = f_str.read()
                    expected = f_path.read()
                    assert result == expected

    # å®šä¹‰å¦ä¸€ä¸ªæµ‹è¯•å‡½æ•°ï¼Œæµ‹è¯•å°† DataFrame å†™å…¥ HDF5 æ ¼å¼æ–‡ä»¶å¹¶æ¯”è¾ƒç»“æœ
    def test_write_fspath_hdf5(self):
        # åŒ test_write_fspath_allï¼Œä½† HDF5 æ–‡ä»¶ä¸ä¸€å®šæ˜¯å­—èŠ‚å®Œå…¨ç›¸åŒçš„ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        # å› æ­¤ï¼Œè¯»å–åæ¯”è¾ƒæ•°æ®çš„ç›¸ç­‰æ€§
        pytest.importorskip("tables")

        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ DataFrame
        df = pd.DataFrame({"A": [1, 2]})
        
        # ç¡®ä¿å­—ç¬¦ä¸²å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„å‚æ•°çš„æœ‰æ•ˆæ€§å¹¶è¿›è¡Œæ¸…ç†
        p1 = tm.ensure_clean("string")
        p2 = tm.ensure_clean("fspath")

        # ä½¿ç”¨ä¸¤ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ†åˆ«æ‰“å¼€å­—ç¬¦ä¸²å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„
        with p1 as string, p2 as fspath:
            # ä½¿ç”¨è‡ªå®šä¹‰çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„å¯¹è±¡åˆ›å»ºè·¯å¾„
            mypath = CustomFSPath(fspath)
            
            # å°† DataFrame å†™å…¥ HDF5 æ ¼å¼æ–‡ä»¶ï¼Œä½¿ç”¨ç›¸åŒçš„é”®å "bar"
            df.to_hdf(mypath, key="bar")
            df.to_hdf(string, key="bar")

            # è¯»å–å¹¶æ¯”è¾ƒ HDF5 æ–‡ä»¶ä¸­çš„æ•°æ®
            result = pd.read_hdf(fspath, key="bar")
            expected = pd.read_hdf(string, key="bar")

        # ä½¿ç”¨æµ‹è¯•æ¡†æ¶æä¾›çš„æ–¹æ³•æ¯”è¾ƒä¸¤ä¸ª DataFrame æ˜¯å¦ç›¸ç­‰
        tm.assert_frame_equal(result, expected)
@pytest.fixture
def mmap_file(datapath):
    return datapath("io", "data", "csv", "test_mmap.csv")


class TestMMapWrapper:
    @pytest.mark.skipif(WASM, reason="limited file system access on WASM")
    def test_constructor_bad_file(self, mmap_file):
        # åˆ›å»ºä¸€ä¸ªä¸æ˜¯æ–‡ä»¶çš„ StringIO å¯¹è±¡
        non_file = StringIO("I am not a file")
        # å®šä¹‰ä¸€ä¸ªåŒ¿åå‡½æ•°ï¼Œæ¨¡æ‹Ÿ fileno æ–¹æ³•è¿”å›å€¼ä¸º -1
        non_file.fileno = lambda: -1

        # æ ¹æ®å¹³å°ä¸åŒè®¾ç½®ä¸åŒçš„é”™è¯¯æ¶ˆæ¯å’Œå¼‚å¸¸ç±»
        if is_platform_windows():
            msg = "The parameter is incorrect"
            err = OSError
        else:
            msg = "[Errno 22]"
            err = mmap.error

        # ä½¿ç”¨ pytest æ¥éªŒè¯è°ƒç”¨ _maybe_memory_map æ–¹æ³•æ—¶æ˜¯å¦ä¼šæŠ›å‡ºç‰¹å®šçš„å¼‚å¸¸
        with pytest.raises(err, match=msg):
            icom._maybe_memory_map(non_file, True)

        # æ‰“å¼€çœŸå®çš„æ–‡ä»¶ï¼Œç¡®ä¿å…¶æ­£å¸¸æ‰“å¼€
        with open(mmap_file, encoding="utf-8") as target:
            pass

        # ä½¿ç”¨ pytest æ¥éªŒè¯å½“æ–‡ä»¶å…³é—­åè°ƒç”¨ _maybe_memory_map æ–¹æ³•æ˜¯å¦ä¼šæŠ›å‡ºç‰¹å®šçš„å¼‚å¸¸
        msg = "I/O operation on closed file"
        with pytest.raises(ValueError, match=msg):
            icom._maybe_memory_map(target, True)

    @pytest.mark.skipif(WASM, reason="limited file system access on WASM")
    def test_next(self, mmap_file):
        # æ‰“å¼€æ–‡ä»¶ä»¥è¯»å–å†…å®¹
        with open(mmap_file, encoding="utf-8") as target:
            # è¯»å–æ–‡ä»¶æ‰€æœ‰è¡Œ
            lines = target.readlines()

            # ä½¿ç”¨ icom.get_handle æ–¹æ³•å¤„ç†æ–‡ä»¶å¥æŸ„ï¼Œç¡®ä¿æ–‡ä»¶é€šè¿‡å†…å­˜æ˜ å°„æ–¹å¼æ‰“å¼€
            with icom.get_handle(
                target, "r", is_text=True, memory_map=True
            ) as wrappers:
                wrapper = wrappers.handle
                # æ–­è¨€å¤„ç†å™¨çš„ç¼“å†²åŒºä¸º mmap.mmap ç±»å‹
                assert isinstance(wrapper.buffer.buffer, mmap.mmap)

                # é€è¡Œæ¯”è¾ƒå¤„ç†å™¨è¿”å›çš„ä¸‹ä¸€è¡Œå†…å®¹ä¸å®é™…æ–‡ä»¶ä¸­çš„å†…å®¹
                for line in lines:
                    next_line = next(wrapper)
                    assert next_line.strip() == line.strip()

                # ä½¿ç”¨ pytest éªŒè¯æ–‡ä»¶è¯»å–åˆ°æœ«å°¾æ—¶è°ƒç”¨ next æ–¹æ³•æ˜¯å¦ä¼šæŠ›å‡º StopIteration å¼‚å¸¸
                with pytest.raises(StopIteration, match=r"^$"):
                    next(wrapper)

    def test_unknown_engine(self):
        # ç¡®ä¿åœ¨æµ‹è¯•æœŸé—´è·¯å¾„å¹²å‡€ï¼Œåˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„ DataFrameï¼Œå¹¶å°†å…¶ä¿å­˜ä¸º CSV æ–‡ä»¶
        with tm.ensure_clean() as path:
            df = pd.DataFrame(
                1.1 * np.arange(120).reshape((30, 4)),
                columns=pd.Index(list("ABCD"), dtype=object),
                index=pd.Index([f"i-{i}" for i in range(30)], dtype=object),
            )
            df.to_csv(path)
            # ä½¿ç”¨ pytest éªŒè¯è°ƒç”¨ pd.read_csv æ—¶ä½¿ç”¨æœªçŸ¥çš„å¼•æ“å‚æ•°æ˜¯å¦ä¼šæŠ›å‡º ValueError å¼‚å¸¸
            with pytest.raises(ValueError, match="Unknown engine"):
                pd.read_csv(path, engine="pyt")

    def test_binary_mode(self):
        """
        'encoding' shouldn't be passed to 'open' in binary mode.

        GH 35058
        """
        # ç¡®ä¿åœ¨æµ‹è¯•æœŸé—´è·¯å¾„å¹²å‡€ï¼Œåˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„ DataFrameï¼Œå¹¶å°†å…¶ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ¨¡å¼çš„ CSV æ–‡ä»¶
        with tm.ensure_clean() as path:
            df = pd.DataFrame(
                1.1 * np.arange(120).reshape((30, 4)),
                columns=pd.Index(list("ABCD"), dtype=object),
                index=pd.Index([f"i-{i}" for i in range(30)], dtype=object),
            )
            df.to_csv(path, mode="w+b")
            # ä½¿ç”¨ tm.assert_frame_equal æ–¹æ³•éªŒè¯ DataFrame åœ¨è¯»å–åä¸åŸå§‹ DataFrame æ˜¯å¦ç›¸ç­‰
            tm.assert_frame_equal(df, pd.read_csv(path, index_col=0))

    @pytest.mark.parametrize("encoding", ["utf-16", "utf-32"])
    @pytest.mark.parametrize("compression_", ["bz2", "xz"])
    # å®šä¹‰ä¸€ä¸ªæµ‹è¯•æ–¹æ³•ï¼Œç”¨äºæ£€æŸ¥åœ¨æŒ‡å®šç¼–ç å’Œå‹ç¼©æ–¹å¼ä¸‹æ˜¯å¦ç¼ºå°‘ UTF BOMï¼ˆå­—èŠ‚é¡ºåºæ ‡è®°ï¼‰è­¦å‘Š
    def test_warning_missing_utf_bom(self, encoding, compression_):
        """
        bz2 and xz do not write the byte order mark (BOM) for utf-16/32.

        https://stackoverflow.com/questions/55171439

        GH 35681
        """
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ•°å€¼æ•°æ®çš„ Pandas DataFrameï¼Œ30è¡Œ4åˆ—
        df = pd.DataFrame(
            1.1 * np.arange(120).reshape((30, 4)),
            columns=pd.Index(list("ABCD"), dtype=object),
            index=pd.Index([f"i-{i}" for i in range(30)], dtype=object),
        )
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¸Šä¸‹æ–‡ç¡®ä¿æ“ä½œåæ–‡ä»¶ç³»ç»Ÿçš„å¹²å‡€çŠ¶æ€
        with tm.ensure_clean() as path:
            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç¡®ä¿åœ¨å†™å…¥æ—¶äº§ç”Ÿ Unicode è­¦å‘Šï¼Œå¹¶åŒ¹é…æŒ‡å®šçš„è­¦å‘Šä¿¡æ¯
            with tm.assert_produces_warning(UnicodeWarning, match="byte order mark"):
                df.to_csv(path, compression=compression_, encoding=encoding)

            # è¯»å–æ“ä½œåº”è¯¥å¤±è´¥ï¼ˆå¦åˆ™ä¸éœ€è¦è­¦å‘Šï¼‰
            # å®šä¹‰ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åŒ¹é…å¯èƒ½çš„ Unicode é”™è¯¯ä¿¡æ¯
            msg = (
                r"UTF-\d+ stream does not start with BOM|"
                r"'utf-\d+' codec can't decode byte"
            )
            # ä½¿ç”¨ pytest æ–­è¨€åº”æŠ›å‡º UnicodeErrorï¼Œå¹¶åŒ¹é…å®šä¹‰çš„é”™è¯¯æ¶ˆæ¯
            with pytest.raises(UnicodeError, match=msg):
                pd.read_csv(path, compression=compression_, encoding=encoding)
# å®šä¹‰ä¸€ä¸ªæµ‹è¯•å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥æ˜¯å¦ä¸º fsspec URL
def test_is_fsspec_url():
    # æ–­è¨€ä»¥ä¸‹ URL æ˜¯ fsspec URL
    assert icom.is_fsspec_url("gcs://pandas/somethingelse.com")
    assert icom.is_fsspec_url("gs://pandas/somethingelse.com")
    # ä¸‹é¢è¿™ä¸ªæ˜¯å”¯ä¸€ä¸éœ€è¦ fsspec å¤„ç†çš„è¿œç¨‹ URL
    assert not icom.is_fsspec_url("http://pandas/somethingelse.com")
    assert not icom.is_fsspec_url("random:pandas/somethingelse.com")
    assert not icom.is_fsspec_url("/local/path")
    assert not icom.is_fsspec_url("relative/local/path")
    # å­—ç¬¦ä¸²ä¸­çš„ fsspec URL ä¸åº”è¯¥è¢«è¯†åˆ«
    assert not icom.is_fsspec_url("this is not fsspec://url")
    assert not icom.is_fsspec_url("{'url': 'gs://pandas/somethingelse.com'}")
    # æ¥å—ç¬¦åˆ RFC 3986 æ ‡å‡†çš„æ‰€æœ‰ URL
    assert icom.is_fsspec_url("RFC-3986+compliant.spec://something")

# å‚æ•°åŒ–æµ‹è¯•ï¼Œæµ‹è¯•ä¸åŒçš„ç¼–ç å’Œæ ¼å¼
@pytest.mark.parametrize("encoding", [None, "utf-8"])
@pytest.mark.parametrize("format", ["csv", "json"])
def test_codecs_encoding(encoding, format):
    # GH39247
    # åˆ›å»ºä¸€ä¸ªæœŸæœ›çš„ DataFrame
    expected = pd.DataFrame(
        1.1 * np.arange(120).reshape((30, 4)),
        columns=pd.Index(list("ABCD"), dtype=object),
        index=pd.Index([f"i-{i}" for i in range(30)], dtype=object),
    )
    # ç¡®ä¿åœ¨ä¸´æ—¶è·¯å¾„ä¸Šè¿›è¡Œæ“ä½œ
    with tm.ensure_clean() as path:
        # ä½¿ç”¨ç¼–ç æ–¹å¼æ‰“å¼€æ–‡ä»¶ï¼Œå†™å…¥æœŸæœ›çš„æ ¼å¼æ•°æ®
        with codecs.open(path, mode="w", encoding=encoding) as handle:
            getattr(expected, f"to_{format}")(handle)
        # ä½¿ç”¨ç¼–ç æ–¹å¼æ‰“å¼€æ–‡ä»¶ï¼Œè¯»å–æ•°æ®åˆ° DataFrame
        with codecs.open(path, mode="r", encoding=encoding) as handle:
            if format == "csv":
                df = pd.read_csv(handle, index_col=0)
            else:
                df = pd.read_json(handle)
    # æ–­è¨€æœŸæœ›çš„ DataFrame å’Œè¯»å–å‡ºçš„ DataFrame ç›¸ç­‰
    tm.assert_frame_equal(expected, df)

# æµ‹è¯•è·å–ç¼–ç å™¨å’Œè§£ç å™¨
def test_codecs_get_writer_reader():
    # GH39247
    # åˆ›å»ºä¸€ä¸ªæœŸæœ›çš„ DataFrame
    expected = pd.DataFrame(
        1.1 * np.arange(120).reshape((30, 4)),
        columns=pd.Index(list("ABCD"), dtype=object),
        index=pd.Index([f"i-{i}" for i in range(30)], dtype=object),
    )
    # ç¡®ä¿åœ¨ä¸´æ—¶è·¯å¾„ä¸Šè¿›è¡Œæ“ä½œ
    with tm.ensure_clean() as path:
        # ä½¿ç”¨ UTF-8 ç¼–ç æ‰“å¼€æ–‡ä»¶ï¼Œå†™å…¥ DataFrame çš„ CSV æ ¼å¼æ•°æ®
        with open(path, "wb") as handle:
            with codecs.getwriter("utf-8")(handle) as encoded:
                expected.to_csv(encoded)
        # ä½¿ç”¨ UTF-8 è§£ç æ‰“å¼€æ–‡ä»¶ï¼Œè¯»å–æ•°æ®åˆ° DataFrame
        with open(path, "rb") as handle:
            with codecs.getreader("utf-8")(handle) as encoded:
                df = pd.read_csv(encoded, index_col=0)
    # æ–­è¨€æœŸæœ›çš„ DataFrame å’Œè¯»å–å‡ºçš„ DataFrame ç›¸ç­‰
    tm.assert_frame_equal(expected, df)

# å‚æ•°åŒ–æµ‹è¯•ï¼Œæµ‹è¯•ä¸åŒçš„ io ç±»å‹ã€æ¨¡å¼å’Œé”™è¯¯æ¶ˆæ¯
@pytest.mark.parametrize(
    "io_class,mode,msg",
    [
        (BytesIO, "t", "a bytes-like object is required, not 'str'"),
        (StringIO, "b", "string argument expected, got 'bytes'"),
    ],
)
def test_explicit_encoding(io_class, mode, msg):
    # GH39247; æ­¤æµ‹è¯•ç¡®ä¿å¦‚æœç”¨æˆ·æä¾› mode="*t" æˆ– "*b"ï¼Œåˆ™ä½¿ç”¨å®ƒ
    # åœ¨è¿™ä¸ªæµ‹è¯•æ¡ˆä¾‹ä¸­ï¼Œæ•…æ„è¯·æ±‚é”™è¯¯çš„æ¨¡å¼ä¼šå¯¼è‡´é”™è¯¯
    expected = pd.DataFrame(
        1.1 * np.arange(120).reshape((30, 4)),
        columns=pd.Index(list("ABCD"), dtype=object),
        index=pd.Index([f"i-{i}" for i in range(30)], dtype=object),
    )
    # ä½¿ç”¨æŒ‡å®šçš„è¾“å…¥è¾“å‡ºç±»å®ä¾‹åŒ–ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯¹è±¡ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™å˜é‡buffer
    with io_class() as buffer:
        # ä½¿ç”¨pytestæ–­è¨€æ•è·é¢„æœŸçš„TypeErrorå¼‚å¸¸ï¼Œå¹¶ä¸”å¼‚å¸¸æ¶ˆæ¯å¿…é¡»åŒ¹é…msgå‚æ•°
        with pytest.raises(TypeError, match=msg):
            # è°ƒç”¨expectedå¯¹è±¡çš„to_csvæ–¹æ³•ï¼Œå°†å…¶å†…å®¹ä»¥æŒ‡å®šæ¨¡å¼"w{mode}"å†™å…¥åˆ°bufferä¸­
            expected.to_csv(buffer, mode=f"w{mode}")
@pytest.mark.parametrize("encoding_errors", ["strict", "replace"])
@pytest.mark.parametrize("format", ["csv", "json"])
def test_encoding_errors(encoding_errors, format):
    # æ ‡è®°æµ‹è¯•ç”¨ä¾‹ï¼Œä½¿ç”¨å‚æ•°åŒ–æµ‹è¯•ï¼Œæµ‹è¯•ç¼–ç é”™è¯¯å¤„ç†
    msg = "'utf-8' codec can't decode byte"
    bad_encoding = b"\xe4"

    if format == "csv":
        # å¦‚æœæ ¼å¼ä¸ºcsvï¼Œåˆ›å»ºåŒ…å«é”™è¯¯ç¼–ç çš„å†…å®¹
        content = b"," + bad_encoding + b"\n" + bad_encoding * 2 + b"," + bad_encoding
        reader = partial(pd.read_csv, index_col=0)
    else:
        # å¦‚æœæ ¼å¼ä¸ºjsonï¼Œåˆ›å»ºåŒ…å«é”™è¯¯ç¼–ç çš„å†…å®¹
        content = (
            b'{"'
            + bad_encoding * 2
            + b'": {"'
            + bad_encoding
            + b'":"'
            + bad_encoding
            + b'"}}'
        )
        reader = partial(pd.read_json, orient="index")
    
    # åœ¨ä¸´æ—¶è·¯å¾„ä¸Šåˆ›å»ºæ–‡ä»¶ï¼Œå¹¶å†™å…¥å†…å®¹
    with tm.ensure_clean() as path:
        file = Path(path)
        file.write_bytes(content)

        if encoding_errors != "replace":
            # å¦‚æœä¸æ˜¯ä½¿ç”¨æ›¿æ¢ç­–ç•¥æ¥å¤„ç†ç¼–ç é”™è¯¯ï¼Œé¢„æœŸä¼šæŠ›å‡ºUnicodeDecodeErrorå¼‚å¸¸
            with pytest.raises(UnicodeDecodeError, match=msg):
                reader(path, encoding_errors=encoding_errors)
        else:
            # ä½¿ç”¨æ›¿æ¢ç­–ç•¥æ¥å¤„ç†ç¼–ç é”™è¯¯ï¼Œè¯»å–æ–‡ä»¶å¹¶éªŒè¯ç»“æœ
            df = reader(path, encoding_errors=encoding_errors)
            decoded = bad_encoding.decode(errors=encoding_errors)
            expected = pd.DataFrame({decoded: [decoded]}, index=[decoded * 2])
            tm.assert_frame_equal(df, expected)


@pytest.mark.parametrize("encoding_errors", [0, None])
def test_encoding_errors_badtype(encoding_errors):
    # æ ‡è®°æµ‹è¯•ç”¨ä¾‹ï¼Œä½¿ç”¨å‚æ•°åŒ–æµ‹è¯•ï¼Œæµ‹è¯•é”™è¯¯çš„ç¼–ç ç±»å‹å¤„ç†
    content = StringIO("A,B\n1,2\n3,4\n")
    reader = partial(pd.read_csv, encoding_errors=encoding_errors)
    expected_error = "encoding_errors must be a string, got "
    expected_error += f"{type(encoding_errors).__name__}"
    # é¢„æœŸä¼šæŠ›å‡ºå€¼é”™è¯¯ï¼ŒåŒ¹é…é”™è¯¯æ¶ˆæ¯
    with pytest.raises(ValueError, match=expected_error):
        reader(content)


def test_bad_encdoing_errors():
    # æ ‡è®°æµ‹è¯•ç”¨ä¾‹ï¼Œæµ‹è¯•é”™è¯¯çš„ç¼–ç å¤„ç†
    with tm.ensure_clean() as path:
        # ä½¿ç”¨pytesté¢„æœŸä¼šæŠ›å‡ºæŸ¥æ‰¾é”™è¯¯å¤„ç†å™¨åå­—æ—¶çš„æŸ¥æ‰¾é”™è¯¯å¼‚å¸¸
        with pytest.raises(LookupError, match="unknown error handler name"):
            icom.get_handle(path, "w", errors="bad")


@pytest.mark.skipif(WASM, reason="limited file system access on WASM")
def test_errno_attribute():
    # æ ‡è®°æµ‹è¯•ç”¨ä¾‹ï¼Œä½¿ç”¨å‚æ•°åŒ–æµ‹è¯•ï¼Œæµ‹è¯•é”™è¯¯å·å±æ€§
    with pytest.raises(FileNotFoundError, match="\\[Errno 2\\]") as err:
        # é¢„æœŸä¼šæŠ›å‡ºæ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯ï¼ŒåŒ¹é…ç‰¹å®šé”™è¯¯æ¶ˆæ¯
        pd.read_csv("doesnt_exist")
        assert err.errno == errno.ENOENT


def test_fail_mmap():
    # æ ‡è®°æµ‹è¯•ç”¨ä¾‹ï¼Œæµ‹è¯•å†…å­˜æ˜ å°„å¤±è´¥æƒ…å†µ
    with pytest.raises(UnsupportedOperation, match="fileno"):
        with BytesIO() as buffer:
            icom.get_handle(buffer, "rb", memory_map=True)


def test_close_on_error():
    # æ ‡è®°æµ‹è¯•ç”¨ä¾‹ï¼Œæµ‹è¯•é”™è¯¯æ—¶çš„å…³é—­å¤„ç†
    class TestError:
        def close(self):
            raise OSError("test")

    with pytest.raises(OSError, match="test"):
        with BytesIO() as buffer:
            with icom.get_handle(buffer, "rb") as handles:
                handles.created_handles.append(TestError())


@pytest.mark.parametrize(
    "reader",
    [
        pd.read_csv,
        pd.read_fwf,
        pd.read_excel,
        pd.read_feather,
        pd.read_hdf,
        pd.read_stata,
        pd.read_sas,
        pd.read_json,
        pd.read_pickle,
    ],
)
def test_pickle_reader(reader):
    # æ ‡è®°å‚æ•°åŒ–æµ‹è¯•ç”¨ä¾‹ï¼Œæµ‹è¯•ä¸åŒçš„æ•°æ®æ ¼å¼è¯»å–å™¨
    # æœªå®Œå…¨æ·»åŠ æ³¨é‡Šï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè¡¥å……
    # ä½¿ç”¨ BytesIO åˆ›å»ºä¸€ä¸ªå†…å­˜ç¼“å†²åŒºå¯¹è±¡ï¼Œå¯ä»¥åœ¨å…¶ä¸­å­˜å‚¨æ•°æ®
    with BytesIO() as buffer:
        # ä½¿ç”¨ pickle åº“å°† reader å¯¹è±¡åºåˆ—åŒ–å¹¶å­˜å‚¨åˆ° buffer ä¸­
        pickle.dump(reader, buffer)
```