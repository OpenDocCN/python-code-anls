# AutoGPT源码解析 4

# `autogpts/autogpt/autogpt/app/configurator.py`

这是一个 Autogpt 模型的配置模块。它通过颜色化显示信息来帮助用户设置和组织他们的自定义配置。

配置模块包含以下组件：

1. 从 `__future__` 获取所有实用的未来方法。
2. 导入 `logging` 模块以记录调试信息。
3. 导入 `pathlib` 模块以导入路径。
4. 导入 `typing` 中的 `Literal` 类型。
5. 导入 `Optional` 类型。
6. 从 `click` 模块导入 `Click` 类。
7. 从 `colorama` 模块导入 `Fore` 和 `Back` 类。
8. 从 `typing.util` 模块导入 `ApiManager` 类。
9. 从 `autogpt.config` 模块导入 `Config` 类。
10. 从 `autogpt.llm.api_manager` 模块导入 `ApiManager` 类。
11. 从 `autogpt.logs.helpers` 模块导入 `print_attribute` 和 `request_user_double_check` 函数。

配置模块的主要作用是帮助用户设置 Autogpt 模型的配置。通过使用 `Click` 类，用户可以轻松地设置和查看设置。通过导入 `ApiManager` 类，用户可以访问一个集中的 API 来配置他们的模型。通过导入 `logging` 模块，调试信息将记录到日志中。


```py
"""Configurator module."""
from __future__ import annotations

import logging
from pathlib import Path
from typing import Literal, Optional

import click
from colorama import Back, Fore, Style

from autogpt import utils
from autogpt.config import Config
from autogpt.config.config import GPT_3_MODEL, GPT_4_MODEL
from autogpt.llm.api_manager import ApiManager
from autogpt.logs.helpers import print_attribute, request_user_double_check
```

这是一段 Python 代码，它包括了几个 Python 函数，这些函数的作用是：

1. 检查是否允许用户在空格键中输入文本，如果是，则返回 True，否则返回 False。
2. 创建一个 YAML 文件并读取其中的设置，如果文件存在并且格式正确，则将当前设置应用到配置中，并返回设置的文件路径。
3. 创建一个名为 "ai_settings_file" 的文件，如果文件存在，则读取其中的设置，否则创建一个名为 "default_settings.yaml" 的文件，并将设置写入其中。
4. 如果配置文件 "ai_settings_file" 存在，则将配置对象中的 "ai_settings_file" 属性设置为文件路径，并将 "skip_reprompt" 设置为 True。
5. 如果用户允许下载文件，则创建一个名为 "browser_name" 的变量，并将设置对象中的 "browser_name" 属性设置为浏览器名称。
6. 如果用户允许下载文件，则创建一个名为 "allow_downloads" 的布尔变量，并将设置对象中的 "allow_downloads" 属性设置为 True。
7. 如果用户在设置中禁用了回复信，则创建一个名为 "skip_news" 的布尔变量，并将设置对象中的 "skip_news" 属性设置为 True。
8. 如果没有设置跳过回复信，则会输出 "Skip Re-prompt" 的消息，并在配置中设置 "skip_reprompt" 为 True。




```py
from autogpt.memory.vector import get_supported_memory_backends

logger = logging.getLogger(__name__)


def apply_overrides_to_config(
    config: Config,
    continuous: bool = False,
    continuous_limit: Optional[int] = None,
    ai_settings_file: Optional[Path] = None,
    prompt_settings_file: Optional[Path] = None,
    skip_reprompt: bool = False,
    speak: bool = False,
    debug: bool = False,
    gpt3only: bool = False,
    gpt4only: bool = False,
    memory_type: str = "",
    browser_name: str = "",
    allow_downloads: bool = False,
    skip_news: bool = False,
) -> None:
    """Updates the config object with the given arguments.

    Args:
        continuous (bool): Whether to run in continuous mode
        continuous_limit (int): The number of times to run in continuous mode
        ai_settings_file (Path): The path to the ai_settings.yaml file
        prompt_settings_file (Path): The path to the prompt_settings.yaml file
        skip_reprompt (bool): Whether to skip the re-prompting messages at the beginning of the script
        speak (bool): Whether to enable speak mode
        debug (bool): Whether to enable debug mode
        gpt3only (bool): Whether to enable GPT3.5 only mode
        gpt4only (bool): Whether to enable GPT4 only mode
        memory_type (str): The type of memory backend to use
        browser_name (str): The name of the browser to use when using selenium to scrape the web
        allow_downloads (bool): Whether to allow AutoGPT to download files natively
        skips_news (bool): Whether to suppress the output of latest news on startup
    """
    config.debug_mode = False
    config.continuous_mode = False
    config.tts_config.speak_mode = False

    if debug:
        print_attribute("Debug mode", "ENABLED")
        config.debug_mode = True

    if continuous:
        print_attribute("Continuous Mode", "ENABLED", title_color=Fore.YELLOW)
        logger.warning(
            "Continuous mode is not recommended. It is potentially dangerous and may"
            " cause your AI to run forever or carry out actions you would not usually"
            " authorise. Use at your own risk.",
        )
        config.continuous_mode = True

        if continuous_limit:
            print_attribute("Continuous Limit", continuous_limit)
            config.continuous_limit = continuous_limit

    # Check if continuous limit is used without continuous mode
    if continuous_limit and not continuous:
        raise click.UsageError("--continuous-limit can only be used with --continuous")

    if speak:
        print_attribute("Speak Mode", "ENABLED")
        config.tts_config.speak_mode = True

    # Set the default LLM models
    if gpt3only:
        print_attribute("GPT3.5 Only Mode", "ENABLED")
        # --gpt3only should always use gpt-3.5-turbo, despite user's FAST_LLM config
        config.fast_llm = GPT_3_MODEL
        config.smart_llm = GPT_3_MODEL
    elif (
        gpt4only
        and check_model(GPT_4_MODEL, model_type="smart_llm", config=config)
        == GPT_4_MODEL
    ):
        print_attribute("GPT4 Only Mode", "ENABLED")
        # --gpt4only should always use gpt-4, despite user's SMART_LLM config
        config.fast_llm = GPT_4_MODEL
        config.smart_llm = GPT_4_MODEL
    else:
        config.fast_llm = check_model(config.fast_llm, "fast_llm", config=config)
        config.smart_llm = check_model(config.smart_llm, "smart_llm", config=config)

    if memory_type:
        supported_memory = get_supported_memory_backends()
        chosen = memory_type
        if chosen not in supported_memory:
            logger.warning(
                extra={
                    "title": "ONLY THE FOLLOWING MEMORY BACKENDS ARE SUPPORTED:",
                    "title_color": Fore.RED,
                },
                msg=f"{supported_memory}",
            )
            print_attribute(
                "Defaulting to", config.memory_backend, title_color=Fore.YELLOW
            )
        else:
            config.memory_backend = chosen

    if skip_reprompt:
        print_attribute("Skip Re-prompt", "ENABLED")
        config.skip_reprompt = True

    if ai_settings_file:
        file = ai_settings_file

        # Validate file
        (validated, message) = utils.validate_yaml_file(file)
        if not validated:
            logger.fatal(extra={"title": "FAILED FILE VALIDATION:"}, msg=message)
            request_user_double_check()
            exit(1)

        print_attribute("Using AI Settings File", file)
        config.ai_settings_file = config.project_root / file
        config.skip_reprompt = True

    if prompt_settings_file:
        file = prompt_settings_file

        # Validate file
        (validated, message) = utils.validate_yaml_file(file)
        if not validated:
            logger.fatal(extra={"title": "FAILED FILE VALIDATION:"}, msg=message)
            request_user_double_check()
            exit(1)

        print_attribute("Using Prompt Settings File", file)
        config.prompt_settings_file = config.project_root / file

    if browser_name:
        config.selenium_web_browser = browser_name

    if allow_downloads:
        print_attribute("Native Downloading", "ENABLED")
        logger.warn(
            msg=f"{Back.LIGHTYELLOW_EX}AutoGPT will now be able to download and save files to your machine.{Back.RESET}"
            " It is recommended that you monitor any files it downloads carefully.",
        )
        logger.warn(
            msg=f"{Back.RED + Style.BRIGHT}ALWAYS REMEMBER TO NEVER OPEN FILES YOU AREN'T SURE OF!{Style.RESET_ALL}",
        )
        config.allow_downloads = True

    if skip_news:
        config.skip_news = True


```

这段代码是一个函数 `check_model()`，它接受一个参数 `model_name`、一个参数 `model_type` 和一个参数 `config`。这个函数的作用是检查 `model_name` 模型是否可用，如果不可用，就返回 `gpt-3.5-turbo`。

具体实现包括以下步骤：

1. 从 `config` 中获取 `openai_credentials`。
2. 创建一个 `ApiManager` 类。
3. 从 `ApiManager` 类中获取所有可用的模型。
4. 遍历所有模型，检查模型是否与 `model_name` 匹配。
5. 如果匹配到一个模型，就返回这个模型。
6. 如果所有模型都不匹配 `model_name`，就输出一条警告信息，然后返回 `"gpt-3.5-turbo"`。

这个函数的输入参数为 `model_name`、`model_type` 和 `config`。函数内部通过对 `openai_credentials` 和 `ApiManager` 类的使用，来获取并检查可用的模型。


```py
def check_model(
    model_name: str,
    model_type: Literal["smart_llm", "fast_llm"],
    config: Config,
) -> str:
    """Check if model is available for use. If not, return gpt-3.5-turbo."""
    openai_credentials = config.get_openai_credentials(model_name)
    api_manager = ApiManager()
    models = api_manager.get_models(**openai_credentials)

    if any(model_name in m["id"] for m in models):
        return model_name

    logger.warn(
        f"You do not have access to {model_name}. Setting {model_type} to gpt-3.5-turbo."
    )
    return "gpt-3.5-turbo"

```

# `autogpts/autogpt/autogpt/app/main.py`

这段代码定义了一个程序的主入口点，可以被CLI或其他前端应用程序调用。它包含了以下几行注释：

```py
# 定义了一个枚举类型，名为Personas，包括员工，经理，普通人等
enum.Personas = Personas
```

```py
# 定义了一个日志类，用于记录应用程序的日志信息
logging.Logger
```

```py
# 定义了一个数学类，包含了一些基本的数学运算
math
```

```py
# 定义了一个正则表达式，用于匹配字符串中的某些模式
re
```

```py
# 定义了一个信号处理类，用于处理操作系统中的信号（SIGX）
signal
```

```py
# 定义了一个系统类，包含了一些与操作系统相关的功能
sys
```

```py
# 导入了一些路径类，例如PathLike和AbsolutePath
from pathlib import Path
```

```py
# 导入了一个SecretStr类型的类，用于管理应用程序的配置文件
from pydantic import SecretStr
```

```py
# 定义了一个Personas类型的类，用于表示应用程序中的不同角色
class Persona(enum.Personas):
   # 定义了一个名为“经理”的Persona类型
   Manager = Persona
```

```py
# 定义了一个应用程序主入口点函数
def main(跳转的URL: str, **kwargs) -> Optional[int]:
   # 在这里，可以执行应用程序的一些设置，例如：设置日志等级，设置存储数据库等
   # 这里我们先不管这些设置，直接返回一个数字，表示应用程序可以运行的最大时间
   max_run_time = 3600
   # 这里我们创建了一个Persona类型的实例，用于表示应用程序的当前状态
   persona = Persona.Manager
   # 将这个Persona实例的值设置为True，表示应用程序已经准备好了
   persona.ready = True
   # 这里我们返回应用程序可以运行的最大时间，表示我们的应用程序可能运行的最长时间
   return max_run_time
```

这段代码定义了一个应用程序的主入口点，可以被CLI或其他前端应用程序调用。它使用了Python的标准库，并定义了一些枚举类型、日志类、数学类、正则表达式类、信号处理类、系统类以及Persona类型的类。这些类和函数的具体实现将在应用程序的代码中进行。


```py
"""The application entry point.  Can be invoked by a CLI or any other front end application."""
import enum
import logging
import math
import re
import signal
import sys
from pathlib import Path
from types import FrameType
from typing import TYPE_CHECKING, Optional

from colorama import Fore, Style
from forge.sdk.db import AgentDB
from pydantic import SecretStr

```

这段代码是一个条件判断语句，它会根据当前是否开启了类型检查来执行不同的代码。

具体来说，如果当前开启了类型检查，那么下面的代码将会被执行：

```py
from autogpt.agents.agent import Agent
from autogpt.agent_factory.configurators import configure_agent_with_state, create_agent
from autogpt.agent_factory.profile_generator import generate_agent_profile_for_task
from autogpt.agent_manager import AgentManager
```

这段代码会创建一个自动评估（agent）代理，并使用`configure_agent_with_state`配置器来设置其状态。接下来，它将使用`create_agent`来创建一个代理实例。最后，它将使用`generate_agent_profile_for_task`来生成代理的描述。

如果当前没有开启类型检查，那么`configure_agent_with_state`配置器将不会被调用，而`create_agent`和`generate_agent_profile_for_task`仍然会被执行。


```py
if TYPE_CHECKING:
    from autogpt.agents.agent import Agent

from autogpt.agent_factory.configurators import configure_agent_with_state, create_agent
from autogpt.agent_factory.profile_generator import generate_agent_profile_for_task
from autogpt.agent_manager import AgentManager
from autogpt.agents import AgentThoughts, CommandArgs, CommandName
from autogpt.agents.utils.exceptions import AgentTerminated, InvalidAgentResponseError
from autogpt.config import (
    AIDirectives,
    AIProfile,
    Config,
    ConfigBuilder,
    assert_config_has_openai_api_key,
)
```

这段代码是一个AutogPT的插件，其目的是为了给用户提供一个更好的AutogPT的使用体验。具体来说，它实现了以下功能：

1. 导入了一些必要的模块和函数：从autogpt的core模块中导入ModelProviderCredentials，从OpenAI模块中导入OpenAIProvider，从autogpt的runner模块中导入coroutine，从print_attribute函数中导入print_attribute，从speak函数中导入speak，从InstallPluginDependencies函数中导入install_plugin_dependencies，从scans_plugins函数中导入scan_plugins，以及定义了Spinner，从配置器模块中应用配置，从设置AI设置器的函数中应用设置，从Spinner中获取数据等等。

2. 实现了插件的配置功能：从configurator模块中应用配置，从setup模块中应用设置，从spinner模块中获取数据，从util模块中获取一些有用的函数，如打印注释，打印合法警告，打印Python版本信息等等。

3. 实现了Markdown风格的输出：使用Markdown风格的格式输出了AutogPT的版本信息和一些帮助信息，如插件列表和插件版本号等。

4. 实现了Speak函数：使用Speak函数将AutogPT的输出文本转换为Markdown格式，这样用户就可以更方便地阅读文档和聊天内容了。

5. 实现了InstallPluginDependencies函数：从插件依赖管理器中安装必要的依赖项，以确保AutogPT运行得更加高效和稳定。

6. 实现了scan_plugins函数：扫描插件库中的所有插件，以确保AutogPT的插件生态的健康和完整。

这段代码是一个AutogPT插件，主要目的是为了提供一个更友好、更有效的AutogPT使用体验。


```py
from autogpt.core.resource.model_providers import ModelProviderCredentials
from autogpt.core.resource.model_providers.openai import OpenAIProvider
from autogpt.core.runner.client_lib.utils import coroutine
from autogpt.logs.config import configure_chat_plugins, configure_logging
from autogpt.logs.helpers import print_attribute, speak
from autogpt.plugins import scan_plugins
from scripts.install_plugin_deps import install_plugin_dependencies

from .configurator import apply_overrides_to_config
from .setup import apply_overrides_to_ai_settings, interactively_revise_ai_settings
from .spinner import Spinner
from .utils import (
    clean_input,
    get_legal_warning,
    markdown_to_ansi_style,
    print_git_branch_info,
    print_motd,
    print_python_version_info,
)


```

This appears to be a Python script that is using the LLM (Let's Learn to Model) framework to revise AI settings and create an agent to interact with a workspace.

The script takes several arguments:

- `ai_profile`: This is the AI profile to use.
- `directives`: This is the directive to use for the AI.
- `app_config`: This is the configuration file for the app.
- `llm_provider`: This is the LLM provider to use (e.g. AWS, etc.).

It also depends on `create_agent`, `create_agent_dir`, `get_agent_dir`, `attach_fs`, `set_id`, `state_file_path`, `file_manager`, `task`, and `agent`.

The script firsts Revises AI settings by passing the specified AI profile, directives, and app config to the `ai_revise_ai_settings` function.

Then, it creates an agent by passing the specified AI profile, directives, app config, LLM provider, and other arguments to the `create_agent` function.

Finally, it attaches the file system to the agent and saves the state to a file.

It also allows the user to save the agent state as another agent ID by using the `clean_input` function.


```py
@coroutine
async def run_auto_gpt(
    continuous: bool,
    continuous_limit: int,
    ai_settings: Optional[Path],
    prompt_settings: Optional[Path],
    skip_reprompt: bool,
    speak: bool,
    debug: bool,
    gpt3only: bool,
    gpt4only: bool,
    memory_type: str,
    browser_name: str,
    allow_downloads: bool,
    skip_news: bool,
    workspace_directory: Path,
    install_plugin_deps: bool,
    override_ai_name: str = "",
    override_ai_role: str = "",
    resources: Optional[list[str]] = None,
    constraints: Optional[list[str]] = None,
    best_practices: Optional[list[str]] = None,
    override_directives: bool = False,
):
    config = ConfigBuilder.build_config_from_env()

    # TODO: fill in llm values here
    assert_config_has_openai_api_key(config)

    apply_overrides_to_config(
        config=config,
        continuous=continuous,
        continuous_limit=continuous_limit,
        ai_settings_file=ai_settings,
        prompt_settings_file=prompt_settings,
        skip_reprompt=skip_reprompt,
        speak=speak,
        debug=debug,
        gpt3only=gpt3only,
        gpt4only=gpt4only,
        memory_type=memory_type,
        browser_name=browser_name,
        allow_downloads=allow_downloads,
        skip_news=skip_news,
    )

    # Set up logging module
    configure_logging(
        debug_mode=debug,
        plain_output=config.plain_output,
        tts_config=config.tts_config,
    )

    llm_provider = _configure_openai_provider(config)

    logger = logging.getLogger(__name__)

    if config.continuous_mode:
        for line in get_legal_warning().split("\n"):
            logger.warn(
                extra={
                    "title": "LEGAL:",
                    "title_color": Fore.RED,
                    "preserve_color": True,
                },
                msg=markdown_to_ansi_style(line),
            )

    if not config.skip_news:
        print_motd(config, logger)
        print_git_branch_info(logger)
        print_python_version_info(logger)

    if install_plugin_deps:
        install_plugin_dependencies()

    config.plugins = scan_plugins(config, config.debug_mode)
    configure_chat_plugins(config)

    # Let user choose an existing agent to run
    agent_manager = AgentManager(config.app_data_dir)
    existing_agents = agent_manager.list_agents()
    load_existing_agent = ""
    if existing_agents:
        print(
            "Existing agents\n---------------\n"
            + "\n".join(f"{i} - {id}" for i, id in enumerate(existing_agents, 1))
        )
        load_existing_agent = await clean_input(
            config,
            "Enter the number or name of the agent to run, or hit enter to create a new one:",
        )
        if re.match(r"^\d+$", load_existing_agent):
            load_existing_agent = existing_agents[int(load_existing_agent) - 1]
        elif load_existing_agent and load_existing_agent not in existing_agents:
            raise ValueError(f"Unknown agent '{load_existing_agent}'")

    # Either load existing or set up new agent state
    agent = None
    agent_state = None

    ############################
    # Resume an Existing Agent #
    ############################
    if load_existing_agent:
        agent_state = agent_manager.retrieve_state(load_existing_agent)
        while True:
            answer = await clean_input(config, "Resume? [Y/n]")
            if answer.lower() == "y":
                break
            elif answer.lower() == "n":
                agent_state = None
                break
            else:
                print("Please respond with 'y' or 'n'")

    if agent_state:
        agent = configure_agent_with_state(
            state=agent_state,
            app_config=config,
            llm_provider=llm_provider,
        )
        apply_overrides_to_ai_settings(
            ai_profile=agent.state.ai_profile,
            directives=agent.state.directives,
            override_name=override_ai_name,
            override_role=override_ai_role,
            resources=resources,
            constraints=constraints,
            best_practices=best_practices,
            replace_directives=override_directives,
        )

        # If any of these are specified as arguments,
        #  assume the user doesn't want to revise them
        if not any(
            [
                override_ai_name,
                override_ai_role,
                resources,
                constraints,
                best_practices,
            ]
        ):
            ai_profile, ai_directives = await interactively_revise_ai_settings(
                ai_profile=agent.state.ai_profile,
                directives=agent.state.directives,
                app_config=config,
            )
        else:
            logger.info("AI config overrides specified through CLI; skipping revision")

    ######################
    # Set up a new Agent #
    ######################
    if not agent:
        task = await clean_input(
            config,
            "Enter the task that you want AutoGPT to execute,"
            " with as much detail as possible:",
        )
        base_ai_directives = AIDirectives.from_file(config.prompt_settings_file)

        ai_profile, task_oriented_ai_directives = await generate_agent_profile_for_task(
            task,
            app_config=config,
            llm_provider=llm_provider,
        )
        ai_directives = base_ai_directives + task_oriented_ai_directives
        apply_overrides_to_ai_settings(
            ai_profile=ai_profile,
            directives=ai_directives,
            override_name=override_ai_name,
            override_role=override_ai_role,
            resources=resources,
            constraints=constraints,
            best_practices=best_practices,
            replace_directives=override_directives,
        )

        # If any of these are specified as arguments,
        #  assume the user doesn't want to revise them
        if not any(
            [
                override_ai_name,
                override_ai_role,
                resources,
                constraints,
                best_practices,
            ]
        ):
            ai_profile, ai_directives = await interactively_revise_ai_settings(
                ai_profile=ai_profile,
                directives=ai_directives,
                app_config=config,
            )
        else:
            logger.info("AI config overrides specified through CLI; skipping revision")

        agent = create_agent(
            task=task,
            ai_profile=ai_profile,
            directives=ai_directives,
            app_config=config,
            llm_provider=llm_provider,
        )
        agent.attach_fs(agent_manager.get_agent_dir(agent.state.agent_id))

        if not agent.config.allow_fs_access:
            logger.info(
                f"{Fore.YELLOW}NOTE: All files/directories created by this agent"
                f" can be found inside its workspace at:{Fore.RESET} {agent.workspace.root}",
                extra={"preserve_color": True},
            )

    #################
    # Run the Agent #
    #################
    try:
        await run_interaction_loop(agent)
    except AgentTerminated:
        agent_id = agent.state.agent_id
        logger.info(f"Saving state of {agent_id}...")

        # Allow user to Save As other ID
        save_as_id = (
            await clean_input(
                config,
                f"Press enter to save as '{agent_id}', or enter a different ID to save to:",
            )
            or agent_id
        )
        if save_as_id and save_as_id != agent_id:
            agent.set_id(
                new_id=save_as_id,
                new_agent_dir=agent_manager.get_agent_dir(save_as_id),
            )
            # TODO: clone workspace if user wants that
            # TODO: ... OR allow many-to-one relations of agents and workspaces

        agent.state.save_to_json_file(agent.file_manager.state_file_path)


```

这段代码是一个名为 `run_auto_gpt_server` 的异步函数，它定义了一个用于运行 GPT-3 和 GPT-4 模型的服务器。以下是代码的作用：

1. 定义了输入参数 `prompt_settings`，`debug`，`gpt3only`，`gpt4only`，`memory_type` 和 `browser_name`，它们用于设置 GPT 模型的配置。
2. 引入了 `AgentProtocolServer` 和 `ConfigBuilder` 类，以及 `assert_config_has_openai_api_key` 和 `scan_plugins` 函数，这些函数在代码中没有直接使用，但它们的目的是确保 GPT 服务器可以正常工作并提供一些配置和功能。
3. 创建了一个 `AgentProtocolServer` 实例，该实例将负责处理客户端的请求并与 GPT 模型进行交互。
4. 将 `database` 对象设置为用于存储数据的数据库，并将 `server` 实例设置为 `AgentProtocolServer` 实例的实例。
5. 使用 `start` 方法启动 GPT 服务器并将其运行在后台。

总的来说，这段代码定义了一个用于运行 GPT 模型的服务器，它可以通过命令行或将其集成到应用程序中使用。用户可以通过设置不同的参数来 customize 服务器的功能。


```py
@coroutine
async def run_auto_gpt_server(
    prompt_settings: Optional[Path],
    debug: bool,
    gpt3only: bool,
    gpt4only: bool,
    memory_type: str,
    browser_name: str,
    allow_downloads: bool,
    install_plugin_deps: bool,
):
    from .agent_protocol_server import AgentProtocolServer

    config = ConfigBuilder.build_config_from_env()

    # TODO: fill in llm values here
    assert_config_has_openai_api_key(config)

    apply_overrides_to_config(
        config=config,
        prompt_settings_file=prompt_settings,
        debug=debug,
        gpt3only=gpt3only,
        gpt4only=gpt4only,
        memory_type=memory_type,
        browser_name=browser_name,
        allow_downloads=allow_downloads,
    )

    # Set up logging module
    configure_logging(
        debug_mode=debug,
        plain_output=config.plain_output,
        tts_config=config.tts_config,
    )

    llm_provider = _configure_openai_provider(config)

    if install_plugin_deps:
        install_plugin_dependencies()

    config.plugins = scan_plugins(config, config.debug_mode)

    # Set up & start server
    database = AgentDB("sqlite:///data/ap_server.db", debug_enabled=False)
    server = AgentProtocolServer(
        app_config=config, database=database, llm_provider=llm_provider
    )
    await server.start()


```

这段代码是一个函数，名为 `_configure_openai_provider`，它接受一个 `Config` 对象作为参数，并返回一个配置了 OpenAIProvider 的对象。

函数内部首先检查 `config` 对象中是否有 OpenAI API 密钥，如果没有，就抛出一个 `RuntimeError`。如果密钥已配置，那么就创建一个 `OpenAIProvider` 对象，并返回它。

具体来说，函数内部首先从 `config` 对象中复制一个默认的 `OpenAIProvider` 设置，然后设置 `openai_api_key` 和 `openai_api_base` 字段为从 `config` 对象中获得的 OpenAI API 密钥和 API 基础。如果 `openai_api_type` 和 `openai_api_version` 字段已配置，则设置这两个字段的值。最后，创建一个 `OpenAIProvider` 对象，并设置其 `settings` 和 `logger` 属性。


```py
def _configure_openai_provider(config: Config) -> OpenAIProvider:
    """Create a configured OpenAIProvider object.

    Args:
        config: The program's configuration.

    Returns:
        A configured OpenAIProvider object.
    """
    if config.openai_api_key is None:
        raise RuntimeError("OpenAI key is not configured")

    openai_settings = OpenAIProvider.default_settings.copy(deep=True)
    openai_settings.credentials = ModelProviderCredentials(
        api_key=SecretStr(config.openai_api_key),
        # TODO: support OpenAI Azure credentials
        api_base=SecretStr(config.openai_api_base) if config.openai_api_base else None,
        api_type=SecretStr(config.openai_api_type) if config.openai_api_type else None,
        api_version=SecretStr(config.openai_api_version)
        if config.openai_api_version
        else None,
    )
    return OpenAIProvider(
        settings=openai_settings,
        logger=logging.getLogger("OpenAIProvider"),
    )


```

这段代码是一个函数，名为 `_get_cycle_budget`，它接收两个参数，一个是 `continuous_mode`，另一个是 `continuous_limit`。函数的作用是将 `continuous_mode` 和 `continuous_limit` 下的配置代码进行翻译，得到一个 `cycle_budget` 和一个 `count_of_cycles_remaining` 变量。

具体来说，如果 `continuous_mode` 为 `True`，则将 `continuous_limit` 如果为 `None`，则将其设置为 `math.inf`，否则将其设置为 `continuous_limit`。这样，当用户不进行任何交互操作时，函数将返回 `continuous_limit`，表示用户可以连续运行的最大循环次数。如果 `continuous_mode` 为 `False`，则将 `continuous_limit` 设置为 `1`，表示用户每次交互操作后可以运行一次循环。

函数返回的 `cycle_budget` 用于表示用户可以在不进行任何交互操作的情况下连续运行的最大循环次数。


```py
def _get_cycle_budget(continuous_mode: bool, continuous_limit: int) -> int | float:
    # Translate from the continuous_mode/continuous_limit config
    # to a cycle_budget (maximum number of cycles to run without checking in with the
    # user) and a count of cycles_remaining before we check in..
    if continuous_mode:
        cycle_budget = continuous_limit if continuous_limit else math.inf
    else:
        cycle_budget = 1

    return cycle_budget


class UserFeedback(str, enum.Enum):
    """Enum for user feedback."""

    AUTHORIZE = "GENERATE NEXT COMMAND JSON"
    EXIT = "EXIT"
    TEXT = "TEXT"


```

This is a implementation of an intelligent agent that can perform text-based commands. The agent has a limited number of cycles in its budget, and it can either execute a pre-defined command or receive a text input to perform a specific task.

When the user interacts with the agent, they can choose to execute a pre-defined command or use the text input to provide custom instructions. If the user chooses to execute a pre-defined command, the agent will use the system'sexecute method to execute the command. If the user provides a text input, the agent will use the agent'slearn method to parse the input and perform a corresponding task.

The agent also has a stop button that can be pressed to stop the agent from running. If the user presses the stop button, the agent will stop execution immediately and reset the remaining cycles.

Overall, the agent appears to be a simple text-based agent that can perform various tasks based on user input.


```py
async def run_interaction_loop(
    agent: "Agent",
) -> None:
    """Run the main interaction loop for the agent.

    Args:
        agent: The agent to run the interaction loop for.

    Returns:
        None
    """
    # These contain both application config and agent config, so grab them here.
    legacy_config = agent.legacy_config
    ai_profile = agent.ai_profile
    logger = logging.getLogger(__name__)

    cycle_budget = cycles_remaining = _get_cycle_budget(
        legacy_config.continuous_mode, legacy_config.continuous_limit
    )
    spinner = Spinner("Thinking...", plain_output=legacy_config.plain_output)
    stop_reason = None

    def graceful_agent_interrupt(signum: int, frame: Optional[FrameType]) -> None:
        nonlocal cycle_budget, cycles_remaining, spinner, stop_reason
        if stop_reason:
            logger.error("Quitting immediately...")
            sys.exit()
        if cycles_remaining in [0, 1]:
            logger.warning("Interrupt signal received: shutting down gracefully.")
            logger.warning(
                "Press Ctrl+C again if you want to stop AutoGPT immediately."
            )
            stop_reason = AgentTerminated("Interrupt signal received")
        else:
            restart_spinner = spinner.running
            if spinner.running:
                spinner.stop()

            logger.error(
                "Interrupt signal received: stopping continuous command execution."
            )
            cycles_remaining = 1
            if restart_spinner:
                spinner.start()

    def handle_stop_signal() -> None:
        if stop_reason:
            raise stop_reason

    # Set up an interrupt signal for the agent.
    signal.signal(signal.SIGINT, graceful_agent_interrupt)

    #########################
    # Application Main Loop #
    #########################

    # Keep track of consecutive failures of the agent
    consecutive_failures = 0

    while cycles_remaining > 0:
        logger.debug(f"Cycle budget: {cycle_budget}; remaining: {cycles_remaining}")

        ########
        # Plan #
        ########
        handle_stop_signal()
        # Have the agent determine the next action to take.
        with spinner:
            try:
                (
                    command_name,
                    command_args,
                    assistant_reply_dict,
                ) = await agent.propose_action()
            except InvalidAgentResponseError as e:
                logger.warn(f"The agent's thoughts could not be parsed: {e}")
                consecutive_failures += 1
                if consecutive_failures >= 3:
                    logger.error(
                        "The agent failed to output valid thoughts"
                        f" {consecutive_failures} times in a row. Terminating..."
                    )
                    raise AgentTerminated(
                        "The agent failed to output valid thoughts"
                        f" {consecutive_failures} times in a row."
                    )
                continue

        consecutive_failures = 0

        ###############
        # Update User #
        ###############
        # Print the assistant's thoughts and the next command to the user.
        update_user(
            ai_profile,
            command_name,
            command_args,
            assistant_reply_dict,
            speak_mode=legacy_config.tts_config.speak_mode,
        )

        ##################
        # Get user input #
        ##################
        handle_stop_signal()
        if cycles_remaining == 1:  # Last cycle
            user_feedback, user_input, new_cycles_remaining = await get_user_feedback(
                legacy_config,
                ai_profile,
            )

            if user_feedback == UserFeedback.AUTHORIZE:
                if new_cycles_remaining is not None:
                    # Case 1: User is altering the cycle budget.
                    if cycle_budget > 1:
                        cycle_budget = new_cycles_remaining + 1
                    # Case 2: User is running iteratively and
                    #   has initiated a one-time continuous cycle
                    cycles_remaining = new_cycles_remaining + 1
                else:
                    # Case 1: Continuous iteration was interrupted -> resume
                    if cycle_budget > 1:
                        logger.info(
                            f"The cycle budget is {cycle_budget}.",
                            extra={
                                "title": "RESUMING CONTINUOUS EXECUTION",
                                "title_color": Fore.MAGENTA,
                            },
                        )
                    # Case 2: The agent used up its cycle budget -> reset
                    cycles_remaining = cycle_budget + 1
                logger.info(
                    "-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=",
                    extra={"color": Fore.MAGENTA},
                )
            elif user_feedback == UserFeedback.EXIT:
                logger.warn("Exiting...")
                exit()
            else:  # user_feedback == UserFeedback.TEXT
                command_name = "human_feedback"
        else:
            user_input = ""
            # First log new-line so user can differentiate sections better in console
            print()
            if cycles_remaining != math.inf:
                # Print authorized commands left value
                print_attribute(
                    "AUTHORIZED_COMMANDS_LEFT", cycles_remaining, title_color=Fore.CYAN
                )

        ###################
        # Execute Command #
        ###################
        # Decrement the cycle counter first to reduce the likelihood of a SIGINT
        # happening during command execution, setting the cycles remaining to 1,
        # and then having the decrement set it to 0, exiting the application.
        if command_name != "human_feedback":
            cycles_remaining -= 1

        if not command_name:
            continue

        handle_stop_signal()

        result = await agent.execute(command_name, command_args, user_input)

        if result.status == "success":
            logger.info(result, extra={"title": "SYSTEM:", "title_color": Fore.YELLOW})
        elif result.status == "error":
            logger.warn(
                f"Command {command_name} returned an error: {result.error or result.reason}"
            )


```

这段代码是一个人工智能程序的函数，它的目的是根据用户给出的命令，更新用户的配置和数据，然后向用户输出助理的回复和下一条命令。

具体来说，代码的作用可以拆分成以下几个步骤：

1. 定义了函数参数五个变量：ai_profile、command_name、command_args、assistant_reply_dict和speak_mode，分别表示AI的个性签名、用户要执行的命令、命令的参数、助理的回复和用户是否要大声读出来。

2. 在函数内输出logger信息，包括时间是“print_assistant_thoughts()”函数输出的，用来在日志中记录用户手册中没有的日志。

3. 如果用户开启了大声读出来，程序会先输出一条消息然后提示用户“I want to execute”命令。

4. 通过两个if语句，第一个if语句用来输出命令和参数，第二个if语句用来在日志中记录。输出结果包含命令名称和参数，格式化后的大写颜色可以突出显示当前正在执行的命令。

5. 通过print()函数输出新的日志条。

6. 通过Fore.CYAN函数输出颜色为绿色，高亮显示的条。

7. 通过print()函数输出新的日志条。

8. 通过print()函数输出新的日志条。


```py
def update_user(
    ai_profile: AIProfile,
    command_name: CommandName,
    command_args: CommandArgs,
    assistant_reply_dict: AgentThoughts,
    speak_mode: bool = False,
) -> None:
    """Prints the assistant's thoughts and the next command to the user.

    Args:
        config: The program's configuration.
        ai_profile: The AI's personality/profile
        command_name: The name of the command to execute.
        command_args: The arguments for the command.
        assistant_reply_dict: The assistant's reply.
    """
    logger = logging.getLogger(__name__)

    print_assistant_thoughts(
        ai_name=ai_profile.ai_name,
        assistant_reply_json_valid=assistant_reply_dict,
        speak_mode=speak_mode,
    )

    if speak_mode:
        speak(f"I want to execute {command_name}")

    # First log new-line so user can differentiate sections better in console
    print()
    logger.info(
        f"COMMAND = {Fore.CYAN}{remove_ansi_escape(command_name)}{Style.RESET_ALL}  "
        f"ARGUMENTS = {Fore.CYAN}{command_args}{Style.RESET_ALL}",
        extra={
            "title": "NEXT ACTION:",
            "title_color": Fore.CYAN,
            "preserve_color": True,
        },
    )


```

This is a Python script that appears to be an implementation of a command-line interface for a robotics project. The script provides several options for the user to customize their experience, including the ability to run a program, authorize a user, and exit the program.

The script first prompts the user to press enter to continue or escape. If the user enters the authorized key (config.authorise_key), the program will run and prompt the user to enter a number of continuous tasks (config.authorise_key -N). The program will then start new cycles based on the number entered by the user. If the user enters the exit key or presses the escape key, the program will exit. If the user does not enter any other key, the program will exit with a warning.

The script also provides a way for the user to provide feedback to the robotics project. The user can press the config.chat_messages_enabled ?statement to see the user's messages in the chat.


```py
async def get_user_feedback(
    config: Config,
    ai_profile: AIProfile,
) -> tuple[UserFeedback, str, int | None]:
    """Gets the user's feedback on the assistant's reply.

    Args:
        config: The program's configuration.
        ai_profile: The AI's configuration.

    Returns:
        A tuple of the user's feedback, the user's input, and the number of
        cycles remaining if the user has initiated a continuous cycle.
    """
    logger = logging.getLogger(__name__)

    # ### GET USER AUTHORIZATION TO EXECUTE COMMAND ###
    # Get key press: Prompt the user to press enter to continue or escape
    # to exit
    logger.info(
        f"Enter '{config.authorise_key}' to authorise command, "
        f"'{config.authorise_key} -N' to run N continuous commands, "
        f"'{config.exit_key}' to exit program, or enter feedback for "
        f"{ai_profile.ai_name}..."
    )

    user_feedback = None
    user_input = ""
    new_cycles_remaining = None

    while user_feedback is None:
        # Get input from user
        if config.chat_messages_enabled:
            console_input = await clean_input(config, "Waiting for your response...")
        else:
            console_input = await clean_input(
                config, Fore.MAGENTA + "Input:" + Style.RESET_ALL
            )

        # Parse user input
        if console_input.lower().strip() == config.authorise_key:
            user_feedback = UserFeedback.AUTHORIZE
        elif console_input.lower().strip() == "":
            logger.warn("Invalid input format.")
        elif console_input.lower().startswith(f"{config.authorise_key} -"):
            try:
                user_feedback = UserFeedback.AUTHORIZE
                new_cycles_remaining = abs(int(console_input.split(" ")[1]))
            except ValueError:
                logger.warn(
                    f"Invalid input format. "
                    f"Please enter '{config.authorise_key} -N'"
                    " where N is the number of continuous tasks."
                )
        elif console_input.lower() in [config.exit_key, "exit"]:
            user_feedback = UserFeedback.EXIT
        else:
            user_feedback = UserFeedback.TEXT
            user_input = console_input

    return user_feedback, user_input, new_cycles_remaining


```

Additionally, you may want to consider the following improvements:

1. Add a few more options to the `remove_ansi_escape` function. For example, you could add an option to remove ANSI escape codes that are not truly ANSI escape codes.
2. Consider adding a docstring for the `remove_ansi_escape` function, explaining what ANSI escape codes are and why they are being removed.
3. You may also want to consider adding support for `assistant_thoughts_plan` and `assistant_thoughts_criticism` keys to indicate that they are expected to be strings.

Here's an updated version of the `remove_ansi_escape` function with these improvements:
```py
def remove_ansi_escape(text):
   # Remove ANSI escape codes that are not truly ANSI escape codes
   return "".join(
       c for c in text if not c.isdigit() and not c.isalpha() and not c.isspace()
   )
```
And here's an updated version of the `print_attribute` function with the added docstring:
```py
def print_attribute(title, text, **kwargs):
   # Format the text using the given title and any additional options.
   # Add a few more options to the present implementation.
   return f"{title.upper()} {text}"
```
You can also consider adding a docstring for the `print_attribute` function explaining what the function does and any options that are available.


```py
def print_assistant_thoughts(
    ai_name: str,
    assistant_reply_json_valid: dict,
    speak_mode: bool = False,
) -> None:
    logger = logging.getLogger(__name__)

    assistant_thoughts_reasoning = None
    assistant_thoughts_plan = None
    assistant_thoughts_speak = None
    assistant_thoughts_criticism = None

    assistant_thoughts = assistant_reply_json_valid.get("thoughts", {})
    assistant_thoughts_text = remove_ansi_escape(assistant_thoughts.get("text", ""))
    if assistant_thoughts:
        assistant_thoughts_reasoning = remove_ansi_escape(
            assistant_thoughts.get("reasoning", "")
        )
        assistant_thoughts_plan = remove_ansi_escape(assistant_thoughts.get("plan", ""))
        assistant_thoughts_criticism = remove_ansi_escape(
            assistant_thoughts.get("criticism", "")
        )
        assistant_thoughts_speak = remove_ansi_escape(
            assistant_thoughts.get("speak", "")
        )
    print_attribute(
        f"{ai_name.upper()} THOUGHTS", assistant_thoughts_text, title_color=Fore.YELLOW
    )
    print_attribute("REASONING", assistant_thoughts_reasoning, title_color=Fore.YELLOW)
    if assistant_thoughts_plan:
        print_attribute("PLAN", "", title_color=Fore.YELLOW)
        # If it's a list, join it into a string
        if isinstance(assistant_thoughts_plan, list):
            assistant_thoughts_plan = "\n".join(assistant_thoughts_plan)
        elif isinstance(assistant_thoughts_plan, dict):
            assistant_thoughts_plan = str(assistant_thoughts_plan)

        # Split the input_string using the newline character and dashes
        lines = assistant_thoughts_plan.split("\n")
        for line in lines:
            line = line.lstrip("- ")
            logger.info(line.strip(), extra={"title": "- ", "title_color": Fore.GREEN})
    print_attribute(
        "CRITICISM", f"{assistant_thoughts_criticism}", title_color=Fore.YELLOW
    )

    # Speak the assistant's thoughts
    if assistant_thoughts_speak:
        if speak_mode:
            speak(assistant_thoughts_speak)
        else:
            print_attribute("SPEAK", assistant_thoughts_speak, title_color=Fore.YELLOW)


```

这段代码定义了一个名为 `remove_ansi_escape` 的函数，接受一个字符串参数 `s`，并返回一个经过处理的剩余字符串。

函数的作用是通过调用 `s.replace("\x1B", "")` 来实现的。首先，将 `\x1B` 转义序列（即 Unicode 编码中的 ASCII 码 1B）替换为空字符串，使得所有 Ansible escape 转义序列的字符（如 `\x0A`、`\x0D` 等）都变成普通字符。然后，去除转义序列，得到一个不包含 Ansible escape 转义序列的字符串。

因此，这段代码的作用是移除字符串中的 Ansible escape 转义序列，使得结果可以被作为普通字符进行操作。


```py
def remove_ansi_escape(s: str) -> str:
    return s.replace("\x1B", "")

```

# `autogpts/autogpt/autogpt/app/setup.py`

这段代码的主要作用是设置AI的设置，包括它的目标、指令和约束。它通过读取用户提供的AI配置文件（一般是.ini格式的配置文件），并覆盖默认设置，从而允许用户自定义AI的行为。

具体来说，这段代码实现以下功能：

1. 读取AI配置文件中的设置，包括AI名称、AI角色、指令、资源和约束等。
2. 如果用户指定了AI名称，则将其设置为指定的名称。
3. 如果用户指定了AI角色，则将其设置为指定的角色。
4. 如果用户指定了指令，则将其添加到AI的指令列表中。
5. 如果用户指定了资源，则将其添加到AI的资源列表中。
6. 如果用户指定了约束，则将其添加到AI的约束列表中。
7. 如果用户指定了最佳实践，则将其添加到AI的最佳实践中。

最后，将设置应用到AI的配置中，从而使AI的行为遵循这些设置。


```py
"""Set up the AI and its goals"""
import logging
from typing import Optional

from autogpt.app.utils import clean_input
from autogpt.config import AIDirectives, AIProfile, Config
from autogpt.logs.helpers import print_attribute

logger = logging.getLogger(__name__)


def apply_overrides_to_ai_settings(
    ai_profile: AIProfile,
    directives: AIDirectives,
    override_name: str = "",
    override_role: str = "",
    replace_directives: bool = False,
    resources: Optional[list[str]] = None,
    constraints: Optional[list[str]] = None,
    best_practices: Optional[list[str]] = None,
):
    if override_name:
        ai_profile.ai_name = override_name
    if override_role:
        ai_profile.ai_role = override_role

    if replace_directives:
        if resources:
            directives.resources = resources
        if constraints:
            directives.constraints = constraints
        if best_practices:
            directives.best_practices = best_practices
    else:
        if resources:
            directives.resources += resources
        if constraints:
            directives.constraints += constraints
        if best_practices:
            directives.best_practices += best_practices


```

It seems like this is a Python implementation of an AI language model. The `clean_input` function and the `print_attribute` function are not defined in this code snippet, so it is unclear what they do. It would be helpful to understand the context and purpose of these functions in order to provide a more accurate and useful response.


```py
async def interactively_revise_ai_settings(
    ai_profile: AIProfile,
    directives: AIDirectives,
    app_config: Config,
):
    """Interactively revise the AI settings.

    Args:
        ai_profile (AIConfig): The current AI profile.
        ai_directives (AIDirectives): The current AI directives.
        app_config (Config): The application configuration.

    Returns:
        AIConfig: The revised AI settings.
    """
    logger = logging.getLogger("revise_ai_profile")

    revised = False

    while True:
        # Print the current AI configuration
        print_ai_settings(
            title="Current AI Settings" if not revised else "Revised AI Settings",
            ai_profile=ai_profile,
            directives=directives,
            logger=logger,
        )

        if (
            await clean_input(app_config, "Continue with these settings? [Y/n]")
            or app_config.authorise_key
        ) == app_config.authorise_key:
            break

        # Ask for revised ai_profile
        ai_profile.ai_name = (
            await clean_input(
                app_config, "Enter AI name (or press enter to keep current):"
            )
            or ai_profile.ai_name
        )
        ai_profile.ai_role = (
            await clean_input(
                app_config, "Enter new AI role (or press enter to keep current):"
            )
            or ai_profile.ai_role
        )

        # Revise constraints
        for i, constraint in enumerate(directives.constraints):
            print_attribute(f"Constraint {i+1}:", f'"{constraint}"')
            new_constraint = (
                await clean_input(
                    app_config,
                    f"Enter new constraint {i+1} (press enter to keep current, or '-' to remove):",
                )
                or constraint
            )
            if new_constraint == "-":
                directives.constraints.remove(constraint)
            elif new_constraint:
                directives.constraints[i] = new_constraint

        # Add new constraints
        while True:
            new_constraint = await clean_input(
                app_config,
                "Press enter to finish, or enter a constraint to add:",
            )
            if not new_constraint:
                break
            directives.constraints.append(new_constraint)

        # Revise resources
        for i, resource in enumerate(directives.resources):
            print_attribute(f"Resource {i+1}:", f'"{resource}"')
            new_resource = (
                await clean_input(
                    app_config,
                    f"Enter new resource {i+1} (press enter to keep current, or '-' to remove):",
                )
                or resource
            )
            if new_resource == "-":
                directives.resources.remove(resource)
            elif new_resource:
                directives.resources[i] = new_resource

        # Add new resources
        while True:
            new_resource = await clean_input(
                app_config,
                "Press enter to finish, or enter a resource to add:",
            )
            if not new_resource:
                break
            directives.resources.append(new_resource)

        # Revise best practices
        for i, best_practice in enumerate(directives.best_practices):
            print_attribute(f"Best Practice {i+1}:", f'"{best_practice}"')
            new_best_practice = (
                await clean_input(
                    app_config,
                    f"Enter new best practice {i+1} (press enter to keep current, or '-' to remove):",
                )
                or best_practice
            )
            if new_best_practice == "-":
                directives.best_practices.remove(best_practice)
            elif new_best_practice:
                directives.best_practices[i] = new_best_practice

        # Add new best practices
        while True:
            new_best_practice = await clean_input(
                app_config,
                "Press enter to finish, or add a best practice to add:",
            )
            if not new_best_practice:
                break
            directives.best_practices.append(new_best_practice)

        revised = True

    return ai_profile, directives


```

这段代码定义了一个名为 `print_ai_settings` 的函数，它接受四个参数：`ai_profile`、`directives`、`logger` 和 `title`。函数的主要作用是输出关于 AI 设置的配置信息。

具体来说，这段代码执行以下操作：

1. 打印标题 "AI Settings"，然后打印一个下划线的 `print_attribute` 函数。
2. 打印 AI 配置文件的名称，以及 AI 的名称和角色。
3. 如果 `directives` 参数中包含 `constraints` 参数，则打印 "Constraints：" 和 "每个约束的详细信息"。否则，打印一个空字符串。
4. 如果 `directives` 参数中包含 `resources` 参数，则打印 "Resources：" 和 "每个资源的详细信息"。否则，打印一个空字符串。
5. 如果 `directives` 参数中包含 `best_practices` 参数，则打印 "Best practices：" 和 "每个最佳实践的详细信息"。否则，打印一个空字符串。
6. 循环遍历 `directives.constraints` 和 `directives.resources` 列表中的每个元素，打印一个带有 `-` 和详细信息的字符串。

通过执行上述操作，函数可以确保在 AI 设置发生变化时，用户获得有关 AI 设置的警告信息。


```py
def print_ai_settings(
    ai_profile: AIProfile,
    directives: AIDirectives,
    logger: logging.Logger,
    title: str = "AI Settings",
):
    print_attribute(title, "")
    print_attribute("-" * len(title), "")
    print_attribute("Name :", ai_profile.ai_name)
    print_attribute("Role :", ai_profile.ai_role)

    print_attribute("Constraints:", "" if directives.constraints else "(none)")
    for constraint in directives.constraints:
        logger.info(f"- {constraint}")
    print_attribute("Resources:", "" if directives.resources else "(none)")
    for resource in directives.resources:
        logger.info(f"- {resource}")
    print_attribute("Best practices:", "" if directives.best_practices else "(none)")
    for best_practice in directives.best_practices:
        logger.info(f"- {best_practice}")

```

# `autogpts/autogpt/autogpt/app/spinner.py`

This is a simple Python class called `Spinner` that initializes a spinner with a specified message, delay between updates, and whether or not to display the spinner. The spinner is updated using a cycle of characters, and the message is printed to the console after each update.

To use the `Spinner` class, you first need to import it, like this:
```py
import spinner
```
Then, you can call the `Spinner` class like this:
```py
spinner_class = spinner.Spinner("This is a test message", 0.2, True)
```
This creates a new `Spinner` object called `spinner_class` with the given message, delay, and whether or not to display the spinner. The `Spinner` object can then be used to spin the spinner like this:
```py
spinner_class.spin()
```
This will start the spinner, which will print the message to the console after each update, with a delay of 0.2 seconds between each update. The spinner will also display the character `"-"` to indicate that it is running. Once you are done using the `Spinner` object, you should call the `stop` method on it, like this:
```py
spinner_class.stop()
```
This will stop the spinner and print the message to the console after the last update.


```py
"""A simple spinner module"""
import itertools
import sys
import threading
import time


class Spinner:
    """A simple spinner class"""

    def __init__(
        self,
        message: str = "Loading...",
        delay: float = 0.1,
        plain_output: bool = False,
    ) -> None:
        """Initialize the spinner class

        Args:
            message (str): The message to display.
            delay (float): The delay between each spinner update.
            plain_output (bool): Whether to display the spinner or not.
        """
        self.plain_output = plain_output
        self.spinner = itertools.cycle(["-", "/", "|", "\\"])
        self.delay = delay
        self.message = message
        self.running = False
        self.spinner_thread = None

    def spin(self) -> None:
        """Spin the spinner"""
        if self.plain_output:
            self.print_message()
            return
        while self.running:
            self.print_message()
            time.sleep(self.delay)

    def print_message(self):
        sys.stdout.write(f"\r{' ' * (len(self.message) + 2)}\r")
        sys.stdout.write(f"{next(self.spinner)} {self.message}\r")
        sys.stdout.flush()

    def start(self):
        self.running = True
        self.spinner_thread = threading.Thread(target=self.spin)
        self.spinner_thread.start()

    def stop(self):
        self.running = False
        if self.spinner_thread is not None:
            self.spinner_thread.join()
        sys.stdout.write(f"\r{' ' * (len(self.message) + 2)}\r")
        sys.stdout.flush()

    def __enter__(self):
        """Start the spinner"""
        self.start()
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback) -> None:
        """Stop the spinner

        Args:
            exc_type (Exception): The exception type.
            exc_value (Exception): The exception value.
            exc_traceback (Exception): The exception traceback.
        """
        self.stop()

```