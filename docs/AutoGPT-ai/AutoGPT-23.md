# AutoGPTæºç è§£æ 23

# AutoGPT Forge: Crafting Intelligent AgentÂ Logic

![Header](../../../docs/content/imgs/quickstart/t3_01.png)
**By Craig Swift & [Ryan Brandt](https://github.com/paperMoose)**

Hey there! Ready for part 3 of our AutoGPT Forge tutorial series? If you missed the earlier parts, catch up here:

- [Getting Started](001_getting_started.md)
- [Blueprint of an Agent](002_blueprint_of_an_agent.md)

Now, let's get hands-on! We'll use an LLM to power our agent and complete a task. The challenge? Making the agent write "Washington" to a .txt file. We won't give it step-by-step instructionsâ€”just the task. Let's see our agent in action and watch it figure out the steps on its own!


## Get Your Smart Agent Project Ready

Make sure you've set up your project and created an agent as described in our initial guide. If you skipped that part, [click here](#) to get started. Once you're done, come back, and we'll move forward.

In the image below, you'll see my "SmartAgent" and the agent.py file inside the 'forge' folder. That's where we'll be adding our LLM-based logic. If you're unsure about the project structure or agent functions from our last guide, don't worry. We'll cover the basics as we go!

![SmartAgent](../../../docs/content/imgs/quickstart/t3_02.png)

---

## The Task Lifecycle

The lifecycle of a task, from its creation to execution, is outlined in the agent protocol. In simple terms: a task is initiated, its steps are systematically executed, and it concludes once completed.

Want your agent to perform an action? Start by dispatching a create_task request. This crucial step involves specifying the task details, much like how you'd send a prompt to ChatGPT, using the input field. If you're giving this a shot on your own, the UI is your best friend; it effortlessly handles all the API calls on your behalf.

When the agent gets this, it runs the create_task function. The code `super().create_task(task_request)` takes care of protocol steps. It then logs the task's start. For this guide, you don't need to change this function.

```python
async def create_task(self, task_request: TaskRequestBody) -> Task:
    """
    The agent protocol, which is the core of the Forge, works by creating a task and then
    executing steps for that task. This method is called when the agent is asked to create
    a task.

    We are hooking into function to add a custom log message. Though you can do anything you
    want here.
    """
    task = await super().create_task(task_request)
    LOG.info(
        f"ğŸ“¦ Task created: {task.task_id} input: {task.input[:40]}{'...' if len(task.input) > 40 else ''}"
    )
    return task
```py

After starting a task, the `execute_step` function runs until all steps are done. Here's a basic view of `execute_step`. I've left out the detailed comments for simplicity, but you'll find them in your project.

```python
async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
    # An example that
      step = await self.db.create_step(
          task_id=task_id, input=step_request, is_last=True
      )

      self.workspace.write(task_id=task_id, path="output.txt", data=b"Washington D.C")

      await self.db.create_artifact(
          task_id=task_id,
          step_id=step.step_id,
          file_name="output.txt",
          relative_path="",
          agent_created=True,
      )
      
      step.output = "Washington D.C"

      LOG.info(f"\tâœ… Final Step completed: {step.step_id}")

      return step
```py

Here's the breakdown of the 'write file' process in four steps:

1. **Database Step Creation**: The first stage is all about creating a step within the database, an essential aspect of the agent protocol. You'll observe that while setting up this step, we've flagged it with `is_last=True`. This signals to the agent protocol that no more steps are pending. For the purpose of this guide, let's work under the assumption that our agent will only tackle single-step tasks. However, hang tight for future tutorials, where we'll level up and let the agent determine its completion point.

2. **File Writing**: Next, we pen down "Washington D.C." using the workspace.write function.

3. **Artifact Database Update**: After writing, we record the file in the agent's artifact database.

4. **Step Output & Logging**: Finally, we set the step output to match the file content, log the executed step, and use the step object.

With the 'write file' process clear, let's make our agent smarter and more autonomous. Ready to dive in?

---

## Building the Foundations For Our SmartÂ Agent

First, we need to update the `execute_step()` function. Instead of a fixed solution, it should use the given request.

To do this, we'll fetch the task details using the provided `task_id`:

```python
task = await self.db.get_task(task_id)
```py

Next, remember to create a database record and mark it as a single-step task with `is_last=True`:

```python
step = await self.db.create_step(
    task_id=task_id, input=step_request, is_last=True
)
```py

Your updated `execute_step` function will look like this:

```python
async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
    # Get the task details
    task = await self.db.get_task(task_id)

    # Add a new step to the database
    step = await self.db.create_step(
        task_id=task_id, input=step_request, is_last=True
    )
    return step
```py

Now that we've set this up, let's move to the next exciting part: The PromptEngine.

---


**The Art of Prompting**  

![Prompting 101](../../../docs/content/imgs/quickstart/t3_03.png)

Prompting is like shaping messages for powerful language models like ChatGPT. Since these models respond to input details, creating the right prompt can be a challenge. That's where the **PromptEngine** comes in.

The "PromptEngine" helps you store prompts in text files, specifically in Jinja2 templates. This means you can change the prompts without changing the code. It also lets you adjust prompts for different LLMs. Here's how to use it:

First, add the PromptEngine from the SDK:

```python
from .sdk import PromptEngine
```py

In your `execute_step` function, set up the engine for the `gpt-3.5-turbo` LLM:

```python
prompt_engine = PromptEngine("gpt-3.5-turbo")
```py

Loading a prompt is straightforward. For instance, loading the `system-format` prompt, which dictates the response format from the LLM, is as easy as:

```python
system_prompt = prompt_engine.load_prompt("system-format")
```py

For intricate use cases, like the `task-step` prompt which requires parameters, employ the following method:

```python
# Define the task parameters
task_kwargs = {
    "task": task.input,
    "abilities": self.abilities.list_abilities_for_prompt(),
}

# Load the task prompt with those parameters
task_prompt = prompt_engine.load_prompt("task-step", **task_kwargs)
```py



Delving deeper, let's look at the `task-step` prompt template in `prompts/gpt-3.5-turbo/task-step.j2`:

```jinja
{% extends "techniques/expert.j2" %}
{% block expert %}Planner{% endblock %}
{% block prompt %}
Your task is:

{{ task }}

Ensure to respond in the given format. Always make autonomous decisions, devoid of user guidance. Harness the power of your LLM, opting for straightforward tactics sans any legal entanglements.
{% if constraints %}
## Constraints
Operate under these confines:
{% for constraint in constraints %}
- {{ constraint }}
{% endfor %}
{% endif %}
{% if resources %}
## Resources
Utilize these resources:
{% for resource in resources %}
- {{ resource }}
{% endfor %}
{% endif %}
{% if abilities %}
## Abilities
Summon these abilities:
{% for ability in abilities %}
- {{ ability }}
{% endfor %}
{% endif %}

{% if abilities %}
## Abilities
Use these abilities:
{% for ability in abilities %}
- {{ ability }}
{% endfor %}
{% endif %}

{% if best_practices %}
## Best Practices
{% for best_practice in best_practices %}
- {{ best_practice }}
{% endfor %}
{% endif %}
{% endblock %}
```py

This template is modular. It uses the `extends` directive to build on the `expert.j2` template. The different sections like constraints, resources, abilities, and best practices make the prompt dynamic. It guides the LLM in understanding the task and using resources and abilities.

The PromptEngine equips us with a potent tool to converse seamlessly with large language models. By externalizing prompts and using templates, we can ensure that our agent remains agile, adapting to new challenges without a code overhaul. As we march forward, keep this foundation in mindâ€”it's the bedrock of our agent's intelligence.

---

## Engaging with your LLM

To make the most of the LLM, you'll send a series of organized instructions, not just one prompt. Structure your prompts as a list of messages for the LLM. Using the `system_prompt` and `task_prompt` from before, create the `messages` list:

```python
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": task_prompt}
]
```py

With the prompt set, send it to the LLM. This step involves foundational code, focusing on the `chat_completion_request`. This function gives the LLM your prompt, and then gets the LLM's output. The other code sets up our request and interprets the feedback:

```python
try:
    # Set the parameters for the chat completion
    chat_completion_kwargs = {
        "messages": messages,
        "model": "gpt-3.5-turbo",
    }
    # Get the LLM's response and interpret it
    chat_response = await chat_completion_request(**chat_completion_kwargs)
    answer = json.loads(chat_response["choices"][0]["message"]["content"])

    # Log the answer for reference
    LOG.info(pprint.pformat(answer))

except json.JSONDecodeError as e:
    # Handle JSON decoding errors
    LOG.error(f"Can't decode chat response: {chat_response}")
except Exception as e:
    # Handle other errors
    LOG.error(f"Can't get chat response: {e}")
```py

Extracting clear messages from LLM outputs can be complex. Our method is simple and works with GPT-3.5 and GPT-4. Future guides will show more ways to interpret LLM outputs. The goal? To go beyond JSON, as some LLMs work best with other response types. Stay tuned!

---


## Using and Creating Abilities

Abilities are the gears and levers that enable the agent to interact with tasks at hand. Let's unpack the mechanisms behind these abilities and how you can harness, and even extend, them.

In the SDK, there's a `abilities` folder containing `registry.py`, `finish.py`, and a `file_system` subfolder. You can also add your own abilities here. `registry.py` is the main file for abilities. It contains the `@ability` decorator and the `AbilityRegister` class. This class actively tracks abilities and outlines their function. The base Agent class includes a default ability register available via `self.abilities`. It looks like this:

```python
self.abilities = AbilityRegister(self)
```py

The `AbilityRegister` has two key methods. `list_abilities_for_prompt` prepares abilities for prompts. `run_ability` makes the ability work. An ability is a function with the `@ability` decorator. It must have specific parameters, including the agent and `task_id`.

```python
@ability(
    name="write_file",
    description="Write data to a file",
    parameters=[
        {
            "name": "file_path",
            "description": "Path to the file",
            "type": "string",
            "required": True,
        },
        {
            "name": "data",
            "description": "Data to write to the file",
            "type": "bytes",
            "required": True,
        },
    ],
    output_type="None",
)
async def write_file(agent, task_id: str, file_path: str, data: bytes) -> None:
    pass
```py

The `@ability` decorator defines the ability's details, like its identity (name), functionality (description), and operational parameters.

## Example of a Custom Ability: Webpage Fetcher

```python
import requests

@ability(
  name="fetch_webpage",
  description="Retrieve the content of a webpage",
  parameters=[
      {
          "name": "url",
          "description": "Webpage URL",
          "type": "string",
          "required": True,
      }
  ],
  output_type="string",
)
async def fetch_webpage(agent, task_id: str, url: str) -> str:
  response = requests.get(url)
  return response.text
```py

This ability, `fetch_webpage`, accepts a URL as input and returns the HTML content of the webpage as a string. Custom abilities let you add more features to your agent. They can integrate other tools and libraries to enhance its functions. To make a custom ability, you need to understand the structure and add technical details. With abilities like "fetch_webpage", your agent can handle complex tasks efficiently.

## Running an Ability

Now that you understand abilities and how to create them, let's use them. The last piece is the `execute_step` function. Our goal is to understand the agent's response, find the ability, and use it. 

First, we get the ability details from the agent's answer:

```python
# Extract the ability from the answer
ability = answer["ability"]
```py

With the ability details, we use it. We call the `run_ability` function:

```python
# Run the ability and get the output
# We don't actually use the output in this example
output = await self.abilities.run_ability(
    task_id, ability["name"], **ability["args"]
)
```py

Here, weâ€™re invoking the specified ability. The task_id ensures continuity, ability['name'] pinpoints the exact function, and the arguments (ability["args"]) provide necessary context.

Finally, we make the step's output show the agent's thinking:

```python
# Set the step output to the "speak" part of the answer
step.output = answer["thoughts"]["speak"]

# Return the completed step
return step
```py

And there you have it! Your first Smart Agent, sculpted with precision and purpose, stands ready to take on challenges. The stage is set. Itâ€™s showtime!

Here is what your function should look like:

```python
async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
    # Firstly we get the task this step is for so we can access the task input
    task = await self.db.get_task(task_id)

    # Create a new step in the database
    step = await self.db.create_step(
        task_id=task_id, input=step_request, is_last=True
    )

    # Log the message
    LOG.info(f"\tâœ… Final Step completed: {step.step_id} input: {step.input[:19]}")

    # Initialize the PromptEngine with the "gpt-3.5-turbo" model
    prompt_engine = PromptEngine("gpt-3.5-turbo")

    # Load the system and task prompts
    system_prompt = prompt_engine.load_prompt("system-format")

    # Initialize the messages list with the system prompt
    messages = [
        {"role": "system", "content": system_prompt},
    ]
    # Define the task parameters
    task_kwargs = {
        "task": task.input,
        "abilities": self.abilities.list_abilities_for_prompt(),
    }

    # Load the task prompt with the defined task parameters
    task_prompt = prompt_engine.load_prompt("task-step", **task_kwargs)

    # Append the task prompt to the messages list
    messages.append({"role": "user", "content": task_prompt})

    try:
        # Define the parameters for the chat completion request
        chat_completion_kwargs = {
            "messages": messages,
            "model": "gpt-3.5-turbo",
        }
        # Make the chat completion request and parse the response
        chat_response = await chat_completion_request(**chat_completion_kwargs)
        answer = json.loads(chat_response["choices"][0]["message"]["content"])

        # Log the answer for debugging purposes
        LOG.info(pprint.pformat(answer))

    except json.JSONDecodeError as e:
        # Handle JSON decoding errors
        LOG.error(f"Unable to decode chat response: {chat_response}")
    except Exception as e:
        # Handle other exceptions
        LOG.error(f"Unable to generate chat response: {e}")

    # Extract the ability from the answer
    ability = answer["ability"]

    # Run the ability and get the output
    # We don't actually use the output in this example
    output = await self.abilities.run_ability(
        task_id, ability["name"], **ability["args"]
    )

    # Set the step output to the "speak" part of the answer
    step.output = answer["thoughts"]["speak"]

    # Return the completed step
    return step
```py

## Interacting with your Agent
> âš ï¸ Heads up: The UI and benchmark are still in the oven, so they might be a tad glitchy.

With the heavy lifting of crafting our Smart Agent behind us, itâ€™s high time to see it in action. Kick things off by firing up the agent with this command:
```bash
./run agent start SmartAgent.
```py

Once your digital playground is all set, your terminal should light up with:
```bash


       d8888          888             .d8888b.  8888888b. 88888888888 
      d88888          888            d88P  Y88b 888   Y88b    888     
     d88P888          888            888    888 888    888    888     
    d88P 888 888  888 888888 .d88b.  888        888   d88P    888     
   d88P  888 888  888 888   d88""88b 888  88888 8888888P"     888     
  d88P   888 888  888 888   888  888 888    888 888           888     
 d8888888888 Y88b 888 Y88b. Y88..88P Y88b  d88P 888           888     
d88P     888  "Y88888  "Y888 "Y88P"   "Y8888P88 888           888     
                                                                      
                                                                      
                                                                      
                8888888888                                            
                888                                                   
                888                                                   
                8888888  .d88b.  888d888 .d88b.   .d88b.              
                888     d88""88b 888P"  d88P"88b d8P  Y8b             
                888     888  888 888    888  888 88888888             
                888     Y88..88P 888    Y88b 888 Y8b.                 
                888      "Y88P"  888     "Y88888  "Y8888              
                                             888                      
                                        Y8b d88P                      
                                         "Y88P"                v0.1.0


[2023-09-27 15:39:07,832] [forge.sdk.agent] [INFO]      ğŸ“  Agent server starting on http://localhost:8000

```py
1. **Get Started**
   - Click the link to access the AutoGPT Agent UI.

2. **Login**
   - Log in using your Gmail or Github credentials.

3. **Navigate to Benchmarking**
   - Look to the left, and you'll spot a trophy icon. Click it to enter the benchmarking arena.
  
![Benchmarking page of the AutoGPT UI](../../../docs/content/imgs/quickstart/t3_04.png)

4. **Select the 'WriteFile' Test**
   - Choose the 'WriteFile' test from the available options.

5. **Initiate the Test Suite**
   - Hit 'Initiate test suite' to start the benchmarking process.

6. **Monitor in Real-Time**
   - Keep your eyes on the right panel as it displays real-time output.

7. **Check the Console**
   - For additional information, you can also monitor your console for progress updates and messages.
```bash
ğŸ“  ğŸ“¦ Task created: 70518b75-0104-49b0-923e-f607719d042b input: Write the word 'Washington' to a .txt fi...
ğŸ“      âœ… Final Step completed: a736c45f-65a5-4c44-a697-f1d6dcd94d5c input: y
```
If you see this, you've done it!

8. **Troubleshooting**
   - If you encounter any issues or see cryptic error messages, don't worry. Just hit the retry button. Remember, LLMs are powerful but may occasionally need some guidance.

## Wrap Up
- Stay tuned for our next tutorial, where we'll enhance the agent's capabilities by adding memory!

## Keep Exploring
- Keep experimenting and pushing the boundaries of AI. Happy coding! ğŸš€

## Wrap Up
In our next tutorial, weâ€™ll further refine this process, enhancing the agentâ€™s capabilities, through the addition of memory!

Until then, keep experimenting and pushing the boundaries of AI. Happy coding! ğŸš€


# Memory Integration: Enabling Your Agent to Remember and Learn

## Introduction
- Importance of Memory Integration in AI Agents
- Overview of Memory Mechanisms in AutoGPT

## Section 1: Understanding Memory Integration
- Concept of Memory in AI Agents
- Types of Memory: Short-term vs. Long-term

## Section 2: Implementing Memory in Your Agent
- Setting up Memory Structures in the Forge Environment
- Utilizing Agent Protocol for Memory Integration

## Section 3: Developing Learning Mechanisms
- Creating Learning Algorithms for Your Agent
- Implementing Learning Mechanisms using Task and Artifact Schemas

## Section 4: Testing and Optimizing Memory Integration
- Employing AGBenchmark for Memory Testing
- Optimizing Memory for Enhanced Performance and Efficiency

## Section 5: Best Practices in Memory Integration
- Tips and Strategies for Effective Memory Integration
- Avoiding Common Pitfalls in Memory Development

## Conclusion
- Recap of the Tutorial
- Future Directions in Memory Integration

## Additional Resources

From **The Rise and Potential of Large Language Model Based Agents: A Survey** *Zhiheng Xi (Fudan University) et al. arXiv.* [[paper](https://arxiv.org/abs/2305.14497)] [[code](https://github.com/woooodyy/llm-agent-paper-list)]

##### Memory capability

###### Raising the length limit of Transformers

- [2023/05] **Randomized Positional Encodings Boost Length Generalization of Transformers.** *Anian Ruoss (DeepMind) et al. arXiv.* [[paper](https://arxiv.org/abs/2305.16843)] [[code](https://github.com/google-deepmind/randomized_positional_encodings)]
- [2023-03] **CoLT5: Faster Long-Range Transformers with Conditional Computation.** *Joshua Ainslie (Google Research) et al. arXiv.* [[paper](https://arxiv.org/abs/2303.09752)]
- [2022/03] **Efficient Classification of Long Documents Using Transformers.** *Hyunji Hayley Park (Illinois University) et al. arXiv.* [[paper](https://arxiv.org/abs/2203.11258)] [[code](https://github.com/amazon-science/efficient-longdoc-classification)]
- [2021/12] **LongT5: Efficient Text-To-Text Transformer for Long Sequences.** *Mandy Guo (Google Research) et al. arXiv.* [[paper](https://arxiv.org/abs/2112.07916)] [[code](https://github.com/google-research/longt5)]
- [2019/10] **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.** *Michael Lewis(Facebook AI) et al. arXiv.* [[paper](https://arxiv.org/abs/1910.13461)] [[code](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bart)]

###### Summarizing memory

- [2023/08] **ExpeL: LLM Agents Are Experiential Learners.** *Andrew Zhao (Tsinghua University) et al. arXiv.* [[paper](https://arxiv.org/abs/2308.10144)] [[code]([https://github.com/thunlp/ChatEval](https://github.com/Andrewzh112/ExpeL))]
- [2023/08] **ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.** *Chi-Min Chan (Tsinghua University) et al. arXiv.* [[paper](https://arxiv.org/abs/2308.07201)] [[code](https://github.com/thunlp/ChatEval)]
- [2023/05] **MemoryBank: Enhancing Large Language Models with Long-Term Memory.** *Wanjun Zhong (Harbin Institute of Technology) et al. arXiv.* [[paper](https://arxiv.org/abs/2305.10250)] [[code](https://github.com/zhongwanjun/memorybank-siliconfriend)]
- [2023/04] **Generative Agents: Interactive Simulacra of Human Behavior.** *Joon Sung Park (Stanford University) et al. arXiv.* [[paper](https://arxiv.org/abs/2304.03442)] [[code](https://github.com/joonspk-research/generative_agents)]
- [2023/04] **Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System.** *Xinnian Liang(Beihang University) et al. arXiv.* [[paper](https://arxiv.org/abs/2304.13343)] [[code](https://github.com/wbbeyourself/scm4llms)]
- [2023/03] **Reflexion: Language Agents with Verbal Reinforcement Learning.** *Noah Shinn (Northeastern University) et al. arXiv.* [[paper](https://arxiv.org/abs/2303.11366)] [[code](https://github.com/noahshinn024/reflexion)]
- [2023/05] **RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text.** Wangchunshu Zhou (AIWaves) et al. arXiv.* [[paper](https://arxiv.org/pdf/2305.13304.pdf)] [[code](https://github.com/aiwaves-cn/RecurrentGPT)]  


###### Compressing memories with vectors or data structures

- [2023/07] **Communicative Agents for Software Development.** *Chen Qian (Tsinghua University) et al. arXiv.* [[paper](https://arxiv.org/abs/2307.07924)] [[code](https://github.com/openbmb/chatdev)]
- [2023/06] **ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.** *Chenxu Hu(Tsinghua University) et al. arXiv.* [[paper](https://arxiv.org/abs/2306.03901)] [[code](https://github.com/huchenxucs/ChatDB)]
- [2023/05] **Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.** *Xizhou Zhu (Tsinghua University) et al. arXiv.* [[paper](https://arxiv.org/abs/2305.17144)] [[code](https://github.com/OpenGVLab/GITM)]
- [2023/05] **RET-LLM: Towards a General Read-Write Memory for Large Language Models.** *Ali Modarressi (LMU Munich) et al. arXiv.* [[paper](https://arxiv.org/abs/2305.14322)] [[code](https://github.com/tloen/alpaca-lora)]
- [2023/05] **RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text.** Wangchunshu Zhou (AIWaves) et al. arXiv.* [[paper](https://arxiv.org/pdf/2305.13304.pdf)] [[code](https://github.com/aiwaves-cn/RecurrentGPT)]

##### Memory retrieval

- [2023/08] **Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents.** *Ziheng Huang(University of Californiaâ€”San Diego) et al. arXiv.* [[paper](https://arxiv.org/abs/2308.01542)]
- [2023/08] **AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.** *Jiaju Lin (PTA Studio) et al. arXiv.* [[paper](https://arxiv.org/abs/2308.04026)] [[project page](https://www.agentsims.com/)] [[code](https://github.com/py499372727/AgentSims/)] 
- [2023/06] **ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.** *Chenxu Hu(Tsinghua University) et al. arXiv.* [[paper](https://arxiv.org/abs/2306.03901)] [[code](https://github.com/huchenxucs/ChatDB)]
- [2023/05] **MemoryBank: Enhancing Large Language Models with Long-Term Memory.** *Wanjun Zhong (Harbin Institute of Technology) et al. arXiv.* [[paper](https://arxiv.org/abs/2305.10250)] [[code](https://github.com/zhongwanjun/memorybank-siliconfriend)]
- [2023/04] **Generative Agents: Interactive Simulacra of Human Behavior.** *Joon Sung Park (Stanford) et al. arXiv.* [[paper](https://arxiv.org/abs/2304.03442)] [[code](https://github.com/joonspk-research/generative_agents)]
- [2023/05] **RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text.** Wangchunshu Zhou (AIWaves) et al. arXiv.* [[paper](https://arxiv.org/pdf/2305.13304.pdf)] [[code](https://github.com/aiwaves-cn/RecurrentGPT)]

## Appendix
- Examples of Memory Integration Implementations
- Glossary of Memory-Related Terms


# Auto-GPT Benchmarks

Built for the purpose of benchmarking the performance of agents regardless of how they work.

Objectively know how well your agent is performing in categories like code, retrieval, memory, and safety.

Save time and money while doing it through smart dependencies. The best part? It's all automated.

## Scores:

<img width="733" alt="Screenshot 2023-07-25 at 10 35 01 AM" src="https://github.com/Significant-Gravitas/Auto-GPT-Benchmarks/assets/9652976/98963e0b-18b9-4b17-9a6a-4d3e4418af70">

## Ranking overall:

- 1- [Beebot](https://github.com/AutoPackAI/beebot)
- 2- [mini-agi](https://github.com/muellerberndt/mini-agi)
- 3- [Auto-GPT](https://github.com/Significant-Gravitas/AutoGPT)

## Detailed results:

<img width="733" alt="Screenshot 2023-07-25 at 10 42 15 AM" src="https://github.com/Significant-Gravitas/Auto-GPT-Benchmarks/assets/9652976/39be464c-c842-4437-b28a-07d878542a83">

[Click here to see the results and the raw data!](https://docs.google.com/spreadsheets/d/1WXm16P2AHNbKpkOI0LYBpcsGG0O7D8HYTG5Uj0PaJjA/edit#gid=203558751)!

More agents coming soon !


# `benchmark/server.py`

è¿™æ®µä»£ç ä½¿ç”¨äº†å¤šç§Pythonåº“ï¼ŒåŒ…æ‹¬fastapiã€jsonã€loggingå’Œshutilï¼Œä½œç”¨æ˜¯ä¸ºäº†è§£å†³ç‰¹å®šçš„ä»»åŠ¡æˆ–é—®é¢˜ã€‚ä¸‹é¢æ˜¯æ¯éƒ¨åˆ†ä»£ç çš„ä½œç”¨ï¼š

1. `import io`ï¼šç”¨äºå¯¼å…¥ioåº“ï¼Œä»¥ä¾¿æˆ‘ä»¬ä½¿ç”¨å…¶ä¸­çš„æµå¯¹è±¡ï¼ˆå¦‚æ–‡ä»¶æµæˆ–å­—ç¬¦æµç­‰ï¼‰ã€‚
2. `import json`ï¼šç”¨äºå¯¼å…¥jsonåº“ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿæ–¹ä¾¿åœ°è§£æå’Œç”ŸæˆJSONæ ¼å¼çš„æ•°æ®ã€‚
3. `import logging`ï¼šç”¨äºå¯¼å…¥loggingåº“ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿæ–¹ä¾¿åœ°ä½¿ç”¨æ—¥å¿—è®°å½•åŠŸèƒ½ã€‚
4. `import shutil`ï¼šç”¨äºå¯¼å…¥shutilåº“ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿæ–¹ä¾¿åœ°å¤„ç†æ–‡ä»¶å’Œç›®å½•æ“ä½œã€‚
5. `from pathlib import Path`ï¼šç”¨äºä»pathlibåº“ä¸­å¯¼å…¥Pathå¯¹è±¡ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨å®ƒä»¬æ¥è¡¨ç¤ºæ–‡ä»¶æˆ–ç›®å½•çš„è·¯å¾„ã€‚
6. `from random import randint`ï¼šç”¨äºä»randomåº“ä¸­å¯¼å…¥randintå‡½æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆä¸€ä¸ªéšæœºçš„æ•´æ•°ã€‚
7. `from typing import Annotated, Any, Dict, List`ï¼šç”¨äºä»typingåº“ä¸­å¯¼å…¥Annotatedã€Anyã€Dictå’ŒListç±»å‹ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿå®šä¹‰è¿™äº›ç±»å‹ã€‚
8. `from fastapi import FastAPI, File, Form, HTTPException, UploadFile`ï¼šç”¨äºå¯¼å…¥fastapiåº“ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨å®ƒä»¬æ¥åˆ›å»ºAPIå’Œå¤„ç†æ–‡ä»¶ä¸Šä¼ ã€‚
9. `from fastapi.responses import StreamingResponse`ï¼šç”¨äºå¯¼å…¥fastapi.responsesåº“ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨å®ƒä»¬æ¥è¿”å›ä¸åŒç±»å‹çš„å“åº”ï¼ˆå¦‚JSONã€å›¾ç‰‡ç­‰ï¼‰ã€‚
10. `from pydantic import BaseModel`ï¼šç”¨äºå¯¼å…¥pydanticåº“ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨å®ƒä»¬æ¥å®šä¹‰APIå’Œæ•°æ®æ¨¡å‹ã€‚


```py
import io
import json
import logging
import shutil
from pathlib import Path
from random import randint
from typing import Annotated, Any, Dict, List

from fastapi import FastAPI, File, Form, HTTPException, UploadFile
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

```

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ª FastAPI åº”ç”¨å’Œä¸€ç»„ Artifactsã€‚

FastAPI æ˜¯ä¸€ä¸ªç”¨äºæ„å»º API çš„ç°ä»£ Python web æ¡†æ¶ã€‚è¿™é‡Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª FastAPI åº”ç”¨ï¼Œè¡¨ç¤ºæˆ‘ä»¬çš„ API å°†å¦‚ä½•å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚ã€‚

Artifacts æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ª Artifact éƒ½æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªäºŒè¿›åˆ¶æ•°æ®ã€ä¸€ä¸ªæˆ–å¤šä¸ªé”®å€¼å¯¹ï¼ˆå³ç›¸å¯¹è·¯å¾„ï¼‰å’Œä¸€ä¸ªåä¸º ArtifactID çš„éšæœºç”Ÿæˆçš„å­—ç¬¦ä¸²ã€‚

`Task` æ˜¯ä¸€ä¸ªç±»ï¼Œå®ƒæ˜¯ FastAPI çš„æ¨¡å‹ï¼Œå®šä¹‰äº†ä¸Šä¼ æ–‡ä»¶çš„è¾“å…¥å‚æ•°ã€‚

`@app.post("/agent/tasks/{task_id}/artifacts")` æ˜¯ FastAPI çš„è·¯ç”±ï¼Œå½“å®¢æˆ·ç«¯å‘å‡ºä¸€ä¸ª /agent/tasks/{task_id}/artifacts è¯·æ±‚æ—¶ï¼Œè¿™ä¸ªè·¯ç”±å°†è¢«è°ƒç”¨ã€‚è¿™ä¸ªè·¯ç”±å°†æ¥æ”¶ä¸€ä¸ªä»»åŠ¡ ID å’Œä¸€ä¸ªæ–‡ä»¶ï¼Œç„¶åå°†æ–‡ä»¶ä¸Šä¼ åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚

`file` æ˜¯æ–‡ä»¶å‚æ•°ï¼Œå®ƒé€šè¿‡ `Annotated[UploadFile, File()]` ç±»å‹è¿›è¡Œçº¦æŸï¼Œåªèƒ½ä¸Šä¼ æ–‡ä»¶æˆ–è€…ä» `File` ç±»ä¸­è¯»å–æ–‡ä»¶ã€‚

`relative_path` æ˜¯å¯é€‰çš„ï¼Œå®ƒæŒ‡å®šäº†æ–‡ä»¶åœ¨ API è¯·æ±‚ä¸­çš„è·¯å¾„ã€‚å¦‚æœ `relative_path` è¢«æä¾›ï¼Œåˆ™å®ƒå°†è¢«ç”¨ä½œæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ã€‚

`artifacts` åˆ—è¡¨ç”¨äºå­˜å‚¨ Artifactsï¼Œå®ƒè¢«åˆå§‹åŒ–ä¸ºä¸€ä¸ªç©ºåˆ—è¡¨ã€‚

`upload_file` æ˜¯ `Task` ç±»çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºä¸Šä¼ æ–‡ä»¶åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚è¿™ä¸ªæ–¹æ³•å°†è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¹¶å°†å…¶å­˜å‚¨ä¸º `artifact_data` å­—å…¸ï¼Œç„¶åå°†å…¶æ·»åŠ åˆ° `artifacts` åˆ—è¡¨ä¸­ã€‚

æœ€åï¼Œ`logger.info` å°†ç”¨äºè®°å½•ä¸Šä¼ æ–‡ä»¶çš„æ—¥å¿—ä¿¡æ¯ï¼Œ`return` è¡¨ç¤ºè·¯ç”±çš„å“åº”ï¼Œå…¶ä¸­ `artifact_id` æ˜¯éšæœºç”Ÿæˆçš„å­—ç¬¦ä¸²ï¼Œ`file_name` æ˜¯æ–‡ä»¶åï¼Œ`relative_path` æ˜¯æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœæä¾›äº†çš„è¯ï¼‰ã€‚


```py
app = FastAPI()
artifacts: List[Dict[str, Any]] = []


class Task(BaseModel):
    input: str


@app.post("/agent/tasks/{task_id}/artifacts")
async def upload_file(
    task_id: str, file: Annotated[UploadFile, File()], relative_path: str = Form("")
) -> Dict[str, Any]:
    logger.info(
        "Uploading file for task_id: %s with relative path: %s", task_id, relative_path
    )
    absolute_directory_path = Path(__file__).parent.absolute()
    save_path = (
        absolute_directory_path
        / "agent/gpt-engineer"
        / "projects/my-new-project/workspace"
    )

    random_string = str(randint(0, 100000))
    while random_string in artifacts:
        random_string = str(randint(0, 100000))

    artifact_data = await file.read()
    artifacts.append(
        {
            "binary": artifact_data,
            "relative_path": relative_path,
            "file_name": file.filename,
            "artifact_id": random_string,
        }
    )

    print(artifacts)
    return {
        "artifact_id": random_string,
        "file_name": "file_name",
        "relative_path": "relative_path",
    }


```

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª Python ç¼–å†™çš„ Flask åº”ç”¨ç¨‹åºä¸­çš„ä¸¤ä¸ªè·¯ç”±ï¼Œåˆ†åˆ«æ˜¯ `/agent/tasks/{task_id}/artifacts` å’Œ `/agent/tasks/{task_id}/artifacts/{artifact_id}`ã€‚å®ƒä»¬çš„ä½œç”¨æ˜¯è·å–æŒ‡å®šä»»åŠ¡ä¸‹çš„è‰ºæœ¯å“åˆ—è¡¨å’ŒæŒ‡å®šè‰ºæœ¯å“çš„ä¿¡æ¯ã€‚

å…·ä½“æ¥è¯´ï¼Œç¬¬ä¸€ä¸ªè·¯ç”± `/agent/tasks/{task_id}/artifacts` å°†ä¼šè·å–åŒ…å«æŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰è‰ºæœ¯å“åˆ—è¡¨ã€‚è¿™ä¸ªè·¯ç”±ä¸­çš„ `{task_id}` å‚æ•°å°†ä¼šè¢«æ›¿æ¢ä¸ºä»»åŠ¡ IDï¼Œå› æ­¤å¦‚æœä½ åœ¨ä¸€ä¸ªä»£ç†ç¨‹åºä¸­è¿è¡Œäº† `app.get("/agent/tasks/123456/artifacts")`ï¼Œå®ƒå°†ä¼šè·å–åˆ°ä»£ç†ç¨‹åºä¸­æ‰€æœ‰ä»»åŠ¡ä¸‹çš„è‰ºæœ¯å“åˆ—è¡¨ã€‚è¿™ä¸ªè·¯ç”±ä¸­çš„ `artifacts` å‚æ•°å°†ä¼šè¢«è§£ææˆä¸€ä¸ª JSON å“åº”ï¼Œå…¶ä¸­åŒ…å«æŒ‡å®šä»»åŠ¡ä¸‹çš„æ‰€æœ‰è‰ºæœ¯å“å¯¹è±¡ã€‚ç”±äº `artifacts` æ˜¯ä¸€ä¸ª JSON æ•°æ®ç»“æ„ï¼Œå› æ­¤ `get_files()` å‡½æ•°å°†ä½œä¸ºä¸€ä¸ª `List[Dict[str, Any]]` ç±»å‹çš„å‡½æ•°ï¼Œå®ƒå°†ä¼šè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ–‡ä»¶åçš„åˆ—è¡¨ã€‚

ç¬¬äºŒä¸ªè·¯ç”± `/agent/tasks/{task_id}/artifacts/{artifact_id}` å°†ä¼šè·å–æŒ‡å®šè‰ºæœ¯å“çš„ä¿¡æ¯ã€‚è¿™ä¸ªè·¯ç”±ä¸­çš„ `{task_id}` å‚æ•°å°†ä¼šè¢«æ›¿æ¢ä¸ºä»»åŠ¡ IDï¼Œå› æ­¤å¦‚æœä½ åœ¨ä¸€ä¸ªä»£ç†ç¨‹åºä¸­è¿è¡Œäº† `app.get("/agent/tasks/123456/artifacts/678901")`ï¼Œå®ƒå°†ä¼šè·å–åˆ°ä»£ç†ç¨‹åºä¸­æŒ‡å®šä»»åŠ¡ä¸‹çš„æŒ‡å®šè‰ºæœ¯å“çš„ä¿¡æ¯ã€‚è¿™ä¸ªè·¯ç”±ä¸­çš„ `{artifact_id}` å‚æ•°å°†ä¼šè¢«æ›¿æ¢ä¸ºè‰ºæœ¯å“ IDï¼Œå› æ­¤å¦‚æœä½ åœ¨ä¸€ä¸ªä»£ç†ç¨‹åºä¸­è¿è¡Œäº† `app.get("/agent/tasks/123456/artifacts/678901")`ï¼Œå®ƒå°†ä¼šè·å–åˆ°æŒ‡å®šä»»åŠ¡ä¸‹çš„æŒ‡å®šè‰ºæœ¯å“çš„ä¿¡æ¯ã€‚

åœ¨ `get_file()` å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ `artifacts` æ•°æ®ç»“æ„æŸ¥æ‰¾åˆ°äº†æŒ‡å®šè‰ºæœ¯å“çš„ä¿¡æ¯ï¼Œå¹¶è¿”å›äº†è¯¥è‰ºæœ¯å“çš„ä¿¡æ¯ as a file-like objectã€‚æˆ‘ä»¬é€šè¿‡ `for` å¾ªç¯æ¥æŸ¥æ‰¾æŒ‡å®šè‰ºæœ¯å“ï¼Œå¦‚æœæŸ¥æ‰¾åˆ°äº†è¯¥è‰ºæœ¯å“ï¼Œå°±è¿”å›äº†è¯¥æ–‡ä»¶çš„ä¿¡æ¯ï¼Œå¦åˆ™è¿”å›äº†ä¸€ä¸ª HTTP 404 é”™è¯¯å¹¶è¿”å›äº†ä¸€ä¸ªé”™è¯¯æ¶ˆæ¯ã€‚


```py
@app.get("/agent/tasks/{task_id}/artifacts")
async def get_files() -> List[Dict[str, Any]]:
    logger.info("Fetching list of files for task")
    return artifacts


@app.get("/agent/tasks/{task_id}/artifacts/{artifact_id}")
async def get_file(artifact_id: str):
    for artifact in artifacts:
        if artifact["artifact_id"] == artifact_id:
            break
    else:
        logger.error("Attempt to access nonexistent artifact with ID: %s", artifact_id)
        raise HTTPException(status_code=404, detail="Artifact not found")

    logger.info("Fetching artifact with ID: %s", artifact_id)
    # find aritifact where artifact_id = artifact_id

    for artifact in artifacts:
        if artifact["artifact_id"] == artifact_id:
            return StreamingResponse(
                io.BytesIO(artifact["binary"]),
                media_type="application/octet-stream",
                headers={"Content-Disposition": f"attachment; filename=test.txt"},
            )
    # return 404
    return HTTPException(status_code=404, detail="Artifact not found")


```

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ï¼Œä½¿ç”¨Pythonçš„ä½œå¼Šæ¥å£asyncioæ¥ç¼–å†™ã€‚

å®ƒçš„ä½œç”¨æ˜¯å½“æœ‰ä¸€ä¸ªè¯·æ±‚åˆ°è¾¾æ—¶ï¼Œä»¥postçš„æ–¹å¼å‘é€ä¸€ä¸ªä»»åŠ¡ç¼–å·ï¼ˆtask_idï¼‰ï¼Œå¹¶è·å–è¯¥ä»»åŠ¡çš„æ‰€æœ‰æ­¥éª¤ã€‚

å…·ä½“æ¥è¯´ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªå‚æ•°ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„æ­¥éª¤ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨æœ¬åœ°å˜é‡ä¸­ã€‚ç„¶åï¼Œå®ƒå°†æ­¥éª¤æ·»åŠ åˆ°ä»»åŠ¡å…ƒæ•°æ®ä¸­ï¼Œä»¥ä¾¿åœ¨ä»»åŠ¡æˆåŠŸå®Œæˆåå‘å®¢æˆ·ç«¯å‘é€ã€‚

æ­¥éª¤å…ƒæ•°æ®åŒ…å«è¾“å…¥ã€é™„åŠ è¾“å…¥ã€ä»»åŠ¡ç¼–å·ã€æ­¥éª¤ç¼–å·ã€åç§°ã€çŠ¶æ€ã€è¾“å‡ºã€é™„åŠ è¾“å‡ºã€è‰ºæœ¯å“å’Œæ˜¯å¦æ˜¯æœ€åä¸€æ­¥ç­‰ä¿¡æ¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªæ˜¯ç®€å•åœ°åˆ›å»ºä¸€ä¸ªéšæœºæ­¥éª¤ï¼Œå¹¶å°†å®ƒæ·»åŠ åˆ°ä»»åŠ¡å…ƒæ•°æ®ä¸­ã€‚

æœ€åï¼Œå®ƒè¿˜è¿”å›ä¸€ä¸ªè¡¨ç¤ºå·²åˆ›å»ºæ­¥éª¤çš„å¸ƒå°”å€¼ï¼Œä»¥ä¾¿åœ¨ç¨åçš„æ£€æŸ¥ä¸­è¿›è¡Œä½¿ç”¨ã€‚


```py
@app.post("/agent/tasks/{task_id}/steps")
async def create_steps(task_id: str):
    logger.info("Creating step for task_id: %s", task_id)
    return {
        "input": "random",
        "additional_input": {},
        "task_id": task_id,
        "step_id": "random_step",
        "name": "random",
        "status": "created",
        "output": "random",
        "additional_output": {},
        "artifacts": [],
        "is_last": True,
    }


```

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ï¼Œç”¨äºåœ¨ç‹—ç„•åº”ç”¨ä¸­åˆ›å»ºæ–°ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡å‘ `/agent/tasks` è·¯å¾„å‘é€ä¸€ä¸ª POST è¯·æ±‚æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œå¹¶å°†ä»»åŠ¡çš„ä¿¡æ¯å­˜å‚¨åœ¨ `artifacts` é›†åˆä¸­ã€‚

å½“è¿™æ®µä»£ç åœ¨æ²¡æœ‰å…¶ä»–ä»£ç ç›´æ¥è°ƒç”¨æ—¶ï¼Œå®ƒä¼šåœ¨åˆ›å»ºæ–°ä»»åŠ¡æ—¶æ‰§è¡Œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†æ¸…é™¤ `artifacts` é›†åˆå¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ–°ä»»åŠ¡ä¿¡æ¯çš„å­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª `input` å­—æ®µï¼Œç”¨äºæŒ‡å®šä»»åŠ¡æ¥æ”¶è€…çš„è¾“å…¥ï¼Œä»¥åŠä¸€ä¸ª `additional_input` å­—æ®µï¼Œç”¨äºå­˜å‚¨ä»»åŠ¡çš„å…¶ä»–é™„åŠ ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å°† `task_id` å­—æ®µè®¾ç½®ä¸º `static_task_id`ï¼Œå¹¶å°† `artifacts` åˆ—è¡¨åˆå§‹åŒ–ä¸ºä¸€ä¸ªç©ºåˆ—è¡¨ã€‚

å¦‚æœè¿™æ®µä»£ç åœ¨è¢«ç§°ä¸ºä¸»ç¨‹åºçš„ç¨‹åºä¸­è¢«è°ƒç”¨ï¼Œå®ƒå°†ä½¿ç”¨ `uvicorn` å‡½æ•°æœåŠ¡å™¨è¿è¡Œè¯¥åº”ç”¨ç¨‹åºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†ç›‘å¬æ‰€æœ‰ç»è¿‡ `/agent/tasks` è·¯å¾„çš„è¯·æ±‚ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„åˆ›å»ºä»»åŠ¡æ“ä½œã€‚


```py
@app.post("/agent/tasks")
async def create_tasks(task: Task):
    artifacts.clear()
    return {
        "input": "random",
        "additional_input": {},
        "task_id": "static_task_id",
        "artifacts": [],
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)

```

# `benchmark/agbenchmark/agent_api_interface.py`

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯å®ç°ä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºè¿è¡ŒAG Benchmark agentçš„å‘½ä»¤è¡Œå·¥å…·ã€‚å…·ä½“å®ç°åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. å¯¼å…¥éœ€è¦ç”¨åˆ°çš„åº“ï¼šimport json, logging, os, pathlib, time, typing
2. å¯¼å…¥AG Benchmark agentçš„ç±»ï¼šfrom agbenchmark.__main__ import TEMP_FOLDER_ABS_PATH, UPDATES_JSON_PATH
3. å¯¼å…¥æ–‡ä»¶æ“ä½œç±»ï¼šimport pathlib
4. å¯¼å…¥ç”¨äºè·å–å‘½ä»¤è¡Œå‚æ•°çš„åº“ï¼šimport sys
5. åˆå§‹åŒ–AG Benchmark agentï¼šfrom agbenchmark.agent_interface import get_list_of_file_paths
6. åˆ›å»ºä¸€ä¸ªAgbenchmarkç±»ï¼šfrom agbenchmark.agent_protocol_client import (
   AgentApi,
   ApiClient,
   Configuration,
   TaskRequestBody,
)
7. å®šä¹‰è·å–æ–‡ä»¶åˆ—è¡¨çš„æ–¹æ³•ï¼šclass Agbenchmark:
   def __init__(self, folder_path: str = TEMP_FOLDER_ABS_PATH):
       self.folder_path = folder_path
       if not os.path.exists(folder_path):
           self.folder_path = pathlib.Path(os.path.join(sys.path[0], folder_path))
       self.api_client = AgentApi()

   def get_file_paths(self) -> Optional[List[str]]:
       return self.api_client.list_file_paths(self.folder_path)

   def run_agent(self, agent_options: Dict[str, Any]) -> Optional[Dict[str, Any]]:
       return self.api_client.run_agent(agent_options, self.folder_path)

   def run_suite(self, suite_options: Dict[str, Any]) -> Optional[Dict[str, Any]]:
       return self.run_agent(suite_options)

   def run(self) -> Optional[Dict[str, Any]]:
       folder_path = self.folder_path
       options = {"files": [f"{folder_path}/**/*"]}
       return self.run_suite(options)
```py

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªAgbenchmarkç±»ï¼Œç”¨äºè¿è¡ŒAG Benchmark agentçš„å‘½ä»¤è¡Œå·¥å…·ã€‚è¯¥ç±»åŒ…å«ä»¥ä¸‹æ–¹æ³•ï¼š

* `__init__`ï¼šåˆå§‹åŒ–AG Benchmark agentï¼Œå¹¶è®¾ç½®ä¸€ä¸ªæ–‡ä»¶æ“ä½œç±»`Agbenchmark`ã€‚
* `get_file_paths`ï¼šæ–¹æ³•ç”¨äºè·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„ï¼Œå¹¶åœ¨éœ€è¦æ—¶åˆ›å»ºæ–‡ä»¶å¤¹ã€‚
* `run_agent`ï¼šæ–¹æ³•ç”¨äºä¸AG Benchmark agenté€šä¿¡ï¼Œå¹¶è·å–æˆ–è®¾ç½®ä¸€äº›å‚æ•°ã€‚
* `run_suite`ï¼šæ–¹æ³•ç”¨äºè¿è¡Œä¸€ä¸ªæµ‹è¯•ç»„åˆï¼ŒåŒ…æ‹¬è¿è¡Œ`run_agent`æ–¹æ³•ã€‚
* `run`ï¼šæ–¹æ³•ç”¨äºè¿è¡Œæ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬è¿è¡Œ`run_suite`æ–¹æ³•ã€‚

é€šè¿‡è°ƒç”¨è¿™äº›æ–¹æ³•ï¼Œå¯ä»¥å®ç°AG Benchmark agentå‘½ä»¤è¡Œå·¥å…·çš„æ‰€æœ‰åŠŸèƒ½ã€‚


```
import json
import logging
import os
import pathlib
import time
from typing import Any, Dict, Optional

from agbenchmark.__main__ import TEMP_FOLDER_ABS_PATH, UPDATES_JSON_PATH
from agbenchmark.agent_interface import get_list_of_file_paths
from agbenchmark.agent_protocol_client import (
    AgentApi,
    ApiClient,
    Configuration,
    TaskRequestBody,
)
```py

This is a Python function that runs an API agent to perform a task and returns its results. The function takes in several parameters, including a `ChallengeData` object, which contains the task data, and a dictionary of configuration settings. The function uses the `AgentApi` class to interact with the API, and the `TaskRequestBody` class to construct the request for the task. The function also makes use of the `append_updates_file` and `upload_artifacts` functions to handle the uploading of artifacts and the artifact location. The function runs for a specified timeout, and if the timeout is reached it will raise a `TimeoutError`.


```
from agbenchmark.agent_protocol_client.models.step import Step
from agbenchmark.utils.data_types import ChallengeData

LOG = logging.getLogger(__name__)


async def run_api_agent(
    task: ChallengeData, config: Dict[str, Any], artifacts_location: str, timeout: int
) -> None:
    host_value = None

    configuration = Configuration(host=config["AgentBenchmarkConfig"].host + "/ap/v1")
    async with ApiClient(configuration) as api_client:
        api_instance = AgentApi(api_client)
        task_request_body = TaskRequestBody(input=task.task)

        start_time = time.time()
        response = await api_instance.create_agent_task(
            task_request_body=task_request_body
        )
        task_id = response.task_id

        await upload_artifacts(
            api_instance, artifacts_location, task_id, "artifacts_in"
        )

        i = 1
        steps_remaining = True
        while steps_remaining:
            # Read the existing JSON data from the file

            step = await api_instance.execute_agent_task_step(task_id=task_id)
            await append_updates_file(step)

            print(f"[{task.name}] - step {step.name} ({i}. request)")
            i += 1

            if time.time() - start_time > timeout:
                raise TimeoutError("Time limit exceeded")
            if not step or step.is_last:
                steps_remaining = False
        # if we're calling a mock agent, we "cheat" and give the correct artifacts to pass the tests
        if os.getenv("IS_MOCK"):
            await upload_artifacts(
                api_instance, artifacts_location, task_id, "artifacts_out"
            )

        await copy_agent_artifacts_into_temp_folder(api_instance, task_id)


```py

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª Python å‡½æ•°ï¼Œåä¸º `copy_agent_artifacts_into_temp_folder`ï¼Œå®ƒå®ç°äº†å°†æŒ‡å®šä»»åŠ¡ï¼ˆ`task_id`ï¼‰çš„ä»£ç†ä»»åŠ¡ï¼ˆ`api_instance`ï¼‰ä¸­ç”Ÿæˆçš„ artifacts å¤åˆ¶åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ä¸­ã€‚

å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

1. ä» `api_instance` ä¸­ä¸‹è½½æŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰æœºä»¶ï¼ˆæˆ– artifactsï¼‰çš„å‰¯æœ¬ã€‚
2. éå†ä¸‹è½½çš„æ¯ä¸ªæœºä»¶ï¼ˆæˆ– artifactsï¼‰ã€‚
3. å¦‚æœæœºä»¶ï¼ˆæˆ– artifactsï¼‰æœ‰ç›¸å¯¹è·¯å¾„ï¼Œåˆ™å°†å…¶å¤åˆ¶åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ä¸­ã€‚
4. åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹ï¼ˆå¦‚æœå®ƒä»¬ä¸å­˜åœ¨çš„è¯ï¼‰ã€‚
5. å†™å…¥æ–‡ä»¶å†…å®¹ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ®µä»£ç ä½¿ç”¨äº† `pathlib` åŒ…æ¥ç®¡ç†æ–‡ä»¶å’Œç›®å½•æ“ä½œï¼Œå¹¶ä½¿ç”¨äº† `LOG.info` å‡½æ•°æ¥è¾“å‡ºæ—¥å¿—ä¿¡æ¯ã€‚å¦å¤–ï¼Œç”±äºè¿™æ®µä»£ç ä½¿ç”¨äº† `await` å…³é”®å­—ï¼Œå› æ­¤éœ€è¦ç¡®ä¿ `api_instance` å¼•ç”¨äº†ä¸€ä¸ªå¯ä»¥è°ƒç”¨ `download_agent_task_artifact` å‡½æ•°çš„ Api å®¢æˆ·ç«¯ã€‚


```
async def copy_agent_artifacts_into_temp_folder(api_instance, task_id):
    artifacts = await api_instance.list_agent_task_artifacts(task_id=task_id)
    for artifact in artifacts.artifacts:
        # current absolute path of the directory of the file
        directory_location = pathlib.Path(TEMP_FOLDER_ABS_PATH)
        if artifact.relative_path:
            path = (
                artifact.relative_path
                if not artifact.relative_path.startswith("/")
                else artifact.relative_path[1:]
            )
            directory_location = pathlib.Path(
                os.path.dirname(directory_location / path)
            )
            LOG.info(f"Creating directory {directory_location}")

        directory_location.mkdir(parents=True, exist_ok=True)

        file_path = directory_location / artifact.file_name
        LOG.info(f"Writing file {file_path}")
        with open(file_path, "wb") as f:
            content = await api_instance.download_agent_task_artifact(
                task_id=task_id, artifact_id=artifact.artifact_id
            )

            f.write(content)


```py

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º `append_updates_file` çš„å¼‚æ­¥å‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ªåä¸º `step` çš„å‚æ•°ã€‚å‡½æ•°çš„ä½œç”¨æ˜¯å°† `create_update_json` å‡½æ•°ç”Ÿæˆçš„æ–°çš„æ›´æ–°ä¿¡æ¯æ·»åŠ åˆ°å·²æœ‰çš„æ•°æ®åˆ—è¡¨ä¸­ï¼Œç„¶åå°†æ›´æ–°åçš„æ•°æ®åˆ—è¡¨å†™å›åˆ°æ–‡ä»¶ä¸­ã€‚

å…·ä½“æ¥è¯´ï¼Œä»£ç é¦–å…ˆæ‰“å¼€åä¸º `UPDATES_JSON_PATH` çš„æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¸­çš„æ•°æ®è¯»å–åˆ°å†…å­˜ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªå˜é‡ `existing_data`ã€‚ç„¶åï¼Œå®šä¹‰äº†ä¸€ä¸ªåä¸º `create_update_json` çš„å‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ªå‚æ•° `step`ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªå‚æ•°åˆ›å»ºä¸€ä¸ªæ–°çš„æ›´æ–°ä¿¡æ¯å¯¹è±¡ã€‚

æ¥ç€ï¼Œä»£ç ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼å°†æ–°çš„æ›´æ–°ä¿¡æ¯å¯¹è±¡æ·»åŠ åˆ° `existing_data` åˆ—è¡¨ä¸­ï¼Œå¹¶å°†æ›´æ–°åçš„åˆ—è¡¨å†™å›åˆ°æ–‡ä»¶ä¸­ï¼Œæœ€ååœ¨å‡½æ•°å†…éƒ¨å…³é—­æ–‡ä»¶ã€‚


```
async def append_updates_file(step: Step):
    with open(UPDATES_JSON_PATH, "r") as file:
        existing_data = json.load(file)
    # Append the new update to the existing array
    new_update = create_update_json(step)

    existing_data.append(new_update)
    # Write the updated array back to the file
    with open(UPDATES_JSON_PATH, "w") as file:
        file.write(json.dumps(existing_data, indent=2))


async def upload_artifacts(
    api_instance: ApiClient, artifacts_location: str, task_id: str, type: str
) -> None:
    for file_path in get_list_of_file_paths(artifacts_location, type):
        relative_path: Optional[str] = "/".join(
            file_path.split(f"{type}/", 1)[-1].split("/")[:-1]
        )
        if not relative_path:
            relative_path = None

        await api_instance.upload_agent_task_artifacts(
            task_id=task_id, file=file_path, relative_path=relative_path
        )


```py

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º `create_update_json` çš„å‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ªåä¸º `step` çš„å‚æ•°ï¼Œä»£è¡¨ä¸€ä¸ª `Step` å¯¹è±¡ã€‚

å‡½æ•°å†…éƒ¨å…ˆè·å–å½“å‰æ—¶é—´æˆ³å¹¶å°†å…¶å­˜å‚¨åœ¨æ•´æ•°å˜é‡ `now` ä¸­ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªå­—å…¸å¯¹è±¡ `content`ï¼Œè¯¥å¯¹è±¡åŒ…å«ä¸¤ä¸ªé”®ï¼Œåˆ†åˆ«ä¸º `"content"` å’Œ `"timestamp"`ã€‚

é”® `"content"` çš„å€¼ä¸º `step` å¯¹è±¡ä»¥å…¶ `to_dict()` æ–¹æ³•è¿”å›çš„ JSON å¯¹è±¡çš„ Python ä»£ç è¡¨ç¤ºï¼Œè€Œé”® `"timestamp"` çš„å€¼ä¸ºå½“å‰æ—¶é—´æˆ³ã€‚

æœ€åï¼Œå‡½æ•°è¿”å› `content` å¯¹è±¡ã€‚

è¯¥å‡½æ•°çš„ä½œç”¨æ˜¯åˆ›å»ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªé”® `"content"` çš„å€¼ä¸º `step` å¯¹è±¡çš„ Python ä»£ç è¡¨ç¤ºï¼Œä»¥åŠä¸€ä¸ªé”® `"timestamp"` çš„å€¼ä¸ºå½“å‰æ—¶é—´æˆ³ã€‚


```
def create_update_json(step: Step):
    now = int(time.time())
    content = {"content": step.to_dict(), "timestamp": now}

    return content

```py

# `benchmark/agbenchmark/agent_interface.py`

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯ï¼š

1. å¯¼å…¥ `os`ã€`shutil` å’Œ `sys` æ¨¡å—ã€‚
2. ä» `sys.environment` ä¸­è¯»å– `HELICONE_GRAPHQL_LOGS` ç¯å¢ƒå˜é‡ï¼Œå¦‚æœå½“å‰ç¯å¢ƒä¸­å­˜åœ¨è¯¥å˜é‡ï¼Œåˆ™å°†å…¶å€¼å­˜å‚¨åˆ°ä¸€ä¸ªåä¸º `helicone_graphql_logs` çš„å¸¸é‡ä¸­ï¼Œå¦åˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ç¯å¢ƒå˜é‡å¹¶å°†å…¶å€¼è®¾ç½®ä¸º `False`ã€‚
3. ä» `dotenv` åŒ…ä¸­ä½¿ç”¨ `load_dotenv` å‡½æ•°åŠ è½½æ˜¯å¦å­˜åœ¨åä¸º `helicone_graphql_logs` çš„ç¯å¢ƒå˜é‡ã€‚
4. ä» `agbenchmark.execute_sub_process` ç±»ä¸­ä½¿ç”¨ `execute_subprocess` å‡½æ•°æ‰§è¡Œåä¸º `graphql` çš„å‘½ä»¤ï¼Œå¹¶å°† `helicone_graphql_logs` å’Œ `graphql` ä½œä¸ºå‚æ•°ä¼ é€’ç»™è¯¥å‡½æ•°ã€‚
5. å¯¹ `graphql` å‘½ä»¤çš„è¾“å‡ºè¿›è¡Œå¤„ç†ï¼Œä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼è·å–æ‰€æœ‰è¾“å‡ºè¡Œï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åˆ°ä¸€ä¸ªåä¸º `outputs` çš„åˆ—è¡¨ä¸­ã€‚
6. å°† `graphql` å‘½ä»¤çš„è¾“å‡ºä½œä¸º `helicone_graphql_logs` ç¯å¢ƒå˜é‡çš„æ›´æ–°å€¼ï¼Œå¦‚æœ `helicone_graphql_logs` çš„å€¼ä¸º `True`ï¼Œåˆ™æ‰§è¡Œ `graphql` å‘½ä»¤å¹¶å°† `helicone_graphql_logs` ä½œä¸ºå‚æ•°ä¼ é€’ç»™è¯¥å‘½ä»¤ï¼Œå¦åˆ™å°† `False` ä½œä¸º `helicone_graphql_logs` çš„å€¼ã€‚


```
import os
import shutil
import sys
from typing import List

from dotenv import load_dotenv

from agbenchmark.execute_sub_process import execute_subprocess

load_dotenv()

helicone_graphql_logs = os.getenv("HELICONE_GRAPHQL_LOGS")
HELICONE_GRAPHQL_LOGS = (
    helicone_graphql_logs.lower() == "true" if helicone_graphql_logs else False
)


```py



è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª Python å‡½æ•°ï¼Œåä¸º `run_agent`ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå‚æ•° `task` å’Œ `timeout`ã€‚è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯åœ¨ç»™å®šè¶…æ—¶æ—¶é—´å’Œä»»åŠ¡çš„æƒ…å†µä¸‹è¿è¡Œ `agbenchmark_config.benchmarks` è„šæœ¬ï¼Œå¹¶å°†ä»»åŠ¡è·¯å¾„ä½œä¸ºå‚æ•°ä¼ é€’ç»™å®ƒã€‚

å…·ä½“æ¥è¯´ï¼Œå‡½æ•°ä¼šåœ¨æ‰§è¡Œ `agbenchmark_config.benchmarks` è„šæœ¬ä¹‹å‰è¾“å‡ºä¸€æ¡æ¶ˆæ¯ï¼Œè¯´æ˜æ­£åœ¨è¿è¡Œè¿™ä¸ªä»»åŠ¡ï¼Œç„¶åä½¿ç”¨ `sys.executable` æ‰§è¡Œ `agbenchmark_config.benchmarks` è„šæœ¬ï¼Œå¹¶å°† `task` å’Œ `timeout` ä½œä¸ºå‚æ•°ä¼ é€’ç»™å®ƒã€‚æœ€åï¼Œå‡½æ•°é€šè¿‡è°ƒç”¨ `execute_subprocess` å‡½æ•°æ¥è¿è¡Œ `agbenchmark_config.benchmarks` è„šæœ¬ï¼Œè¿™ä¸ªå‡½æ•°æ¥å—ä¸€ä¸ªå‘½ä»¤è¡Œåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å« `sys.executable` å’Œ `-m` å‚æ•°ï¼Œä»¥åŠ `timeout` å‚æ•°ã€‚

å¦å¤–ï¼Œå‡½æ•° `get_list_of_file_paths` æ¥æ”¶ä¸¤ä¸ªå‚æ•° `challenge_dir_path` å’Œ `artifact_folder_name`ï¼Œè¿”å›åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å« `artbenchmark_agent_interface.py` æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ–‡ä»¶åã€‚


```
def run_agent(task: str, timeout: int) -> None:
    print(f"Running agbenchmark/benchmarks.py with timeout {timeout}")

    command = [sys.executable, "-m", "agbenchmark_config.benchmarks", str(task)]

    execute_subprocess(command, timeout)


def get_list_of_file_paths(
    challenge_dir_path: str, artifact_folder_name: str
) -> List[str]:
    # this file is at agbenchmark\agent_interface.py
    source_dir = os.path.join(
        challenge_dir_path,
        artifact_folder_name,
    )
    if not os.path.exists(source_dir):
        return []
    return [os.path.join(source_dir, file_name) for file_name in os.listdir(source_dir)]


```py

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º `copy_artifacts_into_temp_folder` çš„å‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ª `workspace` å‚æ•°ï¼Œä¸€ä¸ª `artifact_folder_name` å‚æ•°å’Œä¸€ä¸ª `challenge_dir_path` å‚æ•°ã€‚è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å°† `artifact_folder_name` ä¸­çš„æ–‡ä»¶å¤åˆ¶åˆ°æŒ‡å®šçš„ä¸´æ—¶æ–‡ä»¶å¤¹ä¸­ï¼Œå¦‚æœç›®æ ‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™ä¼šåˆ›å»ºå®ƒã€‚

å…·ä½“æ¥è¯´ï¼Œå‡½æ•°é¦–å…ˆé€šè¿‡è°ƒç”¨ `get_list_of_file_paths` å‡½æ•°è·å– `artifact_folder_name` ä¸­æ‰€æœ‰æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ã€‚æ¥ç€ï¼Œå‡½æ•°éå†æ¯ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œåˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºç³»ç»Ÿæ–‡ä»¶ç³»ç»Ÿä¸­ï¼ˆé€šå¸¸å¯ä»¥é€šè¿‡åœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ `ls` å‘½ä»¤æ¥éªŒè¯ï¼‰ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä½¿ç”¨ `shutil.copy` å‡½æ•°å°†æ–‡ä»¶å¤åˆ¶åˆ° `workspace` å‚æ•°æŒ‡å®šçš„ç›®æ ‡æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™ä¼šåˆ›å»ºå®ƒã€‚


```
def copy_artifacts_into_temp_folder(
    workspace: str | dict[str, str], artifact_folder_name: str, challenge_dir_path: str
) -> None:
    file_paths = get_list_of_file_paths(challenge_dir_path, artifact_folder_name)
    for file_path in file_paths:
        if os.path.isfile(file_path):
            shutil.copy(file_path, workspace)

```py

# `benchmark/agbenchmark/app.py`

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯å®ç°ä¸€ä¸ªåŸºå‡†æµ‹è¯•çš„æµç¨‹ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. å¯¼å…¥éœ€è¦çš„åº“ï¼šdatetimeã€uuidã€collectionsã€dequeã€pathlibã€httpxã€‚

2. ä»collections.defaultdictåˆ›å»ºä¸€ä¸ªä»»åŠ¡å®ä½“ï¼Œä½¿ç”¨dequeå­˜å‚¨ä»»åŠ¡é˜Ÿåˆ—ã€‚

3. ä½¿ç”¨pathlibçš„Pathç±»åˆ›å»ºä¸€ä¸ªåŸºå‡†æµ‹è¯•æŠ¥å‘Šçš„è·¯å¾„ã€‚

4. ä»httpxåº“åˆ›å»ºä¸€ä¸ªHTTPå®¢æˆ·ç«¯ï¼Œç”¨äºå’Œè¿œç¨‹æœåŠ¡å™¨é€šä¿¡ã€‚

5. ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯ç¯å¢ƒå˜é‡ï¼‰è®¾ç½®Agbenchmark agentçš„APIé…ç½®ã€‚

6. å®ç°ä¸€ä¸ªæŠ¥å‘Šç±»ï¼Œç”¨äºè®°å½•æ¯ä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ç»“æœï¼ŒåŒ…æ‹¬ä»»åŠ¡çš„IDã€å¯åŠ¨æ—¶é—´ã€ç»“æŸæ—¶é—´å’Œç»“æœï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰ã€‚

7. å®ç°ä¸€ä¸ªåŸºå‡†æµ‹è¯•ç±»ï¼Œç”¨äºæ‰§è¡Œä¸€ä¸ªæµ‹è¯•ä»»åŠ¡ï¼ŒåŒ…æ‹¬è®¾ç½®ä»»åŠ¡çš„IDã€æƒé‡ã€å †æ ˆå’ŒåŸºå‡†æµ‹è¯•æŠ¥å‘Šã€‚

8. åœ¨æµ‹è¯•æŠ¥å‘Šä¸­ç»Ÿè®¡æ¯ä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œå¹¶è¾“å‡ºç»“æœã€‚


```
import datetime
import uuid
from collections import defaultdict, deque
from pathlib import Path

import httpx

from agbenchmark.agent_protocol_client import (
    AgentApi,
    ApiClient,
    ApiException,
    Configuration,
)
from agbenchmark.reports.processing.report_types_v2 import BenchmarkRun
from agbenchmark.schema import TaskEvalRequestBody
```py

è¿™æ®µä»£ç ä½¿ç”¨äº†å¤šä¸ªæ¨¡å—å’Œå‡½æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å—å’Œå‡½æ•°æ¥è§£é‡Šå®ƒä»¬çš„ä½œç”¨ã€‚è¿™é‡Œä¸»è¦ fastapi å’Œ agbenchmarkï¼Œå¯ä»¥å…ˆç®€è¦ä»‹ç»è¿™ä¸¤ä¸ªæ¨¡å—çš„åŠŸèƒ½å’Œä½œç”¨ã€‚

1. fastapiï¼šFastAPI æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºç°ä»£ Web åº”ç”¨ç¨‹åºçš„ FastAPI åº“ï¼Œæä¾›äº† RESTful API çš„å¿«é€Ÿå¼€å‘å’Œè¿è¡Œã€‚

2. agbenchmarkï¼šAgbenchmark æ˜¯ AgileBenchmark çš„ç¼©å†™ï¼Œæ˜¯ä¸€ä¸ªåŸºäº FastAPI åº“çš„ Agile æµ‹è¯•å’Œè¿è¡Œå·¥å…·ï¼Œæä¾›äº†è‡ªåŠ¨åŒ–æµ‹è¯•ã€éƒ¨ç½²å’Œç®¡ç†ç­‰åŠŸèƒ½ã€‚

ç°åœ¨æ¥çœ‹è¿™æ®µä»£ç çš„å…·ä½“ä½œç”¨ï¼š

1. ä» agbenchmark.utils.utils æ¨¡å—ä¸­å¼•å…¥äº† write_pretty_json å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„ä½œç”¨æ˜¯å†™å…¥ä¸€ä¸ªæ¼‚äº®çš„ JSON æ•°æ®ã€‚

2. ä» Configuration ç±»ä¸­ç»§æ‰¿äº† Configuration æ¥å£ï¼Œè¿™ä¸ªæ¥å£å¯èƒ½å®šä¹‰äº†ä¸€äº›é…ç½®å‚æ•°ï¼Œä½†å¹¶ä¸åŒ…å«å…·ä½“çš„æ•°æ®å†™å…¥æ“ä½œã€‚

3. ä» Configuration ç±»ä¸­ç»§æ‰¿äº† Configuration æ¥å£ï¼Œè¿™ä¸ªæ¥å£å¯èƒ½å®šä¹‰äº†ä¸€äº›é…ç½®å‚æ•°ï¼Œä½†å¹¶ä¸åŒ…å«å…·ä½“çš„æ•°æ®å†™å…¥æ“ä½œã€‚

4. åˆ›å»ºäº†ä¸€ä¸ª ApiRouterï¼Œè¿™ä¸ªè·¯ç”±å™¨ä½¿ç”¨ FastAPI åº“ä½œä¸º API çš„åŸºç¡€ï¼Œä¸º v1 è·¯å¾„ä¸‹çš„ç”¨æˆ·æä¾›äº†ä¸€ä¸ªç®€å•çš„ HTTP è¯·æ±‚å¤„ç†ã€‚

5. åœ¨ ApiRouter ä¸­å®šä¹‰äº†ä¸¤ä¸ªæ–¹æ³•ï¼šget å’Œ postï¼Œåˆ†åˆ«å¯¹åº”ç€è·å–å’Œå‘é€è¯·æ±‚çš„åŠŸèƒ½ã€‚

6. åœ¨ post æ–¹æ³•çš„å›è°ƒå‡½æ•°ä¸­ï¼Œé€šè¿‡è°ƒç”¨ agbenchmark.utils.contrib.psutil åº“ä¸å½“å‰ç³»ç»Ÿçš„ psutil å·¥å…·ç±»æ¥è·å–ç³»ç»Ÿçš„ CPU ä½¿ç”¨æƒ…å†µï¼Œå¹¶å°†å…¶ä½œä¸ºè¯·æ±‚çš„ä¿¡æ¯è¿”å›ã€‚

è¿™æ®µä»£ç ä¸»è¦ä½œç”¨æ˜¯ä¸ºäº†ä¸€ä¸ªåŸºäº FastAPI å’Œ AgileBenchmark çš„æµ‹è¯•å’Œè¿è¡Œå·¥å…·ï¼Œæä¾›äº†ä¸€ä¸ªç®€å•çš„ API è·¯ç”±å¤„ç†ï¼Œå¹¶åœ¨è·¯ç”±å¤„ç†ä¸­åŠ å…¥äº†ä¸€äº›é¢å¤–çš„ functionalityï¼Œæ¯”å¦‚å°†ç³»ç»Ÿ CPU ä½¿ç”¨æƒ…å†µä½œä¸ºè¯·æ±‚ä¿¡æ¯è¿”å›ã€‚


```
from agbenchmark.utils.utils import write_pretty_json

configuration = Configuration(host="http://localhost:8000" + "/ap/v1")

import json
import os
import sys
from typing import Any, Optional

import psutil
from fastapi import APIRouter, FastAPI
from fastapi import (
    HTTPException as FastAPIHTTPException,  # Import HTTPException from FastAPI
)
from fastapi import Request, Response
```py

è¿™æ®µä»£ç ä½¿ç”¨äº†ä¸‰ä¸ªæ¨¡å—ï¼šfastapiã€agbenchmarkå’Œpydanticã€‚å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç ï¼š

1. ä»fastapiæ¨¡å—ä¸­å¯¼å…¥äº†ä¸€ä¸ªåä¸ºCORSMiddlewareçš„ç±»ï¼Œè¯¥ç±»å®ç°äº†è·¨åŸŸèµ„æºå…±äº«ï¼ˆCORSï¼‰åŠŸèƒ½ã€‚
2. ä»agbenchmarkæ¨¡å—ä¸­å¯¼å…¥äº†execute_subprocesså’ŒTaskã€TaskRequestBodyä¸¤ä¸ªç±»ï¼Œè¿™äº›ç±»ç”¨äºæ‰§è¡Œå’Œç®¡ç†æµ‹è¯•ç”¨ä¾‹ã€‚
3. ä»./__init__.py__æ–‡ä»¶ä¸­å¯¼å…¥äº†ä¸€ä¸ªåä¸ºFastAPIçš„ç±»ï¼Œè¯¥ç±»æ˜¯FastAPIé¡¹ç›®çš„åŸºç±»ã€‚
4. ä»fastapiæ¨¡å—ä¸­å¯¼å…¥äº†osæ¨¡å—ä¸­çš„find_absolute_benchmark_pathå‡½æ•°ï¼Œè¯¥å‡½æ•°ç”¨äºæŸ¥æ‰¾åŸºå‡†æµ‹è¯•æ‰€åœ¨çš„ç›®å½•ã€‚
5. åœ¨è®¾ç½®å®ŒåŸºå‡†æµ‹è¯•ç›®å½•åï¼Œå°†å½“å‰å·¥ä½œç›®å½•ï¼ˆä¹Ÿå°±æ˜¯å½“å‰ç›®å½•ï¼‰åˆ‡æ¢åˆ°åŸºå‡†æµ‹è¯•ç›®å½•ã€‚
6. åˆ›å»ºäº†ä¸€ä¸ªåä¸ºrouterçš„APIè·¯ç”±å™¨ï¼Œç”¨äºå®šä¹‰APIçš„è·¯ç”±ã€‚
7. ä»fastapiæ¨¡å—ä¸­å¯¼å…¥äº†BaseModelå’ŒExtraä¸¤ä¸ªç±»ï¼Œè¿™äº›ç±»ç”¨äºå®šä¹‰APIå“åº”çš„æ•°æ®ç»“æ„å’Œä¼ é€’ç»™å¤–éƒ¨APIçš„å‚æ•°ã€‚


```
from fastapi.middleware.cors import CORSMiddleware

from agbenchmark.execute_sub_process import execute_subprocess
from agbenchmark.schema import Task, TaskRequestBody

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from fastapi import FastAPI
from pydantic import BaseModel, Extra

router = APIRouter()
import glob

# Change the current working directory to the benchmark path
# home_path = find_absolute_benchmark_path()
# os.chdir(home_path)

```py

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯æ‰§è¡Œä¸€ä¸ªåä¸º"general_command"çš„å‘½ä»¤ï¼Œå…¶ä¸­åŒ…æ‹¬è¿è¡Œ"poetry"å‘½ä»¤ã€"run"å‘½ä»¤ã€"agbenchmark"å‘½ä»¤ã€"start"å‘½ä»¤ä»¥åŠä¸€ä¸ªåä¸º"--backend"çš„å‚æ•°ã€‚åŒæ—¶ï¼Œå®ƒè¿˜å®šä¹‰äº†ä¸€ä¸ªåä¸º"challenges_path"çš„è·¯å¾„ï¼Œç”¨äºå­˜å‚¨ä¸€ä¸ªåä¸º"challenges"çš„æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªæ–‡ä»¶å¤¹ä¸­å¯èƒ½åŒ…å«ä¸€äº›ä»¥"data.json"ä¸ºæ‰©å±•åçš„JSONæ–‡ä»¶ã€‚

å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç ä¸‹é¢çš„å‡ ä¸ªæ­¥éª¤ï¼š

1. å¯¼å…¥psutilæ¨¡å—ï¼Œç”¨äºè·å–ä¸os.path.dirname(__file__)ç›¸å¯¹è·¯å¾„çš„å½“å‰å·¥ä½œç›®å½•ã€‚
2. åˆ›å»ºä¸€ä¸ªåä¸º"challenges_path"çš„æ–‡ä»¶å¤¹ï¼Œå¦‚æœè¿™ä¸ªæ–‡ä»¶å¤¹ä¸å­˜åœ¨çš„è¯ã€‚
3. å¾ªç¯éå†"challenges_path"æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬ä»¥"data.json"ä¸ºæ‰©å±•åçš„æ–‡ä»¶ã€‚
4. å°†éå†åˆ°çš„æ‰€æœ‰JSONæ–‡ä»¶çš„å†…å®¹å­˜å‚¨åˆ°"CHALLENGES"å­—å…¸ä¸­ï¼Œå…¶ä¸­é”®ä¸ºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå€¼ä¸º{"line_count": 0, "test_cases": 0}ã€‚
5. éå†"CHALLENGES"å­—å…¸ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹ï¼Œå…¶ä¸­é”®ä¸ºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå€¼ä¸º{"line_count": 0, "test_cases": 0}ã€‚
6. å°†æ¯ä¸ªæ–‡ä»¶çš„"line_count"å€¼å’Œ"test_cases"å€¼ä½œä¸ºå‡½æ•°å‚æ•°ä¼ å…¥ï¼Œå®ç°è‡ªåŠ¨æµ‹è¯•ã€‚


```
general_command = ["poetry", "run", "agbenchmark", "start", "--backend"]

import psutil

challenges_path = os.path.join(os.path.dirname(__file__), "challenges")

json_files = deque(
    glob.glob(
        f"{challenges_path}/**/data.json",
        recursive=True,
    )
)

CHALLENGES = {}
task_informations = defaultdict(dict)

```py

è¿™æ®µä»£ç ä½¿ç”¨äº†Pythonçš„uuidåº“å’Œjsonåº“ï¼Œå®ç°äº†ç”Ÿæˆå”¯ä¸€IDå¹¶å†™å…¥JSONæ–‡ä»¶çš„åŠŸèƒ½ã€‚

while True:
   # å–å‡ºjsonæ–‡ä»¶åå¹¶å°†å…¶æ‰“å°
   json_file = json_files.popleft()
   print(f"Processing JSON file: {json_file}")

   # ä½¿ç”¨with openæ‰“å¼€æ–‡ä»¶è¯»å–æ•°æ®
   with open(json_file, "r") as file:
       # ä½¿ç”¨json.loadè¯»å–æ–‡ä»¶ä¸­çš„JSONæ•°æ®å¹¶å­˜å‚¨åˆ°dataå˜é‡ä¸­
       data = json.load(file)

       # å¦‚æœeval_idä¸åœ¨æ•°æ®ä¸­ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°dataä¸­
       if "eval_id" not in data:
           data["eval_id"] = str(uuid.uuid4())

   # ä½¿ç”¨write_pretty_jsonå°†æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ä¸­
   write_pretty_json(data, json_file)
   # å­˜å‚¨åˆ°Challengeså˜é‡ä¸­
   CHALLENGES[data["eval_id"]] = data
   CHALLENGES[data["eval_id"]]["path"] = json_file

   # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç»§ç»­å¾ªç¯
   if not os.path.isfile(json_file):
       continue


```
while json_files:
    json_file = json_files.popleft()

    with open(json_file, "r") as file:
        data = json.load(file)

        if "eval_id" not in data:
            data["eval_id"] = str(uuid.uuid4())
        # this will sort all the keys of the JSON systematically so that the order is always the same
        write_pretty_json(data, json_file)
        # ok
        CHALLENGES[data["eval_id"]] = data
        CHALLENGES[data["eval_id"]]["path"] = json_file


```py

è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯æ‰¾å‡ºæ²¡æœ‰ä½¿ç”¨uvicornçš„åŸºå‡†æµ‹è¯•è¿›ç¨‹ã€‚å®ƒä½¿ç”¨äº†ä¸¤ä¸ªpsutilåº“å‡½æ•°ï¼Œç¬¬ä¸€ä¸ªpsutil.process_iterå‡½æ•°ç”¨äºè·å–æ‰€æœ‰è¿›ç¨‹çš„pidï¼Œç¬¬äºŒä¸ªpsutil.process_iterå‡½æ•°ç”¨äºè·å–æ¯ä¸ªè¿›ç¨‹çš„ä¿¡æ¯å­—å…¸ã€‚

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬éå†äº†æ‰€æœ‰è¿›ç¨‹ï¼Œå¹¶ä½¿ç”¨psutilåº“çš„è¿‡æ»¤å™¨æ¥è·å–æ¯ä¸ªè¿›ç¨‹çš„ä¿¡æ¯å­—å…¸ã€‚æˆ‘ä»¬ä½¿ç”¨and()è¿ç®—ç¬¦æ¥è·å–ç¬¦åˆæ¡ä»¶çš„è¿›ç¨‹ï¼Œå³åŒæ—¶æ»¡è¶³â€œagbenchmarkâ€å’Œâ€œuvicornâ€çš„æ¡ä»¶ã€‚å¦‚æœä¸€ä¸ªè¿›ç¨‹çš„ä¿¡æ¯ä¸­åŒ…å«â€œagbenchmarkâ€å¹¶ä¸”ä¸åŒ…å«â€œuvicornâ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å…¶æ·»åŠ åˆ°pidsåˆ—è¡¨ä¸­ã€‚

æœ€åï¼Œæˆ‘ä»¬è¿”å›pidsåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è¿›ç¨‹çš„pidã€‚


```
def find_agbenchmark_without_uvicorn():
    pids = []
    for process in psutil.process_iter(
        attrs=[
            "pid",
            "cmdline",
            "name",
            "username",
            "status",
            "cpu_percent",
            "memory_info",
            "create_time",
            "cwd",
            "connections",
        ]
    ):
        try:
            # Convert the process.info dictionary values to strings and concatenate them
            full_info = " ".join([str(v) for k, v in process.info.items()])

            if "agbenchmark" in full_info and "uvicorn" not in full_info:
                pids.append(process.info["pid"])
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass
    return pids


```py

è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºCreateReportRequestçš„ç±»ï¼Œå®ƒç»§æ‰¿è‡ªPythonä¸­çš„BaseModelç±»ã€‚è¿™ä¸ªç±»çš„ç›®çš„æ˜¯å®šä¹‰ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºåœ¨æµ‹è¯•è¿è¡Œæ—¶åˆ›å»ºæŠ¥å‘Šã€‚

å…·ä½“æ¥è¯´ï¼Œè¿™ä¸ªç±»åŒ…å«ä»¥ä¸‹æˆå‘˜å˜é‡ï¼š

- test: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºè¦è¿è¡Œçš„æµ‹è¯•çš„IDã€‚
- test_run_id: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºè¿è¡Œæµ‹è¯•çš„æŠ¥å‘ŠIDã€‚
- category: ä¸€ä¸ªå¯é€‰çš„å­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºä¸€ä¸ªæˆ–å¤šä¸ªæµ‹è¯•ç±»åˆ«ã€‚
- mock: ä¸€ä¸ªå¯é€‰çš„å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦åœ¨æµ‹è¯•ä¸­ä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿçš„ã€ä¸ä¼šè¾“å‡ºä»»ä½•ç»“æœçš„è¿è¡Œã€‚

è¿™ä¸ªç±»çš„é…ç½®ç±»è¿˜åŒ…å«ä¸€ä¸ª Extra.forbid å±æ€§ï¼Œç”¨äºç¦æ­¢ä»»ä½•é¢å¤–çš„é…ç½®å­—æ®µã€‚

åœ¨ upates_list å˜é‡ä¸­ï¼Œæœ‰ä¸¤ä¸ªç©ºåˆ—è¡¨ï¼Œä¼¼ä¹æ²¡æœ‰è¿›è¡Œä½¿ç”¨ã€‚å¦å¤–ï¼Œä»ä»£ç ä¸­æ— æ³•ç¡®å®š test å’Œ test_run_id å˜é‡æ˜¯åœ¨è¿è¡Œæ—¶è¿˜æ˜¯ç”¨äºé…ç½®æ—¶è¿›è¡Œåˆå§‹åŒ–çš„ã€‚


```
class CreateReportRequest(BaseModel):
    test: str = None
    test_run_id: str = None
    # category: Optional[str] = []
    mock: Optional[bool] = False

    class Config:
        extra = Extra.forbid  # this will forbid any extra fields


updates_list = []

updates_list = []

import json

```py

è¿™æ®µä»£ç ä½¿ç”¨äº†Pythonçš„FastAPIæ¡†æ¶æ¥å®ç°Webåº”ç”¨ç¨‹åºçš„å¼€å‘ï¼Œä¸»è¦ä½œç”¨æ˜¯å®šä¹‰äº†ä¸€ä¸ªåä¸ºoriginsçš„åˆ—è¡¨ï¼ŒåŒ…å«äº†å¤šä¸ªWebæœåŠ¡å™¨åœ°å€ï¼Œè¿™äº›åœ°å€éƒ½æ˜¯ç”¨æ¥å¯åŠ¨FastAPIåº”ç”¨ç¨‹åºçš„ä¸­é—´ä»¶(å³FastAPIä¸­é—´ä»¶)çš„æ¥å£åœ°å€ã€‚

å…·ä½“æ¥è¯´ï¼Œoriginsåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªæ ¼å¼ä¸º"<IPåœ°å€ï¼šç«¯å£å·>"çš„å…ƒç»„ï¼Œå…¶ä¸­IPåœ°å€è¡¨ç¤ºWebæœåŠ¡å™¨çš„ä½ç½®ï¼Œç«¯å£å·è¡¨ç¤ºè¯¥æœåŠ¡å™¨æ‰€ç›‘å¬çš„ç«¯å£ã€‚è¿™äº›å…ƒç»„ç»„ç»„æˆäº†FastAPIåº”ç”¨ç¨‹åºçš„ä¸­é—´ä»¶ï¼Œç”¨äºå°†è¯·æ±‚è½¬å‘åˆ°ç›¸åº”çš„WebæœåŠ¡å™¨ä¸Šè¿›è¡Œå¤„ç†ï¼Œå¹¶è¿”å›å¤„ç†ç»“æœç»™FastAPIåº”ç”¨ç¨‹åºã€‚

åœ¨ FastAPIåº”ç”¨ç¨‹åºä¸­ï¼Œæ·»åŠ CORSMiddlewareæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„è¿‡ç¨‹ï¼Œå¯ä»¥è‡ªåŠ¨å°†æ‰€æœ‰å…è®¸çš„åŸŸåæ·»åŠ åˆ°originsåˆ—è¡¨ä¸­ã€‚è€Œåœ¨è¿™ä¸ªå…·ä½“çš„å®ç°ä¸­ï¼Œä½¿ç”¨äº†allow_credentials=Trueå’Œallow_methods=["*"]ä¸¤ä¸ªå‚æ•°ï¼Œè¡¨ç¤ºå…è®¸åº”ç”¨ç¨‹åºå‘é€åŒ…æ‹¬ç”¨æˆ·åå’Œå¯†ç ç­‰èº«ä»½éªŒè¯ä¿¡æ¯ä»¥åŠæ‰€æœ‰HTTPæ–¹æ³•(åŒ…æ‹¬GETã€POSTç­‰)çš„è¯·æ±‚ã€‚


```
origins = [
    "http://localhost:8000",
    "http://localhost:8080",
    "http://127.0.0.1:5000",
    "http://localhost:5000",
]
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


```py

This code looks like it is a Flask router for a report generation tool. It has a route for running a single test against an benchmark tool called Agbenchmark, and another route for generating a report of the test results.

The `run_single_test` function takes a `CreateReportRequest` object as its argument, which must contain the configuration options for running the test (e.g. the path to the benchmark tool, the test to run, and optionally the category to test). This function starts the benchmark tool in the background and returns when the test finishes.

The `reports` folder is used to store the test results, and is automatically created if it does not already exist. The last folder in the folder is assumed to be the output folder for the test report, which is read from here.

It appears that the code also includes some additional functionality, such as finding and listing all folders in the current working directory, and sorting the folders based on their names. However, these features are not part of the route for generating reports, so I assume they are not used in this case.


```
def stream_output(pipe):
    for line in pipe:
        print(line, end="")


@router.post("/reports")
def run_single_test(body: CreateReportRequest) -> Any:
    pids = find_agbenchmark_without_uvicorn()
    print(f"pids already running with agbenchmark: {pids}")
    print(body.dict())
    # it's a hack because other parts of the code are using sys.argv
    print(os.getcwd())
    command_options = ["agbenchmark"]
    # if body.category:
    #     sys.argv.append(f"--category={body.category}")
    command_options.append(f"--test={body.test}")
    if body.mock:
        command_options.append("--mock")

    execute_subprocess(command_options, 200)
    import json
    from pathlib import Path

    print("finished running")
    # List all folders in the current working directory
    path_reports = Path.cwd() / "agbenchmark_config" / "reports"
    folders = [folder for folder in path_reports.iterdir() if folder.is_dir()]

    # Sort the folders based on their names
    sorted_folders = sorted(folders, key=lambda x: x.name)

    # Get the last folder
    last_folder = sorted_folders[-1] if sorted_folders else None

    # Read report.json from this folder
    if last_folder:
        report_path = last_folder / "report.json"
        print(report_path)
        if report_path.exists():
            with report_path.open() as file:
                data = json.load(file)
            print(data)
        else:
            print(f"'report.json' does not exist in '{last_folder}'")
    else:
        print("No folders found.")

    return Response(
        content=json.dumps(data),
        status_code=200,
        media_type="application/json",
    )


```py

This is a Flask endpoint that serves as the main entry point for a Flask application. It reads data from a "update.json" file (the path to which is provided via the UPDATES\_JSON\_PATH environment variable), converts it to JSON, and returns it to the client. The data in the JSON file is filtered based on the "timestamp" field, which is lower than a specified query parameter (last\_update\_time). If the query parameter is not provided, the endpoint returns an error message.


```
import json
from typing import Any

from fastapi import FastAPI, Request, Response


@router.get("/updates")
def get_updates(request: Request) -> Any:
    from agbenchmark.__main__ import UPDATES_JSON_PATH

    try:
        # Read data from the "update.json" file (provide the correct file path)
        with open(UPDATES_JSON_PATH, "r") as file:
            data = json.load(file)

        # Get the last_update_time from the query parameter
        query_param = request.query_params.get("last_update_time")

        if query_param is None:
            # Handle the case when last_update_time is not provided
            print("ERROR: last_update_time parameter is missing")
            return Response(
                content=json.dumps({"error": "last_update_time parameter is missing"}),
                status_code=400,
                media_type="application/json",
                headers={"Content-Type": "application/json"},
            )

        # Convert query_param to a Unix timestamp (assuming it's in seconds as a string)
        query_timestamp = int(query_param)

        # Filter the data based on the timestamp (keep timestamps before query_timestamp)
        filtered_data = [item for item in data if item["timestamp"] > query_timestamp]

        # Extract only the "content" field from each item
        filtered_data = [item["content"] for item in filtered_data]

        # Convert the filtered data to JSON
        filtered_json = json.dumps(filtered_data, indent=2)

        print("INFO: Returning filtered data to the client")
        return Response(
            content=filtered_json,
            status_code=200,
            media_type="application/json",
            headers={"Content-Type": "application/json"},
        )
    except FileNotFoundError:
        print("ERROR: File not found: updates.json")
        return Response(
            content=json.dumps({"error": "File not found"}),
            status_code=404,
            media_type="application/json",
            headers={"Content-Type": "application/json"},
        )


```py

This is an Agbenchmark agent that is configured to upload the results of a benchmark test to a specified endpoint. The agent has a task id of "50da533e-3904-4401-8a07-c49adf88b5eb" and an input of "Write the word 'Washington' to a .txt file" and an additional input of "python/code". The task has been created and the agent is waiting for the benchmark start time to be recorded. If the benchmark start time has not been recorded within the specified timeout, the agent will upload the benchmark results to the specified endpoint. The endpoint that the agent is using is not specified in this code.


```
@router.post("/agent/tasks", tags=["agent"], response_model=Task)
async def create_agent_task(task_eval_request: TaskEvalRequestBody) -> Task:
    """
    Creates a new task using the provided TaskRequestBody and returns a Task.

    Args:
        request (Request): FastAPI request object.
        task (TaskRequestBody): The task request containing input and additional input data.

    Returns:
        Task: A new task with task_id, input, additional_input, and empty lists for artifacts and steps.

    Example:
        Request (TaskRequestBody defined in schema.py):
            {
                "input": "Write the words you receive to the file 'output.txt'.",
                "additional_input": "python/code"
            }

        Response (Task defined in schema.py):
            {
                "task_id": "50da533e-3904-4401-8a07-c49adf88b5eb",
                "input": "Write the word 'Washington' to a .txt file",
                "additional_input": "python/code",
                "artifacts": [],
            }
    """
    from agbenchmark.agent_api_interface import upload_artifacts

    try:
        async with ApiClient(configuration) as api_client:
            api_instance = AgentApi(api_client)
            task_input = CHALLENGES[task_eval_request.eval_id]["task"]

            task_request_body = TaskRequestBody(input=task_input)
            task_response = await api_instance.create_agent_task(
                task_request_body=task_request_body
            )
            task_informations[task_response.task_id][
                "benchmark_start_time"
            ] = datetime.datetime.now(datetime.timezone.utc).strftime(
                "%Y-%m-%dT%H:%M:%S+00:00"
            )
            task_informations[task_response.task_id][
                "eval_id"
            ] = task_eval_request.eval_id
            await upload_artifacts(
                api_instance,
                str(Path(CHALLENGES[task_eval_request.eval_id]["path"]).parent),
                task_response.task_id,
                "artifacts_in",
            )
            return Response(
                content=task_response.json(),
                status_code=200,
                media_type="application/json",
            )
    except ApiException as e:
        print(f"Error whilst trying to create a task: {task_eval_request}")
        return Response(
            content=json.dumps({"error": "Internal server error"}),
            status_code=500,
            media_type="application/json",
        )


```py

è¿™æ®µä»£ç ä½¿ç”¨äº†Pythonä¸­çš„å¼‚æ­¥ç¼–ç¨‹åº“`aiohttp`å’Œ`httpx`ã€‚å®ƒæ˜¯ä¸€ä¸ªä»£ç†URLï¼Œå¯ä»¥åœ¨ä»»åŠ¡æ­¥åº¦çš„URLä¸Šæ‰§è¡Œå¼‚æ­¥æ“ä½œã€‚

å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º`proxy`çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶ä¸€ä¸ª`Request`å¯¹è±¡å’Œä¸€ä¸ªä»»åŠ¡IDã€‚å‡½æ•°å†…éƒ¨ä½¿ç”¨`httpx.Timeout`åˆ›å»ºä¸€ä¸ªHTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œå¹¶ä½¿ç”¨`httpx.AsyncClient`å‘é€POSTè¯·æ±‚ã€‚å®ƒå°†è¯·æ±‚çš„æ–°URLè®¾ç½®ä¸º`http://localhost:8000/ap/v1/agent/tasks/{task_id}/steps`ï¼Œå¹¶å°†è¯·æ±‚çš„æ•°æ®ä½œä¸ºå‚æ•°ä¼ é€’ç»™`client.post()`æ–¹æ³•ã€‚å‡½æ•°ä½¿ç”¨`async with`è¯­å¥æ¥ç¡®ä¿åœ¨å‡½æ•°å†…éƒ¨èµ„æºå’ŒURLéƒ½å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨å‡½æ•°å¤–éƒ¨ç»§ç»­æ‰§è¡Œåç»­æ“ä½œã€‚

å‡½æ•°è¿”å›ä¸€ä¸ª`Response`å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªåŸå§‹è¯·æ±‚çš„å“åº”å†…å®¹ä»¥åŠå“åº”çŠ¶æ€ç ã€‚


```
@router.post("/agent/tasks/{task_id}/steps")
async def proxy(request: Request, task_id: str):
    timeout = httpx.Timeout(300.0, read=300.0)  # 5 minutes
    async with httpx.AsyncClient(timeout=timeout) as client:
        # Construct the new URL
        new_url = f"http://localhost:8000/ap/v1/agent/tasks/{task_id}/steps"

        # Forward the request
        response = await client.post(
            new_url,
            data=await request.body(),
            headers=dict(request.headers),
        )

        # Return the response from the forwarded request
        return Response(content=response.content, status_code=response.status_code)


```py

It seems like you are providing a Python function that takes a JSON file path as input and returns information about a benchmark.

The function first reads the JSON file and parses it into an object using the `json.loads()` method. Then, it calls the `BenchmarkRun.parse_obj()` method from the `BenchmarkRun` class to parse the object into a `BenchmarkRun` object.

If there are any errors, such as an internal server error, the function returns a response with a 500 status code and an error message.

It's worth noting that the `json.dumps()` method is used to convert the `BenchmarkRun` object to a JSON string that can be returned by the API.


```
@router.post("/agent/tasks/{task_id}/evaluations")
async def create_evaluation(task_id: str) -> deque:
    from agbenchmark.__main__ import TEMP_FOLDER_ABS_PATH
    from agbenchmark.agent_api_interface import copy_agent_artifacts_into_temp_folder
    from agbenchmark.agent_interface import copy_artifacts_into_temp_folder
    from agbenchmark.generate_test import create_challenge

    try:
        async with ApiClient(configuration) as api_client:
            api_instance = AgentApi(api_client)
            await copy_agent_artifacts_into_temp_folder(api_instance, task_id)
        # add custom python
        data = CHALLENGES[task_informations[task_id]["eval_id"]]

        artifact_path = str(Path(data["path"]).parent)
        copy_artifacts_into_temp_folder(
            TEMP_FOLDER_ABS_PATH, "custom_python", artifact_path
        )
        json_file = CHALLENGES[task_informations[task_id]["eval_id"]]["path"]
        json_files = deque()

        _, challenge_class = create_challenge(data, json_file, json_files)
        challenge_instance = challenge_class()
        scores = challenge_instance.get_scores(config={})
        test_name = "Test" + data["name"]
        is_score_100 = 1 in scores["values"]

        info_details = {
            "repository_info": {
                "repo_url": None,
                "team_name": None,
                "benchmark_git_commit_sha": None,
                "agent_git_commit_sha": None,
            },
            "run_details": {
                "run_id": None,
                "command": "agbenchmark" + " --test=" + test_name,
                "completion_time": None,
                "benchmark_start_time": task_informations[task_id][
                    "benchmark_start_time"
                ],
                "test_name": data["name"],
            },
            "task_info": {
                "data_path": data["path"].split("benchmark/", 1)[-1],
                "is_regression": None,
                "category": data["category"],
                "task": data["task"],
                "answer": data["ground"]["answer"],
                "description": data["info"]["description"],
            },
            "metrics": {
                "difficulty": None,
                "success": is_score_100,
                "attempted": True,
                "success_percentage": None,
                "cost": None,
                "run_time": None,
            },
            "reached_cutoff": None,
            "config": {},
        }

        BenchmarkRun.parse_obj(info_details)

        print(json.dumps(info_details, indent=4))
        return Response(
            content=json.dumps(info_details),
            status_code=200,
            media_type="application/json",
        )
    except ApiException as e:
        print(f"Error whilst trying to evaluate the task: {task_id}")
        return Response(
            content=json.dumps({"error": "Internal server error"}),
            status_code=500,
            media_type="application/json",
        )
    # path = Path(json_file).resolve()


```py

è¿™æ®µä»£ç æ˜¯ä½¿ç”¨ Flask-Router åº“ä¸­çš„ include_router æ–¹æ³•ï¼Œç”¨äºå°†ä¸€ä¸ªå·²ç»å®šä¹‰å¥½çš„è·¯ç”±ï¼ˆrouterï¼‰æ·»åŠ åˆ°å½“å‰åº”ç”¨ç¨‹åºï¼ˆappï¼‰çš„è·¯ç”±æ ‘ä¸­ã€‚

å…·ä½“æ¥è¯´ï¼Œè¿™æ®µä»£ç å°†ä¼šåœ¨ app çš„è·¯ç”±æ ‘ä¸­æ·»åŠ ä¸€ä¸ªåä¸º "/ap/v1" çš„å‰ç¼€ï¼Œè¿™ä¸ªå‰ç¼€å°†ä¼šè¢«æ·»åŠ åˆ°æ‰€æœ‰åŒ¹é…è¿™ä¸ªå‰ç¼€çš„è·¯ç”±é¡¹ä¸Šã€‚è¿™æ ·ï¼Œå½“ç”¨æˆ·è®¿é—® URL ä¸­çš„ "/ap/v1" æ—¶ï¼Œå°±ä¼šåŒ¹é…åˆ° app ä¸­å®šä¹‰å¥½çš„è·¯ç”±ï¼Œç„¶åæ‰§è¡Œè¯¥è·¯ç”±å¯¹åº”çš„å¤„ç†é€»è¾‘ã€‚

å‡è®¾ app ä¸­æœ‰ä¸€ä¸ªåä¸º "my_route" çš„è·¯ç”±é¡¹ï¼Œè¯¥è·¯ç”±é¡¹å¤„ç†å™¨ä¸º "handle_my_route"ï¼Œé‚£ä¹ˆå½“ç”¨æˆ·è®¿é—® URL "/ap/v1/my_route" æ—¶ï¼Œè¯¥è·¯ç”±é¡¹ä¸­çš„ "handle_my_route" å°†ä¼šè¢« executedï¼Œæ‰§è¡Œå®Œç•¢åï¼Œæœ€ç»ˆè¿”å›ç»“æœ "hello"ç»™å®¢æˆ·ç«¯ã€‚


```
app.include_router(router, prefix="/ap/v1")

```