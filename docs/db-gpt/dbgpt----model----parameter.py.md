# `.\DB-GPT-src\dbgpt\model\parameter.py`

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# 导入必要的库
import os
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, Optional, Tuple, Union

# 导入自定义的模块
from dbgpt.util.parameter_utils import BaseParameters, BaseServerParameters

# 定义枚举类型，表示工作类型
class WorkerType(str, Enum):
    LLM = "llm"
    TEXT2VEC = "text2vec"

    @staticmethod
    def values():
        return [item.value for item in WorkerType]

    @staticmethod
    def to_worker_key(worker_name, worker_type: Union[str, "WorkerType"]) -> str:
        """Generate worker key from worker name and worker type

        Args:
            worker_name (str): Worker name (eg., chatglm2-6b)
            worker_type (Union[str, "WorkerType"]): Worker type (eg., 'llm', or [`WorkerType.LLM`])

        Returns:
            str: Generated worker key
        """
        if "@" in worker_name:
            raise ValueError(f"Invaild symbol '@' in your worker name {worker_name}")
        if isinstance(worker_type, WorkerType):
            worker_type = worker_type.value
        return f"{worker_name}@{worker_type}"

    @staticmethod
    def parse_worker_key(worker_key: str) -> Tuple[str, str]:
        """Parse worker name and worker type from worker key

        Args:
            worker_key (str): Worker key generated by [`WorkerType.to_worker_key`]

        Returns:
            Tuple[str, str]: Worker name and worker type
        """
        return tuple(worker_key.split("@"))


@dataclass
class ModelControllerParameters(BaseServerParameters):
    port: Optional[int] = field(
        default=8000, metadata={"help": "Model Controller deploy port"}
    )
    registry_type: Optional[str] = field(
        default="embedded",
        metadata={
            "help": "Registry type: embedded, database...",
            "valid_values": ["embedded", "database"],
        },
    )
    registry_db_type: Optional[str] = field(
        default="mysql",
        metadata={
            "help": "Registry database type, now only support sqlite and mysql, it is "
                    "valid when registry_type is database",
            "valid_values": ["mysql", "sqlite"],
        },
    )
    registry_db_name: Optional[str] = field(
        default="dbgpt",
        metadata={
            "help": "Registry database name, just for database, it is valid when "
                    "registry_type is database, please set to full database path for sqlite"
        },
    )
    registry_db_host: Optional[str] = field(
        default=None,
        metadata={
            "help": "Registry database host, just for database, it is valid when "
                    "registry_type is database"
        },
    )
    registry_db_port: Optional[int] = field(
        default=None,
        metadata={
            "help": "Registry database port, just for database, it is valid when "
                    "registry_type is database"
        },
    )
    registry_db_user: Optional[str] = field(
        default=None,
        metadata={
            "help": "Registry database user, just for database, it is valid when "
            "registry_type is database"
        },
    )
    registry_db_password: Optional[str] = field(
        default=None,
        metadata={
            "help": "Registry database password, just for database, it is valid when "
            "registry_type is database. We recommend to use environment variable to "
            "store password, you can set it in your environment variable like "
            "export CONTROLLER_REGISTRY_DB_PASSWORD='your_password'"
        },
    )
    registry_db_pool_size: Optional[int] = field(
        default=5,
        metadata={
            "help": "Registry database pool size, just for database, it is valid when "
            "registry_type is database"
        },
    )
    registry_db_max_overflow: Optional[int] = field(
        default=10,
        metadata={
            "help": "Registry database max overflow, just for database, it is valid "
            "when registry_type is database"
        },
    )

    heartbeat_interval_secs: Optional[int] = field(
        default=20, metadata={"help": "The interval for checking heartbeats (seconds)"}
    )
    heartbeat_timeout_secs: Optional[int] = field(
        default=60,
        metadata={
            "help": "The timeout for checking heartbeats (seconds), it will be set "
            "unhealthy if the worker is not responding in this time"
        },
    )

    log_file: Optional[str] = field(
        default="dbgpt_model_controller.log",
        metadata={
            "help": "The filename to store log",
        },
    )
    tracer_file: Optional[str] = field(
        default="dbgpt_model_controller_tracer.jsonl",
        metadata={
            "help": "The filename to store tracer span records",
        },
    )
    tracer_storage_cls: Optional[str] = field(
        default=None,
        metadata={
            "help": "The storage class to storage tracer span records",
        },
    )


注释：


# 注册表数据库的用户名称，仅适用于数据库类型的注册表，当注册表类型为数据库时有效
registry_db_user: Optional[str] = field(
    default=None,
    metadata={
        "help": "Registry database user, just for database, it is valid when "
        "registry_type is database"
    },
)

# 注册表数据库的密码，仅适用于数据库类型的注册表，当注册表类型为数据库时有效。建议使用环境变量存储密码，
# 您可以在环境变量中设置，如 export CONTROLLER_REGISTRY_DB_PASSWORD='your_password'
registry_db_password: Optional[str] = field(
    default=None,
    metadata={
        "help": "Registry database password, just for database, it is valid when "
        "registry_type is database. We recommend to use environment variable to "
        "store password, you can set it in your environment variable like "
        "export CONTROLLER_REGISTRY_DB_PASSWORD='your_password'"
    },
)

# 注册表数据库的连接池大小，仅适用于数据库类型的注册表，当注册表类型为数据库时有效
registry_db_pool_size: Optional[int] = field(
    default=5,
    metadata={
        "help": "Registry database pool size, just for database, it is valid when "
        "registry_type is database"
    },
)

# 注册表数据库的最大溢出量，仅适用于数据库类型的注册表，当注册表类型为数据库时有效
registry_db_max_overflow: Optional[int] = field(
    default=10,
    metadata={
        "help": "Registry database max overflow, just for database, it is valid "
        "when registry_type is database"
    },
)

# 心跳间隔秒数，用于检查心跳的时间间隔（秒）
heartbeat_interval_secs: Optional[int] = field(
    default=20, metadata={"help": "The interval for checking heartbeats (seconds)"}
)

# 心跳超时秒数，用于检查心跳的超时时间（秒），如果工作进程在此时间内未响应，则将其设置为不健康状态
heartbeat_timeout_secs: Optional[int] = field(
    default=60,
    metadata={
        "help": "The timeout for checking heartbeats (seconds), it will be set "
        "unhealthy if the worker is not responding in this time"
    },
)

# 日志文件名，用于存储日志
log_file: Optional[str] = field(
    default="dbgpt_model_controller.log",
    metadata={
        "help": "The filename to store log",
    },
)

# 追踪器文件名，用于存储追踪器跨度记录
tracer_file: Optional[str] = field(
    default="dbgpt_model_controller_tracer.jsonl",
    metadata={
        "help": "The filename to store tracer span records",
    },
)

# 追踪器存储类，用于存储追踪器跨度记录的存储类
tracer_storage_cls: Optional[str] = field(
    default=None,
    metadata={
        "help": "The storage class to storage tracer span records",
    },
)
@dataclass
class ModelAPIServerParameters(BaseServerParameters):
    port: Optional[int] = field(
        default=8100, metadata={"help": "Model API server deploy port"}
    )
    controller_addr: Optional[str] = field(
        default="http://127.0.0.1:8000",
        metadata={"help": "The Model controller address to connect"},
    )

    api_keys: Optional[str] = field(
        default=None,
        metadata={"help": "Optional list of comma separated API keys"},
    )
    embedding_batch_size: Optional[int] = field(
        default=None, metadata={"help": "Embedding batch size"}
    )

    log_file: Optional[str] = field(
        default="dbgpt_model_apiserver.log",
        metadata={
            "help": "The filename to store log",
        },
    )
    tracer_file: Optional[str] = field(
        default="dbgpt_model_apiserver_tracer.jsonl",
        metadata={
            "help": "The filename to store tracer span records",
        },
    )
    tracer_storage_cls: Optional[str] = field(
        default=None,
        metadata={
            "help": "The storage class to storage tracer span records",
        },
    )



# 定义 ModelAPIServerParameters 类，继承自 BaseServerParameters
@dataclass
class BaseModelParameters(BaseParameters):
    model_name: str = field(metadata={"help": "Model name", "tags": "fixed"})
    model_path: str = field(metadata={"help": "Model path", "tags": "fixed"})



# 定义 BaseModelParameters 类，继承自 BaseParameters
@dataclass
class ModelWorkerParameters(BaseServerParameters, BaseModelParameters):
    worker_type: Optional[str] = field(
        default=None,
        metadata={"valid_values": WorkerType.values(), "help": "Worker type"},
    )
    model_alias: Optional[str] = field(
        default=None,
        metadata={"help": "model alias"},
    )
    worker_class: Optional[str] = field(
        default=None,
        metadata={"help": "Model worker class, dbgpt.model.cluster.DefaultModelWorker"},
    )
    model_type: Optional[str] = field(
        default="huggingface",
        metadata={
            "help": "Model type: huggingface, llama.cpp, proxy and vllm",
            "tags": "fixed",
        },
    )

    port: Optional[int] = field(
        default=8001, metadata={"help": "Model worker deploy port"}
    )
    limit_model_concurrency: Optional[int] = field(
        default=5, metadata={"help": "Model concurrency limit"}
    )
    standalone: Optional[bool] = field(
        default=False,
        metadata={"help": "Standalone mode. If True, embedded Run ModelController"},
    )
    register: Optional[bool] = field(
        default=True, metadata={"help": "Register current worker to model controller"}
    )
    worker_register_host: Optional[str] = field(
        default=None,
        metadata={
            "help": "The ip address of current worker to register to ModelController. "
            "If None, the address is automatically determined"
        },
    )
    controller_addr: Optional[str] = field(
        default=None, metadata={"help": "The Model controller address to register"}
    )



# 定义 ModelWorkerParameters 类，继承自 BaseServerParameters 和 BaseModelParameters
    # 定义一个可选的布尔类型字段，用于指定是否向模型控制器发送心跳信号，默认为True
    send_heartbeat: Optional[bool] = field(
        default=True, metadata={"help": "Send heartbeat to model controller"}
    )
    # 定义一个可选的整数类型字段，表示发送心跳信号的时间间隔（秒），默认为20秒
    heartbeat_interval: Optional[int] = field(
        default=20, metadata={"help": "The interval for sending heartbeats (seconds)"}
    )

    # 定义一个可选的字符串类型字段，表示存储日志的文件名，默认为"dbgpt_model_worker_manager.log"
    log_file: Optional[str] = field(
        default="dbgpt_model_worker_manager.log",
        metadata={
            "help": "The filename to store log",
        },
    )
    # 定义一个可选的字符串类型字段，表示存储追踪器跨度记录的文件名，默认为"dbgpt_model_worker_manager_tracer.jsonl"
    tracer_file: Optional[str] = field(
        default="dbgpt_model_worker_manager_tracer.jsonl",
        metadata={
            "help": "The filename to store tracer span records",
        },
    )
    # 定义一个可选的字符串类型字段，表示存储追踪器跨度记录的存储类，默认为None
    tracer_storage_cls: Optional[str] = field(
        default=None,
        metadata={
            "help": "The storage class to storage tracer span records",
        },
    )
@dataclass
class BaseEmbeddingModelParameters(BaseModelParameters):
    # 定义了一个基础的嵌入模型参数类，继承自BaseModelParameters

    def build_kwargs(self, **kwargs) -> Dict:
        # 构建关键字参数的方法，但此处未实现具体功能，仅占位
        pass

    def is_rerank_model(self) -> bool:
        """Check if the model is a rerank model"""
        # 检查当前模型是否为重新排序模型，始终返回False，子类需要重写该方法来更准确地判断
        return False


@dataclass
class EmbeddingModelParameters(BaseEmbeddingModelParameters):
    # 嵌入模型参数类，继承自BaseEmbeddingModelParameters

    device: Optional[str] = field(
        default=None,
        metadata={
            "help": "Device to run model. If None, the device is automatically determined"
        },
    )

    normalize_embeddings: Optional[bool] = field(
        default=None,
        metadata={
            "help": "Determines whether the model's embeddings should be normalized."
        },
    )

    rerank: Optional[bool] = field(
        default=False, metadata={"help": "Whether the model is a rerank model"}
    )

    max_length: Optional[int] = field(
        default=None,
        metadata={
            "help": "Max length for input sequences. Longer sequences will be "
            "truncated. If None, max length of the model will be used, just for rerank"
            " model now."
        },
    )

    def build_kwargs(self, **kwargs) -> Dict:
        # 构建关键字参数的方法，根据设定的参数设定model_kwargs和encode_kwargs
        model_kwargs, encode_kwargs = None, None
        if self.device:
            model_kwargs = {"device": self.device}
        if self.normalize_embeddings:
            encode_kwargs = {"normalize_embeddings": self.normalize_embeddings}
        if model_kwargs:
            kwargs["model_kwargs"] = model_kwargs
        if self.is_rerank_model():
            kwargs["max_length"] = self.max_length
        if encode_kwargs:
            kwargs["encode_kwargs"] = encode_kwargs
        return kwargs

    def is_rerank_model(self) -> bool:
        """Check if the model is a rerank model"""
        # 检查当前模型是否为重新排序模型，根据rerank属性的设置返回相应值
        return self.rerank


@dataclass
class ModelParameters(BaseModelParameters):
    # 模型参数类，继承自BaseModelParameters

    device: Optional[str] = field(
        default=None,
        metadata={
            "help": "Device to run model. If None, the device is automatically determined"
        },
    )
    model_type: Optional[str] = field(
        default="huggingface",
        metadata={
            "help": "Model type: huggingface, llama.cpp, proxy and vllm",
            "tags": "fixed",
        },
    )
    prompt_template: Optional[str] = field(
        default=None,
        metadata={
            "help": f"Prompt template. If None, the prompt template is automatically "
            f"determined from model path"
        },
    )
    max_context_size: Optional[int] = field(
        default=4096, metadata={"help": "Maximum context size"}
    )

    num_gpus: Optional[int] = field(
        default=None,
        metadata={
            "help": "The number of gpus you expect to use, if it is empty, use all of them as much as possible"
        },
    )
    max_gpu_memory: Optional[str] = field(
        default=None,
        metadata={
            "help": "The maximum memory limit of each GPU, only valid in multi-GPU configuration"
        },
    )
    cpu_offloading: Optional[bool] = field(
        default=False, metadata={"help": "CPU offloading"}
    )
    load_8bit: Optional[bool] = field(
        default=False, metadata={"help": "8-bit quantization"}
    )
    load_4bit: Optional[bool] = field(
        default=False, metadata={"help": "4-bit quantization"}
    )
    quant_type: Optional[str] = field(
        default="nf4",
        metadata={
            "valid_values": ["nf4", "fp4"],
            "help": "Quantization datatypes, `fp4` (four bit float) and `nf4` (normal four bit float), only valid when load_4bit=True",
        },
    )
    use_double_quant: Optional[bool] = field(
        default=True,
        metadata={"help": "Nested quantization, only valid when load_4bit=True"},
    )
    compute_dtype: Optional[str] = field(
        default=None,
        metadata={
            "valid_values": ["bfloat16", "float16", "float32"],
            "help": "Model compute type",
        },
    )
    trust_remote_code: Optional[bool] = field(
        default=True, metadata={"help": "Trust remote code"}
    )
    verbose: Optional[bool] = field(
        default=False, metadata={"help": "Show verbose output."}
    )



    cpu_offloading: Optional[bool] = field(
        default=False, metadata={"help": "指示是否进行 CPU 卸载处理"}
    )
    load_8bit: Optional[bool] = field(
        default=False, metadata={"help": "8 位量化"}
    )
    load_4bit: Optional[bool] = field(
        default=False, metadata={"help": "4 位量化"}
    )
    quant_type: Optional[str] = field(
        default="nf4",
        metadata={
            "valid_values": ["nf4", "fp4"],
            "help": "量化数据类型，`fp4`（四位浮点数）和 `nf4`（普通四位浮点数），仅在 load_4bit=True 时有效",
        },
    )
    use_double_quant: Optional[bool] = field(
        default=True,
        metadata={"help": "嵌套量化，仅在 load_4bit=True 时有效"},
    )
    compute_dtype: Optional[str] = field(
        default=None,
        metadata={
            "valid_values": ["bfloat16", "float16", "float32"],
            "help": "模型计算类型",
        },
    )
    trust_remote_code: Optional[bool] = field(
        default=True, metadata={"help": "信任远程代码"}
    )
    verbose: Optional[bool] = field(
        default=False, metadata={"help": "显示详细输出"}
    )
# 使用 `dataclass` 装饰器定义 `LlamaCppModelParameters` 类，继承自 `ModelParameters`
@dataclass
class LlamaCppModelParameters(ModelParameters):
    # 随机种子，用于 llama-cpp 模型；-1 表示随机选择种子
    seed: Optional[int] = field(
        default=-1, metadata={"help": "Random seed for llama-cpp models. -1 for random"}
    )
    # 线程数，如果为 None，则自动确定线程数
    n_threads: Optional[int] = field(
        default=None,
        metadata={
            "help": "Number of threads to use. If None, the number of threads is automatically determined"
        },
    )
    # 调用 llama_eval 时批处理的最大提示令牌数
    n_batch: Optional[int] = field(
        default=512,
        metadata={
            "help": "Maximum number of prompt tokens to batch together when calling llama_eval"
        },
    )
    # GPU 层的数量，设置为 1000000000 表示所有层都使用 GPU
    n_gpu_layers: Optional[int] = field(
        default=1000000000,
        metadata={
            "help": "Number of layers to offload to the GPU, Set this to 1000000000 to offload all layers to the GPU."
        },
    )
    # 组查询注意力，对于 llama-2 70b 必须为 8
    n_gqa: Optional[int] = field(
        default=None,
        metadata={"help": "Grouped-query attention. Must be 8 for llama-2 70b."},
    )
    # RMS 标准化的 epsilon 值，llama-2 模型使用 5e-6 作为一个良好的值
    rms_norm_eps: Optional[float] = field(
        default=5e-06, metadata={"help": "5e-6 is a good value for llama-2 models."}
    )
    # 缓存容量的最大值，例如：2000MiB, 2GiB；未指定单位时默认为字节
    cache_capacity: Optional[str] = field(
        default=None,
        metadata={
            "help": "Maximum cache capacity. Examples: 2000MiB, 2GiB. When provided without units, bytes will be assumed."
        },
    )
    # 是否偏好 CPU，如果 GPU 可用，默认会优先选择 GPU，除非配置 prefer_cpu=False
    prefer_cpu: Optional[bool] = field(
        default=False,
        metadata={
            "help": "If a GPU is available, it will be preferred by default, unless prefer_cpu=False is configured."
        },
    )


# 使用 `dataclass` 装饰器定义 `ProxyModelParameters` 类，继承自 `BaseModelParameters`
@dataclass
class ProxyModelParameters(BaseModelParameters):
    # 代理服务器的 URL，例如：https://api.openai.com/v1/chat/completions
    proxy_server_url: str = field(
        metadata={
            "help": "Proxy server url, such as: https://api.openai.com/v1/chat/completions"
        },
    )

    # 当前代理 LLM 的 API 密钥
    proxy_api_key: str = field(
        metadata={"tags": "privacy", "help": "The api key of current proxy LLM"},
    )

    # API 的基本地址，例如：https://api.openai.com/v1；如果为 None，则首先使用 proxy_api_base
    proxy_api_base: str = field(
        default=None,
        metadata={
            "help": "The base api address, such as: https://api.openai.com/v1. If None, we will use proxy_api_base first"
        },
    )

    # 当前代理 LLM 的应用程序 ID（仅适用于 spark 代理 LLM）
    proxy_api_app_id: Optional[str] = field(
        default=None,
        metadata={
            "help": "The app id for current proxy LLM(Just for spark proxy LLM now)."
        },
    )

    # 当前代理 LLM 的应用程序密钥（仅适用于 spark 代理 LLM）
    proxy_api_secret: Optional[str] = field(
        default=None,
        metadata={
            "help": "The app secret for current proxy LLM(Just for spark proxy LLM now)."
        },
    )

    # 当前代理模型的 API 类型，例如 Azure
    proxy_api_type: Optional[str] = field(
        default=None,
        metadata={
            "help": "The api type of current proxy the current proxy model, if you use Azure, it can be: azure"
        },
    )

    # 当前代理模型的 API 版本
    proxy_api_version: Optional[str] = field(
        default=None,
        metadata={"help": "The api version of current proxy the current model"},
    )
    # 定义一个可选的字符串变量 http_proxy，其默认值为环境变量中的 http_proxy 或者 https_proxy
    http_proxy: Optional[str] = field(
        default=os.environ.get("http_proxy") or os.environ.get("https_proxy"),
        metadata={"help": "The http or https proxy to use openai"},
    )

    # 定义一个可选的字符串变量 proxyllm_backend，默认为 None，用于指定传递给当前代理服务器的模型名称
    proxyllm_backend: Optional[str] = field(
        default=None,
        metadata={
            "help": "The model name actually pass to current proxy server url, such "
            "as gpt-3.5-turbo, gpt-4, chatglm_pro, chatglm_std and so on"
        },
    )

    # 定义一个可选的字符串变量 model_type，默认为 "proxy"，用于指定模型的类型，可能值包括 huggingface, llama.cpp, proxy 和 vllm
    model_type: Optional[str] = field(
        default="proxy",
        metadata={
            "help": "Model type: huggingface, llama.cpp, proxy and vllm",
            "tags": "fixed",
        },
    )

    # 定义一个可选的字符串变量 device，默认为 None，用于指定运行模型的设备，如果为 None，则自动确定设备
    device: Optional[str] = field(
        default=None,
        metadata={
            "help": "Device to run model. If None, the device is automatically "
            "determined"
        },
    )

    # 定义一个可选的字符串变量 prompt_template，默认为 None，用于指定提示模板，如果为 None，则从模型路径自动确定模板
    prompt_template: Optional[str] = field(
        default=None,
        metadata={
            "help": f"Prompt template. If None, the prompt template is automatically "
            f"determined from model path"
        },
    )

    # 定义一个可选的整数变量 max_context_size，默认为 4096，用于指定最大上下文大小
    max_context_size: Optional[int] = field(
        default=4096, metadata={"help": "Maximum context size"}
    )

    # 定义一个可选的字符串变量 llm_client_class，默认为 None，用于指定 llm 客户端的类名
    llm_client_class: Optional[str] = field(
        default=None,
        metadata={
            "help": "The class name of llm client, such as "
            "dbgpt.model.proxy.llms.proxy_model.ProxyModel"
        },
    )

    # 定义 __post_init__ 方法，用于初始化对象后的额外操作
    def __post_init__(self):
        # 如果 proxy_server_url 不存在但 proxy_api_base 存在，则设置 proxy_server_url 为拼接的字符串
        if not self.proxy_server_url and self.proxy_api_base:
            self.proxy_server_url = f"{self.proxy_api_base}/chat/completions"
@dataclass
class ProxyEmbeddingParameters(BaseEmbeddingModelParameters):
    proxy_server_url: str = field(
        metadata={
            "help": "Proxy base url(OPENAI_API_BASE), such as https://api.openai.com/v1"
        },
    )
    proxy_api_key: str = field(
        metadata={
            "tags": "privacy",
            "help": "The api key of the current embedding model(OPENAI_API_KEY)",
        },
    )
    device: Optional[str] = field(
        default=None,
        metadata={"help": "Device to run model. Not working for proxy embedding model"},
    )
    proxy_api_type: Optional[str] = field(
        default=None,
        metadata={
            "help": "The api type of current proxy the current embedding model(OPENAI_API_TYPE), if you use Azure, it can be: azure"
        },
    )
    proxy_api_version: Optional[str] = field(
        default=None,
        metadata={
            "help": "The api version of current proxy the current embedding model(OPENAI_API_VERSION)"
        },
    )
    proxy_backend: Optional[str] = field(
        default="text-embedding-ada-002",
        metadata={
            "help": "The model name actually pass to current proxy server url, such as text-embedding-ada-002"
        },
    )

    proxy_deployment: Optional[str] = field(
        default="text-embedding-ada-002",
        metadata={"help": "To support Azure OpenAI Service custom deployment names"},
    )

    rerank: Optional[bool] = field(
        default=False, metadata={"help": "Whether the model is a rerank model"}
    )

    def build_kwargs(self, **kwargs) -> Dict:
        # 构建参数字典，用于初始化模型
        params = {
            "openai_api_base": self.proxy_server_url,
            "openai_api_key": self.proxy_api_key,
            "openai_api_type": self.proxy_api_type if self.proxy_api_type else None,
            "openai_api_version": (
                self.proxy_api_version if self.proxy_api_version else None
            ),
            "model": self.proxy_backend,
            "deployment": (
                self.proxy_deployment if self.proxy_deployment else self.proxy_backend
            ),
        }
        # 添加额外的关键字参数到参数字典中
        for k, v in kwargs.items():
            params[k] = v
        return params

    def is_rerank_model(self) -> bool:
        """Check if the model is a rerank model"""
        return self.rerank


_EMBEDDING_PARAMETER_CLASS_TO_NAME_CONFIG = {
    ProxyEmbeddingParameters: "proxy_openai,proxy_azure,proxy_http_openapi,proxy_ollama,proxy_tongyi,rerank_proxy_http_openapi",
}

EMBEDDING_NAME_TO_PARAMETER_CLASS_CONFIG = {}


def _update_embedding_config():
    global EMBEDDING_NAME_TO_PARAMETER_CLASS_CONFIG
    # 更新嵌入配置的函数
    for param_cls, models in _EMBEDDING_PARAMETER_CLASS_TO_NAME_CONFIG.items():
        models = [m.strip() for m in models.split(",")]
        # 遍历每个模型名称并更新到全局配置中
        for model in models:
            if model not in EMBEDDING_NAME_TO_PARAMETER_CLASS_CONFIG:
                EMBEDDING_NAME_TO_PARAMETER_CLASS_CONFIG[model] = param_cls


_update_embedding_config()
```