# `D:\src\scipysrc\scikit-learn\examples\gaussian_process\plot_gpr_noisy.py`

```
"""
=========================================================================
Ability of Gaussian process regression (GPR) to estimate data noise-level
=========================================================================

This example shows the ability of the
:class:`~sklearn.gaussian_process.kernels.WhiteKernel` to estimate the noise
level in the data. Moreover, we show the importance of kernel hyperparameters
initialization.
"""

# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

# %%
# Data generation
# ---------------
#
# We will work in a setting where `X` will contain a single feature. We create a
# function that will generate the target to be predicted. We will add an
# option to add some noise to the generated target.
import numpy as np

# Define a function to generate target values based on a sine function
def target_generator(X, add_noise=False):
    # Generate a clean signal with a sine function
    target = 0.5 + np.sin(3 * X)
    # Optionally add Gaussian noise to the signal
    if add_noise:
        rng = np.random.RandomState(1)
        target += rng.normal(0, 0.3, size=target.shape)
    return target.squeeze()


# %%
# Let's have a look to the target generator where we will not add any noise to
# observe the signal that we would like to predict.
X = np.linspace(0, 5, num=30).reshape(-1, 1)
y = target_generator(X, add_noise=False)

# %%
import matplotlib.pyplot as plt

# Plot the clean signal generated by target_generator
plt.plot(X, y, label="Expected signal")
plt.legend()
plt.xlabel("X")
_ = plt.ylabel("y")

# %%
# The target is transforming the input `X` using a sine function. Now, we will
# generate few noisy training samples. To illustrate the noise level, we will
# plot the true signal together with the noisy training samples.
rng = np.random.RandomState(0)
X_train = rng.uniform(0, 5, size=20).reshape(-1, 1)
y_train = target_generator(X_train, add_noise=True)

# %%
# Plot the clean signal and overlay noisy training samples as observations
plt.plot(X, y, label="Expected signal")
plt.scatter(
    x=X_train[:, 0],
    y=y_train,
    color="black",
    alpha=0.4,
    label="Observations",
)
plt.legend()
plt.xlabel("X")
_ = plt.ylabel("y")

# %%
# Optimisation of kernel hyperparameters in GPR
# ---------------------------------------------
#
# Now, we will create a
# :class:`~sklearn.gaussian_process.GaussianProcessRegressor`
# using an additive kernel adding a
# :class:`~sklearn.gaussian_process.kernels.RBF` and
# :class:`~sklearn.gaussian_process.kernels.WhiteKernel` kernels.
# The :class:`~sklearn.gaussian_process.kernels.WhiteKernel` is a kernel that
# will able to estimate the amount of noise present in the data while the
# :class:`~sklearn.gaussian_process.kernels.RBF` will serve at fitting the
# non-linearity between the data and the target.
#
# However, we will show that the hyperparameter space contains several local
# minima. It will highlights the importance of initial hyperparameter values.
#
# We will create a model using a kernel with a high noise level and a large
# length scale, which will explain all variations in the data by noise.
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
# %%
# 创建一个包含 RBF 核和 WhiteKernel 的混合核，用于高斯过程回归
kernel = 1.0 * RBF(length_scale=1e1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(
    noise_level=1, noise_level_bounds=(1e-5, 1e1)
)
# 创建高斯过程回归对象，使用上述定义的核，并设置 alpha 参数为 0.0
gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)
# 使用训练数据 X_train 和 y_train 进行拟合
gpr.fit(X_train, y_train)
# 对测试数据 X 进行预测，同时返回预测均值 y_mean 和标准差 y_std
y_mean, y_std = gpr.predict(X, return_std=True)

# %%
# 绘制期望信号曲线
plt.plot(X, y, label="Expected signal")
# 绘制观测点
plt.scatter(x=X_train[:, 0], y=y_train, color="black", alpha=0.4, label="Observations")
# 绘制误差线条，显示预测的均值和标准差
plt.errorbar(X, y_mean, y_std)
# 添加图例
plt.legend()
# 设置 x 轴和 y 轴的标签
plt.xlabel("X")
plt.ylabel("y")
# 设置标题，显示初始化的核函数、优化后的核函数和对数边际似然
_ = plt.title(
    (
        f"Initial: {kernel}\nOptimum: {gpr.kernel_}\nLog-Marginal-Likelihood: "
        f"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}"
    ),
    fontsize=8,
)

# %%
# 我们看到最优核仍然具有较高的噪声水平和更大的长度尺度。
# 此外，我们观察到模型并未提供可靠的预测。
#
# 现在，我们将使用更大的长度尺度初始化 :class:`~sklearn.gaussian_process.kernels.RBF`，
# 并将 :class:`~sklearn.gaussian_process.kernels.WhiteKernel` 的噪声水平下限设为较小值。
kernel = 1.0 * RBF(length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(
    noise_level=1e-2, noise_level_bounds=(1e-10, 1e1)
)
# 创建新的高斯过程回归对象，使用更新后的核
gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)
# 使用训练数据 X_train 和 y_train 进行拟合
gpr.fit(X_train, y_train)
# 对测试数据 X 进行预测，同时返回预测均值 y_mean 和标准差 y_std
y_mean, y_std = gpr.predict(X, return_std=True)

# %%
# 首先，我们看到该模型的预测比之前模型更为精确：这个新模型能够估计出无噪声的函数关系。
#
# 查看核函数超参数时，我们发现找到的最佳组合具有比第一个模型更小的噪声水平和更短的长度尺度。
#
# 我们可以检查不同超参数下的 :class:`~sklearn.gaussian_process.GaussianProcessRegressor`
# 的对数边际似然 (LML)，以了解局部最小值。
from matplotlib.colors import LogNorm

# 创建长度尺度和噪声水平的对数网格
length_scale = np.logspace(-2, 4, num=50)
noise_level = np.logspace(-2, 1, num=50)
length_scale_grid, noise_level_grid = np.meshgrid(length_scale, noise_level)

# 计算不同超参数组合下的对数边际似然值
log_marginal_likelihood = [
    gpr.log_marginal_likelihood(theta=np.log([0.36, scale, noise]))
    for scale, noise in zip(length_scale_grid.ravel(), noise_level_grid.ravel())
]
log_marginal_likelihood = np.reshape(
    log_marginal_likelihood, newshape=noise_level_grid.shape
)

# %%
# 设置对数边际似然值的等高线图，用于可视化不同超参数组合的似然性
vmin, vmax = (-log_marginal_likelihood).min(), 50
level = np.around(np.logspace(np.log10(vmin), np.log10(vmax), num=50), decimals=1)
plt.contour(
    length_scale_grid,
    noise_level_grid,
    -log_marginal_likelihood,
    levels=level,
    norm=LogNorm(vmin=vmin, vmax=vmax),
)
    # 使用 LogNorm 对象进行数据的对数标准化，指定最小值为 vmin，最大值为 vmax
    norm = LogNorm(vmin=vmin, vmax=vmax),
)
# 添加一个颜色条到当前图形中
plt.colorbar()
# 设置 x 轴为对数尺度
plt.xscale("log")
# 设置 y 轴为对数尺度
plt.yscale("log")
# 设置 x 轴标签
plt.xlabel("Length-scale")
# 设置 y 轴标签
plt.ylabel("Noise-level")
# 设置图形标题
plt.title("Log-marginal-likelihood")
# 显示绘制的图形
plt.show()



# %%
# 我们可以看到存在两个局部最小值，这些最小值对应之前找到的超参数组合。
# 根据超参数的初始值不同，梯度优化可能会收敛到最佳模型，也可能不会。
# 因此，对于不同的初始值，重复进行优化是非常重要的。
```