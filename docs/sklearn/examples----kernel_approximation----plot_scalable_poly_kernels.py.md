# `D:\src\scipysrc\scikit-learn\examples\kernel_approximation\plot_scalable_poly_kernels.py`

```
"""
======================================================
Scalable learning with polynomial kernel approximation
======================================================

.. currentmodule:: sklearn.kernel_approximation

This example illustrates the use of :class:`PolynomialCountSketch` to
efficiently generate polynomial kernel feature-space approximations.
This is used to train linear classifiers that approximate the accuracy
of kernelized ones.

We use the Covtype dataset [2], trying to reproduce the experiments on the
original paper of Tensor Sketch [1], i.e. the algorithm implemented by
:class:`PolynomialCountSketch`.

First, we compute the accuracy of a linear classifier on the original
features. Then, we train linear classifiers on different numbers of
features (`n_components`) generated by :class:`PolynomialCountSketch`,
approximating the accuracy of a kernelized classifier in a scalable manner.

"""

# Authors: The scikit-learn developers
# SPDX-License-Identifier: BSD-3-Clause

# %%
# Preparing the data
# ------------------
#
# Load the Covtype dataset, which contains 581,012 samples
# with 54 features each, distributed among 6 classes. The goal of this dataset
# is to predict forest cover type from cartographic variables only
# (no remotely sensed data). After loading, we transform it into a binary
# classification problem to match the version of the dataset in the
# LIBSVM webpage [2], which was the one used in [1].

from sklearn.datasets import fetch_covtype

# Fetch the Covtype dataset and return features X and labels y
X, y = fetch_covtype(return_X_y=True)

# Convert labels: class 2 becomes 1 (positive class), all others become 0
y[y != 2] = 0
y[y == 2] = 1  

# %%
# Partitioning the data
# ---------------------
#
# Split the dataset into training and testing sets:
# - X_train, y_train: 5,000 samples for training
# - X_test, y_test: 10,000 samples for testing
# Set random_state for reproducibility.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=5_000, test_size=10_000, random_state=42
)

# %%
# Feature normalization
# ---------------------
#
# Create a pipeline to:
# - Scale features to the range [0, 1]
# - Normalize features to unit length
# This preprocessing step matches the format used in the LIBSVM webpage [2]
# and the original Tensor Sketch paper [1].

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import MinMaxScaler, Normalizer

mm = make_pipeline(MinMaxScaler(), Normalizer())
X_train = mm.fit_transform(X_train)
X_test = mm.transform(X_test)

# %%
# Establishing a baseline model
# -----------------------------
#
# Train a linear Support Vector Machine (SVM) on the original features.
# Measure and store the training time and accuracy on the test set.
# Store results in the dictionary `results`.

import time
from sklearn.svm import LinearSVC

results = {}

lsvm = LinearSVC()
start = time.time()
lsvm.fit(X_train, y_train)
lsvm_time = time.time() - start
lsvm_score = 100 * lsvm.score(X_test, y_test)

results["LSVM"] = {"time": lsvm_time, "score": lsvm_score}
# 输出线性 SVM 在原始特征上的得分，保留两位小数显示
print(f"Linear SVM score on raw features: {lsvm_score:.2f}%")

# %%
# 建立核近似模型
# -------------------------------------------
#
# 然后我们使用 :class:`PolynomialCountSketch` 训练线性 SVM，使用不同的 `n_components` 值生成特征，
# 展示这些核特征近似如何提高线性分类的准确性。在典型的应用场景中，`n_components` 应大于输入表示中的特征数量，
# 以便在线性分类方面实现改进。作为经验法则，评估分数 / 运行时间成本的最佳值通常在 `n_components` 约为特征数的 10 倍时达到，
# 尽管这可能取决于具体的数据集。请注意，由于原始样本具有 54 个特征，四次多项式核的显式特征映射将具有约 850 万个特征（确切地说是 54^4）。
# 由于 :class:`PolynomialCountSketch`，我们可以将大部分区分信息压缩成一个更紧凑的表示。
# 在本例中，我们仅运行一次实验（`n_runs` = 1），但实际应该多次重复实验以弥补 :class:`PolynomialCountSketch` 的随机性质。

from sklearn.kernel_approximation import PolynomialCountSketch

n_runs = 1
N_COMPONENTS = [250, 500, 1000, 2000]

for n_components in N_COMPONENTS:
    ps_lsvm_time = 0
    ps_lsvm_score = 0
    for _ in range(n_runs):
        # 创建流水线，包括 PolynomialCountSketch 和 LinearSVC
        pipeline = make_pipeline(
            PolynomialCountSketch(n_components=n_components, degree=4),
            LinearSVC(),
        )

        start = time.time()
        # 在训练集上拟合流水线
        pipeline.fit(X_train, y_train)
        # 计算训练时间
        ps_lsvm_time += time.time() - start
        # 计算测试集上的得分
        ps_lsvm_score += 100 * pipeline.score(X_test, y_test)

    # 计算平均训练时间和平均得分
    ps_lsvm_time /= n_runs
    ps_lsvm_score /= n_runs

    # 将结果存入字典
    results[f"LSVM + PS({n_components})"] = {
        "time": ps_lsvm_time,
        "score": ps_lsvm_score,
    }
    # 输出带有 PolynomialCountSketch 特征的线性 SVM 得分
    print(
        f"Linear SVM score on {n_components} PolynomialCountSketch "
        + f"features: {ps_lsvm_score:.2f}%"
    )

# %%
# 建立核化 SVM 模型
# -------------------------------------
#
# 训练一个核化 SVM，看看 :class:`PolynomialCountSketch` 如何近似核的性能。这可能需要一些时间，
# 因为 SVC 类的可伸缩性相对较差。这也是核近似器如此有用的原因：

from sklearn.svm import SVC

ksvm = SVC(C=500.0, kernel="poly", degree=4, coef0=0, gamma=1.0)

start = time.time()
# 在训练集上拟合核化 SVM
ksvm.fit(X_train, y_train)
# 计算训练时间
ksvm_time = time.time() - start
# 计算测试集上的得分
ksvm_score = 100 * ksvm.score(X_test, y_test)

# 将结果存入字典
results["KSVM"] = {"time": ksvm_time, "score": ksvm_score}
# 输出在原始特征上的核 SVM 得分
print(f"Kernel-SVM score on raw features: {ksvm_score:.2f}%")

# %%
# 比较结果
# ---------------------
#
# 导入 matplotlib 的 pyplot 模块，用于绘制图表
import matplotlib.pyplot as plt

# 创建一个新的图形和一个包含单个子图的坐标轴
fig, ax = plt.subplots(figsize=(7, 7))

# 绘制线性 SVM 方法的训练时间和准确率散点图
ax.scatter(
    [
        results["LSVM"]["time"],  # 线性 SVM 的训练时间
    ],
    [
        results["LSVM"]["score"],  # 线性 SVM 的准确率
    ],
    label="Linear SVM",  # 图例标签：线性 SVM
    c="green",  # 点的颜色为绿色
    marker="^",  # 点的形状为三角形
)

# 绘制带多项式 CountSketch 的线性 SVM 方法的训练时间和准确率散点图
ax.scatter(
    [
        results["LSVM + PS(250)"]["time"],  # 带多项式 CountSketch 的线性 SVM 的训练时间
    ],
    [
        results["LSVM + PS(250)"]["score"],  # 带多项式 CountSketch 的线性 SVM 的准确率
    ],
    label="Linear SVM + PolynomialCountSketch",  # 图例标签：带多项式 CountSketch 的线性 SVM
    c="blue",  # 点的颜色为蓝色
)

# 遍历 N_COMPONENTS 中的组件数，绘制对应方法的训练时间和准确率散点图，并添加注释
for n_components in N_COMPONENTS:
    ax.scatter(
        [
            results[f"LSVM + PS({n_components})"]["time"],  # 带多项式 CountSketch 的线性 SVM 的训练时间
        ],
        [
            results[f"LSVM + PS({n_components})"]["score"],  # 带多项式 CountSketch 的线性 SVM 的准确率
        ],
        c="blue",  # 点的颜色为蓝色
    )
    ax.annotate(
        f"n_comp.={n_components}",  # 注释内容显示组件数
        (
            results[f"LSVM + PS({n_components})"]["time"],  # X 坐标：带多项式 CountSketch 的线性 SVM 的训练时间
            results[f"LSVM + PS({n_components})"]["score"],  # Y 坐标：带多项式 CountSketch 的线性 SVM 的准确率
        ),
        xytext=(-30, 10),  # 注释相对于数据点的偏移量
        textcoords="offset pixels",  # 注释文本的坐标系
    )

# 绘制核 SVM 方法的训练时间和准确率散点图
ax.scatter(
    [
        results["KSVM"]["time"],  # 核 SVM 的训练时间
    ],
    [
        results["KSVM"]["score"],  # 核 SVM 的准确率
    ],
    label="Kernel SVM",  # 图例标签：核 SVM
    c="red",  # 点的颜色为红色
    marker="x",  # 点的形状为叉号
)

# 设置 X 轴标签为 "Training time (s)"
ax.set_xlabel("Training time (s)")

# 设置 Y 轴标签为 "Accuracy (%)"
ax.set_ylabel("Accuracy (%)")

# 添加图例，显示在图表中
ax.legend()

# 显示图表
plt.show()
```