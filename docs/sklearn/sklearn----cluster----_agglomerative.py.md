# `D:\src\scipysrc\scikit-learn\sklearn\cluster\_agglomerative.py`

```
# 导入警告模块，用于处理警告信息
import warnings
# 导入堆操作相关函数，用于处理堆数据结构
from heapq import heapify, heappop, heappush, heappushpop
# 导入整数和实数类型，用于参数类型检查
from numbers import Integral, Real

# 导入科学计算库 NumPy
import numpy as np
# 导入稀疏矩阵处理模块
from scipy import sparse
# 从 SciPy 稀疏矩阵图模块中导入连接组件函数
from scipy.sparse.csgraph import connected_components

# 导入基础估计器、特征输出前缀类、聚类混合类和拟合上下文管理器
from ..base import (
    BaseEstimator,
    ClassNamePrefixFeaturesOutMixin,
    ClusterMixin,
    _fit_context,
)
# 导入距离度量类
from ..metrics import DistanceMetric
# 导入64位度量映射
from ..metrics._dist_metrics import METRIC_MAPPING64
# 导入有效度量函数和成对距离计算函数
from ..metrics.pairwise import _VALID_METRICS, paired_distances
# 导入数组检查函数
from ..utils import check_array
# 导入整型浮点型字典
from ..utils._fast_dict import IntFloatDict
# 导入参数验证工具函数
from ..utils._param_validation import (
    HasMethods,
    Hidden,
    Interval,
    StrOptions,
    validate_params,
)
# 导入图处理工具函数中的连接组件修正函数
from ..utils.graph import _fix_connected_components
# 导入内存检查函数
from ..utils.validation import check_memory

# 导入层次聚类快速算法模块，忽略类型检查错误
from . import _hierarchical_fast as _hierarchical  # type: ignore
# 导入特征聚合转换类
from ._feature_agglomeration import AgglomerationTransform

###############################################################################
# 对于非完全连接的图形


def _fix_connectivity(X, connectivity, affinity):
    """
    Fixes the connectivity matrix.

    The different steps are:

    - copies it
    - makes it symmetric
    - converts it to LIL if necessary
    - completes it if necessary.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Feature matrix representing `n_samples` samples to be clustered.

    connectivity : sparse matrix, default=None
        Connectivity matrix. Defines for each sample the neighboring samples
        following a given structure of the data. The matrix is assumed to
        be symmetric and only the upper triangular half is used.
        Default is `None`, i.e, the Ward algorithm is unstructured.

    affinity : {"euclidean", "precomputed"}, default="euclidean"
        Which affinity to use. At the moment `precomputed` and
        ``euclidean`` are supported. `euclidean` uses the
        negative squared Euclidean distance between points.

    Returns
    -------
    connectivity : sparse matrix
        The fixed connectivity matrix.

    n_connected_components : int
        The number of connected components in the graph.
    """
    # 获取样本数量
    n_samples = X.shape[0]
    # 检查连接矩阵形状是否与样本数量相符
    if connectivity.shape[0] != n_samples or connectivity.shape[1] != n_samples:
        raise ValueError(
            "Wrong shape for connectivity matrix: %s when X is %s"
            % (connectivity.shape, X.shape)
        )

    # 将连接矩阵转为对称矩阵
    connectivity = connectivity + connectivity.T

    # 如果连接矩阵不是稀疏矩阵，则转换为 LIL 格式的稀疏矩阵
    if not sparse.issparse(connectivity):
        connectivity = sparse.lil_matrix(connectivity)

    # 返回修正后的连接矩阵和图中的连通组件数
    return connectivity, connected_components(connectivity)
    # `connectivity` is a sparse matrix at this point
    如果 `connectivity` 不是 lil 格式的稀疏矩阵，则转换为 lil 格式
    if connectivity.format != "lil":
        connectivity = connectivity.tolil()

    # 计算连接组件的数量和对应的标签
    n_connected_components, labels = connected_components(connectivity)

    # 如果连接组件的数量大于1，则发出警告并进行处理以避免提前停止树的生长
    if n_connected_components > 1:
        warnings.warn(
            "the number of connected components of the "
            "connectivity matrix is %d > 1. Completing it to avoid "
            "stopping the tree early." % n_connected_components,
            stacklevel=2,
        )
        # XXX: 我们是否可以在不完全连接矩阵的情况下完成操作？
        # 调用函数 `_fix_connected_components` 完成对连接组件的修复
        connectivity = _fix_connected_components(
            X=X,
            graph=connectivity,
            n_connected_components=n_connected_components,
            component_labels=labels,
            metric=affinity,
            mode="connectivity",
        )

    # 返回处理后的连接矩阵和连接组件的数量
    return connectivity, n_connected_components
def _single_linkage_tree(
    connectivity,
    n_samples,
    n_nodes,
    n_clusters,
    n_connected_components,
    return_distance,
):
    """
    Perform single linkage clustering on sparse data via the minimum
    spanning tree from scipy.sparse.csgraph, then using union-find to label.
    The parent array is then generated by walking through the tree.
    """
    from scipy.sparse.csgraph import minimum_spanning_tree

    # explicitly cast connectivity to ensure safety
    connectivity = connectivity.astype(np.float64, copy=False)

    # Ensure zero distances aren't ignored by setting them to "epsilon"
    epsilon_value = np.finfo(dtype=connectivity.data.dtype).eps
    connectivity.data[connectivity.data == 0] = epsilon_value

    # Use scipy.sparse.csgraph to generate a minimum spanning tree
    mst = minimum_spanning_tree(connectivity.tocsr())

    # Convert the graph to scipy.cluster.hierarchy array format
    mst = mst.tocoo()

    # Undo the epsilon values
    mst.data[mst.data == epsilon_value] = 0

    # Convert COO matrix to an array of (row, col, data) for edges
    mst_array = np.vstack([mst.row, mst.col, mst.data]).T

    # Sort edges of the min_spanning_tree by weight
    mst_array = mst_array[np.argsort(mst_array.T[2], kind="mergesort"), :]

    # Convert edge list into standard hierarchical clustering format
    single_linkage_tree = _hierarchical._single_linkage_label(mst_array)
    children_ = single_linkage_tree[:, :2].astype(int)

    # Compute parents
    parent = np.arange(n_nodes, dtype=np.intp)
    for i, (left, right) in enumerate(children_, n_samples):
        if n_clusters is not None and i >= n_nodes:
            break
        if left < n_nodes:
            parent[left] = i
        if right < n_nodes:
            parent[right] = i

    if return_distance:
        distances = single_linkage_tree[:, 2]
        return children_, n_connected_components, n_samples, parent, distances
    return children_, n_connected_components, n_samples, parent


注释：


# 定义单链接聚类树构建函数
def _single_linkage_tree(
    connectivity,
    n_samples,
    n_nodes,
    n_clusters,
    n_connected_components,
    return_distance,
):
    """
    通过 scipy.sparse.csgraph 中的最小生成树实现稀疏数据的单链接聚类，
    然后使用并查集进行标记。
    然后通过遍历树来生成父节点数组。
    """
    from scipy.sparse.csgraph import minimum_spanning_tree

    # 将 connectivity 显式地转换为 np.float64 类型，以确保安全性
    connectivity = connectivity.astype(np.float64, copy=False)

    # 确保不会忽略零距离，将它们设置为 "epsilon" 值
    epsilon_value = np.finfo(dtype=connectivity.data.dtype).eps
    connectivity.data[connectivity.data == 0] = epsilon_value

    # 使用 scipy.sparse.csgraph 生成最小生成树
    mst = minimum_spanning_tree(connectivity.tocsr())

    # 将图转换为 scipy.cluster.hierarchy 格式的数组
    mst = mst.tocoo()

    # 恢复 epsilon 值
    mst.data[mst.data == epsilon_value] = 0

    # 将 COO 矩阵转换为边的 (行号, 列号, 数据) 数组
    mst_array = np.vstack([mst.row, mst.col, mst.data]).T

    # 按权重对最小生成树的边进行排序
    mst_array = mst_array[np.argsort(mst_array.T[2], kind="mergesort"), :]

    # 将边列表转换为标准的层次聚类格式
    single_linkage_tree = _hierarchical._single_linkage_label(mst_array)
    children_ = single_linkage_tree[:, :2].astype(int)

    # 计算父节点数组
    parent = np.arange(n_nodes, dtype=np.intp)
    for i, (left, right) in enumerate(children_, n_samples):
        if n_clusters is not None and i >= n_nodes:
            break
        if left < n_nodes:
            parent[left] = i
        if right < n_nodes:
            parent[right] = i

    # 如果需要返回距离，则返回结果中包括距离数组
    if return_distance:
        distances = single_linkage_tree[:, 2]
        return children_, n_connected_components, n_samples, parent, distances
    # 否则只返回聚类结果和相关信息
    return children_, n_connected_components, n_samples, parent
    connectivity : {array-like, sparse matrix}, default=None
        # 连通性矩阵。对于每个样本，定义了数据的特定结构下的相邻样本。
        # 矩阵假定是对称的，只使用上三角部分。
        # 默认为None，即Ward算法未指定结构。

    n_clusters : int, default=None
        # `n_clusters` 应该小于 `n_samples`。在树的构建过程中提前停止到 `n_clusters`。
        # 如果聚类数目与样本数目相比不小，则这有助于减少计算时间。
        # 在这种情况下，完整的树结构未被计算，因此 'children' 输出有限，应使用 'parents' 输出。
        # 此选项仅在指定连接矩阵时有效。

    return_distance : bool, default=False
        # 如果为 `True`，返回聚类之间的距离。

    Returns
    -------
    children : ndarray of shape (n_nodes-1, 2)
        # 每个非叶节点的子节点。小于 `n_samples` 的值对应于树的叶子节点，即原始样本。
        # 节点 `i` 大于或等于 `n_samples` 是一个非叶节点，其子节点是 `children_[i - n_samples]`。
        # 或者在第 i 次迭代中，children[i][0] 和 children[i][1] 被合并形成节点 `n_samples + i`。

    n_connected_components : int
        # 图中连通分量的数量。

    n_leaves : int
        # 树中叶子节点的数量。

    parents : ndarray of shape (n_nodes,) or None
        # 每个节点的父节点。仅当指定连接矩阵时返回，否则返回 'None'。

    distances : ndarray of shape (n_nodes-1,)
        # 如果 `return_distance` 设置为 `True`，则返回距离（为了兼容性）。
        # 节点之间的距离。`distances[i]` 对应于节点 `children[i, 1]` 和 `children[i, 2]` 的加权欧氏距离。
        # 如果节点是树的叶子节点，则 `distances[i]` 是它们的非加权欧氏距离。
        # 距离按照以下方式更新（来自scipy.hierarchy.linkage）：

        # 新的条目 :math:`d(u,v)` 计算如下：

        # .. math::

        #    d(u,v) = \\sqrt{\\frac{|v|+|s|}
        #                        {T}d(v,s)^2
        #                 + \\frac{|v|+|t|}
        #                        {T}d(v,t)^2
        #                 - \\frac{|v|}
        #                        {T}d(s,t)^2}

        # 其中 :math:`u` 是新组成的聚类，包含聚类 :math:`s` 和 :math:`t`，
        # :math:`v` 是森林中未使用的聚类， :math:`T=|v|+|s|+|t|`，
        # :math:`|*|` 是其参数的基数。这也被称为增量算法。

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.cluster import ward_tree  # 从 sklearn 中导入 ward_tree 函数
    >>> X = np.array([[1, 2], [1, 4], [1, 0],  # 创建一个二维 NumPy 数组 X
    ...               [4, 2], [4, 4], [4, 0]])
    >>> children, n_connected_components, n_leaves, parents = ward_tree(X)  # 调用 ward_tree 函数，并接收返回的四个变量
    >>> children  # 打印变量 children，显示聚类的树结构
    array([[0, 1],
           [3, 5],
           [2, 6],
           [4, 7],
           [8, 9]])
    >>> n_connected_components  # 打印 n_connected_components，显示连接的组件数
    1
    >>> n_leaves  # 打印 n_leaves，显示叶子节点数
    6
    """
    X = np.asarray(X)  # 将 X 转换为 NumPy 数组
    if X.ndim == 1:  # 如果 X 是一维数组，则改变形状为二维数组
        X = np.reshape(X, (-1, 1))
    n_samples, n_features = X.shape  # 获取数组 X 的样本数和特征数

    if connectivity is None:  # 如果连接矩阵为空
        from scipy.cluster import hierarchy  # 导入 hierarchy 模块

        if n_clusters is not None:  # 如果指定了聚类数目，发出警告信息
            warnings.warn(
                (
                    "Partial build of the tree is implemented "
                    "only for structured clustering (i.e. with "
                    "explicit connectivity). The algorithm "
                    "will build the full tree and only "
                    "retain the lower branches required "
                    "for the specified number of clusters"
                ),
                stacklevel=2,
            )
        X = np.require(X, requirements="W")  # 要求 X 满足特定要求
        out = hierarchy.ward(X)  # 使用 ward 方法构建树
        children_ = out[:, :2].astype(np.intp)  # 提取出树的子节点信息并转换数据类型为整型

        if return_distance:  # 如果需要返回距离信息
            distances = out[:, 2]  # 获取距离信息
            return children_, 1, n_samples, None, distances  # 返回子节点、连接组件数、样本数、父节点信息和距离信息
        else:
            return children_, 1, n_samples, None  # 返回子节点、连接组件数、样本数和空的父节点信息

    connectivity, n_connected_components = _fix_connectivity(
        X, connectivity, affinity="euclidean"
    )  # 修正连接性并获取连接组件数

    if n_clusters is None:  # 如果未指定聚类数目
        n_nodes = 2 * n_samples - 1  # 计算节点数
    else:
        if n_clusters > n_samples:  # 如果聚类数目大于样本数，引发错误
            raise ValueError(
                "Cannot provide more clusters than samples. "
                "%i n_clusters was asked, and there are %i "
                "samples." % (n_clusters, n_samples)
            )
        n_nodes = 2 * n_samples - n_clusters  # 计算节点数

    # 创建惯性矩阵
    coord_row = []  # 初始化行坐标列表
    coord_col = []  # 初始化列坐标列表
    A = []  # 初始化列表 A
    for ind, row in enumerate(connectivity.rows):  # 遍历连接性矩阵的行
        A.append(row)  # 将行添加到列表 A 中
        # 仅保留上三角形的矩阵
        row = [i for i in row if i < ind]  # 过滤掉大于当前行索引的列索引
        coord_row.extend(  # 扩展行坐标列表
            len(row) * [
                ind,
            ]
        )
        coord_col.extend(row)  # 扩展列坐标列表

    coord_row = np.array(coord_row, dtype=np.intp, order="C")  # 将行坐标转换为 NumPy 数组
    coord_col = np.array(coord_col, dtype=np.intp, order="C")  # 将列坐标转换为 NumPy 数组

    # 创建矩阵的矩
    moments_1 = np.zeros(n_nodes, order="C")  # 初始化 moments_1 数组
    moments_1[:n_samples] = 1  # 将前 n_samples 个元素设置为 1
    moments_2 = np.zeros((n_nodes, n_features), order="C")  # 初始化 moments_2 数组
    moments_2[:n_samples] = X  # 将前 n_samples 行设置为 X 的内容
    inertia = np.empty(len(coord_row), dtype=np.float64, order="C")  # 初始化惯性数组
    _hierarchical.compute_ward_dist(moments_1, moments_2, coord_row, coord_col, inertia)  # 计算惯性矩阵的距离
    inertia = list(zip(inertia, coord_row, coord_col))  # 将惯性、行坐标和列坐标组成元组并转换为列表
    heapify(inertia)  # 对惯性进行堆排序

    # 准备主要字段
    parent = np.arange(n_nodes, dtype=np.intp)  # 创建父节点数组
    # 创建一个布尔类型的数组，所有节点都标记为已使用
    used_node = np.ones(n_nodes, dtype=bool)
    # 初始化一个空列表，用于存储子节点
    children = []
    # 如果需要返回距离信息，则初始化一个空的数组，用于存储距离值
    if return_distance:
        distances = np.empty(n_nodes - n_samples)

    # 创建一个布尔类型的数组，表示节点是否被访问过，初始值为未访问
    not_visited = np.empty(n_nodes, dtype=bool, order="C")

    # 递归合并循环
    for k in range(n_samples, n_nodes):
        # 从惯性堆中弹出合并节点及其惯性值
        while True:
            inert, i, j = heappop(inertia)
            if used_node[i] and used_node[j]:
                break
        # 设置父节点关系
        parent[i], parent[j] = k, k
        # 将合并的节点加入到子节点列表中
        children.append((i, j))
        # 标记合并的节点为已使用
        used_node[i] = used_node[j] = False
        # 如果需要返回距离信息，则存储惯性值
        if return_distance:
            distances[k - n_samples] = inert

        # 更新节点的时刻信息
        moments_1[k] = moments_1[i] + moments_1[j]
        moments_2[k] = moments_2[i] + moments_2[j]

        # 更新结构矩阵 A 和惯性矩阵
        coord_col = []
        # 将未访问节点数组重置为全是未访问状态
        not_visited.fill(1)
        not_visited[k] = 0
        # 获取节点 i 和 j 的所有父节点
        _hierarchical._get_parents(A[i], coord_col, parent, not_visited)
        _hierarchical._get_parents(A[j], coord_col, parent, not_visited)
        # 使用列表推导式更新结构矩阵 A，比 for 循环更快
        [A[col].append(k) for col in coord_col]
        # 将 coord_col 添加到 A 的末尾
        A.append(coord_col)
        # 将 coord_col 转换为 numpy 数组，以提高缓存效率
        coord_col = np.array(coord_col, dtype=np.intp, order="C")
        # 创建一个与 coord_col 形状相同的数组，并填充为 k
        coord_row = np.empty(coord_col.shape, dtype=np.intp, order="C")
        coord_row.fill(k)
        n_additions = len(coord_row)
        ini = np.empty(n_additions, dtype=np.float64, order="C")

        # 计算 Ward 距离
        _hierarchical.compute_ward_dist(moments_1, moments_2, coord_row, coord_col, ini)

        # 使用列表推导式将新的惯性值加入堆中，比 for 循环更快
        [heappush(inertia, (ini[idx], k, coord_col[idx])) for idx in range(n_additions)]

    # 将叶子节点分离到 children 中（到目前为止是空列表）
    n_leaves = n_samples
    # 对 children 进行排序，以便与非结构化版本输出一致
    children = [c[::-1] for c in children]
    children = np.array(children)  # 返回 numpy 数组以提高缓存效率

    if return_distance:
        # 将距离值乘以根号2，以便与非结构化版本比较
        distances = np.sqrt(2.0 * distances)
        return children, n_connected_components, n_leaves, parent, distances
    else:
        return children, n_connected_components, n_leaves, parent
`
# single average and complete linkage
def linkage_tree(
    X,
    connectivity=None,
    n_clusters=None,
    linkage="complete",
    affinity="euclidean",
    return_distance=False,
):
    """Linkage agglomerative clustering based on a Feature matrix.

    The inertia matrix uses a Heapq-based representation.

    This is the structured version, that takes into account some topological
    structure between samples.

    Read more in the :ref:`User Guide <hierarchical_clustering>`.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Feature matrix representing `n_samples` samples to be clustered.

    connectivity : sparse matrix, default=None
        Connectivity matrix. Defines for each sample the neighboring samples
        following a given structure of the data. The matrix is assumed to
        be symmetric and only the upper triangular half is used.
        Default is `None`, i.e, the Ward algorithm is unstructured.

    n_clusters : int, default=None
        Stop early the construction of the tree at `n_clusters`. This is
        useful to decrease computation time if the number of clusters is
        not small compared to the number of samples. In this case, the
        complete tree is not computed, thus the 'children' output is of
        limited use, and the 'parents' output should rather be used.
        This option is valid only when specifying a connectivity matrix.

    linkage : {"average", "complete", "single"}, default="complete"
        Which linkage criteria to use. The linkage criterion determines which
        distance to use between sets of observation.
            - "average" uses the average of the distances of each observation of
              the two sets.
            - "complete" or maximum linkage uses the maximum distances between
              all observations of the two sets.
            - "single" uses the minimum of the distances between all
              observations of the two sets.

    affinity : str or callable, default='euclidean'
        Which metric to use. Can be 'euclidean', 'manhattan', or any
        distance known to paired distance (see metric.pairwise).

    return_distance : bool, default=False
        Whether or not to return the distances between the clusters.

    Returns
    -------
    children : ndarray of shape (n_nodes-1, 2)
        The children of each non-leaf node. Values less than `n_samples`
        correspond to leaves of the tree which are the original samples.
        A node `i` greater than or equal to `n_samples` is a non-leaf
        node and has children `children_[i - n_samples]`. Alternatively
        at the i-th iteration, children[i][0] and children[i][1]
        are merged to form node `n_samples + i`.

    n_connected_components : int
        The number of connected components in the graph.

    n_leaves : int
        The number of leaves in the tree.
    """
    # parents : ndarray of shape (n_nodes, ) or None
    # 每个节点的父节点数组。当指定了连接矩阵时返回，否则返回None。

    # distances : ndarray of shape (n_nodes-1,)
    # 当 `return_distance` 设置为 `True` 时返回。
    # distances[i] 表示当 children[i][0] 和 children[i][1] 合并时它们之间的距离。

    # See Also
    # --------
    # ward_tree : 使用Ward linkage进行层次聚类。
    """
    X = np.asarray(X)
    # 将输入数据转换为NumPy数组
    if X.ndim == 1:
        # 如果输入数据是一维的，则将其重塑为二维的(-1, 1)形状
        X = np.reshape(X, (-1, 1))
    n_samples, n_features = X.shape

    # linkage_choices 字典，用于选择合适的链接函数
    linkage_choices = {
        "complete": _hierarchical.max_merge,
        "average": _hierarchical.average_merge,
        "single": None,
    }  # Single linkage is handled differently
    try:
        join_func = linkage_choices[linkage]
    except KeyError as e:
        # 如果给定的链接类型不在支持的选项中，则引发错误
        raise ValueError(
            "Unknown linkage option, linkage should be one of %s, but %s was given"
            % (linkage_choices.keys(), linkage)
        ) from e

    # 当使用余弦相似度并且 X 包含零向量时，抛出错误
    if affinity == "cosine" and np.any(~np.any(X, axis=1)):
        raise ValueError("Cosine affinity cannot be used when X contains zero vectors")
    # 如果连接性参数为 None，则导入 scipy.cluster.hierarchy 并且引入 PIL 库
    if connectivity is None:
        from scipy.cluster import hierarchy  # imports PIL
        
        # 如果指定了 n_clusters 参数，则发出警告，因为部分树的构建仅对结构化聚类有效
        if n_clusters is not None:
            warnings.warn(
                (
                    "Partial build of the tree is implemented "
                    "only for structured clustering (i.e. with "
                    "explicit connectivity). The algorithm "
                    "will build the full tree and only "
                    "retain the lower branches required "
                    "for the specified number of clusters"
                ),
                stacklevel=2,
            )
        
        # 如果 affinity 参数为 "precomputed"
        if affinity == "precomputed":
            # 对于 hierarchy.linkage 函数在预计算数据上的使用，
            # 第一个参数应为一个形状与 sklearn.metrics.pairwise_distances 返回相同的 ndarray。
            if X.shape[0] != X.shape[1]:
                raise ValueError(
                    f"Distance matrix should be square, got matrix of shape {X.shape}"
                )
            # 获取上三角矩阵的索引
            i, j = np.triu_indices(X.shape[0], k=1)
            # 提取上三角部分作为 X 的新值
            X = X[i, j]
        
        # 如果 affinity 参数为 "l2"
        elif affinity == "l2":
            # 转换为 scipy 能理解的标准度量方式
            affinity = "euclidean"
        
        # 如果 affinity 参数为 "l1" 或 "manhattan"
        elif affinity in ("l1", "manhattan"):
            # 转换为 "cityblock"，即曼哈顿距离
            affinity = "cityblock"
        
        # 如果 affinity 参数是可调用的函数
        elif callable(affinity):
            # 使用 affinity 函数处理 X 数据
            X = affinity(X)
            # 获取上三角矩阵的索引
            i, j = np.triu_indices(X.shape[0], k=1)
            # 提取上三角部分作为 X 的新值
            X = X[i, j]
        
        # 如果 linkage 参数为 "single"，且 affinity 不是 "precomputed"，并且 affinity 在 METRIC_MAPPING64 中
        if (
            linkage == "single"
            and affinity != "precomputed"
            and not callable(affinity)
            and affinity in METRIC_MAPPING64
        ):
            # 获取快速的 Cython 度量函数
            dist_metric = DistanceMetric.get_metric(affinity)
            
            # Cython 程序需要连续的数组
            X = np.ascontiguousarray(X, dtype=np.double)
            
            # 使用 _hierarchical.mst_linkage_core 函数进行最小生成树的计算
            mst = _hierarchical.mst_linkage_core(X, dist_metric)
            # 按权重对最小生成树的边进行排序
            mst = mst[np.argsort(mst.T[2], kind="mergesort"), :]
            
            # 将边列表转换为标准的层次聚类格式
            out = _hierarchical.single_linkage_label(mst)
        
        else:
            # 使用 hierarchy.linkage 函数进行层次聚类计算
            out = hierarchy.linkage(X, method=linkage, metric=affinity)
        
        # 提取出层次聚类结果的子节点信息
        children_ = out[:, :2].astype(int, copy=False)
        
        # 如果 return_distance 参数为 True，则返回距离数组
        if return_distance:
            distances = out[:, 2]
            return children_, 1, n_samples, None, distances
        # 否则返回聚类结果的子节点信息及其他默认值
        return children_, 1, n_samples, None
    
    # 如果连接性参数不为 None，则调用 _fix_connectivity 函数修正连接性
    connectivity, n_connected_components = _fix_connectivity(
        X, connectivity, affinity=affinity
    )
    
    # 将 connectivity 转换为 COO 格式，并将对角线元素设为零
    connectivity = connectivity.tocoo()
    diag_mask = connectivity.row != connectivity.col
    connectivity.row = connectivity.row[diag_mask]
    connectivity.col = connectivity.col[diag_mask]
    connectivity.data = connectivity.data[diag_mask]
    del diag_mask
    # 如果亲和性（距离度量方式）为"precomputed"
    if affinity == "precomputed":
        # 从预先计算好的距离矩阵中提取所需的距离数据，转换为 np.float64 类型，避免复制
        distances = X[connectivity.row, connectivity.col].astype(np.float64, copy=False)
    else:
        # 否则，使用指定的距离度量方法计算所有数据点之间的距离
        # FIXME 我们计算了所有的距离，但实际上我们可以只计算“有意义”的距离
        distances = paired_distances(
            X[connectivity.row], X[connectivity.col], metric=affinity
        )
    # 将计算得到的距离数据存储在连接性对象的数据属性中
    connectivity.data = distances

    # 如果未指定聚类数目
    if n_clusters is None:
        # 计算节点数目，通常是构建树时的节点总数
        n_nodes = 2 * n_samples - 1
    else:
        # 否则，确保聚类数目不超过样本数目
        assert n_clusters <= n_samples
        # 根据指定的聚类数目计算节点数目
        n_nodes = 2 * n_samples - n_clusters

    # 如果链接方式为"single"
    if linkage == "single":
        # 返回通过单链接法构建的层次聚类树
        return _single_linkage_tree(
            connectivity,
            n_samples,
            n_nodes,
            n_clusters,
            n_connected_components,
            return_distance,
        )

    # 如果需要返回距离信息
    if return_distance:
        # 创建一个空的距离数组，用于存储后续计算的距离
        distances = np.empty(n_nodes - n_samples)
    
    # 创建一个空的对象数组 A，用于存储连接矩阵的每一行数据
    A = np.empty(n_nodes, dtype=object)
    # 创建一个空列表 inertia，用于存储权重边对象

    # 将连接矩阵转换为 LIL 格式，以便快速访问行数据
    connectivity = connectivity.tolil()
    # 将连接矩阵存储在 IntFloatDict 的列表中
    for ind, (data, row) in enumerate(zip(connectivity.data, connectivity.rows)):
        # 使用 IntFloatDict 对象存储每一行的数据和对应的列索引
        A[ind] = IntFloatDict(
            np.asarray(row, dtype=np.intp), np.asarray(data, dtype=np.float64)
        )
        # 只保留上三角形部分的数据用于堆操作
        # 使用生成器表达式提高性能，遍历并存储权重边对象
        inertia.extend(
            _hierarchical.WeightedEdge(d, ind, r) for r, d in zip(row, data) if r < ind
        )
    # 释放连接矩阵占用的内存
    del connectivity

    # 对权重边列表进行堆化操作，以便后续的合并过程
    heapify(inertia)

    # 初始化主要字段
    # parent 数组用于存储节点的父节点索引
    parent = np.arange(n_nodes, dtype=np.intp)
    # used_node 数组标记节点是否被使用
    used_node = np.ones(n_nodes, dtype=np.intp)
    # children 列表用于存储每个节点的子节点列表

    # 递归合并过程的开始
    for k in range(n_samples, n_nodes):
        # 对于每个新的合并节点 k，执行以下操作：

        # 识别合并的边
        while True:
            edge = heappop(inertia)
            if used_node[edge.a] and used_node[edge.b]:
                break

        i = edge.a
        j = edge.b

        if return_distance:
            # 如果需要返回距离信息，则存储当前边的权重到 distances 数组中
            distances[k - n_samples] = edge.weight

        # 将节点 i 和 j 设置为节点 k 的父节点，并将它们作为一个新的子节点对加入 children 列表中
        parent[i] = parent[j] = k
        children.append((i, j))

        # 更新集群中每个元素的数量
        n_i = used_node[i]
        n_j = used_node[j]
        used_node[k] = n_i + n_j
        used_node[i] = used_node[j] = False

        # 更新结构矩阵 A 和惯性矩阵
        # 使用 join_func 函数对 A[i] 和 A[j] 进行聪明的 'min' 或 'max' 操作，得到 coord_col
        coord_col = join_func(A[i], A[j], used_node, n_i, n_j)
        for col, d in coord_col:
            # 将新的列 col 和相应的距离 d 添加到 A[k] 中
            A[col].append((k, d))
            # 使用 coord_col 中的信息更新惯性队列 inertia
            heappush(inertia, _hierarchical.WeightedEdge(d, k, col))
        # 将 A[i] 和 A[j] 清空以节省内存
        A[i] = A[j] = []

    # 将叶子节点从 children 中分离（此时 children 是一个空列表）
    n_leaves = n_samples

    # 为了实现高效的缓存，返回 numpy 数组形式的 children
    children = np.array(children)[:, ::-1]

    if return_distance:
        # 如果需要返回距离信息，则返回 children, n_connected_components, n_leaves, parent, distances
        return children, n_connected_components, n_leaves, parent, distances
    # 否则，返回 children, n_connected_components, n_leaves, parent
    return children, n_connected_components, n_leaves, parent
# Matching names to tree-building strategies
def _complete_linkage(*args, **kwargs):
    # 设置链接方式为“complete”
    kwargs["linkage"] = "complete"
    # 调用 linkage_tree 函数，使用设置好的参数
    return linkage_tree(*args, **kwargs)


def _average_linkage(*args, **kwargs):
    # 设置链接方式为“average”
    kwargs["linkage"] = "average"
    # 调用 linkage_tree 函数，使用设置好的参数
    return linkage_tree(*args, **kwargs)


def _single_linkage(*args, **kwargs):
    # 设置链接方式为“single”
    kwargs["linkage"] = "single"
    # 调用 linkage_tree 函数，使用设置好的参数
    return linkage_tree(*args, **kwargs)


_TREE_BUILDERS = dict(
    ward=ward_tree,
    complete=_complete_linkage,
    average=_average_linkage,
    single=_single_linkage,
)

###############################################################################
# Functions for cutting hierarchical clustering tree


def _hc_cut(n_clusters, children, n_leaves):
    """Function cutting the ward tree for a given number of clusters.

    Parameters
    ----------
    n_clusters : int or ndarray
        The number of clusters to form.

    children : ndarray of shape (n_nodes-1, 2)
        The children of each non-leaf node. Values less than `n_samples`
        correspond to leaves of the tree which are the original samples.
        A node `i` greater than or equal to `n_samples` is a non-leaf
        node and has children `children_[i - n_samples]`. Alternatively
        at the i-th iteration, children[i][0] and children[i][1]
        are merged to form node `n_samples + i`.

    n_leaves : int
        Number of leaves of the tree.

    Returns
    -------
    labels : array [n_samples]
        Cluster labels for each point.
    """
    if n_clusters > n_leaves:
        raise ValueError(
            "Cannot extract more clusters than samples: "
            f"{n_clusters} clusters were given for a tree with {n_leaves} leaves."
        )
    # In this function, we store nodes as a heap to avoid recomputing
    # the max of the nodes: the first element is always the smallest
    # We use negated indices as heaps work on smallest elements, and we
    # are interested in largest elements
    # children[-1] is the root of the tree
    # 将根节点插入堆中，用于维护合并的节点
    nodes = [-(max(children[-1]) + 1)]
    for _ in range(n_clusters - 1):
        # 由于堆的性质，nodes[0] 是最小元素
        these_children = children[-nodes[0] - n_leaves]
        # 插入两个子节点并移除最大节点
        heappush(nodes, -these_children[0])
        heappushpop(nodes, -these_children[1])
    label = np.zeros(n_leaves, dtype=np.intp)
    for i, node in enumerate(nodes):
        # 为每个节点设置标签，根据_hc_get_descendent函数计算
        label[_hierarchical._hc_get_descendent(-node, children, n_leaves)] = i
    return label


###############################################################################
    # 定义变量 n_clusters，表示要寻找的簇的数量，可以是整数或 None，默认为 2
    n_clusters : int or None, default=2
        # 如果 distance_threshold 不是 None，则 n_clusters 必须是 None
        The number of clusters to find. It must be ``None`` if
        ``distance_threshold`` is not ``None``.

    # 定义变量 metric，表示用于计算链接的距离度量标准，可以是字符串或可调用对象，默认为 "euclidean"
    metric : str or callable, default="euclidean"
        # 用于计算链接的度量标准，可以是 "euclidean", "l1", "l2", "manhattan", "cosine" 或 "precomputed"
        # 如果 linkage 是 "ward"，则只接受 "euclidean"
        # 如果是 "precomputed"，则需要将距离矩阵作为拟合方法的输入
        Metric used to compute the linkage. Can be "euclidean", "l1", "l2",
        "manhattan", "cosine", or "precomputed". If linkage is "ward", only
        "euclidean" is accepted.

        .. versionadded:: 1.2

        .. deprecated:: 1.4
           `metric=None` is deprecated in 1.4 and will be removed in 1.6.
           Let `metric` be the default value (i.e. `"euclidean"`) instead.

    # 定义变量 memory，表示用于缓存树计算结果的对象或路径，默认为 None
    memory : str or object with the joblib.Memory interface, default=None
        # 用于缓存树计算结果的对象或路径
        Used to cache the output of the computation of the tree.
        By default, no caching is done. If a string is given, it is the
        path to the caching directory.

    # 定义变量 connectivity，表示连接性矩阵，用于定义数据结构中每个样本的邻居关系，默认为 None
    connectivity : array-like, sparse matrix, or callable, default=None
        # 连接性矩阵，用于定义数据结构中每个样本的邻居关系
        Connectivity matrix. Defines for each sample the neighboring
        samples following a given structure of the data.
        This can be a connectivity matrix itself or a callable that transforms
        the data into a connectivity matrix, such as derived from
        `kneighbors_graph`. Default is ``None``, i.e, the
        hierarchical clustering algorithm is unstructured.

        For an example of connectivity matrix using
        :class:`~sklearn.neighbors.kneighbors_graph`, see
        :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`.

    # 定义变量 compute_full_tree，表示是否在达到 n_clusters 后提前停止树的构建，默认为 'auto'
    compute_full_tree : 'auto' or bool, default='auto'
        # 是否在达到 n_clusters 后提前停止树的构建，对于减少计算时间很有用，如果簇的数量与样本数目相比不是很小
        # 只有在指定连接性矩阵时才有用。注意，当变化簇的数量并使用缓存时，计算完整树可能更有利。
        # 如果 distance_threshold 不是 None，则必须设置为 True
        Stop early the construction of the tree at ``n_clusters``. This is
        useful to decrease computation time if the number of clusters is not
        small compared to the number of samples. This option is useful only
        when specifying a connectivity matrix. Note also that when varying the
        number of clusters and using caching, it may be advantageous to compute
        the full tree. It must be ``True`` if ``distance_threshold`` is not
        ``None``. By default `compute_full_tree` is "auto", which is equivalent
        to `True` when `distance_threshold` is not `None` or that `n_clusters`
        is inferior to the maximum between 100 or `0.02 * n_samples`.
        Otherwise, "auto" is equivalent to `False`.
    linkage : {'ward', 'complete', 'average', 'single'}, default='ward'
        # 聚类时使用的链接标准。链接标准决定了观察集之间使用的距离。
        # 算法将合并最小化此标准的聚类对。

        - 'ward'：最小化合并后聚类的方差。
        - 'average'：使用两个集合中每个观察的距离的平均值。
        - 'complete' 或 'maximum'：使用两个集合中所有观察之间的最大距离。
        - 'single'：使用两个集合中所有观察之间的最小距离。

        .. versionadded:: 0.20
            添加了 'single' 选项。

        有关比较不同链接标准的示例，请参见
        :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`。

    distance_threshold : float, default=None
        # 聚合过程中的链接距离阈值。高于或等于此阈值的聚类将不会合并。
        # 如果不为 ``None``，则 ``n_clusters`` 必须为 ``None``，且
        # ``compute_full_tree`` 必须为 ``True``。

        .. versionadded:: 0.21

    compute_distances : bool, default=False
        # 即使未使用 `distance_threshold`，也计算聚类之间的距离。
        # 可用于制作树状图可视化，但会增加计算和内存开销。

        .. versionadded:: 0.24

        有关树状图可视化的示例，请参见
        :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`。

    Attributes
    ----------
    n_clusters_ : int
        # 算法找到的聚类数。如果 ``distance_threshold=None``，则等于给定的
        # ``n_clusters``。

    labels_ : ndarray of shape (n_samples)
        # 每个点的聚类标签。

    n_leaves_ : int
        # 层次树中的叶子节点数。

    n_connected_components_ : int
        # 图中估计的连通分量数。

        .. versionadded:: 0.21
            添加了 ``n_connected_components_`` 以替代 ``n_components_``。

    n_features_in_ : int
        # 在拟合过程中看到的特征数目。

        .. versionadded:: 0.24

    feature_names_in_ : ndarray of shape (`n_features_in_`,)
        # 在拟合过程中看到的特征名称。仅当 `X` 的特征名全为字符串时定义。

        .. versionadded:: 1.0

    children_ : array-like of shape (n_samples-1, 2)
        # 每个非叶节点的子节点。小于 `n_samples` 的值对应于树的叶子节点，
        # 即原始样本。大于或等于 `n_samples` 的节点 `i` 是非叶节点，其子节点
        # `children_[i - n_samples]` 表示在第 i 次迭代中合并的节点 `n_samples + i`。
    # distances_ : array-like of shape (n_nodes-1,)
    #     存储节点间距离的数组，形状为 (n_nodes-1,)
    #     只有在使用 distance_threshold 或设置 compute_distances=True 时才会计算。

    # See Also
    # --------
    # FeatureAgglomeration : 用于特征聚合的凝聚聚类，不同于样本聚类。
    # ward_tree : 使用 ward 连接的层次聚类。

    # Examples
    # --------
    # >>> from sklearn.cluster import AgglomerativeClustering
    # >>> import numpy as np
    # >>> X = np.array([[1, 2], [1, 4], [1, 0],
    # ...               [4, 2], [4, 4], [4, 0]])
    # >>> clustering = AgglomerativeClustering().fit(X)
    # >>> clustering
    # AgglomerativeClustering()
    # >>> clustering.labels_
    # array([1, 1, 1, 0, 0, 0])
    """

    _parameter_constraints: dict = {
        "n_clusters": [Interval(Integral, 1, None, closed="left"), None],
        "metric": [
            StrOptions(set(_VALID_METRICS) | {"precomputed"}),
            callable,
            Hidden(None),
        ],
        "memory": [str, HasMethods("cache"), None],
        "connectivity": ["array-like", "sparse matrix", callable, None],
        "compute_full_tree": [StrOptions({"auto"}), "boolean"],
        "linkage": [StrOptions(set(_TREE_BUILDERS.keys()))],
        "distance_threshold": [Interval(Real, 0, None, closed="left"), None],
        "compute_distances": ["boolean"],
    }

    def __init__(
        self,
        n_clusters=2,
        *,
        metric="euclidean",
        memory=None,
        connectivity=None,
        compute_full_tree="auto",
        linkage="ward",
        distance_threshold=None,
        compute_distances=False,
    ):
        """
        初始化聚类器对象。

        Parameters
        ----------
        n_clusters : int, optional, default=2
            聚类的目标类别数。
        metric : str or callable, optional, default="euclidean"
            距离度量的方法，或者一个可调用的函数。
        memory : str, optional, default=None
            内存缓存的路径。
        connectivity : array-like, sparse matrix or callable, optional, default=None
            样本之间的连接方式。
        compute_full_tree : str or bool, optional, default="auto"
            是否计算完整的聚类树。
        linkage : str, optional, default="ward"
            连接算法的选择。
        distance_threshold : float, optional, default=None
            停止合并聚类的距离阈值。
        compute_distances : bool, optional, default=False
            是否计算节点之间的距离。

        Returns
        -------
        self : object
            返回初始化后的对象实例。
        """
        self.n_clusters = n_clusters
        self.distance_threshold = distance_threshold
        self.memory = memory
        self.connectivity = connectivity
        self.compute_full_tree = compute_full_tree
        self.linkage = linkage
        self.metric = metric
        self.compute_distances = compute_distances

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y=None):
        """从特征或距离矩阵拟合层次聚类模型。

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features) or \
                (n_samples, n_samples)
            训练实例或实例间距离的数组，若 ``metric='precomputed'`` 时使用。
        y : 忽略
            不使用，仅为了符合 API 规范而存在。

        Returns
        -------
        self : object
            返回拟合后的对象实例。
        """
        X = self._validate_data(X, ensure_min_samples=2)
        return self._fit(X)
    # 使用父类的fit_predict方法，对输入的数据进行聚类拟合和预测
    def fit_predict(self, X, y=None):
        """Fit and return the result of each sample's clustering assignment.

        In addition to fitting, this method also return the result of the
        clustering assignment for each sample in the training set.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features) or \
                (n_samples, n_samples)
            Training instances to cluster, or distances between instances if
            ``affinity='precomputed'``.

        y : Ignored
            Not used, present here for API consistency by convention.

        Returns
        -------
        labels : ndarray of shape (n_samples,)
            Cluster labels.
        """
        # 调用父类的fit_predict方法，对输入数据进行聚类拟合和预测，并返回聚类结果标签
        return super().fit_predict(X, y)
class FeatureAgglomeration(
    ClassNamePrefixFeaturesOutMixin, AgglomerativeClustering, AgglomerationTransform
):
    """Agglomerate features.

    Recursively merges pair of clusters of features.

    Read more in the :ref:`User Guide <hierarchical_clustering>`.

    Parameters
    ----------
    n_clusters : int or None, default=2
        The number of clusters to find. It must be ``None`` if
        ``distance_threshold`` is not ``None``.

    metric : str or callable, default="euclidean"
        Metric used to compute the linkage. Can be "euclidean", "l1", "l2",
        "manhattan", "cosine", or "precomputed". If linkage is "ward", only
        "euclidean" is accepted. If "precomputed", a distance matrix is needed
        as input for the fit method.

        .. versionadded:: 1.2

        .. deprecated:: 1.4
           `metric=None` is deprecated in 1.4 and will be removed in 1.6.
           Let `metric` be the default value (i.e. `"euclidean"`) instead.

    memory : str or object with the joblib.Memory interface, default=None
        Used to cache the output of the computation of the tree.
        By default, no caching is done. If a string is given, it is the
        path to the caching directory.

    connectivity : array-like, sparse matrix, or callable, default=None
        Connectivity matrix. Defines for each feature the neighboring
        features following a given structure of the data.
        This can be a connectivity matrix itself or a callable that transforms
        the data into a connectivity matrix, such as derived from
        `kneighbors_graph`. Default is `None`, i.e, the
        hierarchical clustering algorithm is unstructured.

    compute_full_tree : 'auto' or bool, default='auto'
        Stop early the construction of the tree at `n_clusters`. This is useful
        to decrease computation time if the number of clusters is not small
        compared to the number of features. This option is useful only when
        specifying a connectivity matrix. Note also that when varying the
        number of clusters and using caching, it may be advantageous to compute
        the full tree. It must be ``True`` if ``distance_threshold`` is not
        ``None``. By default `compute_full_tree` is "auto", which is equivalent
        to `True` when `distance_threshold` is not `None` or that `n_clusters`
        is inferior to the maximum between 100 or `0.02 * n_samples`.
        Otherwise, "auto" is equivalent to `False`.
    """
    linkage : {"ward", "complete", "average", "single"}, default="ward"
        # 簇的合并策略，确定特征集之间使用的距离度量标准。算法将合并最小化此标准的簇对。

        - "ward" 最小化被合并的簇的方差。
        - "complete" 或最大链接使用两个集合之间所有特征的最大距离。
        - "average" 使用两个集合每个特征距离的平均值。
        - "single" 使用两个集合之间所有特征的最小距离。

    pooling_func : callable, default=np.mean
        # 用于将聚合特征的值组合成单个值的函数，应接受形状为 [M, N] 的数组和关键字参数 `axis=1`，并将其减少为大小为 [M] 的数组。

    distance_threshold : float, default=None
        # 链接距离阈值，高于或等于此阈值时将不合并簇。如果不为 ``None``，则 ``n_clusters`` 必须为 ``None``，并且 ``compute_full_tree`` 必须为 ``True``。

        .. versionadded:: 0.21
            # 在版本 0.21 中添加了此功能。

    compute_distances : bool, default=False
        # 即使未使用 `distance_threshold`，也会计算簇之间的距离。这可以用于生成树形图，但会引入计算和内存开销。

        .. versionadded:: 0.24
            # 在版本 0.24 中添加了此功能。

    Attributes
    ----------
    n_clusters_ : int
        # 算法找到的簇的数量。如果 ``distance_threshold=None``，则将等于给定的 ``n_clusters``。

    labels_ : array-like of (n_features,)
        # 每个特征的簇标签。

    n_leaves_ : int
        # 层次树中的叶子节点数。

    n_connected_components_ : int
        # 图中估计的连接组件数量。

        .. versionadded:: 0.21
            # 在版本 0.21 中，用 ``n_connected_components_`` 替换了 ``n_components_``。

    n_features_in_ : int
        # 在 :term:`fit` 过程中看到的特征数量。

        .. versionadded:: 0.24
            # 在版本 0.24 中添加了此功能。

    feature_names_in_ : ndarray of shape (`n_features_in_`,)
        # 在 :term:`fit` 过程中看到的特征名称。仅在 `X` 具有全部字符串特征名称时定义。

        .. versionadded:: 1.0
            # 在版本 1.0 中添加了此功能。

    children_ : array-like of shape (n_nodes-1, 2)
        # 每个非叶节点的孩子节点。小于 `n_features` 的值对应于树的叶子节点，即原始样本。
        # 第 `i` 个节点大于或等于 `n_features` 是一个非叶节点，其孩子为 `children_[i - n_features]`。
        # 或者在第 `i` 次迭代中，`children[i][0]` 和 `children[i][1]` 被合并形成节点 `n_features + i`。
    # distances_ : array-like of shape (n_nodes-1,)
    #     Distances between nodes in the corresponding place in `children_`.
    #     Only computed if `distance_threshold` is used or `compute_distances`
    #     is set to `True`.

    # See Also
    # --------
    # AgglomerativeClustering : Agglomerative clustering samples instead of
    #     features.
    # ward_tree : Hierarchical clustering with ward linkage.

    # Examples
    # --------
    # >>> import numpy as np
    # >>> from sklearn import datasets, cluster
    # >>> digits = datasets.load_digits()
    # >>> images = digits.images
    # >>> X = np.reshape(images, (len(images), -1))
    # >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)
    # >>> agglo.fit(X)
    # FeatureAgglomeration(n_clusters=32)
    # >>> X_reduced = agglo.transform(X)
    # >>> X_reduced.shape
    # (1797, 32)
    """

    # _parameter_constraints: dict = {
    #     "n_clusters": [Interval(Integral, 1, None, closed="left"), None],
    #     "metric": [
    #         StrOptions(set(_VALID_METRICS) | {"precomputed"}),
    #         callable,
    #         Hidden(None),
    #     ],
    #     "memory": [str, HasMethods("cache"), None],
    #     "connectivity": ["array-like", "sparse matrix", callable, None],
    #     "compute_full_tree": [StrOptions({"auto"}), "boolean"],
    #     "linkage": [StrOptions(set(_TREE_BUILDERS.keys()))],
    #     "pooling_func": [callable],
    #     "distance_threshold": [Interval(Real, 0, None, closed="left"), None],
    #     "compute_distances": ["boolean"],
    # }

    # def __init__(
    #     self,
    #     n_clusters=2,
    #     *,
    #     metric="euclidean",
    #     memory=None,
    #     connectivity=None,
    #     compute_full_tree="auto",
    #     linkage="ward",
    #     pooling_func=np.mean,
    #     distance_threshold=None,
    #     compute_distances=False,
    # ):
    #     super().__init__(
    #         n_clusters=n_clusters,
    #         memory=memory,
    #         connectivity=connectivity,
    #         compute_full_tree=compute_full_tree,
    #         linkage=linkage,
    #         metric=metric,
    #         distance_threshold=distance_threshold,
    #         compute_distances=compute_distances,
    #     )
    #     self.pooling_func = pooling_func

    # @_fit_context(prefer_skip_nested_validation=True)
    # def fit(self, X, y=None):
    #     """Fit the hierarchical clustering on the data.

    #     Parameters
    #     ----------
    #     X : array-like of shape (n_samples, n_features)
    #         The data.

    #     y : Ignored
    #         Not used, present here for API consistency by convention.

    #     Returns
    #     -------
    #     self : object
    #         Returns the transformer.
    #     """
    #     X = self._validate_data(X, ensure_min_features=2)
    #     super()._fit(X.T)
    #     self._n_features_out = self.n_clusters_
    #     return self

    # @property
    # def fit_predict(self):
    #     """Fit and return the result of each sample's clustering assignment."""
    #     raise AttributeError
```