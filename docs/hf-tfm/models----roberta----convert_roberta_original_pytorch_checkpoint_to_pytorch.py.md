# `.\transformers\models\roberta\convert_roberta_original_pytorch_checkpoint_to_pytorch.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸ºUTF-8
# ç‰ˆæƒå£°æ˜ï¼Œå‘ŠçŸ¥ä½¿ç”¨è€…å¯ä»¥åœ¨éµå®ˆè®¸å¯è¯çš„æƒ…å†µä¸‹ä½¿ç”¨è¯¥æ–‡ä»¶
# å¦‚æœéœ€è¦ï¼Œå¯ä»¥ä» http://www.apache.org/licenses/LICENSE-2.0 è·å¾—è®¸å¯è¯å‰¯æœ¬
# æ ¹æ®è®¸å¯è¯ï¼Œåˆ†å‘çš„è½¯ä»¶åŸºäºâ€œåŸæ ·â€åˆ†å‘ï¼Œæ²¡æœ‰ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶ï¼Œä¸è®ºæ˜¯æ˜ç¤ºçš„è¿˜æ˜¯éšå«çš„
# æŸ¥çœ‹è®¸å¯è¯ï¼Œäº†è§£ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶
# å°†RoBERTaæ£€æŸ¥ç‚¹è½¬æ¢ä¸ºPyTorchæ ¼å¼

import argparse  # å¯¼å…¥ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°çš„æ¨¡å—
import pathlib  # æä¾›äº†ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„çš„å®ç”¨åŠŸèƒ½

import fairseq  # å¯¼å…¥ fairseq åº“
import torch  # å¯¼å…¥PyTorchåº“
from fairseq.models.roberta import RobertaModel as FairseqRobertaModel  # ä» fairseq æ¨¡å‹ä¸­å¯¼å…¥ RoBERTa æ¨¡å‹
from fairseq.modules import TransformerSentenceEncoderLayer  # ä» fairseq æ¨¡å—ä¸­å¯¼å…¥ TransformerSentenceEncoderLayer
from packaging import version  # ä» packaging æ¨¡å—ä¸­å¯¼å…¥ version ç±»

from transformers import RobertaConfig, RobertaForMaskedLM, RobertaForSequenceClassification  # ä» transformersåº“ä¸­å¯¼å…¥ç›¸å…³æ¥å£
from transformers.models.bert.modeling_bert import (  # ä» transformers åº“ä¸­å¯¼å…¥ç›¸å…³æ¥å£
    BertIntermediate,
    BertLayer,
    BertOutput,
    BertSelfAttention,
    BertSelfOutput,
)
from transformers.utils import logging  # ä» transformers åº“ä¸­å¯¼å…¥æ—¥å¿—æ¨¡å—

# æ£€æŸ¥ fairseq åº“çš„ç‰ˆæœ¬ï¼Œå¦‚æœå°äº0.9.0ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
if version.parse(fairseq.__version__) < version.parse("0.9.0"):
    raise Exception("requires fairseq >= 0.9.0")

# è®¾ç½®loggingæ¨¡å—çš„è¾“å‡ºçº§åˆ«
logging.set_verbosity_info()
# è·å–loggerå¯¹è±¡
logger = logging.get_logger(__name__)

# ç¤ºä¾‹æ–‡æœ¬
SAMPLE_TEXT = "Hello world! cÃ©cÃ© herlolip"

# å°†RoBERTaæ£€æŸ¥ç‚¹è½¬æ¢ä¸ºPyTorchæ ¼å¼
def convert_roberta_checkpoint_to_pytorch(
    roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool
):
    """
    Copy/paste/tweak roberta's weights to our BERT structure.
    """
    # ä»ç»™å®šè·¯å¾„åŠ è½½é¢„è®­ç»ƒçš„RoBERTaæ¨¡å‹
    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)
    # å…³é—­dropoutï¼Œä»¤æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    roberta.eval()
    # è·å–å¥å­ç¼–ç å™¨
    roberta_sent_encoder = roberta.model.encoder.sentence_encoder
    # åˆ›å»ºRoBERTaé…ç½®å¯¹è±¡
    config = RobertaConfig(
        vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings,
        hidden_size=roberta.args.encoder_embed_dim,
        num_hidden_layers=roberta.args.encoder_layers,
        num_attention_heads=roberta.args.encoder_attention_heads,
        intermediate_size=roberta.args.encoder_ffn_embed_dim,
        max_position_embeddings=514,
        type_vocab_size=1,
        layer_norm_eps=1e-5,  # ä½¿ç”¨fairseqä¸­çš„PyTorché»˜è®¤å€¼
    )
    # å¦‚æœå­˜åœ¨åˆ†ç±»å¤´ï¼Œåˆ™å°†åˆ†ç±»æ ‡ç­¾çš„æ•°é‡æ·»åŠ åˆ°é…ç½®ä¸­
    if classification_head:
        config.num_labels = roberta.model.classification_heads["mnli"].out_proj.weight.shape[0]
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("Our BERT config:", config)

    # åˆ›å»ºRoBERTaæ¨¡å‹ï¼Œå¦‚æœå­˜åœ¨åˆ†ç±»å¤´ï¼Œåˆ™åˆ›å»ºRoBERTaåºåˆ—åˆ†ç±»æ¨¡å‹ï¼Œå¦åˆ™åˆ›å»ºRoBERTaé®è”½è¯­è¨€æ¨¡å‹
    model = RobertaForSequenceClassification(config) if classification_head else RobertaForMaskedLM(config)
    # å…³é—­dropoutï¼Œä»¤æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    model.eval()

    # å¤åˆ¶æ‰€æœ‰æƒé‡
    # å¤åˆ¶åµŒå…¥æƒé‡
    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight
    # å¤åˆ¶ä½ç½®åµŒå…¥æƒé‡
    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight
    # å°†token_typeåµŒå…¥æƒé‡ç½®ä¸º0ï¼Œå› ä¸ºRoBERTaä¸ä½¿ç”¨å®ƒä»¬
    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.roberta.embeddings.token_type_embeddings.weight
    )
    # å°†é¢„è®­ç»ƒæ¨¡å‹ RoBERTa çš„ LayerNorm æƒé‡å’Œåç½®èµ‹å€¼ç»™å½“å‰æ¨¡å‹çš„ RoBERTa çš„åµŒå…¥å±‚çš„ LayerNorm æƒé‡å’Œåç½®
    model.roberta.embeddings.LayerNorm.weight = roberta_sent_encoder.emb_layer_norm.weight
    model.roberta.embeddings.LayerNorm.bias = roberta_sent_encoder.emb_layer_norm.bias

    # éå†æ¯ä¸ªç¼–ç å±‚
    for i in range(config.num_hidden_layers):
        # è·å–å½“å‰å±‚çš„ BertLayer å¯¹è±¡å’Œå¯¹åº”çš„ TransformerSentenceEncoderLayer å¯¹è±¡
        layer: BertLayer = model.roberta.encoder.layer[i]
        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]

        # éªŒè¯è‡ªæ³¨æ„åŠ›å±‚å‚æ•°çš„å½¢çŠ¶æ˜¯å¦ä¸€è‡´
        assert (
            roberta_layer.self_attn.k_proj.weight.data.shape
            == roberta_layer.self_attn.q_proj.weight.data.shape
            == roberta_layer.self_attn.v_proj.weight.data.shape
            == torch.Size((config.hidden_size, config.hidden_size))
        )

        # å°† RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„æƒé‡å’Œåç½®èµ‹å€¼ç»™å½“å‰æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚
        self_attn: BertSelfAttention = layer.attention.self
        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight
        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias
        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight
        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias
        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight
        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias

        # å°† RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚è¾“å‡ºå±‚çš„æƒé‡å’Œåç½®èµ‹å€¼ç»™å½“å‰æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚è¾“å‡ºå±‚
        self_output: BertSelfOutput = layer.attention.output
        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape
        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight
        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias
        self_output.LayerNorm.weight = roberta_layer.self_attn_layer_norm.weight
        self_output.LayerNorm.bias = roberta_layer.self_attn_layer_norm.bias

        # å°† RoBERTa ä¸­ä¸­é—´å±‚çš„æƒé‡å’Œåç½®èµ‹å€¼ç»™å½“å‰æ¨¡å‹çš„ä¸­é—´å±‚
        intermediate: BertIntermediate = layer.intermediate
        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape
        intermediate.dense.weight = roberta_layer.fc1.weight
        intermediate.dense.bias = roberta_layer.fc1.bias

        # å°† RoBERTa ä¸­è¾“å‡ºå±‚çš„æƒé‡å’Œåç½®èµ‹å€¼ç»™å½“å‰æ¨¡å‹çš„è¾“å‡ºå±‚
        bert_output: BertOutput = layer.output
        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape
        bert_output.dense.weight = roberta_layer.fc2.weight
        bert_output.dense.bias = roberta_layer.fc2.bias
        bert_output.LayerNorm.weight = roberta_layer.final_layer_norm.weight
        bert_output.LayerNorm.bias = roberta_layer.final_layer_norm.bias
        # ç¼–ç å±‚ç»“æŸ

    # å¦‚æœæœ‰åˆ†ç±»å¤´éƒ¨ï¼Œåˆ™å°† RoBERTa ä¸­å¯¹åº”çš„æƒé‡å’Œåç½®èµ‹å€¼ç»™å½“å‰æ¨¡å‹çš„åˆ†ç±»å™¨
    if classification_head:
        model.classifier.dense.weight = roberta.model.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = roberta.model.classification_heads["mnli"].dense.bias
        model.classifier.out_proj.weight = roberta.model.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = roberta.model.classification_heads["mnli"].out_proj.bias
    else:
        # å¤åˆ¶ RoBERTa æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´éƒ¨å‚æ•°åˆ°å½“å‰æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´éƒ¨
        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight
        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias
        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias
        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight
        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias

    # æ£€æŸ¥æ˜¯å¦å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚
    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)  # æ‰¹é‡å¤§å°ä¸º1çš„è¾“å…¥

    # ä½¿ç”¨å½“å‰æ¨¡å‹å¤„ç†è¾“å…¥ï¼Œè·å–è¾“å‡º
    our_output = model(input_ids)[0]

    if classification_head:
        # å¦‚æœå­˜åœ¨åˆ†ç±»å¤´éƒ¨ï¼Œåˆ™ä½¿ç”¨ RoBERTa æ¨¡å‹çš„ç‰¹å®šåˆ†ç±»å¤´éƒ¨å¤„ç†è¾“å…¥ç‰¹å¾
        their_output = roberta.model.classification_heads["mnli"](roberta.extract_features(input_ids))
    else:
        # å¦åˆ™ï¼Œä½¿ç”¨ RoBERTa æ¨¡å‹å¤„ç†è¾“å…¥å¹¶è·å–è¾“å‡º
        their_output = roberta.model(input_ids)[0]

    # æ‰“å°å½“å‰æ¨¡å‹è¾“å‡ºå’Œ RoBERTa æ¨¡å‹è¾“å‡ºçš„å½¢çŠ¶
    print(our_output.shape, their_output.shape)

    # è®¡ç®—è¾“å‡ºä¹‹é—´çš„æœ€å¤§ç»å¯¹å·®å¼‚
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    print(f"max_absolute_diff = {max_absolute_diff}")  # å¤§çº¦ 1e-7

    # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºæ˜¯å¦éå¸¸æ¥è¿‘
    success = torch.allclose(our_output, their_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")

    # å¦‚æœè¾“å‡ºä¸æ¥è¿‘ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
    if not success:
        raise Exception("Something went wRoNg")

    # åˆ›å»ºç›®å½•ç”¨äºä¿å­˜ PyTorch æ¨¡å‹
    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
    print(f"Saving model to {pytorch_dump_folder_path}")

    # å°†å½“å‰æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)
# å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥è¿è¡Œï¼Œè€Œä¸æ˜¯ä½œä¸ºè¢«å¯¼å…¥æ¨¡å—ä½¿ç”¨ï¼Œæ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ª argparse è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ ä¸€ä¸ªå¿…éœ€çš„å‚æ•°ï¼Œç”¨æ¥æŒ‡å®š RoBERTa æ¨¡å‹çš„æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    parser.add_argument(
        "--roberta_checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    # æ·»åŠ ä¸€ä¸ªå¿…éœ€çš„å‚æ•°ï¼Œç”¨æ¥æŒ‡å®šè½¬æ¢åçš„ PyTorch æ¨¡å‹è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    # æ·»åŠ ä¸€ä¸ªå¼€å…³å‚æ•°ï¼Œç”¨æ¥æŒ‡å®šæ˜¯å¦è½¬æ¢æ¨¡å‹çš„æœ€åä¸€ä¸ªåˆ†ç±»å¤´
    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # ä½¿ç”¨è§£æå™¨è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªèƒ½å¤Ÿè®¿é—®è§£æåå‚æ•°çš„å‘½åç©ºé—´å¯¹è±¡
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•° convert_roberta_checkpoint_to_pytorchï¼Œå°† RoBERTa æ¨¡å‹è½¬æ¢ä¸º PyTorch æ¨¡å‹
    # ä¼ å…¥å‚æ•°ä¸º RoBERTa æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ã€è½¬æ¢åçš„ PyTorch æ¨¡å‹è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ã€æ˜¯å¦è½¬æ¢æ¨¡å‹çš„æœ€åä¸€ä¸ªåˆ†ç±»å¤´
    convert_roberta_checkpoint_to_pytorch(
        args.roberta_checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
```