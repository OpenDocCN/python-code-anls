# `.\models\roberta\convert_roberta_original_pytorch_checkpoint_to_pytorch.py`

```py
# è®¾ç½® Python æ–‡ä»¶ç¼–ç æ ¼å¼ä¸º UTF-8
# ç‰ˆæƒå£°æ˜å’Œè®¸å¯åè®®ï¼Œè¿™é‡Œæ˜¯ Apache License 2.0
# è¯¦ç»†ä¿¡æ¯å¯å‚è§ http://www.apache.org/licenses/LICENSE-2.0

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import argparse        # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import pathlib         # æä¾›å¤„ç†è·¯å¾„çš„ç±»å’Œå‡½æ•°

import fairseq         # å¯¼å…¥ fairseq åº“
import torch           # å¯¼å…¥ PyTorch åº“
from fairseq.models.roberta import RobertaModel as FairseqRobertaModel  # å¯¼å…¥ Fairseq ä¸­çš„ RoBERTa æ¨¡å‹
from fairseq.modules import TransformerSentenceEncoderLayer  # å¯¼å…¥ Fairseq ä¸­çš„ TransformerSentenceEncoderLayer æ¨¡å—
from packaging import version  # ç”¨äºå¤„ç†ç‰ˆæœ¬å·çš„åº“

from transformers import RobertaConfig, RobertaForMaskedLM, RobertaForSequenceClassification  # å¯¼å…¥ Hugging Face Transformers ä¸­çš„ RoBERTa ç›¸å…³ç±»
from transformers.models.bert.modeling_bert import (  # å¯¼å…¥ Transformers BERT æ¨¡å‹çš„éƒ¨åˆ†ç»„ä»¶ç±»
    BertIntermediate,
    BertLayer,
    BertOutput,
    BertSelfAttention,
    BertSelfOutput,
)
from transformers.utils import logging  # å¯¼å…¥ Transformers çš„æ—¥å¿—æ¨¡å—

# å¦‚æœ fairseq çš„ç‰ˆæœ¬å°äº 0.9.0ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
if version.parse(fairseq.__version__) < version.parse("0.9.0"):
    raise Exception("requires fairseq >= 0.9.0")

# è®¾ç½®æ—¥å¿—è¾“å‡ºçº§åˆ«ä¸º INFO
logging.set_verbosity_info()
# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# ç¤ºä¾‹æ–‡æœ¬
SAMPLE_TEXT = "Hello world! cÃ©cÃ© herlolip"

# å®šä¹‰å‡½æ•°ï¼Œå°† RoBERTa æ¨¡å‹çš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ ¼å¼
def convert_roberta_checkpoint_to_pytorch(
    roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool
):
    """
    å¤åˆ¶/ç²˜è´´/è°ƒæ•´ RoBERTa çš„æƒé‡ä»¥é€‚åº”æˆ‘ä»¬çš„ BERT ç»“æ„ã€‚
    """
    # ä»é¢„è®­ç»ƒçš„ RoBERTa æ£€æŸ¥ç‚¹è·¯å¾„åŠ è½½æ¨¡å‹
    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨ dropout
    roberta.eval()
    # è·å– RoBERTa çš„å¥å­ç¼–ç å™¨
    roberta_sent_encoder = roberta.model.encoder.sentence_encoder
    # åˆ›å»º RoBERTaConfig å¯¹è±¡ï¼Œç”¨äºå®šä¹‰è½¬æ¢åçš„ BERT æ¨¡å‹é…ç½®
    config = RobertaConfig(
        vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings,
        hidden_size=roberta.args.encoder_embed_dim,
        num_hidden_layers=roberta.args.encoder_layers,
        num_attention_heads=roberta.args.encoder_attention_heads,
        intermediate_size=roberta.args.encoder_ffn_embed_dim,
        max_position_embeddings=514,
        type_vocab_size=1,
        layer_norm_eps=1e-5,  # PyTorch é»˜è®¤å€¼ï¼Œä¸ fairseq ä¿æŒä¸€è‡´
    )
    # å¦‚æœéœ€è¦åˆ†ç±»å¤´éƒ¨ï¼Œåˆ™è®¾ç½® num_labels å±æ€§ä¸ºå¯¹åº”åˆ†ç±»å¤´éƒ¨çš„è¾“å‡ºç»´åº¦
    if classification_head:
        config.num_labels = roberta.model.classification_heads["mnli"].out_proj.weight.shape[0]
    # è¾“å‡º BERT æ¨¡å‹çš„é…ç½®ä¿¡æ¯
    print("Our BERT config:", config)

    # åˆ›å»º RoBERTaForSequenceClassification æˆ– RoBERTaForMaskedLM æ¨¡å‹å¯¹è±¡
    model = RobertaForSequenceClassification(config) if classification_head else RobertaForMaskedLM(config)
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # å¼€å§‹å¤åˆ¶æ‰€æœ‰æƒé‡
    # å¤åˆ¶è¯åµŒå…¥æƒé‡
    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight
    # å¤åˆ¶ä½ç½®ç¼–ç æƒé‡
    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight
    # å°† token_type_embeddings çš„æƒé‡æ•°æ®ç½®é›¶ï¼Œå› ä¸º RoBERTa ä¸ä½¿ç”¨ token_type_embeddings
    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.roberta.embeddings.token_type_embeddings.weight
    )  # just zero them out b/c RoBERTa doesn't use them.
    # å°† RoBERTa æ¨¡å‹çš„ LayerNorm æƒé‡å’Œåç½®è®¾ç½®ä¸º RoBERTa å¥å­ç¼–ç å™¨çš„å¯¹åº”æƒé‡å’Œåç½®
    model.roberta.embeddings.LayerNorm.weight = roberta_sent_encoder.emb_layer_norm.weight
    model.roberta.embeddings.LayerNorm.bias = roberta_sent_encoder.emb_layer_norm.bias

    # éå†æ¯ä¸ªéšè—å±‚è¿›è¡Œå‚æ•°è®¾ç½®
    for i in range(config.num_hidden_layers):
        # è·å–å½“å‰å±‚çš„ BertLayer å¯¹è±¡å’Œå¯¹åº”çš„ TransformerSentenceEncoderLayer å¯¹è±¡
        layer: BertLayer = model.roberta.encoder.layer[i]
        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]

        # è®¾ç½®è‡ªæ³¨æ„åŠ›å±‚çš„æƒé‡å’Œåç½®
        self_attn: BertSelfAttention = layer.attention.self
        assert (
            roberta_layer.self_attn.k_proj.weight.data.shape
            == roberta_layer.self_attn.q_proj.weight.data.shape
            == roberta_layer.self_attn.v_proj.weight.data.shape
            == torch.Size((config.hidden_size, config.hidden_size))
        )
        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight
        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias
        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight
        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias
        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight
        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias

        # è®¾ç½®è‡ªæ³¨æ„åŠ›å±‚è¾“å‡ºçš„æƒé‡å’Œåç½®
        self_output: BertSelfOutput = layer.attention.output
        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape
        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight
        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias
        self_output.LayerNorm.weight = roberta_layer.self_attn_layer_norm.weight
        self_output.LayerNorm.bias = roberta_layer.self_attn_layer_norm.bias

        # è®¾ç½®ä¸­é—´å±‚çš„æƒé‡å’Œåç½®
        intermediate: BertIntermediate = layer.intermediate
        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape
        intermediate.dense.weight = roberta_layer.fc1.weight
        intermediate.dense.bias = roberta_layer.fc1.bias

        # è®¾ç½®è¾“å‡ºå±‚çš„æƒé‡å’Œåç½®
        bert_output: BertOutput = layer.output
        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape
        bert_output.dense.weight = roberta_layer.fc2.weight
        bert_output.dense.bias = roberta_layer.fc2.bias
        bert_output.LayerNorm.weight = roberta_layer.final_layer_norm.weight
        bert_output.LayerNorm.bias = roberta_layer.final_layer_norm.bias
        # æœ¬å±‚è®¾ç½®ç»“æŸ

    # å¦‚æœæœ‰åˆ†ç±»å¤´ï¼Œåˆ™è®¾ç½®åˆ†ç±»å™¨çš„æƒé‡å’Œåç½®ä¸º RoBERTa æ¨¡å‹ä¸­æŒ‡å®šåˆ†ç±»å¤´çš„å¯¹åº”æƒé‡å’Œåç½®
    if classification_head:
        model.classifier.dense.weight = roberta.model.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = roberta.model.classification_heads["mnli"].dense.bias
        model.classifier.out_proj.weight = roberta.model.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = roberta.model.classification_heads["mnli"].out_proj.bias
    else:
        # å¦‚æœä¸æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œå¤åˆ¶ RoBERTa æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´éƒ¨æƒé‡å’Œåç½®åˆ°å½“å‰æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´éƒ¨
        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight
        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias
        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias
        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight
        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias

    # æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦äº§ç”Ÿç›¸åŒçš„è¾“å‡ºç»“æœã€‚
    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)  # æ‰¹å¤§å°ä¸º1çš„è¾“å…¥å¼ é‡

    our_output = model(input_ids)[0]
    if classification_head:
        # å¦‚æœæœ‰åˆ†ç±»å¤´éƒ¨ï¼Œä½¿ç”¨ RoBERTa æ¨¡å‹çš„å¯¹åº”åˆ†ç±»å¤´éƒ¨è¿›è¡Œæ¨ç†
        their_output = roberta.model.classification_heads["mnli"](roberta.extract_features(input_ids))
    else:
        # å¦åˆ™ç›´æ¥ä½¿ç”¨ RoBERTa æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œæ¨ç†
        their_output = roberta.model(input_ids)[0]
    print(our_output.shape, their_output.shape)
    # è®¡ç®—è¾“å‡ºå¼ é‡çš„æœ€å¤§ç»å¯¹å·®å¼‚
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    print(f"max_absolute_diff = {max_absolute_diff}")  # çº¦ä¸º 1e-7
    # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºå¼ é‡æ˜¯å¦è¶³å¤Ÿæ¥è¿‘
    success = torch.allclose(our_output, their_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")
    if not success:
        raise Exception("Something went wRoNg")

    # åˆ›å»ºå­˜å‚¨ PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
    print(f"Saving model to {pytorch_dump_folder_path}")
    # å°†å½“å‰æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)
if __name__ == "__main__":
    # å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œè€Œéè¢«å¯¼å…¥ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    parser = argparse.ArgumentParser()
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡

    # å¿…é€‰å‚æ•°
    parser.add_argument(
        "--roberta_checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®š RoBERTa æ¨¡å‹çš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¿…é¡»æä¾›ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¿…é¡»æä¾›ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²

    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦è½¬æ¢æœ€ç»ˆçš„åˆ†ç±»å¤´éƒ¨ï¼Œé‡‡ç”¨å¸ƒå°”æ ‡å¿—æ–¹å¼

    args = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ä¸€ä¸ªå‘½åç©ºé—´å¯¹è±¡ argsï¼ŒåŒ…å«äº†è§£æåçš„å‚æ•°å€¼

    convert_roberta_checkpoint_to_pytorch(
        args.roberta_checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
    # è°ƒç”¨å‡½æ•° convert_roberta_checkpoint_to_pytorchï¼Œä¼ é€’å‘½ä»¤è¡Œå‚æ•°ä¸­æŒ‡å®šçš„ RoBERTa æ£€æŸ¥ç‚¹è·¯å¾„ã€è¾“å‡ºè·¯å¾„å’Œåˆ†ç±»å¤´éƒ¨è½¬æ¢æ ‡å¿—ä½œä¸ºå‚æ•°
```