# `.\transformers\testing_utils.py`

```py
# å¯¼å…¥æ¨¡å—å’Œåº“
import collections  # å¯¼å…¥ collections æ¨¡å—
import contextlib  # å¯¼å…¥ contextlib æ¨¡å—
import doctest  # å¯¼å…¥ doctest æ¨¡å—
import functools  # å¯¼å…¥ functools æ¨¡å—
import importlib  # å¯¼å…¥ importlib æ¨¡å—
import inspect  # å¯¼å…¥ inspect æ¨¡å—
import logging  # å¯¼å…¥ logging æ¨¡å—
import multiprocessing  # å¯¼å…¥ multiprocessing æ¨¡å—
import os  # å¯¼å…¥ os æ¨¡å—
import re  # å¯¼å…¥ re æ¨¡å—
import shlex  # å¯¼å…¥ shlex æ¨¡å—
import shutil  # å¯¼å…¥ shutil æ¨¡å—
import subprocess  # å¯¼å…¥ subprocess æ¨¡å—
import sys  # å¯¼å…¥ sys æ¨¡å—
import tempfile  # å¯¼å…¥ tempfile æ¨¡å—
import time  # å¯¼å…¥ time æ¨¡å—
import unittest  # å¯¼å…¥ unittest æ¨¡å—
from collections import defaultdict  # ä» collections æ¨¡å—å¯¼å…¥ defaultdict ç±»
from collections.abc import Mapping  # ä» collections.abc æ¨¡å—å¯¼å…¥ Mapping ç±»
from io import StringIO  # ä» io æ¨¡å—å¯¼å…¥ StringIO ç±»
from pathlib import Path  # ä» pathlib æ¨¡å—å¯¼å…¥ Path ç±»
from typing import Callable, Dict, Iterable, Iterator, List, Optional, Union  # ä» typing æ¨¡å—å¯¼å…¥è‹¥å¹²ç±»å‹
from unittest import mock  # ä» unittest æ¨¡å—å¯¼å…¥ mock æ¨¡å—
from unittest.mock import patch  # ä» unittest.mock æ¨¡å—å¯¼å…¥ patch ç±»

import urllib3  # å¯¼å…¥ urllib3 æ¨¡å—

from transformers import logging as transformers_logging  # ä» transformers åŒ…ä¸­å¯¼å…¥ logging æ¨¡å—
# ä»æœ¬åœ° integrations æ¨¡å—å¯¼å…¥è‹¥å¹²å‡½æ•°åˆ¤æ–­å¤–éƒ¨åº“æ˜¯å¦å¯ç”¨
from .integrations import (
    is_clearml_available,
    is_optuna_available,
    is_ray_available,
    is_sigopt_available,
    is_tensorboard_available,
    is_wandb_available,
)
# ä»æœ¬åœ° integrations.deepspeed æ¨¡å—å¯¼å…¥æ˜¯å¦å¯ç”¨ deepspeed
from .integrations.deepspeed import is_deepspeed_available
# ä»æœ¬åœ° utils æ¨¡å—å¯¼å…¥è‹¥å¹²å‡½æ•°åˆ¤æ–­å¤–éƒ¨åº“æ˜¯å¦å¯ç”¨
from .utils import (
    is_accelerate_available,
    is_apex_available,
    is_auto_awq_available,
    is_auto_gptq_available,
    is_bitsandbytes_available,
    is_bs4_available,
    is_cv2_available,
    is_cython_available,
    is_decord_available,
    is_detectron2_available,
    is_essentia_available,
    is_faiss_available,
    is_flash_attn_2_available,
    is_flax_available,
    is_fsdp_available,
    is_ftfy_available,
    is_g2p_en_available,
    is_ipex_available,
    is_jieba_available,
    is_jinja_available,
    is_jumanpp_available,
    is_keras_nlp_available,
    is_levenshtein_available,
    is_librosa_available,
    is_natten_available,
    is_nltk_available,
    is_onnx_available,
    is_optimum_available,
    is_pandas_available,
    is_peft_available,
    is_phonemizer_available,
    is_pretty_midi_available,
    is_pyctcdecode_available,
    is_pytesseract_available,
    is_pytest_available,
    is_pytorch_quantization_available,
    is_rjieba_available,
    is_safetensors_available,
    is_scipy_available,
    is_sentencepiece_available,
    is_seqio_available,
    is_soundfile_availble,
    is_spacy_available,
    is_sudachi_available,
    is_tensorflow_probability_available,
    is_tensorflow_text_available,
    is_tf2onnx_available,
    is_tf_available,
    is_timm_available,
    is_tokenizers_available,
    is_torch_available,
    is_torch_bf16_available_on_device,
    is_torch_bf16_cpu_available,
    is_torch_bf16_gpu_available,
    is_torch_fp16_available_on_device,
    is_torch_neuroncore_available,
)
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Torch çš„ NPU åŠŸèƒ½
    is_torch_npu_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Torch çš„ SDPA åŠŸèƒ½
    is_torch_sdpa_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Torch çš„ TensorRT FX åŠŸèƒ½
    is_torch_tensorrt_fx_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Torch çš„ TF32 åŠŸèƒ½
    is_torch_tf32_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Torch çš„ TPU åŠŸèƒ½
    is_torch_tpu_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Torch çš„ XPU åŠŸèƒ½
    is_torch_xpu_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Torchaudio åº“
    is_torchaudio_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ TorchDynamo åº“
    is_torchdynamo_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ TorchVision åº“
    is_torchvision_available,
    # æ£€æŸ¥å½“å‰ç¯å¢ƒä¸‹æ˜¯å¦å¯ç”¨ Vision åŠŸèƒ½ï¼ˆå¯èƒ½æ˜¯ TorchVision çš„ä¸€éƒ¨åˆ†ï¼‰
    is_vision_available,
    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼çš„å‡½æ•°ï¼Œç”¨äºè§£æé…ç½®ç­‰
    strtobool,
# æ£€æŸ¥æ˜¯å¦å¯ç”¨åŠ é€Ÿå™¨
if is_accelerate_available():
    # å¦‚æœå¯ç”¨ï¼Œä»åŠ é€Ÿå™¨çŠ¶æ€æ¨¡å—å¯¼å…¥åŠ é€Ÿå™¨çŠ¶æ€å’Œéƒ¨åˆ†çŠ¶æ€
    from accelerate.state import AcceleratorState, PartialState


# æ£€æŸ¥æ˜¯å¦å¯ç”¨ pytest
if is_pytest_available():
    # å¦‚æœå¯ç”¨ï¼Œä» pytest çš„ doctest æ¨¡å—å¯¼å…¥æ‰€éœ€å†…å®¹
    from _pytest.doctest import (
        Module,  # å¯¼å…¥ Module ç±»
        _get_checker,  # å¯¼å…¥ _get_checker å‡½æ•°
        _get_continue_on_failure,  # å¯¼å…¥ _get_continue_on_failure å‡½æ•°
        _get_runner,  # å¯¼å…¥ _get_runner å‡½æ•°
        _is_mocked,  # å¯¼å…¥ _is_mocked å‡½æ•°
        _patch_unwrap_mock_aware,  # å¯¼å…¥ _patch_unwrap_mock_aware å‡½æ•°
        get_optionflags,  # å¯¼å…¥ get_optionflags å‡½æ•°
        import_path,  # å¯¼å…¥ import_path å‡½æ•°
    )
    # ä» pytest çš„ outcomes æ¨¡å—å¯¼å…¥ skip å‡½æ•°
    from _pytest.outcomes import skip
    # ä» pytest å¯¼å…¥ DoctestItem ç±»
    from pytest import DoctestItem
else:
    # å¦‚æœ pytest ä¸å¯ç”¨ï¼Œå°† Module å’Œ DoctestItem è®¾ä¸º object ç±»çš„å®ä¾‹
    Module = object
    DoctestItem = object


# å®šä¹‰ä¸€ä¸ªå°æ¨¡å‹çš„æ ‡è¯†ç¬¦
SMALL_MODEL_IDENTIFIER = "julien-c/bert-xsmall-dummy"
# å®šä¹‰ä¸€ä¸ªæœªçŸ¥æ¨¡å‹çš„æ ‡è¯†ç¬¦ï¼Œç”¨äºæµ‹è¯•
DUMMY_UNKNOWN_IDENTIFIER = "julien-c/dummy-unknown"
# å®šä¹‰ä¸€ä¸ªå…·æœ‰ä¸åŒåˆ†è¯å™¨çš„è™šæ‹Ÿæ¨¡å‹çš„æ ‡è¯†ç¬¦ï¼Œç”¨äºæµ‹è¯•
DUMMY_DIFF_TOKENIZER_IDENTIFIER = "julien-c/dummy-diff-tokenizer"

# ç”¨äºæµ‹è¯• hub
# å®šä¹‰ç”¨æˆ·
USER = "__DUMMY_TRANSFORMERS_USER__"
# å®šä¹‰ç”¨äºæµ‹è¯•çš„ç«¯ç‚¹
ENDPOINT_STAGING = "https://hub-ci.huggingface.co"

# ä¸æ˜¯å…³é”®çš„ï¼Œä»…åœ¨å—é™çš„ CI å®ä¾‹ä¸Šå¯ç”¨
# å®šä¹‰ä»¤ç‰Œ
TOKEN = "hf_94wBhPGp6KrrTH3KDchhKpRxZwd6dmHWLL"


# ä»ç¯å¢ƒä¸­è§£æå¸ƒå°”å‹æ ‡å¿—
def parse_flag_from_env(key, default=False):
    try:
        # å°è¯•ä»ç¯å¢ƒä¸­è·å–å€¼
        value = os.environ[key]
    except KeyError:
        # å¦‚æœæœªè®¾ç½®é”®ï¼Œåˆ™é»˜è®¤ä¸º `default`
        _value = default
    else:
        # å¦‚æœè®¾ç½®äº†é”®ï¼Œå°†å…¶è½¬æ¢ä¸º True æˆ– False
        try:
            _value = strtobool(value)
        except ValueError:
            # æ”¯æŒæ›´å¤šçš„å€¼ï¼Œä½†è®©æ¶ˆæ¯ä¿æŒç®€å•
            raise ValueError(f"If set, {key} must be yes or no.")
    return _value


# ä»ç¯å¢ƒä¸­è§£ææ•´æ•°
def parse_int_from_env(key, default=None):
    try:
        # å°è¯•ä»ç¯å¢ƒä¸­è·å–å€¼
        value = os.environ[key]
    except KeyError:
        _value = default
    else:
        # å¦‚æœè®¾ç½®äº†é”®ï¼Œå°†å…¶è½¬æ¢ä¸ºæ•´æ•°
        try:
            _value = int(value)
        except ValueError:
            raise ValueError(f"If set, {key} must be a int.")
    return _value


# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œæ…¢æµ‹è¯•çš„æ ‡å¿—
_run_slow_tests = parse_flag_from_env("RUN_SLOW", default=False)
# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œ PyTorch + TensorFlow äº¤å‰æµ‹è¯•çš„æ ‡å¿—
_run_pt_tf_cross_tests = parse_flag_from_env("RUN_PT_TF_CROSS_TESTS", default=True)
# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œ PyTorch + Flax äº¤å‰æµ‹è¯•çš„æ ‡å¿—
_run_pt_flax_cross_tests = parse_flag_from_env("RUN_PT_FLAX_CROSS_TESTS", default=True)
# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œè‡ªå®šä¹‰åˆ†è¯å™¨æµ‹è¯•çš„æ ‡å¿—
_run_custom_tokenizers = parse_flag_from_env("RUN_CUSTOM_TOKENIZERS", default=False)
# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œåœ¨ staging ä¸Šçš„æµ‹è¯•çš„æ ‡å¿—
_run_staging = parse_flag_from_env("HUGGINGFACE_CO_STAGING", default=False)
# ä»ç¯å¢ƒä¸­è§£æ TensorFlow GPU å†…å­˜é™åˆ¶
_tf_gpu_memory_limit = parse_int_from_env("TF_GPU_MEMORY_LIMIT", default=None)
# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œç®¡é“æµ‹è¯•çš„æ ‡å¿—
_run_pipeline_tests = parse_flag_from_env("RUN_PIPELINE_TESTS", default=True)
# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œå·¥å…·æµ‹è¯•çš„æ ‡å¿—
_run_tool_tests = parse_flag_from_env("RUN_TOOL_TESTS", default=False)
# ä»ç¯å¢ƒä¸­è§£æè¿è¡Œç¬¬ä¸‰æ–¹è®¾å¤‡æµ‹è¯•çš„æ ‡å¿—
_run_third_party_device_tests = parse_flag_from_env("RUN_THIRD_PARTY_DEVICE_TESTS", default=False)


# æ ‡è®°ä¸€ä¸ªæµ‹è¯•ä¸ºæ§åˆ¶ PyTorch å’Œ TensorFlow ä¹‹é—´äº¤äº’çš„æµ‹è¯•çš„è£…é¥°å™¨
def is_pt_tf_cross_test(test_case):
    """
    Decorator marking a test as a test that control interactions between PyTorch and TensorFlow.

    PT+TF tests are skipped by default and we can run only them by setting RUN_PT_TF_CROSS_TESTS environment variable
    to a truthy value and selecting the is_pt_tf_cross_test pytest mark.

    """
    # å¦‚æœä¸è¿è¡Œ PyTorch + TensorFlow äº¤å‰æµ‹è¯•æˆ–è€… PyTorch æˆ– TensorFlow ä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•
    if not _run_pt_tf_cross_tests or not is_torch_available() or not is_tf_available():
        return unittest.skip("test is PT+TF test")(test_case)
    else:
        # å°è¯•å¯¼å…¥ pytest æ¨¡å—ï¼Œå¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™è¿”å›åŸå§‹æµ‹è¯•ç”¨ä¾‹
        try:
            import pytest  # We don't need a hard dependency on pytest in the main library
        except ImportError:
            return test_case
        # å¦‚æœå¯¼å…¥æˆåŠŸï¼Œæ‰§è¡Œä¸‹é¢çš„ä»£ç å—
        else:
            # ä½¿ç”¨ pytest.mark.is_pt_tf_cross_test() è£…é¥°å™¨è£…é¥°æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶è¿”å›è£…é¥°åçš„æµ‹è¯•ç”¨ä¾‹
            return pytest.mark.is_pt_tf_cross_test()(test_case)
# ç”¨äºè£…é¥°æµ‹è¯•ï¼Œæ ‡è®°æµ‹è¯•ä¸º PyTorch å’Œ Flax ä¹‹é—´äº¤äº’çš„æµ‹è¯•
def is_pt_flax_cross_test(test_case):
    # å¦‚æœä¸è¿è¡Œ PT+FLAX æµ‹è¯•æˆ–è€… PyTorch æˆ– Flax ä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•
    if not _run_pt_flax_cross_tests or not is_torch_available() or not is_flax_available():
        return unittest.skip("test is PT+FLAX test")(test_case)
    else:
        try:
            import pytest  # æˆ‘ä»¬ä¸éœ€è¦åœ¨ä¸»åº“ä¸­ç¡¬ä¾èµ–äº pytest
        except ImportError:
            return test_case
        else:
            return pytest.mark.is_pt_flax_cross_test()(test_case)


# ç”¨äºè£…é¥°æµ‹è¯•ï¼Œæ ‡è®°æµ‹è¯•ä¸ºåˆ†æ®µæµ‹è¯•
def is_staging_test(test_case):
    # å¦‚æœä¸è¿è¡Œåˆ†æ®µæµ‹è¯•ï¼Œåˆ™è·³è¿‡æµ‹è¯•
    if not _run_staging:
        return unittest.skip("test is staging test")(test_case)
    else:
        try:
            import pytest  # æˆ‘ä»¬ä¸éœ€è¦åœ¨ä¸»åº“ä¸­ç¡¬ä¾èµ–äº pytest
        except ImportError:
            return test_case
        else:
            return pytest.mark.is_staging_test()(test_case)


# ç”¨äºè£…é¥°æµ‹è¯•ï¼Œæ ‡è®°æµ‹è¯•ä¸ºç®¡é“æµ‹è¯•ã€‚å¦‚æœ RUN_PIPELINE_TESTS è®¾ç½®ä¸ºå‡å€¼ï¼Œåˆ™è·³è¿‡æµ‹è¯•ã€‚
def is_pipeline_test(test_case):
    if not _run_pipeline_tests:
        return unittest.skip("test is pipeline test")(test_case)
    else:
        try:
            import pytest  # æˆ‘ä»¬ä¸éœ€è¦åœ¨ä¸»åº“ä¸­ç¡¬ä¾èµ–äº pytest
        except ImportError:
            return test_case
        else:
            return pytest.mark.is_pipeline_test()(test_case)


# ç”¨äºè£…é¥°æµ‹è¯•ï¼Œæ ‡è®°æµ‹è¯•ä¸ºå·¥å…·æµ‹è¯•ã€‚å¦‚æœ RUN_TOOL_TESTS è®¾ç½®ä¸ºå‡å€¼ï¼Œåˆ™è·³è¿‡æµ‹è¯•ã€‚
def is_tool_test(test_case):
    if not _run_tool_tests:
        return unittest.skip("test is a tool test")(test_case)
    else:
        try:
            import pytest  # æˆ‘ä»¬ä¸éœ€è¦åœ¨ä¸»åº“ä¸­ç¡¬ä¾èµ–äº pytest
        except ImportError:
            return test_case
        else:
            return pytest.mark.is_tool_test()(test_case)


# ç”¨äºè£…é¥°æµ‹è¯•ï¼Œæ ‡è®°æµ‹è¯•ä¸ºæ…¢é€Ÿæµ‹è¯•ã€‚æ…¢é€Ÿæµ‹è¯•é»˜è®¤æƒ…å†µä¸‹ä¼šè¢«è·³è¿‡ã€‚è®¾ç½® RUN_SLOW ç¯å¢ƒå˜é‡ä¸ºçœŸå€¼æ¥è¿è¡Œå®ƒä»¬ã€‚
def slow(test_case):
    return unittest.skipUnless(_run_slow_tests, "test is slow")(test_case)


# ç”¨äºè£…é¥°æµ‹è¯•ï¼Œæ ‡è®°æµ‹è¯•ä¸ºå¤ªæ…¢æµ‹è¯•ã€‚æ…¢é€Ÿæµ‹è¯•åœ¨è¢«ä¿®å¤è¿‡ç¨‹ä¸­ä¼šè¢«è·³è¿‡ã€‚æ²¡æœ‰æµ‹è¯•åº”è¯¥æ ‡è®°ä¸º "tooslow"ï¼Œå› ä¸ºè¿™äº›æµ‹è¯•å°†ä¸ä¼šè¢« CI æµ‹è¯•ã€‚
def tooslow(test_case):
    return unittest.skip("test is too slow")(test_case)


# ç”¨äºè£…é¥°æµ‹è¯•ï¼Œæ ‡è®°æµ‹è¯•ä¸ºè‡ªå®šä¹‰åˆ†è¯å™¨æµ‹è¯•ã€‚
def custom_tokenizers(test_case):
    """
    Decorator marking a test for a custom tokenizer.
    """
```  
    # å®šä¹‰ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œç”¨äºè·³è¿‡æµ‹è¯•ç”¨ä¾‹ï¼ˆunittestï¼‰ã€‚
    # Custom tokenizers éœ€è¦é¢å¤–çš„ä¾èµ–é¡¹ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šè¢«è·³è¿‡ã€‚å°†ç¯å¢ƒå˜é‡ RUN_CUSTOM_TOKENIZERS è®¾ç½®ä¸ºçœŸå€¼ä»¥è¿è¡Œå®ƒä»¬ã€‚
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œå¦‚æœ _run_custom_tokenizers ä¸ºçœŸï¼Œåˆ™è¿”å›è£…é¥°è¿‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªè·³è¿‡æµ‹è¯•çš„å‡½æ•°ã€‚
    return unittest.skipUnless(_run_custom_tokenizers, "test of custom tokenizers")(test_case)
# æ ‡è®°éœ€è¦ BeautifulSoup4 çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… BeautifulSoup4 æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_bs4(test_case):
    """
    Decorator marking a test that requires BeautifulSoup4. These tests are skipped when BeautifulSoup4 isn't installed.
    """
    return unittest.skipUnless(is_bs4_available(), "test requires BeautifulSoup4")(test_case)


# æ ‡è®°éœ€è¦ OpenCV çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… OpenCV æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_cv2(test_case):
    """
    Decorator marking a test that requires OpenCV.

    These tests are skipped when OpenCV isn't installed.

    """
    return unittest.skipUnless(is_cv2_available(), "test requires OpenCV")(test_case)


# æ ‡è®°éœ€è¦ Levenshtein çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… Levenshtein æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_levenshtein(test_case):
    """
    Decorator marking a test that requires Levenshtein.

    These tests are skipped when Levenshtein isn't installed.

    """
    return unittest.skipUnless(is_levenshtein_available(), "test requires Levenshtein")(test_case)


# æ ‡è®°éœ€è¦ NLTK çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… NLTK æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_nltk(test_case):
    """
    Decorator marking a test that requires NLTK.

    These tests are skipped when NLTK isn't installed.

    """
    return unittest.skipUnless(is_nltk_available(), "test requires NLTK")(test_case)


# æ ‡è®°éœ€è¦ accelerate çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… accelerate æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_accelerate(test_case):
    """
    Decorator marking a test that requires accelerate. These tests are skipped when accelerate isn't installed.
    """
    return unittest.skipUnless(is_accelerate_available(), "test requires accelerate")(test_case)


# æ ‡è®°éœ€è¦ fsdp çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… fsdp æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_fsdp(test_case, min_version: str = "1.12.0"):
    """
    Decorator marking a test that requires fsdp. These tests are skipped when fsdp isn't installed.
    """
    return unittest.skipUnless(is_fsdp_available(min_version), f"test requires torch version >= {min_version}")(
        test_case
    )


# æ ‡è®°éœ€è¦ g2p_en çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… g2p_en æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_g2p_en(test_case):
    """
    Decorator marking a test that requires g2p_en. These tests are skipped when SentencePiece isn't installed.
    """
    return unittest.skipUnless(is_g2p_en_available(), "test requires g2p_en")(test_case)


# æ ‡è®°éœ€è¦ safetensors çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… safetensors æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_safetensors(test_case):
    """
    Decorator marking a test that requires safetensors. These tests are skipped when safetensors isn't installed.
    """
    return unittest.skipUnless(is_safetensors_available(), "test requires safetensors")(test_case)


# æ ‡è®°éœ€è¦ rjieba çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… rjieba æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_rjieba(test_case):
    """
    Decorator marking a test that requires rjieba. These tests are skipped when rjieba isn't installed.
    """
    return unittest.skipUnless(is_rjieba_available(), "test requires rjieba")(test_case)


# æ ‡è®°éœ€è¦ jieba çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… jieba æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_jieba(test_case):
    """
    Decorator marking a test that requires jieba. These tests are skipped when jieba isn't installed.
    """
    return unittest.skipUnless(is_jieba_available(), "test requires jieba")(test_case)


# æ ‡è®°éœ€è¦ jinja çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… jinja æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_jinja(test_case):
    """
    Decorator marking a test that requires jinja. These tests are skipped when jinja isn't installed.
    """
    return unittest.skipUnless(is_jinja_available(), "test requires jinja")(test_case)


# æ ‡è®°éœ€è¦ tf2onnx çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… tf2onnx æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_tf2onnx(test_case):
    return unittest.skipUnless(is_tf2onnx_available(), "test requires tf2onnx")(test_case)


# æ ‡è®°éœ€è¦ onnx çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœªå®‰è£… onnx æ—¶è·³è¿‡è¿™äº›æµ‹è¯•
def require_onnx(test_case):
    # å¦‚æœ ONNX å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_onnx_available(), "test requires ONNX")(test_case)
# æ ‡è®°ä¸€ä¸ªéœ€è¦ Timm çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_timm(test_case):
    """
    Decorator marking a test that requires Timm.

    These tests are skipped when Timm isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ Timm æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_timm_available(), "test requires Timm")(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ NATTEN çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_natten(test_case):
    """
    Decorator marking a test that requires NATTEN.

    These tests are skipped when NATTEN isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ NATTEN æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_natten_available(), "test requires natten")(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ PyTorch çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_torch(test_case):
    """
    Decorator marking a test that requires PyTorch.

    These tests are skipped when PyTorch isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ PyTorch æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torch_available(), "test requires PyTorch")(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ Flash Attention çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_flash_attn(test_case):
    """
    Decorator marking a test that requires Flash Attention.

    These tests are skipped when Flash Attention isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ Flash Attention æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_flash_attn_2_available(), "test requires Flash Attention")(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ PyTorch's SDPA çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_torch_sdpa(test_case):
    """
    Decorator marking a test that requires PyTorch's SDPA.

    These tests are skipped when requirements are not met (torch version).
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ PyTorch's SDPA æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torch_sdpa_available(), "test requires PyTorch SDPA")(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ PEFT çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_peft(test_case):
    """
    Decorator marking a test that requires PEFT.

    These tests are skipped when PEFT isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ PEFT æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_peft_available(), "test requires PEFT")(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ Torchvision çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_torchvision(test_case):
    """
    Decorator marking a test that requires Torchvision.

    These tests are skipped when Torchvision isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ Torchvision æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torchvision_available(), "test requires Torchvision")(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ PyTorch æˆ– TensorFlow çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_torch_or_tf(test_case):
    """
    Decorator marking a test that requires PyTorch or TensorFlow.

    These tests are skipped when neither PyTorch nor TensorFlow is installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ PyTorch æˆ– TensorFlow æ˜¯å¦å¯ç”¨ï¼Œè‹¥éƒ½ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torch_available() or is_tf_available(), "test requires PyTorch or TensorFlow")(
        test_case
    )


# æ ‡è®°ä¸€ä¸ªéœ€è¦ Intel Extension for PyTorch çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_intel_extension_for_pytorch(test_case):
    """
    Decorator marking a test that requires Intel Extension for PyTorch.

    These tests are skipped when Intel Extension for PyTorch isn't installed or it does not match current PyTorch
    version.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ Intel Extension for PyTorch æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(
        is_ipex_available(),
        "test requires Intel Extension for PyTorch to be installed and match current PyTorch version, see"
        " https://github.com/intel/intel-extension-for-pytorch",
    )(test_case)


# æ ‡è®°ä¸€ä¸ªéœ€è¦ TensorFlow probability çš„æµ‹è¯•çš„è£…é¥°å™¨
def require_tensorflow_probability(test_case):
    """
    Decorator marking a test that requires TensorFlow probability.

    These tests are skipped when TensorFlow probability isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨å‡½æ•°ï¼Œæ£€æŸ¥ TensorFlow probability æ˜¯å¦å¯ç”¨ï¼Œè‹¥ä¸å¯ç”¨åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_tensorflow_probability_available(), "test requires TensorFlow probability")(
        test_case
    )
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºè·³è¿‡æµ‹è¯•ï¼Œé™¤é TensorFlow probability å¯ç”¨
    return unittest.skipUnless(is_tensorflow_probability_available(), "test requires TensorFlow probability")(
        test_case
    )
# æ ‡è®°ä¸€ä¸ªéœ€è¦ torchaudio çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ torchaudio æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_torchaudio(test_case):
    """
    Decorator marking a test that requires torchaudio. These tests are skipped when torchaudio isn't installed.
    """
    return unittest.skipUnless(is_torchaudio_available(), "test requires torchaudio")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ TensorFlow çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ TensorFlow æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_tf(test_case):
    """
    Decorator marking a test that requires TensorFlow. These tests are skipped when TensorFlow isn't installed.
    """
    return unittest.skipUnless(is_tf_available(), "test requires TensorFlow")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ JAX & Flax çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“å…¶ä¸­ä¸€ä¸ªæˆ–ä¸¤è€…æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_flax(test_case):
    """
    Decorator marking a test that requires JAX & Flax. These tests are skipped when one / both are not installed
    """
    return unittest.skipUnless(is_flax_available(), "test requires JAX & Flax")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ SentencePiece çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ SentencePiece æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_sentencepiece(test_case):
    """
    Decorator marking a test that requires SentencePiece. These tests are skipped when SentencePiece isn't installed.
    """
    return unittest.skipUnless(is_sentencepiece_available(), "test requires SentencePiece")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ Seqio çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ Seqio æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_seqio(test_case):
    """
    Decorator marking a test that requires SentencePiece. These tests are skipped when SentencePiece isn't installed.
    """
    return unittest.skipUnless(is_seqio_available(), "test requires Seqio")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ Scipy çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ Scipy æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_scipy(test_case):
    """
    Decorator marking a test that requires Scipy. These tests are skipped when SentencePiece isn't installed.
    """
    return unittest.skipUnless(is_scipy_available(), "test requires Scipy")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ ğŸ¤— Tokenizers çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ ğŸ¤— Tokenizers æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_tokenizers(test_case):
    """
    Decorator marking a test that requires ğŸ¤— Tokenizers. These tests are skipped when ğŸ¤— Tokenizers isn't installed.
    """
    return unittest.skipUnless(is_tokenizers_available(), "test requires tokenizers")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ tensorflow_text çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ tensorflow_text æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_tensorflow_text(test_case):
    """
    Decorator marking a test that requires tensorflow_text. These tests are skipped when tensroflow_text isn't
    installed.
    """
    return unittest.skipUnless(is_tensorflow_text_available(), "test requires tensorflow_text")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ keras_nlp çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ keras_nlp æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_keras_nlp(test_case):
    """
    Decorator marking a test that requires keras_nlp. These tests are skipped when keras_nlp isn't installed.
    """
    return unittest.skipUnless(is_keras_nlp_available(), "test requires keras_nlp")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ pandas çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ pandas æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_pandas(test_case):
    """
    Decorator marking a test that requires pandas. These tests are skipped when pandas isn't installed.
    """
    return unittest.skipUnless(is_pandas_available(), "test requires pandas")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ PyTesseract çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚å½“ PyTesseract æœªå®‰è£…æ—¶ï¼Œè·³è¿‡è¿™äº›æµ‹è¯•ã€‚
def require_pytesseract(test_case):
    """
    Decorator marking a test that requires PyTesseract. These tests are skipped when PyTesseract isn't installed.
    """
    return unittest.skipUnless(is_pytesseract_available(), "test requires PyTesseract")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ PyTorch é‡åŒ–åŠŸèƒ½çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ã€‚æš‚æ—¶ç¼ºå°‘äº†è¿™ä¸ªè£…é¥°å™¨çš„å…·ä½“æ³¨é‡Šã€‚
def require_pytorch_quantization(test_case):
    """
    # è£…é¥°å™¨æ ‡è®°ä¸€ä¸ªéœ€è¦ PyTorch é‡åŒ–å·¥å…·åŒ…çš„æµ‹è¯•ã€‚å½“ PyTorch é‡åŒ–å·¥å…·åŒ…æœªå®‰è£…æ—¶ï¼Œè¿™äº›æµ‹è¯•å°†è¢«è·³è¿‡ã€‚
    """
    # ä½¿ç”¨ unittest.skipUnless() å‡½æ•°è£…é¥°æµ‹è¯•ç”¨ä¾‹ï¼Œå½“ is_pytorch_quantization_available() å‡½æ•°è¿”å› False æ—¶è·³è¿‡æµ‹è¯•ï¼Œ
    # å¹¶æä¾›ä¸€æ¡æ¶ˆæ¯è¯´æ˜æµ‹è¯•éœ€è¦ PyTorch é‡åŒ–å·¥å…·åŒ…
    return unittest.skipUnless(is_pytorch_quantization_available(), "test requires PyTorch Quantization Toolkit")(
        test_case
    )
# æ ‡è®°ä¸€ä¸ªéœ€è¦è§†è§‰ä¾èµ–çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æ²¡æœ‰å®‰è£… torchaudio æ—¶ä¼šè·³è¿‡è¿™äº›æµ‹è¯•
def require_vision(test_case):
    return unittest.skipUnless(is_vision_available(), "test requires vision")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ ftfy çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æ²¡æœ‰å®‰è£… ftfy æ—¶ä¼šè·³è¿‡è¿™äº›æµ‹è¯•
def require_ftfy(test_case):
    return unittest.skipUnless(is_ftfy_available(), "test requires ftfy")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ SpaCy çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æ²¡æœ‰å®‰è£… SpaCy æ—¶ä¼šè·³è¿‡è¿™äº›æµ‹è¯•
def require_spacy(test_case):
    return unittest.skipUnless(is_spacy_available(), "test requires spacy")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ decord çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æ²¡æœ‰å®‰è£… decord æ—¶ä¼šè·³è¿‡è¿™äº›æµ‹è¯•
def require_decord(test_case):
    return unittest.skipUnless(is_decord_available(), "test requires decord")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦å¤š GPU è®¾ç½®ï¼ˆåœ¨ PyTorch ä¸­ï¼‰çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœºå™¨æ²¡æœ‰å¤šä¸ª GPU æ—¶ä¼šè·³è¿‡è¿™äº›æµ‹è¯•
def require_torch_multi_gpu(test_case):
    if not is_torch_available():
        return unittest.skip("test requires PyTorch")(test_case)

    import torch

    return unittest.skipUnless(torch.cuda.device_count() > 1, "test requires multiple GPUs")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦å¤šåŠ é€Ÿå™¨è®¾ç½®ï¼ˆåœ¨ PyTorch ä¸­ï¼‰çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨ï¼Œå½“æœºå™¨æ²¡æœ‰å¤šä¸ªåŠ é€Ÿå™¨æ—¶ä¼šè·³è¿‡è¿™äº›æµ‹è¯•
def require_torch_multi_accelerator(test_case):
    if not is_torch_available():
        return unittest.skip("test requires PyTorch")(test_case)

    return unittest.skipUnless(backend_device_count(torch_device) > 1, "test requires multiple accelerators")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ 0 æˆ– 1 ä¸ª GPU è®¾ç½®ï¼ˆåœ¨ PyTorch ä¸­ï¼‰çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨
def require_torch_non_multi_gpu(test_case):
    if not is_torch_available():
        return unittest.skip("test requires PyTorch")(test_case)

    import torch

    return unittest.skipUnless(torch.cuda.device_count() < 2, "test requires 0 or 1 GPU")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ 0 æˆ– 1 ä¸ªåŠ é€Ÿå™¨è®¾ç½®ï¼ˆåœ¨ PyTorch ä¸­ï¼‰çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨
def require_torch_non_multi_accelerator(test_case):
    if not is_torch_available():
        return unittest.skip("test requires PyTorch")(test_case)

    return unittest.skipUnless(backend_device_count(torch_device) < 2, "test requires 0 or 1 accelerator")(test_case)

# æ ‡è®°ä¸€ä¸ªéœ€è¦ 0 æˆ– 1 æˆ– 2 ä¸ª GPU è®¾ç½®ï¼ˆåœ¨ PyTorch ä¸­ï¼‰çš„æµ‹è¯•ç”¨ä¾‹çš„è£…é¥°å™¨
def require_torch_up_to_2_gpus(test_case):
    # æ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦å¯ç”¨ PyTorch æ¡†æ¶
    if not is_torch_available():
        # å¦‚æœä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•å¹¶è¿”å›ç›¸åº”çš„æ¶ˆæ¯
        return unittest.skip("test requires PyTorch")(test_case)
    
    # å¯¼å…¥ PyTorch æ¡†æ¶
    import torch
    
    # ä»…åœ¨å½“å‰ CUDA è®¾å¤‡æ•°é‡å°äº 3 æ—¶æ‰§è¡Œæµ‹è¯•ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•å¹¶è¿”å›æ¶ˆæ¯
    return unittest.skipUnless(torch.cuda.device_count() < 3, "test requires 0 or 1 or 2 GPUs")(test_case)
# è£…é¥°å™¨ï¼Œæ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦æœ€å¤šä¸¤ä¸ªåŠ é€Ÿå™¨ï¼ˆåœ¨ PyTorch ä¸­ï¼‰
def require_torch_up_to_2_accelerators(test_case):
    # å¦‚æœ PyTorch ä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•
    if not is_torch_available():
        return unittest.skip("test requires PyTorch")(test_case)

    # é™¤éå½“å‰è®¾å¤‡åŠ é€Ÿå™¨æ•°é‡å°äº 3ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(backend_device_count(torch_device) < 3, "test requires 0 or 1 or 2 accelerators")(test_case)


# è£…é¥°å™¨ï¼Œæ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ TPUï¼ˆåœ¨ PyTorch ä¸­ï¼‰
def require_torch_tpu(test_case):
    # é™¤é PyTorch TPU å¯ç”¨ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torch_tpu_available(check_device=False), "test requires PyTorch TPU")(test_case)


# è£…é¥°å™¨ï¼Œæ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ NeuronCoreï¼ˆåœ¨ PyTorch ä¸­ï¼‰
def require_torch_neuroncore(test_case):
    # é™¤é PyTorch NeuronCore å¯ç”¨ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torch_neuroncore_available(check_device=False), "test requires PyTorch NeuronCore")(test_case)


# è£…é¥°å™¨ï¼Œæ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ NPUï¼ˆåœ¨ PyTorch ä¸­ï¼‰
def require_torch_npu(test_case):
    # é™¤é PyTorch NPU å¯ç”¨ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torch_npu_available(), "test requires PyTorch NPU")(test_case)


# è£…é¥°å™¨ï¼Œæ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦å¤šä¸ª NPUï¼ˆåœ¨ PyTorch ä¸­ï¼‰ã€‚è¿™äº›æµ‹è¯•åœ¨æ²¡æœ‰å¤šä¸ª NPU çš„æœºå™¨ä¸Šè·³è¿‡ã€‚
def require_torch_multi_npu(test_case):
    # å¦‚æœ PyTorch NPU ä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•
    if not is_torch_npu_available():
        return unittest.skip("test requires PyTorch NPU")(test_case)

    # é™¤éå½“å‰è®¾å¤‡çš„ NPU æ•°é‡å¤§äº 1ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(torch.npu.device_count() > 1, "test requires multiple NPUs")(test_case)


# è£…é¥°å™¨ï¼Œæ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ XPU å’Œ IPEXã€‚è¿™äº›æµ‹è¯•åœ¨æœªå®‰è£… Intel Extension for PyTorch æˆ–å…¶ç‰ˆæœ¬ä¸åŒ¹é…å½“å‰ PyTorch ç‰ˆæœ¬æ—¶è·³è¿‡ã€‚
def require_torch_xpu(test_case):
    # é™¤é IPEX å’Œ XPU è®¾å¤‡å¯ç”¨ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_torch_xpu_available(), "test requires IPEX and an XPU device")(test_case)


# è£…é¥°å™¨ï¼Œæ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦å¸¦æœ‰ IPEX å’Œè‡³å°‘ä¸€ä¸ª XPU è®¾å¤‡çš„å¤šä¸ª XPU è®¾ç½®ã€‚è¿™äº›æµ‹è¯•åœ¨æ²¡æœ‰ IPEX æˆ–å¤šä¸ª XPU çš„æœºå™¨ä¸Šè·³è¿‡ã€‚
def require_torch_multi_xpu(test_case):
    # å¦‚æœ IPEX å’Œ XPU ä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•
    if not is_torch_xpu_available():
        return unittest.skip("test requires IPEX and atleast one XPU device")(test_case)

    # é™¤éå½“å‰è®¾å¤‡çš„ XPU æ•°é‡å¤§äº 1ï¼Œå¦åˆ™è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(torch.xpu.device_count() > 1, "test requires multiple XPUs")(test_case)


# å¦‚æœ PyTorch å¯ç”¨ï¼Œåˆ™è®¾ç½®ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES="" ä»¥å¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼
if is_torch_available():
    import torch
    # æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦è®¾ç½®äº†åä¸º "TRANSFORMERS_TEST_BACKEND" çš„å˜é‡
    if "TRANSFORMERS_TEST_BACKEND" in os.environ:
        # å¦‚æœè®¾ç½®äº†ï¼Œåˆ™è·å–è¯¥å˜é‡çš„å€¼
        backend = os.environ["TRANSFORMERS_TEST_BACKEND"]
        try:
            # å°è¯•åŠ¨æ€å¯¼å…¥è¯¥å˜é‡æŒ‡å®šçš„æ¨¡å—
            _ = importlib.import_module(backend)
        except ModuleNotFoundError as e:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æŠ›å‡ºæ¨¡å—æœªæ‰¾åˆ°çš„å¼‚å¸¸ï¼Œå¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ModuleNotFoundError(
                f"Failed to import `TRANSFORMERS_TEST_BACKEND` '{backend}'! This should be the name of an installed module. The original error (look up to see its"
                f" traceback):\n{e}"
            ) from e
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦è®¾ç½®äº†åä¸º "TRANSFORMERS_TEST_DEVICE" çš„å˜é‡
    if "TRANSFORMERS_TEST_DEVICE" in os.environ:
        # å¦‚æœè®¾ç½®äº†ï¼Œåˆ™è·å–è¯¥å˜é‡çš„å€¼ä½œä¸º Torch è®¾å¤‡
        torch_device = os.environ["TRANSFORMERS_TEST_DEVICE"]
        try:
            # å°è¯•åˆ›å»º Torch è®¾å¤‡ï¼Œä»¥éªŒè¯æä¾›çš„è®¾å¤‡æ˜¯å¦æœ‰æ•ˆ
            _ = torch.device(torch_device)
        except RuntimeError as e:
            # å¦‚æœåˆ›å»ºè®¾å¤‡æ—¶å‘ç”Ÿé”™è¯¯ï¼Œåˆ™æŠ›å‡ºè¿è¡Œæ—¶é”™è¯¯ï¼Œå¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise RuntimeError(
                f"Unknown testing device specified by environment variable `TRANSFORMERS_TEST_DEVICE`: {torch_device}"
            ) from e
    # å¦‚æœç¯å¢ƒå˜é‡ä¸­æœªè®¾ç½®æµ‹è¯•è®¾å¤‡ï¼Œå¹¶ä¸” CUDA å¯ç”¨ï¼Œåˆ™é€‰æ‹© CUDA è®¾å¤‡
    elif torch.cuda.is_available():
        torch_device = "cuda"
    # å¦‚æœç¬¬ä¸‰æ–¹è®¾å¤‡æµ‹è¯•å¯ç”¨ï¼Œå¹¶ä¸” Torch NPU å¯ç”¨ï¼Œåˆ™é€‰æ‹© NPU è®¾å¤‡
    elif _run_third_party_device_tests and is_torch_npu_available():
        torch_device = "npu"
    # å¦‚æœç¬¬ä¸‰æ–¹è®¾å¤‡æµ‹è¯•å¯ç”¨ï¼Œå¹¶ä¸” Torch XPU å¯ç”¨ï¼Œåˆ™é€‰æ‹© XPU è®¾å¤‡
    elif _run_third_party_device_tests and is_torch_xpu_available():
        torch_device = "xpu"
    # å¦‚æœä»¥ä¸Šæ¡ä»¶éƒ½ä¸æ»¡è¶³ï¼Œåˆ™é€‰æ‹© CPU è®¾å¤‡
    else:
        torch_device = "cpu"
else:
    # å¦‚æœæ²¡æœ‰å…¶ä»–è®¾å¤‡å¯ç”¨ï¼Œåˆ™å°† torch_device è®¾ä¸º None
    torch_device = None

# å¦‚æœ TensorFlow å¯ç”¨ï¼Œåˆ™å¯¼å…¥ TensorFlow åº“
if is_tf_available():
    import tensorflow as tf

# å¦‚æœ Flax å¯ç”¨ï¼Œåˆ™å¯¼å…¥ JAX åº“ï¼Œå¹¶è®¾ç½®é»˜è®¤è®¾å¤‡ä¸ºå½“å‰è®¾å¤‡
if is_flax_available():
    import jax
    # è·å–é»˜è®¤çš„ JAX åç«¯è®¾å¤‡
    jax_device = jax.default_backend()
else:
    # å¦åˆ™å°† jax_device è®¾ä¸º None
    jax_device = None

# ä»¥ä¸‹ä¸ºä¸€ç³»åˆ—è£…é¥°å™¨å‡½æ•°ï¼Œç”¨äºæ ‡è®°éœ€è¦ç‰¹å®šç¯å¢ƒæ”¯æŒçš„æµ‹è¯•ç”¨ä¾‹

# è¦æ±‚ TorchDynamoï¼Œéœ€è¦ TorchDynamo å¯ç”¨
def require_torchdynamo(test_case):
    """Decorator marking a test that requires TorchDynamo"""
    return unittest.skipUnless(is_torchdynamo_available(), "test requires TorchDynamo")(test_case)

# è¦æ±‚ Torch-TensorRT FXï¼Œéœ€è¦ Torch-TensorRT FX å¯ç”¨
def require_torch_tensorrt_fx(test_case):
    """Decorator marking a test that requires Torch-TensorRT FX"""
    return unittest.skipUnless(is_torch_tensorrt_fx_available(), "test requires Torch-TensorRT FX")(test_case)

# è¦æ±‚ Torch GPUï¼Œéœ€è¦ CUDA å’Œ PyTorch å¯ç”¨
def require_torch_gpu(test_case):
    """Decorator marking a test that requires CUDA and PyTorch."""
    return unittest.skipUnless(torch_device == "cuda", "test requires CUDA")(test_case)

# è¦æ±‚ Torch åŠ é€Ÿå™¨ï¼Œéœ€è¦å¯ç”¨çš„åŠ é€Ÿå™¨å’Œ PyTorch
def require_torch_accelerator(test_case):
    """Decorator marking a test that requires an accessible accelerator and PyTorch."""
    return unittest.skipUnless(torch_device is not None and torch_device != "cpu", "test requires accelerator")(test_case)

# è¦æ±‚ Torch fp16ï¼Œéœ€è¦è®¾å¤‡æ”¯æŒ fp16
def require_torch_fp16(test_case):
    """Decorator marking a test that requires a device that supports fp16"""
    return unittest.skipUnless(
        is_torch_fp16_available_on_device(torch_device), "test requires device with fp16 support"
    )(test_case)

# è¦æ±‚ Torch bf16ï¼Œéœ€è¦è®¾å¤‡æ”¯æŒ bf16
def require_torch_bf16(test_case):
    """Decorator marking a test that requires a device that supports bf16"""
    return unittest.skipUnless(
        is_torch_bf16_available_on_device(torch_device), "test requires device with bf16 support"
    )(test_case)

# è¦æ±‚ Torch bf16 GPUï¼Œéœ€è¦ torch>=1.10ï¼Œå¹¶ä¸”ä½¿ç”¨ Ampere GPU æˆ–æ›´æ–°çš„æ¶æ„ï¼Œæˆ– cuda>=11.0
def require_torch_bf16_gpu(test_case):
    """Decorator marking a test that requires torch>=1.10, using Ampere GPU or newer arch with cuda>=11.0"""
    return unittest.skipUnless(
        is_torch_bf16_gpu_available(),
        "test requires torch>=1.10, using Ampere GPU or newer arch with cuda>=11.0",
    )(test_case)

# è¦æ±‚ Torch bf16 CPUï¼Œéœ€è¦ torch>=1.10ï¼Œå¹¶ä¸”ä½¿ç”¨ CPU
def require_torch_bf16_cpu(test_case):
    """Decorator marking a test that requires torch>=1.10, using CPU."""
    return unittest.skipUnless(
        is_torch_bf16_cpu_available(),
        "test requires torch>=1.10, using CPU",
    )(test_case)

# è¦æ±‚ Torch tf32ï¼Œéœ€è¦ Ampere æˆ–æ›´æ–°çš„ GPU æ¶æ„ï¼Œcuda>=11 å’Œ torch>=1.7
def require_torch_tf32(test_case):
    """Decorator marking a test that requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7."""
    return unittest.skipUnless(
        is_torch_tf32_available(), "test requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7"
    )(test_case)

# è¦æ±‚ Detectron2ï¼Œéœ€è¦ detectron2 å¯ç”¨
def require_detectron2(test_case):
    """Decorator marking a test that requires detectron2."""
    return unittest.skipUnless(is_detectron2_available(), "test requires `detectron2`")(test_case)

# è¦æ±‚ Faissï¼Œéœ€è¦ faiss å¯ç”¨
def require_faiss(test_case):
    """Decorator marking a test that requires faiss."""
    return unittest.skipUnless(is_faiss_available(), "test requires `faiss`")(test_case)

# è¦æ±‚ Optunaï¼Œéœ€è¦ optuna å¯ç”¨
def require_optuna(test_case):
    """
    Decorator marking a test that requires optuna.

    These tests are skipped when optuna isn't installed.

    """
    # å¦‚æœ optuna å¯ç”¨ï¼Œåˆ™è·³è¿‡æµ‹è¯•ï¼Œå¦åˆ™æç¤ºæµ‹è¯•éœ€è¦ optuna
    return unittest.skipUnless(is_optuna_available(), "test requires optuna")(test_case)
def require_ray(test_case):
    """
    Decorator marking a test that requires Ray/tune.

    These tests are skipped when Ray/tune isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ Ray/tune çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_ray_available(), "test requires Ray/tune")(test_case)


def require_sigopt(test_case):
    """
    Decorator marking a test that requires SigOpt.

    These tests are skipped when SigOpt isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ SigOpt çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_sigopt_available(), "test requires SigOpt")(test_case)


def require_wandb(test_case):
    """
    Decorator marking a test that requires wandb.

    These tests are skipped when wandb isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ wandb çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_wandb_available(), "test requires wandb")(test_case)


def require_clearml(test_case):
    """
    Decorator marking a test requires clearml.

    These tests are skipped when clearml isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ clearml çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_clearml_available(), "test requires clearml")(test_case)


def require_soundfile(test_case):
    """
    Decorator marking a test that requires soundfile

    These tests are skipped when soundfile isn't installed.

    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ soundfile çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_soundfile_availble(), "test requires soundfile")(test_case)


def require_deepspeed(test_case):
    """
    Decorator marking a test that requires deepspeed
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ deepspeed çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_deepspeed_available(), "test requires deepspeed")(test_case)


def require_apex(test_case):
    """
    Decorator marking a test that requires apex
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ apex çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_apex_available(), "test requires apex")(test_case)


def require_bitsandbytes(test_case):
    """
    Decorator for bits and bytes (bnb) dependency
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ bnb çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_bitsandbytes_available(), "test requires bnb")(test_case)


def require_optimum(test_case):
    """
    Decorator for optimum dependency
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ optimum çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_optimum_available(), "test requires optimum")(test_case)


def require_tensorboard(test_case):
    """
    Decorator for `tensorboard` dependency
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ tensorboard çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_tensorboard_available(), "test requires tensorboard")


def require_auto_gptq(test_case):
    """
    Decorator for auto_gptq dependency
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ auto_gptq çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_auto_gptq_available(), "test requires auto-gptq")(test_case)


def require_auto_awq(test_case):
    """
    Decorator for auto_awq dependency
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ autoawq çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_auto_awq_available(), "test requires autoawq")(test_case)


def require_phonemizer(test_case):
    """
    Decorator marking a test that requires phonemizer
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ phonemizer çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_phonemizer_available(), "test requires phonemizer")(test_case)


def require_pyctcdecode(test_case):
    """
    Decorator marking a test that requires pyctcdecode
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ pyctcdecode çš„æµ‹è¯•ç”¨ä¾‹
    return unittest.skipUnless(is_pyctcdecode_available(), "test requires pyctcdecode")(test_case)


def require_librosa(test_case):
    # Placeholder for require_librosa
    pass
    # å®šä¹‰ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ ‡è®°éœ€è¦ä½¿ç”¨ librosa çš„æµ‹è¯•
    """
    Decorator marking a test that requires librosa
    """
    # è¿”å›ä¸€ä¸ªè£…é¥°å™¨ï¼Œæ ¹æ®æ˜¯å¦å¯ç”¨ librosa å†³å®šæ˜¯å¦è·³è¿‡æµ‹è¯•
    return unittest.skipUnless(is_librosa_available(), "test requires librosa")(test_case)
# æ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ essentia çš„è£…é¥°å™¨
def require_essentia(test_case):
    """
    Decorator marking a test that requires essentia
    """
    return unittest.skipUnless(is_essentia_available(), "test requires essentia")(test_case)


# æ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ pretty_midi çš„è£…é¥°å™¨
def require_pretty_midi(test_case):
    """
    Decorator marking a test that requires pretty_midi
    """
    return unittest.skipUnless(is_pretty_midi_available(), "test requires pretty_midi")(test_case)


# æ£€æŸ¥ç»™å®šå‘½ä»¤æ˜¯å¦å­˜åœ¨
def cmd_exists(cmd):
    return shutil.which(cmd) is not None


# æ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ `/usr/bin/time` çš„è£…é¥°å™¨
def require_usr_bin_time(test_case):
    """
    Decorator marking a test that requires `/usr/bin/time`
    """
    return unittest.skipUnless(cmd_exists("/usr/bin/time"), "test requires /usr/bin/time")(test_case)


# æ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ sudachi çš„è£…é¥°å™¨
def require_sudachi(test_case):
    """
    Decorator marking a test that requires sudachi
    """
    return unittest.skipUnless(is_sudachi_available(), "test requires sudachi")(test_case)


# æ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ jumanpp çš„è£…é¥°å™¨
def require_jumanpp(test_case):
    """
    Decorator marking a test that requires jumanpp
    """
    return unittest.skipUnless(is_jumanpp_available(), "test requires jumanpp")(test_case)


# æ ‡è®°ä¸€ä¸ªæµ‹è¯•éœ€è¦ cython çš„è£…é¥°å™¨
def require_cython(test_case):
    """
    Decorator marking a test that requires jumanpp
    """
    return unittest.skipUnless(is_cython_available(), "test requires cython")(test_case)


# è¿”å›å¯ç”¨ GPU æ•°é‡ï¼ˆä¸ç®¡æ˜¯ä½¿ç”¨ torchã€tf è¿˜æ˜¯ jaxï¼‰
def get_gpu_count():
    """
    Return the number of available gpus (regardless of whether torch, tf or jax is used)
    """
    if is_torch_available():
        import torch

        return torch.cuda.device_count()
    elif is_tf_available():
        import tensorflow as tf

        return len(tf.config.list_physical_devices("GPU"))
    elif is_flax_available():
        import jax

        return jax.device_count()
    else:
        return 0


# è·å–æµ‹è¯•ç›®å½•è·¯å¾„
def get_tests_dir(append_path=None):
    """
    Args:
        append_path: optional path to append to the tests dir path

    Return:
        The full path to the `tests` dir, so that the tests can be invoked from anywhere. Optionally `append_path` is
        joined after the `tests` dir the former is provided.

    """
    # è·å–è°ƒç”¨è¯¥å‡½æ•°çš„æ–‡ä»¶è·¯å¾„
    caller__file__ = inspect.stack()[1][1]
    tests_dir = os.path.abspath(os.path.dirname(caller__file__))

    # å¾ªç¯ç›´åˆ°æ‰¾åˆ°ï¿½ï¿½å« "tests" çš„ç›®å½•
    while not tests_dir.endswith("tests"):
        tests_dir = os.path.dirname(tests_dir)

    # å¦‚æœæä¾›äº† append_pathï¼Œåˆ™å°†å…¶è¿æ¥åˆ° "tests" ç›®å½•åé¢
    if append_path:
        return os.path.join(tests_dir, append_path)
    else:
        return tests_dir


#
# ç”¨äºå¤„ç†æµ‹è¯•æ–‡æœ¬è¾“å‡ºçš„è¾…åŠ©å‡½æ•°
# åŸå§‹ä»£ç æ¥æºäºï¼š
# https://github.com/fastai/fastai/blob/master/tests/utils/text.py


# å½“ä»»ä½•å‡½æ•°åŒ…å« print() è°ƒç”¨å¹¶ä¸”è¢«è¦†ç›–æ—¶ï¼Œæ¯”å¦‚è¿›åº¦æ¡ï¼Œ
# éœ€è¦ç‰¹åˆ«æ³¨æ„ï¼Œå› ä¸ºåœ¨ pytest -s æ•è·çš„è¾“å‡ºï¼ˆcapsys æˆ– contextlib.redirect_stdoutï¼‰
# åŒ…å«ä»»ä½•ä¸´æ—¶æ‰“å°çš„å­—ç¬¦ä¸²ï¼Œåé¢è·Ÿç€ \rã€‚è¿™ä¸ªè¾…åŠ©å‡½æ•°ç¡®ä¿ç¼“å†²åŒºå°†åŒ…å«ç›¸åŒçš„è¾“å‡º
# æ— è®ºæ˜¯å¦åœ¨ pytest ä¸­ä½¿ç”¨ -sï¼Œå°†:
# foo bar\r tar mar\r final message
# è½¬æ¢ä¸º:
# final message
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤„ç†å•ä¸ªå­—ç¬¦ä¸²æˆ–å¤šè¡Œç¼“å†²åŒº
def apply_print_resets(buf):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰ä»¥\rç»“å°¾çš„å†…å®¹ï¼Œè¿”å›å¤„ç†åçš„ç»“æœ
    return re.sub(r"^.*\r", "", buf, 0, re.M)

# æ–­è¨€è¾“å‡ºä¸­åŒ…å«ç‰¹å®šå†…å®¹
def assert_screenout(out, what):
    # å°†è¾“å‡ºå†…å®¹è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶å»é™¤ç‰¹å®šæ ¼å¼çš„å†…å®¹
    out_pr = apply_print_resets(out).lower()
    # åœ¨å¤„ç†åçš„è¾“å‡ºä¸­æŸ¥æ‰¾ç‰¹å®šå†…å®¹ï¼Œå¦‚æœæ‰¾åˆ°åˆ™ç»§ç»­æ‰§è¡Œï¼Œå¦åˆ™æŠ›å‡ºå¼‚å¸¸
    match_str = out_pr.find(what.lower())
    assert match_str != -1, f"expecting to find {what} in output: f{out_pr}"

# å®šä¹‰ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºæ•è·å’Œé‡æ”¾ stdout å’Œ stderr
class CaptureStd:
    """
    Context manager to capture:

        - stdout: replay it, clean it up and make it available via `obj.out`
        - stderr: replay it and make it available via `obj.err`

    Args:
        out (`bool`, *optional*, defaults to `True`): Whether to capture stdout or not.
        err (`bool`, *optional*, defaults to `True`): Whether to capture stderr or not.
        replay (`bool`, *optional*, defaults to `True`): Whether to replay or not.
            By default each captured stream gets replayed back on context's exit, so that one can see what the test was
            doing. If this is a not wanted behavior and the captured data shouldn't be replayed, pass `replay=False` to
            disable this feature.

    Examples:

    ```py
    # to capture stdout only with auto-replay
    with CaptureStdout() as cs:
        print("Secret message")
    assert "message" in cs.out

    # to capture stderr only with auto-replay
    import sys

    with CaptureStderr() as cs:
        print("Warning: ", file=sys.stderr)
    assert "Warning" in cs.err

    # to capture both streams with auto-replay
    with CaptureStd() as cs:
        print("Secret message")
        print("Warning: ", file=sys.stderr)
    assert "message" in cs.out
    assert "Warning" in cs.err

    # to capture just one of the streams, and not the other, with auto-replay
    with CaptureStd(err=False) as cs:
        print("Secret message")
    assert "message" in cs.out
    # but best use the stream-specific subclasses

    # to capture without auto-replay
    with CaptureStd(replay=False) as cs:
        print("Secret message")
    assert "message" in cs.out
    ```"""

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®æ˜¯å¦æ•è· stdout å’Œ stderrï¼Œä»¥åŠæ˜¯å¦é‡æ”¾
    def __init__(self, out=True, err=True, replay=True):
        self.replay = replay

        # å¦‚ï¿½ï¿½ï¿½æ•è· stdoutï¼Œåˆ™åˆ›å»ºä¸€ä¸ª StringIO å¯¹è±¡
        if out:
            self.out_buf = StringIO()
            self.out = "error: CaptureStd context is unfinished yet, called too early"
        else:
            self.out_buf = None
            self.out = "not capturing stdout"

        # å¦‚æœæ•è· stderrï¼Œåˆ™åˆ›å»ºä¸€ä¸ª StringIO å¯¹è±¡
        if err:
            self.err_buf = StringIO()
            self.err = "error: CaptureStd context is unfinished yet, called too early"
        else:
            self.err_buf = None
            self.err = "not capturing stderr"

    # è¿›å…¥ä¸Šä¸‹æ–‡æ—¶æ‰§è¡Œçš„æ–¹æ³•
    def __enter__(self):
        # å¦‚æœæ•è· stdoutï¼Œåˆ™å°† sys.stdout é‡å®šå‘åˆ° StringIO å¯¹è±¡
        if self.out_buf:
            self.out_old = sys.stdout
            sys.stdout = self.out_buf

        # å¦‚æœæ•è· stderrï¼Œåˆ™å°† sys.stderr é‡å®šå‘åˆ° StringIO å¯¹è±¡
        if self.err_buf:
            self.err_old = sys.stderr
            sys.stderr = self.err_buf

        return self
    # å½“é€€å‡ºä¸Šä¸‹æ–‡æ—¶çš„æ“ä½œï¼Œæ¥å—ä»»æ„å¼‚å¸¸ä¿¡æ¯
    def __exit__(self, *exc):
        # å¦‚æœæœ‰è¾“å‡ºç¼“å†²åŒº
        if self.out_buf:
            # æ¢å¤æ ‡å‡†è¾“å‡ºåˆ°å…ˆå‰çŠ¶æ€
            sys.stdout = self.out_old
            # è·å–è¾“å‡ºç¼“å†²åŒºä¸­çš„å†…å®¹
            captured = self.out_buf.getvalue()
            # å¦‚æœéœ€è¦é‡æ”¾
            if self.replay:
                # å°†æ•è·çš„å†…å®¹å†™å›æ ‡å‡†è¾“å‡º
                sys.stdout.write(captured)
            # åº”ç”¨è¾“å‡ºé‡ç½®å¹¶æ›´æ–°å®ä¾‹å˜é‡
            self.out = apply_print_resets(captured)

        # å¦‚æœæœ‰é”™è¯¯è¾“å‡ºç¼“å†²åŒº
        if self.err_buf:
            # æ¢å¤æ ‡å‡†é”™è¯¯è¾“å‡ºåˆ°å…ˆå‰çŠ¶æ€
            sys.stderr = self.err_old
            # è·å–é”™è¯¯è¾“å‡ºç¼“å†²åŒºä¸­çš„å†…å®¹
            captured = self.err_buf.getvalue()
            # å¦‚æœéœ€è¦é‡æ”¾
            if self.replay:
                # å°†æ•è·çš„å†…å®¹å†™å›æ ‡å‡†é”™è¯¯è¾“å‡º
                sys.stderr.write(captured)
            # æ›´æ–°å®ä¾‹å˜é‡
            self.err = captured

    # å®šä¹‰å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼
    def __repr__(self):
        # åˆå§‹åŒ–æ¶ˆæ¯ä¸ºç©ºå­—ç¬¦ä¸²
        msg = ""
        # å¦‚æœå­˜åœ¨è¾“å‡ºç¼“å†²åŒº
        if self.out_buf:
            # æ·»åŠ æ ‡å‡†è¾“å‡ºçš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼åˆ°æ¶ˆæ¯ä¸­
            msg += f"stdout: {self.out}\n"
        # å¦‚æœå­˜åœ¨é”™è¯¯è¾“å‡ºç¼“å†²åŒº
        if self.err_buf:
            # æ·»åŠ æ ‡å‡†é”™è¯¯è¾“å‡ºçš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼åˆ°æ¶ˆæ¯ä¸­
            msg += f"stderr: {self.err}\n"
        # è¿”å›æ¶ˆæ¯
        return msg
# åœ¨æµ‹è¯•ä¸­ï¼Œæœ€å¥½åªæ•è·æ‰€éœ€çš„æµï¼Œå¦åˆ™å¾ˆå®¹æ˜“é”™è¿‡ä¸€äº›ä¸œè¥¿ï¼Œæ‰€ä»¥é™¤ééœ€è¦æ•è·ä¸¤ä¸ªæµï¼Œå¦åˆ™ä½¿ç”¨ä¸‹é¢çš„å­ç±»ï¼ˆè¾“å…¥æ›´å°‘ï¼‰ã€‚
# æˆ–è€…å¯ä»¥é…ç½®`CaptureStd`æ¥ç¦ç”¨ä¸éœ€è¦æµ‹è¯•çš„æµã€‚

class CaptureStdout(CaptureStd):
    """ä¸CaptureStdç›¸åŒï¼Œä½†ä»…æ•è·stdout"""

    def __init__(self, replay=True):
        super().__init__(err=False, replay=replay)


class CaptureStderr(CaptureStd):
    """ä¸CaptureStdç›¸åŒï¼Œä½†ä»…æ•è·stderr"""

    def __init__(self, replay=True):
        super().__init__(out=False, replay=replay)


class CaptureLogger:
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºæ•è·`logging`æµ

    Args:
        logger: 'logging` loggerå¯¹è±¡

    Returns:
        é€šè¿‡`self.out`å¯è·å¾—æ•è·çš„è¾“å‡º

    ç¤ºä¾‹:

    ```py
    >>> from transformers import logging
    >>> from transformers.testing_utils import CaptureLogger

    >>> msg = "Testing 1, 2, 3"
    >>> logging.set_verbosity_info()
    >>> logger = logging.get_logger("transformers.models.bart.tokenization_bart")
    >>> with CaptureLogger(logger) as cl:
    ...     logger.info(msg)
    >>> assert cl.out, msg + "\n"
    ```
    """

    def __init__(self, logger):
        self.logger = logger
        self.io = StringIO()
        self.sh = logging.StreamHandler(self.io)
        self.out = ""

    def __enter__(self):
        self.logger.addHandler(self.sh)
        return self

    def __exit__(self, *exc):
        self.logger.removeHandler(self.sh)
        self.out = self.io.getvalue()

    def __repr__(self):
        return f"captured: {self.out}\n"


@contextlib.contextmanager
def LoggingLevel(level):
    """
    è¿™æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºä¸´æ—¶æ›´æ”¹transformersæ¨¡å—çš„æ—¥å¿—çº§åˆ«ä¸ºæ‰€éœ€å€¼ï¼Œå¹¶åœ¨ä½œç”¨åŸŸç»“æŸæ—¶å°†å…¶æ¢å¤ä¸ºåŸå§‹è®¾ç½®ã€‚

    ç¤ºä¾‹:

    ```py
    with LoggingLevel(logging.INFO):
        AutoModel.from_pretrained("gpt2")  # è°ƒç”¨logger.info()å¤šæ¬¡
    ```
    """

    orig_level = transformers_logging.get_verbosity()
    try:
        transformers_logging.set_verbosity(level)
        yield
    finally:
        transformers_logging.set_verbosity(orig_level)


@contextlib.contextmanager
# æ”¹ç¼–è‡ªhttps://stackoverflow.com/a/64789046/9201239
def ExtendSysPath(path: Union[str, os.PathLike]) -> Iterator[None]:
    """
    ä¸´æ—¶å°†ç»™å®šè·¯å¾„æ·»åŠ åˆ°`sys.path`ä¸­ã€‚

    ç”¨æ³•:

    ```py
    with ExtendSysPath("/path/to/dir"):
        mymodule = importlib.import_module("mymodule")
    ```
    """

    path = os.fspath(path)
    try:
        sys.path.insert(0, path)
        yield
    finally:
        sys.path.remove(path)


class TestCasePlus(unittest.TestCase):
    """
    æ­¤ç±»æ‰©å±•äº†*unittest.TestCase*ï¼Œå…·æœ‰é™„åŠ åŠŸèƒ½ã€‚

    ç‰¹æ€§1: ä¸€ç»„å®Œå…¨è§£æçš„é‡è¦æ–‡ä»¶å’Œç›®å½•è·¯å¾„è®¿é—®å™¨ã€‚
    class TestPaths:
        """
        åœ¨æµ‹è¯•ä¸­é€šå¸¸éœ€è¦çŸ¥é“äº‹ç‰©ç›¸å¯¹äºå½“å‰æµ‹è¯•æ–‡ä»¶çš„ä½ç½®ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªç®€å•çš„é—®é¢˜ï¼Œå› ä¸ºæµ‹è¯•å¯ä»¥ä»å¤šä¸ªç›®å½•è°ƒç”¨ï¼Œæˆ–è€…å¯èƒ½ä½äºå…·æœ‰ä¸åŒæ·±åº¦çš„å­ç›®å½•ä¸­ã€‚è¯¥ç±»é€šè¿‡æ•´ç†æ‰€æœ‰åŸºæœ¬è·¯å¾„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æä¾›äº†æ˜“äºè®¿é—®çš„è®¿é—®å™¨ï¼š
    
        - `pathlib` å¯¹è±¡ï¼ˆå…¨éƒ¨è§£æï¼‰ï¼š
    
           - `test_file_path` - å½“å‰æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆ=`__file__`ï¼‰
           - `test_file_dir` - åŒ…å«å½“å‰æµ‹è¯•æ–‡ä»¶çš„ç›®å½•
           - `tests_dir` - `tests` æµ‹è¯•å¥—ä»¶çš„ç›®å½•
           - `examples_dir` - `examples` æµ‹è¯•å¥—ä»¶çš„ç›®å½•
           - `repo_root_dir` - ä»“åº“çš„ç›®å½•
           - `src_dir` - `src` çš„ç›®å½•ï¼ˆå³ `transformers` å­ç›®å½•æ‰€åœ¨çš„ä½ç½®ï¼‰
    
        - å­—ç¬¦ä¸²åŒ–çš„è·¯å¾„---ä¸ä¸Šè¿°ç›¸åŒï¼Œä½†è¿™äº›è¿”å›è·¯å¾„ä½œä¸ºå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ `pathlib` å¯¹è±¡ï¼š
    
           - `test_file_path_str`
           - `test_file_dir_str`
           - `tests_dir_str`
           - `examples_dir_str`
           - `repo_root_dir_str`
           - `src_dir_str`
    
        åŠŸèƒ½ 2: çµæ´»çš„è‡ªåŠ¨å¯ç§»é™¤ä¸´æ—¶ç›®å½•ï¼Œä¿è¯åœ¨æµ‹è¯•ç»“æŸæ—¶è¢«åˆ é™¤ã€‚
    
        1. åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„ä¸´æ—¶ç›®å½•ï¼š
    
        ```py
        def test_whatever(self):
            tmp_dir = self.get_auto_remove_tmp_dir()
        ```
    
        `tmp_dir` å°†åŒ…å«åˆ›å»ºçš„ä¸´æ—¶ç›®å½•çš„è·¯å¾„ã€‚å®ƒå°†åœ¨æµ‹è¯•ç»“æŸæ—¶è‡ªåŠ¨åˆ é™¤ã€‚
    
    
        2. åˆ›å»ºæˆ‘é€‰æ‹©çš„ä¸´æ—¶ç›®å½•ï¼Œåœ¨æµ‹è¯•å¼€å§‹å‰ç¡®ä¿å®ƒä¸ºç©ºï¼Œå¹¶åœ¨æµ‹è¯•ç»“æŸåä¸æ¸…ç©ºå®ƒã€‚
    
        ```py
        def test_whatever(self):
            tmp_dir = self.get_auto_remove_tmp_dir("./xxx")
        ```
    
        å½“æ‚¨å¸Œæœ›ç›‘è§†ç‰¹å®šç›®å½•å¹¶ç¡®ä¿ä»¥å‰çš„æµ‹è¯•æœªåœ¨å…¶ä¸­ç•™ä¸‹ä»»ä½•æ•°æ®æ—¶ï¼Œè¿™æ˜¯æœ‰ç”¨çš„ã€‚
    
        3. æ‚¨å¯ä»¥é€šè¿‡ç›´æ¥è¦†ç›– `before` å’Œ `after` å‚æ•°æ¥è¦†ç›–å‰ä¸¤ä¸ªé€‰é¡¹ï¼Œå¯¼è‡´ä»¥ä¸‹è¡Œä¸ºï¼š
    
        `before=True`ï¼šä¸´æ—¶ç›®å½•å°†å§‹ç»ˆåœ¨æµ‹è¯•å¼€å§‹æ—¶æ¸…é™¤ã€‚
    
        `before=False`ï¼šå¦‚æœä¸´æ—¶ç›®å½•å·²å­˜åœ¨ï¼Œåˆ™ä»»ä½•ç°æœ‰æ–‡ä»¶å°†ä¿ç•™åœ¨å…¶ä¸­ã€‚
    
        `after=True`ï¼šä¸´æ—¶ç›®å½•å°†å§‹ç»ˆåœ¨æµ‹è¯•ç»“æŸæ—¶åˆ é™¤ã€‚
    
        `after=False`ï¼šä¸´æ—¶ç›®å½•å°†å§‹ç»ˆåœ¨æµ‹è¯•ç»“æŸæ—¶ä¿æŒä¸å˜ã€‚
    
        æ³¨æ„ 1ï¼šä¸ºäº†å®‰å…¨è¿è¡Œç­‰åŒäº `rm -r` çš„æ“ä½œï¼Œåªå…è®¸ä½¿ç”¨æ˜¾å¼ `tmp_dir` çš„é¡¹ç›®ä»“åº“æ£€å‡ºçš„å­ç›®å½•ï¼Œä»¥ä¾¿ä¸ä¼šæ„å¤–åœ°æ¸…ç† `/tmp` æˆ–ç±»ä¼¼çš„æ–‡ä»¶ç³»ç»Ÿçš„é‡è¦éƒ¨åˆ†ã€‚å³è¯·å§‹ç»ˆä¼ é€’ä»¥ `./` å¼€å¤´çš„è·¯å¾„ã€‚
    
        æ³¨æ„ 2ï¼šæ¯ä¸ªæµ‹è¯•éƒ½å¯ä»¥æ³¨å†Œå¤šä¸ªä¸´æ—¶ç›®å½•ï¼Œå¹¶ä¸”é™¤éå¦æœ‰è¦æ±‚ï¼Œå¦åˆ™å®ƒä»¬éƒ½å°†è¢«è‡ªåŠ¨åˆ é™¤ã€‚
    
        åŠŸèƒ½ 3: è·å–è®¾ç½®äº†ç‰¹å®šäºå½“å‰æµ‹è¯•å¥—ä»¶çš„ `PYTHONPATH` çš„ `os.environ` å¯¹è±¡çš„å‰¯æœ¬ã€‚è¿™
        """
        def __init__(self):
            # åˆå§‹åŒ–æµ‹è¯•è·¯å¾„å¯¹è±¡
            self._initialize_test_paths()
    
        def _initialize_test_paths(self):
            # åˆå§‹åŒ–æµ‹è¯•è·¯å¾„
            self.test_file_path = Path(__file__).resolve()
            # å½“å‰æµ‹è¯•æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
            self.test_file_dir = self.test_file_path.parent
            # `tests` æµ‹è¯•å¥—ä»¶çš„ç›®å½•
            self.tests_dir = self.test_file_dir.parent
            # `examples` æµ‹è¯•å¥—ä»¶çš„ç›®å½•
            self.examples_dir = self.tests_dir / "examples"
            # ä»“åº“çš„ç›®å½•
            self.repo_root_dir = self.tests_dir.parent
            # `src` çš„ç›®å½•
            self.src_dir = self.repo_root_dir / "src"
    
            # å°†è·¯å¾„è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            self.test_file_path_str = str(self.test_file_path)
            self.test_file_dir_str = str(self.test_file_dir)
            self.tests_dir_str = str(self.tests_dir)
            self.examples_dir_str = str(self.examples_dir)
            self.repo_root_dir_str = str(self.repo_root_dir)
            self.src_dir_str = str(self.src_dir)
    
        def get_auto_remove_tmp_dir(self, path=None, before=True, after=True):
            # è·å–è‡ªåŠ¨å¯ç§»é™¤ä¸´æ—¶ç›®å½•
            tmp_dir = TemporaryDirectory(prefix="tmp_", dir=path)
            # è¿”å›ä¸´æ—¶ç›®å½•è·¯å¾„
            return tmp_dir.name
    def test_whatever(self):
        # è·å–è®¾ç½®å¥½çš„ç¯å¢ƒå˜é‡
        env = self.get_env()
    ```py

    def setUp(self):
        # get_auto_remove_tmp_dir feature:
        # åˆå§‹åŒ–ç”¨äºè‡ªåŠ¨æ¸…ç†ä¸´æ—¶ç›®å½•çš„åˆ—è¡¨
        self.teardown_tmp_dirs = []

        # è·å–æµ‹è¯•æ–‡ä»¶æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
        self._test_file_path = inspect.getfile(self.__class__)
        path = Path(self._test_file_path).resolve()
        # è·å–æµ‹è¯•æ–‡ä»¶çš„çˆ¶ç›®å½•
        self._test_file_dir = path.parents[0]
        # é€šè¿‡è¿­ä»£æŸ¥æ‰¾é¡¹ç›®çš„æ ¹ç›®å½•
        for up in [1, 2, 3]:
            tmp_dir = path.parents[up]
            # åˆ¤æ–­æ˜¯å¦æ‰¾åˆ°äº†æ ¹ç›®å½•
            if (tmp_dir / "src").is_dir() and (tmp_dir / "tests").is_dir():
                break
        # å¦‚æœæ‰¾åˆ°æ ¹ç›®å½•ï¼Œåˆ™è®¾ç½®æ ¹ç›®å½•è·¯å¾„ï¼›å¦åˆ™ï¼ŒæŠ›å‡ºå¼‚å¸¸
        if tmp_dir:
            self._repo_root_dir = tmp_dir
        else:
            raise ValueError(f"can't figure out the root of the repo from {self._test_file_path}")
        # è®¾ç½®æµ‹è¯•ã€ç¤ºä¾‹å’Œæºä»£ç ç›®å½•è·¯å¾„
        self._tests_dir = self._repo_root_dir / "tests"
        self._examples_dir = self._repo_root_dir / "examples"
        self._src_dir = self._repo_root_dir / "src"

    @property
    def test_file_path(self):
        # è¿”å›æµ‹è¯•æ–‡ä»¶è·¯å¾„
        return self._test_file_path

    @property
    def test_file_path_str(self):
        # è¿”å›æµ‹è¯•æ–‡ä»¶è·¯å¾„çš„å­—ç¬¦ä¸²å½¢å¼
        return str(self._test_file_path)

    @property
    def test_file_dir(self):
        # è¿”å›æµ‹è¯•æ–‡ä»¶æ‰€åœ¨ç›®å½•
        return self._test_file_dir

    @property
    def test_file_dir_str(self):
        # è¿”å›æµ‹è¯•æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„å­—ç¬¦ä¸²å½¢å¼
        return str(self._test_file_dir)

    @property
    def tests_dir(self):
        # è¿”å›æµ‹è¯•ç›®å½•è·¯å¾„
        return self._tests_dir

    @property
    def tests_dir_str(self):
        # è¿”å›æµ‹è¯•ç›®å½•è·¯å¾„çš„å­—ç¬¦ä¸²å½¢å¼
        return str(self._tests_dir)

    @property
    def examples_dir(self):
        # è¿”å›ç¤ºä¾‹ç›®å½•è·¯å¾„
        return self._examples_dir

    @property
    def examples_dir_str(self):
        # è¿”å›ç¤ºä¾‹ç›®å½•è·¯å¾„çš„å­—ç¬¦ä¸²å½¢å¼
        return str(self._examples_dir)

    @property
    def repo_root_dir(self):
        # è¿”å›é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        return self._repo_root_dir

    @property
    def repo_root_dir_str(self):
        # è¿”å›é¡¹ç›®æ ¹ç›®å½•è·¯å¾„çš„å­—ç¬¦ä¸²å½¢å¼
        return str(self._repo_root_dir)

    @property
    def src_dir(self):
        # è¿”å›æºä»£ç ç›®å½•è·¯å¾„
        return self._src_dir

    @property
    def src_dir_str(self):
        # è¿”å›æºä»£ç ç›®å½•è·¯å¾„çš„å­—ç¬¦ä¸²å½¢å¼
        return str(self._src_dir)

    def get_env(self):
        """
        Return a copy of the `os.environ` object that sets up `PYTHONPATH` correctly, depending on the test suite it's
        invoked from. This is useful for invoking external programs from the test suite - e.g. distributed training.

        It always inserts `./src` first, then `./tests` or `./examples` depending on the test suite type and finally
        the preset `PYTHONPATH` if any (all full resolved paths).

        """
        # å¤åˆ¶å½“å‰ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        # æ„å»ºæ­£ç¡®è®¾ç½®äº† `PYTHONPATH` çš„ç¯å¢ƒå˜é‡
        paths = [self.src_dir_str]
        # æ ¹æ®æµ‹è¯•å¥—ä»¶ç±»å‹æ’å…¥ `./tests` æˆ– `./examples`
        if "/examples" in self.test_file_dir_str:
            paths.append(self.examples_dir_str)
        else:
            paths.append(self.tests_dir_str)
        # æ’å…¥é¢„è®¾çš„ `PYTHONPATH`ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œå…¨éƒ½æ˜¯å®Œå…¨è§£æçš„è·¯å¾„ï¼‰
        paths.append(env.get("PYTHONPATH", ""))

        # å°†è·¯å¾„åˆ—è¡¨æ‹¼æ¥æˆå­—ç¬¦ä¸²ï¼Œå¹¶è®¾ç½®åˆ°ç¯å¢ƒå˜é‡ä¸­
        env["PYTHONPATH"] = ":".join(paths)
        return env
    def get_auto_remove_tmp_dir(self, tmp_dir=None, before=None, after=None):
        """
        Args:
            tmp_dir (`string`, *optional*):
                if `None`:
                    - a unique temporary path will be created
                    - sets `before=True` if `before` is `None`
                    - sets `after=True` if `after` is `None`
                else:
                    - `tmp_dir` will be created
                    - sets `before=True` if `before` is `None`
                    - sets `after=False` if `after` is `None`
            before (`bool`, *optional*):
                If `True` and the `tmp_dir` already exists, make sure to empty it right away if `False` and the
                `tmp_dir` already exists, any existing files will remain there.
            after (`bool`, *optional*):
                If `True`, delete the `tmp_dir` at the end of the test if `False`, leave the `tmp_dir` and its contents
                intact at the end of the test.

        Returns:
            tmp_dir(`string`): either the same value as passed via *tmp_dir* or the path to the auto-selected tmp dir
        """
        if tmp_dir is not None:
            # å®šä¹‰è‡ªå®šä¹‰è·¯å¾„æä¾›æ—¶æœ€å¯èƒ½çš„æœŸæœ›è¡Œä¸ºã€‚
            # è¿™å¾ˆå¯èƒ½è¡¨ç¤ºè°ƒè¯•æ¨¡å¼ï¼Œæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªæ˜“äºå®šä½çš„ç›®å½•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
            # 1. åœ¨æµ‹è¯•ä¹‹å‰æ¸…é™¤ï¼ˆå¦‚æœå·²ç»å­˜åœ¨ï¼‰
            # 2. åœ¨æµ‹è¯•ç»“æŸåä¿ç•™
            if before is None:
                before = True
            if after is None:
                after = False

            # ä½¿ç”¨æä¾›çš„è·¯å¾„
            path = Path(tmp_dir).resolve()

            # ä¸ºäº†é¿å…ç ´åæ–‡ä»¶ç³»ç»Ÿçš„éƒ¨åˆ†ï¼Œåªå…è®¸ç›¸å¯¹è·¯å¾„
            if not tmp_dir.startswith("./"):
                raise ValueError(
                    f"`tmp_dir` can only be a relative path, i.e. `./some/path`, but received `{tmp_dir}`"
                )

            # ç¡®ä¿ç›®å½•èµ·å§‹ä¸ºç©º
            if before is True and path.exists():
                shutil.rmtree(tmp_dir, ignore_errors=True)

            path.mkdir(parents=True, exist_ok=True)

        else:
            # å®šä¹‰è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€ä¸´æ—¶è·¯å¾„æ—¶æœ€å¯èƒ½çš„æœŸæœ›è¡Œä¸ºï¼ˆä¸æ˜¯è°ƒè¯•æ¨¡å¼ï¼‰ã€‚
            # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå”¯ä¸€çš„ä¸´æ—¶ç›®å½•ï¼š
            # 1. åœ¨æµ‹è¯•ä¹‹å‰ä¸ºç©ºï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†å§‹ç»ˆä¸ºç©ºï¼‰
            # 2. åœ¨æµ‹è¯•ç»“æŸåå®Œå…¨åˆ é™¤
            if before is None:
                before = True
            if after is None:
                after = True

            # ä½¿ç”¨å”¯ä¸€çš„ä¸´æ—¶ç›®å½•ï¼ˆå§‹ç»ˆä¸ºç©ºï¼Œä¸ç®¡ `before` å¦‚ä½•ï¼‰
            tmp_dir = tempfile.mkdtemp()

        if after is True:
            # æ³¨å†Œä»¥è¿›è¡Œåˆ é™¤
            self.teardown_tmp_dirs.append(tmp_dir)

        return tmp_dir
    def python_one_liner_max_rss(self, one_liner_str):
        """
        Runs the passed python one liner (just the code) and returns how much max cpu memory was used to run the
        program.

        Args:
            one_liner_str (`string`):
                a python one liner code that gets passed to `python -c`

        Returns:
            max cpu memory bytes used to run the program. This value is likely to vary slightly from run to run.

        Requirements:
            this helper needs `/usr/bin/time` to be installed (`apt install time`)

        Example:

        ```
        one_liner_str = 'from transformers import AutoModel; AutoModel.from_pretrained("t5-large")'
        max_rss = self.python_one_liner_max_rss(one_liner_str)
        ```py
        """

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ /usr/bin/time å‘½ä»¤
        if not cmd_exists("/usr/bin/time"):
            raise ValueError("/usr/bin/time is required, install with `apt install time`")

        # å°†å‘½ä»¤å­—ç¬¦ä¸²è§£æä¸ºåˆ—è¡¨
        cmd = shlex.split(f"/usr/bin/time -f %M python -c '{one_liner_str}'")
        # ä½¿ç”¨ CaptureStd ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ•è·æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯
        with CaptureStd() as cs:
            # å¼‚æ­¥æ‰§è¡Œå­è¿›ç¨‹
            execute_subprocess_async(cmd, env=self.get_env())
        # è·å–æœ€å¤§ RSSï¼ˆResident Set Sizeï¼‰å¹¶è½¬æ¢ä¸ºå­—èŠ‚
        max_rss = int(cs.err.split("\n")[-2].replace("stderr: ", "")) * 1024
        return max_rss

    def tearDown(self):
        # get_auto_remove_tmp_dir feature: remove registered temp dirs
        # éå†éœ€è¦æ¸…ç†çš„ä¸´æ—¶ç›®å½•åˆ—è¡¨ï¼Œåˆ é™¤ç›®å½•
        for path in self.teardown_tmp_dirs:
            shutil.rmtree(path, ignore_errors=True)
        self.teardown_tmp_dirs = []
        # å¦‚æœåŠ é€Ÿå™¨å¯ç”¨ï¼Œåˆ™é‡ç½®çŠ¶æ€
        if is_accelerate_available():
            AcceleratorState._reset_state()
            PartialState._reset_state()

            # åˆ é™¤æ‰€æœ‰ç¯å¢ƒå˜é‡ä¸­åŒ…å« `ACCELERATE` çš„å˜é‡
            for k in list(os.environ.keys()):
                if "ACCELERATE" in k:
                    del os.environ[k]
# å®šä¹‰ä¸€ä¸ªä¾¿æ·çš„åŒ…è£…å™¨ï¼Œå…è®¸åœ¨æµ‹è¯•å‡½æ•°ä¸­æ–¹ä¾¿åœ°è®¾ç½®ç¯å¢ƒå˜é‡
def mockenv(**kwargs):
    """
   this is a convenience wrapper, that allows this ::

   @mockenv(RUN_SLOW=True, USE_TF=False) def test_something():
        run_slow = os.getenv("RUN_SLOW", False) use_tf = os.getenv("USE_TF", False)

   """
    return mock.patch.dict(os.environ, kwargs)


# ä¸´æ—¶æ›´æ–° `os.environ` å­—å…¸çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç±»ä¼¼äº mockenv
@contextlib.contextmanager
def mockenv_context(*remove, **update):
    """
    Temporarily updates the `os.environ` dictionary in-place. Similar to mockenv

    The `os.environ` dictionary is updated in-place so that the modification is sure to work in all situations.

    Args:
      remove: Environment variables to remove.
      update: Dictionary of environment variables and values to add/update.
    """
    env = os.environ
    update = update or {}
    remove = remove or []

    # è¢«æ›´æ–°æˆ–åˆ é™¤çš„ç¯å¢ƒå˜é‡åˆ—è¡¨
    stomped = (set(update.keys()) | set(remove)) & set(env.keys())
    # é€€å‡ºæ—¶éœ€è¦æ¢å¤çš„ç¯å¢ƒå˜é‡å’Œå€¼
    update_after = {k: env[k] for k in stomped}
    # é€€å‡ºæ—¶éœ€è¦åˆ é™¤çš„ç¯å¢ƒå˜é‡
    remove_after = frozenset(k for k in update if k not in env)

    try:
        env.update(update)
        [env.pop(k, None) for k in remove]
        yield
    finally:
        env.update(update_after)
        [env.pop(k) for k in remove_after]


# --- pytest conf functions --- #

# é¿å…ä» tests/conftest.py å’Œ examples/conftest.py å¤šæ¬¡è°ƒç”¨ - ç¡®ä¿åªè°ƒç”¨ä¸€æ¬¡
pytest_opt_registered = {}


def pytest_addoption_shared(parser):
    """
    This function is to be called from `conftest.py` via `pytest_addoption` wrapper that has to be defined there.

    It allows loading both `conftest.py` files at once without causing a failure due to adding the same `pytest`
    option.

    """
    option = "--make-reports"
    if option not in pytest_opt_registered:
        parser.addoption(
            option,
            action="store",
            default=False,
            help="generate report files. The value of this option is used as a prefix to report names",
        )
        pytest_opt_registered[option] = 1


def pytest_terminal_summary_main(tr, id):
    """
    Generate multiple reports at the end of test suite run - each report goes into a dedicated file in the current
    directory. The report files are prefixed with the test suite name.

    This function emulates --duration and -rA pytest arguments.

    This function is to be called from `conftest.py` via `pytest_terminal_summary` wrapper that has to be defined
    there.

    Args:
    - tr: `terminalreporter` passed from `conftest.py`
    - id: unique id like `tests` or `examples` that will be incorporated into the final reports filenames - this is
      needed as some jobs have multiple runs of pytest, so we can't have them overwrite each other.
    # å¯¼å…¥æ‰€éœ€æ¨¡å—ï¼Œæ³¨æ„è¿™é‡Œä½¿ç”¨äº† _pytest çš„ç§æœ‰ APIï¼Œè‹¥ pytest è¿›è¡Œå†…éƒ¨æ›´æ”¹å¯èƒ½ä¼šå¯¼è‡´è¯¥åŠŸèƒ½å¤±æ•ˆï¼›åŒæ—¶ï¼Œè°ƒç”¨äº† terminalreporter çš„é»˜è®¤å†…éƒ¨æ–¹æ³•ï¼Œå¯èƒ½ä¼šè¢«å„ç§ `pytest-` æ’ä»¶åŠ«æŒè€Œäº§ç”Ÿå¹²æ‰°ã€‚
    from _pytest.config import create_terminal_writer
    
    # å¦‚æœ id ä¸ºç©ºï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º "tests"
    if not len(id):
        id = "tests"
    
    # è·å– terminalreporter å¯¹åº”çš„é…ç½®ä¿¡æ¯
    config = tr.config
    
    # è·å–åŸå§‹çš„ç»ˆç«¯å†™å…¥å™¨
    orig_writer = config.get_terminal_writer()
    
    # è·å–åŸå§‹çš„ traceback æ˜¾ç¤ºæ–¹å¼
    orig_tbstyle = config.option.tbstyle
    
    # è·å– terminalreporter çš„åŸå§‹æŠ¥å‘Šå­—ç¬¦
    orig_reportchars = tr.reportchars
    
    # åˆ›å»ºæŠ¥å‘Šä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
    dir = f"reports/{id}"
    Path(dir).mkdir(parents=True, exist_ok=True)
    
    # å®šä¹‰ä¸åŒç±»å‹æŠ¥å‘Šçš„æ–‡ä»¶ååŠè·¯å¾„
    report_files = {
        k: f"{dir}/{k}.txt"
        for k in [
            "durations",
            "errors",
            "failures_long",
            "failures_short",
            "failures_line",
            "passes",
            "stats",
            "summary_short",
            "warnings",
        ]
    }
    
    # è‡ªå®šä¹‰è€—æ—¶æŠ¥å‘Š
    # æ³¨æ„ï¼šä¸éœ€è¦è°ƒç”¨ pytest --durations=XX æ¥è·å–å•ç‹¬çš„æŠ¥å‘Š
    # é€‚é…è‡ª https://github.com/pytest-dev/pytest/blob/897f151e/src/_pytest/runner.py#L66
    dlist = []
    for replist in tr.stats.values():
        for rep in replist:
            if hasattr(rep, "duration"):
                dlist.append(rep)
    if dlist:
        dlist.sort(key=lambda x: x.duration, reverse=True)
        with open(report_files["durations"], "w") as f:
            durations_min = 0.05  # sec
            f.write("slowest durations\n")
            for i, rep in enumerate(dlist):
                if rep.duration < durations_min:
                    f.write(f"{len(dlist)-i} durations < {durations_min} secs were omitted")
                    break
                f.write(f"{rep.duration:02.2f}s {rep.when:<8} {rep.nodeid}\n")
    
    # å®šä¹‰ç®€çŸ­å¤±è´¥æŠ¥å‘Š
    def summary_failures_short(tr):
        # æœŸæœ›æŠ¥å‘Šä¸º --tb=long (é»˜è®¤) æ ¼å¼ï¼Œæ­¤å¤„å°†å…¶æˆªæ–­è‡³æœ€åä¸€å¸§
        reports = tr.getreports("failed")
        if not reports:
            return
        tr.write_sep("=", "FAILURES SHORT STACK")
        for rep in reports:
            msg = tr._getfailureheadline(rep)
            tr.write_sep("_", msg, red=True, bold=True)
            # æˆªæ–­å¯é€‰çš„é¢å¤–å‰å¯¼å¸§ï¼Œåªä¿ç•™æœ€åä¸€å¸§
            longrepr = re.sub(r".*_ _ _ (_ ){10,}_ _ ", "", rep.longreprtext, 0, re.M | re.S)
            tr._tw.line(longrepr)
            # æ³¨æ„ï¼šä¸æ‰“å°ä»»ä½• rep.sectionsï¼Œä»¥ä¿æŒæŠ¥å‘Šç®€æ´
    
    # ä½¿ç”¨é¢„å…ˆå‡†å¤‡å¥½çš„æŠ¥å‘Šå‡½æ•°ï¼Œå°†æ—¥å¿—è¾“å‡ºåˆ°ä¸“ç”¨æ–‡ä»¶ä¸­
    # é€‚é…è‡ª https://github.com/pytest-dev/pytest/blob/897f151e/src/_pytest/terminal.py#L814
    # æ³¨æ„ï¼šæŸäº› pytest æ’ä»¶å¯èƒ½ä¼šé€šè¿‡åŠ«æŒé»˜è®¤çš„ `terminalreporter` æ¥äº§ç”Ÿå¹²æ‰°
    # æŠ¥å‘Šå¤±è´¥æ—¶ä½¿ç”¨ line/short/long æ ·å¼
    config.option.tbstyle = "auto"  # å…¨éƒ¨ traceback æ˜¾ç¤º
    # å°†å¤±è´¥é•¿æŠ¥å‘Šå†™å…¥æ–‡ä»¶
    with open(report_files["failures_long"], "w") as f:
        # åˆ›å»ºç»ˆç«¯å†™å…¥å™¨å¹¶å°†å…¶é…ç½®ä¸ºå†™å…¥æ–‡ä»¶
        tr._tw = create_terminal_writer(config, f)
        # æ±‡æ€»å¤±è´¥
        tr.summary_failures()
    
    # å°†å¤±è´¥çŸ­æŠ¥å‘Šå†™å…¥æ–‡ä»¶
    with open(report_files["failures_short"], "w") as f:
        # åˆ›å»ºç»ˆç«¯å†™å…¥å™¨å¹¶å°†å…¶é…ç½®ä¸ºå†™å…¥æ–‡ä»¶
        tr._tw = create_terminal_writer(config, f)
        # ç®€çŸ­æ±‡æ€»å¤±è´¥
        summary_failures_short(tr)
    
    # å°†å¤±è´¥è¡ŒæŠ¥å‘Šå†™å…¥æ–‡ä»¶
    config.option.tbstyle = "line"  # æ¯ä¸ªé”™è¯¯ä¸€è¡Œ
    with open(report_files["failures_line"], "w") as f:
        # åˆ›å»ºç»ˆç«¯å†™å…¥å™¨å¹¶å°†å…¶é…ç½®ä¸ºå†™å…¥æ–‡ä»¶
        tr._tw = create_terminal_writer(config, f)
        # æ±‡æ€»å¤±è´¥
        tr.summary_failures()
    
    # å°†é”™è¯¯æŠ¥å‘Šå†™å…¥æ–‡ä»¶
    with open(report_files["errors"], "w") as f:
        # åˆ›å»ºç»ˆç«¯å†™å…¥å™¨å¹¶å°†å…¶é…ç½®ä¸ºå†™å…¥æ–‡ä»¶
        tr._tw = create_terminal_writer(config, f)
        # æ±‡æ€»é”™è¯¯
        tr.summary_errors()
    
    # å°†è­¦å‘ŠæŠ¥å‘Šå†™å…¥æ–‡ä»¶
    with open(report_files["warnings"], "w") as f:
        # åˆ›å»ºç»ˆç«¯å†™å…¥å™¨å¹¶å°†å…¶é…ç½®ä¸ºå†™å…¥æ–‡ä»¶
        tr._tw = create_terminal_writer(config, f)
        # æ±‡æ€»æ™®é€šè­¦å‘Š
        tr.summary_warnings()
        # æ±‡æ€»æœ€ç»ˆè­¦å‘Š
        tr.summary_warnings()
    
    # è®¾ç½®æŠ¥å‘Šå­—ç¬¦ä»¥æ¨¡æ‹Ÿ `-rA`ï¼ˆç”¨äº summary_passes() å’Œ short_test_summary() ä¸­ï¼‰
    tr.reportchars = "wPpsxXEf"
    
    # è·³è¿‡ `passes` æŠ¥å‘Šï¼Œå› ä¸ºå®ƒå¼€å§‹èŠ±è´¹è¶…è¿‡ 5 åˆ†é’Ÿï¼Œæœ‰æ—¶åœ¨ CircleCI ä¸Šè¶…æ—¶ï¼Œå¦‚æœèŠ±è´¹ > 10 åˆ†é’Ÿï¼ˆå› ä¸ºæ­¤éƒ¨åˆ†ä¸åœ¨ç»ˆç«¯ä¸Šç”Ÿæˆä»»ä½•è¾“å‡ºï¼‰ã€‚
    # ï¼ˆè€Œä¸”ï¼Œä¼¼ä¹åœ¨æ­¤æŠ¥å‘Šä¸­æ²¡æœ‰æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¾ˆå°‘éœ€è¦é˜…è¯»å®ƒï¼‰
    # with open(report_files["passes"], "w") as f:
    #     tr._tw = create_terminal_writer(config, f)
    #     tr.summary_passes()
    
    # å°†ç®€çŸ­æµ‹è¯•æ‘˜è¦å†™å…¥æ–‡ä»¶
    with open(report_files["summary_short"], "w") as f:
        # åˆ›å»ºç»ˆç«¯å†™å…¥å™¨å¹¶å°†å…¶é…ç½®ä¸ºå†™å…¥æ–‡ä»¶
        tr._tw = create_terminal_writer(config, f)
        # æ±‡æ€»ç®€çŸ­æµ‹è¯•æ‘˜è¦
        tr.short_test_summary()
    
    # å°†ç»Ÿè®¡æ‘˜è¦å†™å…¥æ–‡ä»¶
    with open(report_files["stats"], "w") as f:
        # åˆ›å»ºç»ˆç«¯å†™å…¥å™¨å¹¶å°†å…¶é…ç½®ä¸ºå†™å…¥æ–‡ä»¶
        tr._tw = create_terminal_writer(config, f)
        # æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        tr.summary_stats()
    
    # æ¢å¤:
    # æ¢å¤ç»ˆç«¯å†™å…¥å™¨ä¸ºåŸå§‹å†™å…¥å™¨
    tr._tw = orig_writer
    # æ¢å¤æŠ¥å‘Šå­—ç¬¦ä¸ºåŸå§‹æŠ¥å‘Šå­—ç¬¦
    tr.reportchars = orig_reportchars
    # æ¢å¤ traceback æ ¼å¼ä¸ºåŸå§‹æ ¼å¼
    config.option.tbstyle = orig_tbstyle
# --- distributed testing functions --- #

# å¯¼å…¥ asyncio æ¨¡å—
import asyncio  # noqa

# å®šä¹‰ä¸€ä¸ªç”¨äºå­˜å‚¨å­è¿›ç¨‹è¾“å‡ºçš„ç±»
class _RunOutput:
    def __init__(self, returncode, stdout, stderr):
        self.returncode = returncode
        self.stdout = stdout
        self.stderr = stderr

# å¼‚æ­¥è¯»å–æµçš„å‡½æ•°
async def _read_stream(stream, callback):
    while True:
        line = await stream.readline()
        if line:
            callback(line)
        else:
            break

# å¼‚æ­¥æ‰§è¡Œå­è¿›ç¨‹å¹¶å¤„ç†è¾“å‡ºæµçš„å‡½æ•°
async def _stream_subprocess(cmd, env=None, stdin=None, timeout=None, quiet=False, echo=False) -> _RunOutput:
    if echo:
        print("\nRunning: ", " ".join(cmd))

    # åˆ›å»ºå­è¿›ç¨‹
    p = await asyncio.create_subprocess_exec(
        cmd[0],
        *cmd[1:],
        stdin=stdin,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
        env=env,
    )

    # è¯»å–å­è¿›ç¨‹çš„è¾“å‡ºæµ
    out = []
    err = []

    # å¤„ç†è¾“å‡ºæµçš„å›è°ƒå‡½æ•°
    def tee(line, sink, pipe, label=""):
        line = line.decode("utf-8").rstrip()
        sink.append(line)
        if not quiet:
            print(label, line, file=pipe)

    # å¼‚æ­¥ç­‰å¾…è¾“å‡ºæµçš„å¤„ç†
    await asyncio.wait(
        [
            _read_stream(p.stdout, lambda l: tee(l, out, sys.stdout, label="stdout:")),
            _read_stream(p.stderr, lambda l: tee(l, err, sys.stderr, label="stderr:")),
        ],
        timeout=timeout,
    )
    return _RunOutput(await p.wait(), out, err)

# åŒæ­¥æ‰§è¡Œå­è¿›ç¨‹çš„å‡½æ•°
def execute_subprocess_async(cmd, env=None, stdin=None, timeout=180, quiet=False, echo=True) -> _RunOutput:
    loop = asyncio.get_event_loop()
    result = loop.run_until_complete(
        _stream_subprocess(cmd, env=env, stdin=stdin, timeout=timeout, quiet=quiet, echo=echo)
    )

    cmd_str = " ".join(cmd)
    if result.returncode > 0:
        stderr = "\n".join(result.stderr)
        raise RuntimeError(
            f"'{cmd_str}' failed with returncode {result.returncode}\n\n"
            f"The combined stderr from workers follows:\n{stderr}"
        )

    # æ£€æŸ¥å­è¿›ç¨‹æ˜¯å¦æœ‰è¾“å‡º
    if not result.stdout and not result.stderr:
        raise RuntimeError(f"'{cmd_str}' produced no output.")

    return result

# è¿”å› pytest-xdist çš„å·¥ä½œè¿›ç¨‹ç¼–å·
def pytest_xdist_worker_id():
    """
    Returns an int value of worker's numerical id under `pytest-xdist`'s concurrent workers `pytest -n N` regime, or 0
    if `-n 1` or `pytest-xdist` isn't being used.
    """
    # è·å–ç¯å¢ƒå˜é‡ "PYTEST_XDIST_WORKER" çš„å€¼ï¼Œè‹¥ä¸å­˜åœ¨åˆ™é»˜è®¤ä¸º "gw0"
    worker = os.environ.get("PYTEST_XDIST_WORKER", "gw0")
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å­—ç¬¦ä¸²ä¸­ä»¥ "gw" å¼€å¤´çš„éƒ¨åˆ†ä¸ºç©ºå­—ç¬¦ä¸²
    worker = re.sub(r"^gw", "", worker, 0, re.M)
    # å°†ç»“æœè½¬æ¢ä¸ºæ•´æ•°ç±»å‹å¹¶è¿”å›
    return int(worker)
# è¿”å›ä¸€ä¸ªå¯ä»¥ä¼ é€’ç»™ `torch.distributed.launch` çš„ `--master_port` å‚æ•°çš„ç«¯å£å·
def get_torch_dist_unique_port():
    port = 29500
    # å¦‚æœåœ¨ `pytest-xdist` ä¸‹è¿è¡Œï¼Œæ ¹æ® worker id æ·»åŠ ä¸€ä¸ªå¢é‡ï¼Œä»¥é¿å…å¹¶å‘æµ‹è¯•å°è¯•åŒæ—¶ä½¿ç”¨ç›¸åŒçš„ç«¯å£
    uniq_delta = pytest_xdist_worker_id()
    return port + uniq_delta


# ç®€åŒ–å¯¹è±¡ï¼Œå°†æµ®ç‚¹æ•°å››èˆäº”å…¥ï¼Œå°†å¼ é‡/NumPy æ•°ç»„é™çº§ä»¥ä¾¿åœ¨æµ‹è¯•ä¸­è¿›è¡Œç®€å•çš„ç›¸ç­‰æ€§æ£€æŸ¥
def nested_simplify(obj, decimals=3):
    import numpy as np

    if isinstance(obj, list):
        return [nested_simplify(item, decimals) for item in obj]
    if isinstance(obj, tuple):
        return tuple([nested_simplify(item, decimals) for item in obj])
    elif isinstance(obj, np.ndarray):
        return nested_simplify(obj.tolist())
    elif isinstance(obj, Mapping):
        return {nested_simplify(k, decimals): nested_simplify(v, decimals) for k, v in obj.items()}
    elif isinstance(obj, (str, int, np.int64)):
        return obj
    elif obj is None:
        return obj
    elif is_torch_available() and isinstance(obj, torch.Tensor):
        return nested_simplify(obj.tolist(), decimals)
    elif is_tf_available() and tf.is_tensor(obj):
        return nested_simplify(obj.numpy().tolist())
    elif isinstance(obj, float):
        return round(obj, decimals)
    elif isinstance(obj, (np.int32, np.float32)):
        return nested_simplify(obj.item(), decimals)
    else:
        raise Exception(f"Not supported: {type(obj)}")


# æ£€æŸ¥ JSON æ–‡ä»¶æ˜¯å¦å…·æœ‰æ­£ç¡®çš„æ ¼å¼
def check_json_file_has_correct_format(file_path):
    with open(file_path, "r") as f:
        lines = f.readlines()
        if len(lines) == 1:
            # å¦‚æœé•¿åº¦ä¸º 1ï¼Œåˆ™å­—å…¸ä¸ºç©º
            assert lines[0] == "{}"
        else:
            # å¦åˆ™ç¡®ä¿ JSON æ ¼å¼æ­£ç¡®ï¼ˆè‡³å°‘æœ‰ 3 è¡Œï¼‰
            assert len(lines) >= 3
            # æ¯ä¸ªé”®ä¸€è¡Œï¼Œç¼©è¿›åº”ä¸º 2ï¼Œæœ€å°é•¿åº¦ä¸º 3
            assert lines[0].strip() == "{"
            for line in lines[1:-1]:
                left_indent = len(lines[1]) - len(lines[1].lstrip())
                assert left_indent == 2
            assert lines[-1].strip() == "}"


# å°†è¾“å…¥è½¬æ¢ä¸ºäºŒå…ƒç»„
def to_2tuple(x):
    if isinstance(x, collections.abc.Iterable):
        return x
    return (x, x)


# è¿™äº›å·¥å…·ä¸ç¡®ä¿åœ¨è¿è¡Œè„šæœ¬æ—¶æ¥æ”¶åˆ°æ­£ç¡®çš„é”™è¯¯æ¶ˆæ¯æœ‰å…³
class SubprocessCallException(Exception):
    pass


# è¿è¡Œ `command`ï¼Œä½¿ç”¨ `subprocess.check_output`ï¼Œå¯èƒ½è¿”å› `stdout`ã€‚è¿˜å°†æ­£ç¡®æ•è·è¿è¡Œ `command` æ—¶æ˜¯å¦å‘ç”Ÿé”™è¯¯
def run_command(command: List[str], return_stdout=False):
    try:
        output = subprocess.check_output(command, stderr=subprocess.STDOUT)
        if return_stdout:
            if hasattr(output, "decode"):
                output = output.decode("utf-8")
            return output
    # æ•è·å­è¿›ç¨‹è°ƒç”¨æ—¶å¯èƒ½æŠ›å‡ºçš„å¼‚å¸¸ï¼Œå­˜å‚¨åœ¨å˜é‡eä¸­
    except subprocess.CalledProcessError as e:
        # æŠ›å‡ºè‡ªå®šä¹‰çš„SubprocessCallExceptionå¼‚å¸¸ï¼Œå¹¶ä¼ é€’é”™è¯¯ä¿¡æ¯
        raise SubprocessCallException(
            # æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼ŒåŒ…å«å¤±è´¥çš„å‘½ä»¤å’Œé”™è¯¯è¾“å‡º
            f"Command `{' '.join(command)}` failed with the following error:\n\n{e.output.decode()}"
        ) from e
class RequestCounter:
    """
    Helper class that will count all requests made online.

    Might not be robust if urllib3 changes its logging format but should be good enough for us.

    Usage:
    ```py
    with RequestCounter() as counter:
        _ = AutoTokenizer.from_pretrained("hf-internal-testing/tiny-random-bert")
    assert counter["GET"] == 0
    assert counter["HEAD"] == 1
    assert counter.total_calls == 1
    ```py
    """

    def __enter__(self):
        # åˆå§‹åŒ–è¯·æ±‚è®¡æ•°å™¨å­—å…¸
        self._counter = defaultdict(int)
        # å¼€å§‹æ‹¦æˆª urllib3 çš„ debug æ—¥å¿—
        self.patcher = patch.object(urllib3.connectionpool.log, "debug", wraps=urllib3.connectionpool.log.debug)
        # å¯åŠ¨æ‹¦æˆªå™¨
        self.mock = self.patcher.start()
        return self

    def __exit__(self, *args, **kwargs) -> None:
        # éå†æ‹¦æˆªåˆ°çš„æ¯ä¸ªæ—¥å¿—è°ƒç”¨
        for call in self.mock.call_args_list:
            # æå–æ—¥å¿—å†…å®¹
            log = call.args[0] % call.args[1:]
            # éå† HTTP æ–¹æ³•
            for method in ("HEAD", "GET", "POST", "PUT", "DELETE", "CONNECT", "OPTIONS", "TRACE", "PATCH"):
                # å¦‚æœæ—¥å¿—ä¸­åŒ…å«å½“å‰æ–¹æ³•
                if method in log:
                    # å°†è¯¥æ–¹æ³•è®¡æ•°åŠ ä¸€
                    self._counter[method] += 1
                    break
        # åœæ­¢æ‹¦æˆªå™¨
        self.patcher.stop()

    def __getitem__(self, key: str) -> int:
        # è¿”å›æŒ‡å®šæ–¹æ³•çš„è¯·æ±‚è®¡æ•°
        return self._counter[key]

    @property
    def total_calls(self) -> int:
        # è¿”å›æ‰€æœ‰è¯·æ±‚çš„æ€»è®¡æ•°
        return sum(self._counter.values())


def is_flaky(max_attempts: int = 5, wait_before_retry: Optional[float] = None, description: Optional[str] = None):
    """
    To decorate flaky tests. They will be retried on failures.

    Args:
        max_attempts (`int`, *optional*, defaults to 5):
            The maximum number of attempts to retry the flaky test.
        wait_before_retry (`float`, *optional*):
            If provided, will wait that number of seconds before retrying the test.
        description (`str`, *optional*):
            A string to describe the situation (what / where / why is flaky, link to GH issue/PR comments, errors,
            etc.)
    """

    def decorator(test_func_ref):
        @functools.wraps(test_func_ref)
        def wrapper(*args, **kwargs):
            # åˆå§‹åŒ–é‡è¯•æ¬¡æ•°
            retry_count = 1

            # åœ¨è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°å‰å¾ªç¯
            while retry_count < max_attempts:
                try:
                    # è°ƒç”¨æµ‹è¯•å‡½æ•°
                    return test_func_ref(*args, **kwargs)

                except Exception as err:
                    # è¾“å‡ºé”™è¯¯ä¿¡æ¯å’Œé‡è¯•æ¬¡æ•°
                    print(f"Test failed with {err} at try {retry_count}/{max_attempts}.", file=sys.stderr)
                    # å¦‚æœè®¾ç½®äº†é‡è¯•ç­‰å¾…æ—¶é—´ï¼Œåˆ™ç­‰å¾…æŒ‡å®šæ—¶é—´åå†é‡è¯•
                    if wait_before_retry is not None:
                        time.sleep(wait_before_retry)
                    # å¢åŠ é‡è¯•æ¬¡æ•°
                    retry_count += 1

            # è¿”å›æµ‹è¯•å‡½æ•°çš„æœ€ç»ˆç»“æœ
            return test_func_ref(*args, **kwargs)

        return wrapper

    return decorator


def run_test_in_subprocess(test_case, target_func, inputs=None, timeout=None):
    """
    To run a test in a subprocess. In particular, this can avoid (GPU) memory issue.
    """
    Args:
        test_case (`unittest.TestCase`):
            è¿è¡Œ `target_func` çš„æµ‹è¯•ç”¨ä¾‹ã€‚
        target_func (`Callable`):
            å®ç°å®é™…æµ‹è¯•é€»è¾‘çš„å‡½æ•°ã€‚
        inputs (`dict`, *å¯é€‰*, é»˜è®¤ä¸º `None`):
            é€šè¿‡è¾“å…¥é˜Ÿåˆ—ä¼ é€’ç»™ `target_func` çš„è¾“å…¥ã€‚
        timeout (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`):
            ä¼ é€’ç»™è¾“å…¥å’Œè¾“å‡ºé˜Ÿåˆ—çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™æ£€æŸ¥ç¯å¢ƒå˜é‡ `PYTEST_TIMEOUT`ã€‚å¦‚æœä»ä¸º `None`ï¼Œåˆ™å°†å…¶å€¼è®¾ç½®ä¸º `600`ã€‚

    """
    # å¦‚æœæœªæŒ‡å®šè¶…æ—¶æ—¶é—´ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ `PYTEST_TIMEOUT` çš„å€¼ï¼Œè‹¥æœªå®šä¹‰ï¼Œåˆ™è®¾ä¸º `600`ã€‚
    if timeout is None:
        timeout = int(os.environ.get("PYTEST_TIMEOUT", 600))

    # ä½¿ç”¨ "spawn" æ–¹æ³•åˆ›å»ºå¤šè¿›ç¨‹ä¸Šä¸‹æ–‡ã€‚
    start_methohd = "spawn"
    ctx = multiprocessing.get_context(start_methohd)

    # åˆ›å»ºå®¹é‡ä¸º1çš„è¾“å…¥é˜Ÿåˆ—å’Œè¾“å‡ºé˜Ÿåˆ—ã€‚
    input_queue = ctx.Queue(1)
    output_queue = ctx.JoinableQueue(1)

    # æ— æ³•å°† `unittest.TestCase` å‘é€åˆ°å­è¿›ç¨‹ï¼Œå¦åˆ™ä¼šå‡ºç°æœ‰å…³ pickle çš„é—®é¢˜ã€‚
    # å°†è¾“å…¥æ”¾å…¥è¾“å…¥é˜Ÿåˆ—ã€‚
    input_queue.put(inputs, timeout=timeout)

    # åˆ›å»ºä¸€ä¸ªå­è¿›ç¨‹ï¼Œç›®æ ‡å‡½æ•°ä¸º `target_func`ï¼Œå‚æ•°ä¸ºè¾“å…¥é˜Ÿåˆ—ã€è¾“å‡ºé˜Ÿåˆ—å’Œè¶…æ—¶æ—¶é—´ã€‚
    process = ctx.Process(target=target_func, args=(input_queue, output_queue, timeout))
    process.start()

    # å¦‚æœæ— æ³•åŠæ—¶ä»å­è¿›ç¨‹è·å–è¾“å‡ºï¼Œåˆ™ç»ˆæ­¢å­è¿›ç¨‹ä»¥é˜²æ­¢æµ‹è¯•æ— æ³•æ­£å¸¸é€€å‡ºã€‚
    try:
        # è·å–å­è¿›ç¨‹çš„è¾“å‡ºç»“æœã€‚
        results = output_queue.get(timeout=timeout)
        output_queue.task_done()
    except Exception as e:
        # å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œåˆ™ç»ˆæ­¢å­è¿›ç¨‹å¹¶åœ¨æµ‹è¯•ç”¨ä¾‹ä¸­æ ‡è®°ä¸ºå¤±è´¥ã€‚
        process.terminate()
        test_case.fail(e)

    # ç­‰å¾…å­è¿›ç¨‹ç»ˆæ­¢ã€‚
    process.join(timeout=timeout)

    # å¦‚æœç»“æœä¸­å­˜åœ¨é”™è¯¯ï¼Œåˆ™åœ¨æµ‹è¯•ç”¨ä¾‹ä¸­æ ‡è®°ä¸ºå¤±è´¥ã€‚
    if results["error"] is not None:
        test_case.fail(f'{results["error"]}')
"""
The following contains utils to run the documentation tests without having to overwrite any files.

The `preprocess_string` function adds `# doctest: +IGNORE_RESULT` markers on the fly anywhere a `load_dataset` call is
made as a print would otherwise fail the corresonding line.

To skip cuda tests, make sure to call `SKIP_CUDA_DOCTEST=1 pytest --doctest-modules <path_to_files_to_test>
"""

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåœ¨ä¸è¦†ç›–ä»»ä½•æ–‡ä»¶çš„æƒ…å†µä¸‹è¿è¡Œæ–‡æ¡£æµ‹è¯•
def preprocess_string(string, skip_cuda_tests):
    """Prepare a docstring or a `.md` file to be run by doctest.

    The argument `string` would be the whole file content if it is a `.md` file. For a python file, it would be one of
    its docstring. In each case, it may contain multiple python code examples. If `skip_cuda_tests` is `True` and a
    cuda stuff is detective (with a heuristic), this method will return an empty string so no doctest will be run for
    `string`.
    """
    # å®šä¹‰ä»£ç å—çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    codeblock_pattern = r"(```(?:python|py)\s*\n\s*>>> )((?:.*?\n)*?.*?```py)"
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†å­—ç¬¦ä¸²ï¼Œæå–ä»£ç å—
    codeblocks = re.split(re.compile(codeblock_pattern, flags=re.MULTILINE | re.DOTALL), string)
    # åˆå§‹åŒ– CUDA æ£€æµ‹æ ‡å¿—
    is_cuda_found = False
    # éå†ä»£ç å—åˆ—è¡¨
    for i, codeblock in enumerate(codeblocks):
        # åœ¨ä»£ç å—ä¸­å‘ç° `load_dataset` è°ƒç”¨å¹¶ä¸”æ²¡æœ‰ `# doctest: +IGNORE_RESULT` æ ‡è®°æ—¶ï¼Œåœ¨å…¶åæ·»åŠ æ ‡è®°
        if "load_dataset(" in codeblock and "# doctest: +IGNORE_RESULT" not in codeblock:
            codeblocks[i] = re.sub(r"(>>> .*load_dataset\(.*)", r"\1 # doctest: +IGNORE_RESULT", codeblock)
        # å¦‚æœä»£ç å—åŒ…å« CUDA ç›¸å…³å†…å®¹ï¼Œå¹¶ä¸”éœ€è¦è·³è¿‡ CUDA æµ‹è¯•ï¼Œåˆ™å°† CUDA æ£€æµ‹æ ‡å¿—è®¾ä¸º Trueï¼Œå¹¶é€€å‡ºå¾ªç¯
        if (
            (">>>" in codeblock or "..." in codeblock)
            and re.search(r"cuda|to\(0\)|device=0", codeblock)
            and skip_cuda_tests
        ):
            is_cuda_found = True
            break

    # å¦‚æœæ²¡æœ‰å‘ç° CUDA ç›¸å…³å†…å®¹ï¼Œåˆ™å°†ä¿®æ”¹åçš„ä»£ç å—ç»„åˆæˆå­—ç¬¦ä¸²è¿”å›
    modified_string = ""
    if not is_cuda_found:
        modified_string = "".join(codeblocks)

    return modified_string


# å®šä¹‰ä¸€ä¸ªç±»ï¼Œç»§æ‰¿è‡ª doctest.DocTestParserï¼Œç”¨äºè§£æé»‘è‰²æ ¼å¼çš„ä»£ç å—
class HfDocTestParser(doctest.DocTestParser):
    """
    Overwrites the DocTestParser from doctest to properly parse the codeblocks that are formatted with black. This
    means that there are no extra lines at the end of our snippets. The `# doctest: +IGNORE_RESULT` marker is also
    added anywhere a `load_dataset` call is made as a print would otherwise fail the corresponding line.

    Tests involving cuda are skipped base on a naive pattern that should be updated if it is not enough.
    """
    # This regular expression is used to find doctest examples in a
    # string.  It defines three groups: `source` is the source code
    # (including leading indentation and prompts); `indent` is the
    # indentation of the first (PS1) line of the source code; and
    # `want` is the expected output (including leading indentation).
    # fmt: off
    # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºåŒ¹é…æºä»£ç å’ŒæœŸæœ›è¾“å‡º
    _EXAMPLE_RE = re.compile(r'''
        # Source consists of a PS1 line followed by zero or more PS2 lines.
        (?P<source>
            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line
            (?:\n           [ ]*  \.\.\. .*)*)  # PS2 lines
        \n?
        # Want consists of any non-blank lines that do not start with PS1.
        (?P<want> (?:(?![ ]*$)    # Not a blank line
             (?![ ]*>>>)          # Not a line starting with PS1
             # !!!!!!!!!!! HF Specific !!!!!!!!!!!
             (?:(?!```).)*        # Match any character except '`' until a '```py' is found (this is specific to HF because black removes the last line)
             # !!!!!!!!!!! HF Specific !!!!!!!!!!!
             (?:\n|$)  # Match a new line or end of string
          )*)
        ''', re.MULTILINE | re.VERBOSE
    )
    
    # !!!!!!!!!!! HF Specific !!!!!!!!!!!
    # è®¾ç½®æ˜¯å¦è·³è¿‡ CUDA æµ‹è¯•çš„æ ‡å¿—ï¼Œé€šè¿‡æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®æ¥ç¡®å®š
    skip_cuda_tests: bool = bool(os.environ.get("SKIP_CUDA_DOCTEST", False))
    # !!!!!!!!!!! HF Specific !!!!!!!!!!!
    
    # é‡å†™ `parse` æ–¹æ³•ä»¥åŒ…å«å¯¹ CUDA æµ‹è¯•çš„è·³è¿‡ï¼Œå¹¶åœ¨è°ƒç”¨ `super().parse` å‰ç§»é™¤æ—¥å¿—å’Œæ•°æ®é›†æ‰“å°
    def parse(self, string, name="<string>"):
        """
        Overwrites the `parse` method to incorporate a skip for CUDA tests, and remove logs and dataset prints before
        calling `super().parse`
        """
        # é¢„å¤„ç†å­—ç¬¦ä¸²ï¼Œæ ¹æ®æ˜¯å¦è·³è¿‡ CUDA æµ‹è¯•æ¥å†³å®šæ˜¯å¦ç§»é™¤ CUDA ç›¸å…³çš„ä»£ç 
        string = preprocess_string(string, self.skip_cuda_tests)
        # è°ƒç”¨çˆ¶ç±»çš„è§£ææ–¹æ³•
        return super().parse(string, name)
# å®šä¹‰åä¸º HfDoctestModule çš„ç±»ï¼Œç»§æ‰¿è‡ª Module ç±»
class HfDoctestModule(Module):
    """
    Overwrites the `DoctestModule` of the pytest package to make sure the HFDocTestParser is used when discovering
    tests.
    """
    # é‡å†™äº† pytest åŒ…ä¸­çš„ DoctestModuleï¼Œç¡®ä¿åœ¨å‘ç°æµ‹è¯•æ—¶ä½¿ç”¨ HFDocTestParser
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºæ”¶é›† doctest é¡¹
    def collect(self) -> Iterable[DoctestItem]:
        # å®šä¹‰ä¸€ä¸ªç‰¹æ®Šçš„ doctest finderï¼Œç”¨äºä¿®å¤æ ‡å‡†åº“ä¸­çš„ bug
        class MockAwareDocTestFinder(doctest.DocTestFinder):
            """A hackish doctest finder that overrides stdlib internals to fix a stdlib bug.

            https://github.com/pytest-dev/pytest/issues/3456 https://bugs.python.org/issue25532
            """

            # é‡å†™ _find_lineno æ–¹æ³•ä»¥ä¿®å¤æ ‡å‡†åº“çš„ bug
            def _find_lineno(self, obj, source_lines):
                """Doctest code does not take into account `@property`, this
                is a hackish way to fix it. https://bugs.python.org/issue17446

                Wrapped Doctests will need to be unwrapped so the correct line number is returned. This will be
                reported upstream. #8796
                """
                # å¦‚æœ obj æ˜¯ property ç±»å‹ï¼Œåˆ™å°è¯•è·å–å…¶ fget å±æ€§
                if isinstance(obj, property):
                    obj = getattr(obj, "fget", obj)

                # å¦‚æœ obj æœ‰ __wrapped__ å±æ€§ï¼Œåˆ™è·å–å…¶åŸå§‹å¯¹è±¡
                if hasattr(obj, "__wrapped__"):
                    # è·å–è¢«åŒ…è£…çš„ä¸»è¦å¯¹è±¡ä»¥è·å¾—æ­£ç¡®çš„è¡Œå·
                    obj = inspect.unwrap(obj)

                # Type ignored because this is a private function.
                return super()._find_lineno(  # type:ignore[misc]
                    obj,
                    source_lines,
                )

            # é‡å†™ _find æ–¹æ³•ä»¥ä¿®å¤æ ‡å‡†åº“çš„ bug
            def _find(self, tests, obj, name, module, source_lines, globs, seen) -> None:
                # å¦‚æœ obj æ˜¯è¢«æ¨¡æ‹Ÿçš„ï¼Œåˆ™ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œæµ‹è¯•
                if _is_mocked(obj):
                    return
                # ç”¨ _patch_unwrap_mock_aware() ä¸Šä¸‹æ–‡åŒ…è£…å™¨è§£å†³é—®é¢˜
                with _patch_unwrap_mock_aware():
                    # Type ignored because this is a private function.
                    super()._find(  # type:ignore[misc]
                        tests, obj, name, module, source_lines, globs, seen
                    )

        # å¦‚æœè·¯å¾„çš„åç§°ä¸º "conftest.py"ï¼Œåˆ™ä»é…ç½®çš„æ ¹è·¯å¾„ä¸­å¯¼å…¥ conftest æ¨¡å—
        if self.path.name == "conftest.py":
            module = self.config.pluginmanager._importconftest(
                self.path,
                self.config.getoption("importmode"),
                rootpath=self.config.rootpath,
            )
        else:
            # å¦åˆ™ï¼Œå°è¯•å¯¼å…¥ç»™å®šè·¯å¾„çš„æ¨¡å—
            try:
                module = import_path(
                    self.path,
                    root=self.config.rootpath,
                    mode=self.config.getoption("importmode"),
                )
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯è·³è¿‡è¿˜æ˜¯å¼•å‘ ImportError
            except ImportError:
                if self.config.getvalue("doctest_ignore_import_errors"):
                    skip("unable to import module %r" % self.path)
                else:
                    raise

        # åˆ›å»º MockAwareDocTestFinder å®ä¾‹ï¼Œç”¨äºæŸ¥æ‰¾ doctest
        finder = MockAwareDocTestFinder(parser=HfDocTestParser())
        # è·å–é€‰é¡¹æ ‡å¿—
        optionflags = get_optionflags(self)
        # è·å–æµ‹è¯•è¿è¡Œå™¨
        runner = _get_runner(
            verbose=False,
            optionflags=optionflags,
            checker=_get_checker(),
            continue_on_failure=_get_continue_on_failure(self.config),
        )
        # éå†æ‰¾åˆ°çš„æ‰€æœ‰ doctestï¼Œç”Ÿæˆç›¸åº”çš„ DoctestItem
        for test in finder.find(module, module.__name__):
            # å¦‚æœæµ‹è¯•ä¸­åŒ…å«ç¤ºä¾‹ï¼Œåˆ™ç”Ÿæˆå¯¹åº”çš„ DoctestItem
            if test.examples:  # skip empty doctests and cuda
                yield DoctestItem.from_parent(self, name=test.name, runner=runner, dtest=test)
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®è®¾å¤‡ç±»å‹åˆ†å‘æ‰§è¡Œä¸åŒçš„å‡½æ•°
def _device_agnostic_dispatch(device: str, dispatch_table: Dict[str, Callable], *args, **kwargs):
    # å¦‚æœè®¾å¤‡ä¸åœ¨åˆ†å‘è¡¨ä¸­ï¼Œåˆ™è°ƒç”¨é»˜è®¤å‡½æ•°
    if device not in dispatch_table:
        return dispatch_table["default"](*args, **kwargs)

    # è·å–å¯¹åº”è®¾å¤‡çš„å‡½æ•°
    fn = dispatch_table[device]

    # ä¸€äº›ä¸è®¾å¤‡æ— å…³çš„å‡½æ•°ä¼šè¿”å›å€¼ï¼Œéœ€è¦åœ¨ç”¨æˆ·çº§åˆ«å¤„å¯¹ `None` è¿›è¡Œå¤„ç†
    if fn is None:
        return None
    return fn(*args, **kwargs)


# å¦‚æœ Torch å¯ç”¨
if is_torch_available():
    # è®¾å¤‡åç§°åˆ°å¯è°ƒç”¨å‡½æ•°çš„æ˜ å°„ï¼Œç”¨äºæ”¯æŒè®¾å¤‡æ— å…³æµ‹è¯•
    BACKEND_MANUAL_SEED = {"cuda": torch.cuda.manual_seed, "cpu": torch.manual_seed, "default": torch.manual_seed}
    # æ¸…ç©ºç¼“å­˜çš„å‡½æ•°æ˜ å°„ï¼Œå¯¹ CPU è®¾å¤‡å’Œå…¶ä»–è®¾å¤‡å¤„ç†æ–¹å¼ä¸åŒ
    BACKEND_EMPTY_CACHE = {"cuda": torch.cuda.empty_cache, "cpu": None, "default": None}
    # è®¾å¤‡æ•°é‡æŸ¥è¯¢çš„å‡½æ•°æ˜ å°„ï¼Œå¯¹ CPU å’Œå…¶ä»–è®¾å¤‡çš„å¤„ç†æ–¹å¼ä¸åŒ
    BACKEND_DEVICE_COUNT = {"cuda": torch.cuda.device_count, "cpu": lambda: 0, "default": lambda: 1}


# è®¾ç½®éšæœºç§å­çš„åç«¯å‡½æ•°
def backend_manual_seed(device: str, seed: int):
    return _device_agnostic_dispatch(device, BACKEND_MANUAL_SEED, seed)


# æ¸…ç©ºç¼“å­˜çš„åç«¯å‡½æ•°
def backend_empty_cache(device: str):
    return _device_agnostic_dispatch(device, BACKEND_EMPTY_CACHE)


# æŸ¥è¯¢è®¾å¤‡æ•°é‡çš„åç«¯å‡½æ•°
def backend_device_count(device: str):
    return _device_agnostic_dispatch(device, BACKEND_DEVICE_COUNT)


# å¦‚æœ Torch å¯ç”¨
if is_torch_available():
    # å¦‚æœ `TRANSFORMERS_TEST_DEVICE_SPEC` å·²å¯ç”¨ï¼Œéœ€è¦å¯¼å…¥é¢å¤–çš„è®¾å¤‡åˆ°å‡½æ•°æ˜ å°„é¡¹
```  
    # æ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­æ˜¯å¦å­˜åœ¨åä¸º"TRANSFORMERS_TEST_DEVICE_SPEC"çš„é”®
    if "TRANSFORMERS_TEST_DEVICE_SPEC" in os.environ:
        # å¦‚æœå­˜åœ¨ï¼Œè·å–ç¯å¢ƒå˜é‡ä¸­æŒ‡å®šçš„è®¾å¤‡è§„æ ¼æ–‡ä»¶è·¯å¾„
        device_spec_path = os.environ["TRANSFORMERS_TEST_DEVICE_SPEC"]
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦æŒ‡å‘ä¸€ä¸ªå­˜åœ¨çš„æ–‡ä»¶
        if not Path(device_spec_path).is_file():
            # å¦‚æœä¸æ˜¯æ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
            raise ValueError(
                f"Specified path to device spec file is not a file or not found. Received '{device_spec_path}"
            )

        # å°è¯•ä»æ–‡ä»¶è·¯å¾„ä¸­å»é™¤æ‰©å±•åä»¥å¤‡åç»­å¯¼å…¥ - åŒæ—¶éªŒè¯æ˜¯å¦å¯¼å…¥äº†ä¸€ä¸ª Python æ–‡ä»¶
        try:
            import_name = device_spec_path[: device_spec_path.index(".py")]
        except ValueError as e:
            # å¦‚æœæä¾›çš„è®¾å¤‡è§„æ ¼æ–‡ä»¶ä¸æ˜¯ Python æ–‡ä»¶ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
            raise ValueError(f"Provided device spec file was not a Python file! Received '{device_spec_path}") from e

        # å¯¼å…¥è®¾å¤‡è§„æ ¼æ¨¡å—
        device_spec_module = importlib.import_module(import_name)

        # å¯¼å…¥çš„æ–‡ä»¶å¿…é¡»åŒ…å« `DEVICE_NAME`ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™æå‰ç»ˆæ­¢ã€‚
        try:
            # å°è¯•ä»å¯¼å…¥çš„æ¨¡å—ä¸­è·å– `DEVICE_NAME`
            device_name = device_spec_module.DEVICE_NAME
        except AttributeError as e:
            # å¦‚æœæ¨¡å—ä¸åŒ…å« `DEVICE_NAME`ï¼Œåˆ™å¼•å‘å±æ€§é”™è¯¯å¼‚å¸¸
            raise AttributeError("Device spec file did not contain `DEVICE_NAME`") from e

        # å¦‚æœç¯å¢ƒå˜é‡ä¸­å­˜åœ¨"TRANSFORMERS_TEST_DEVICE"ä¸”å…¶å€¼ä¸è®¾å¤‡åç§°ä¸åŒ¹é…ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
        if "TRANSFORMERS_TEST_DEVICE" in os.environ and torch_device != device_name:
            msg = f"Mismatch between environment variable `TRANSFORMERS_TEST_DEVICE` '{torch_device}' and device found in spec '{device_name}'\n"
            msg += "Either unset `TRANSFORMERS_TEST_DEVICE` or ensure it matches device spec name."
            raise ValueError(msg)

        # æ›´æ–° Torch è®¾å¤‡åç§°ä¸ºè®¾å¤‡è§„æ ¼ä¸­çš„åç§°
        torch_device = device_name

        # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºä»è®¾å¤‡è§„æ ¼æ–‡ä»¶ä¸­æ›´æ–°æŒ‡å®šå­—å…¸çš„æ˜ å°„å…³ç³»
        def update_mapping_from_spec(device_fn_dict: Dict[str, Callable], attribute_name: str):
            try:
                # å°è¯•ç›´æ¥å¯¼å…¥å‡½æ•°
                spec_fn = getattr(device_spec_module, attribute_name)
                # å°†å‡½æ•°æ·»åŠ åˆ°æŒ‡å®šå­—å…¸ä¸­
                device_fn_dict[torch_device] = spec_fn
            except AttributeError as e:
                # å¦‚æœå‡½æ•°ä¸å­˜åœ¨ï¼Œå¹¶ä¸”å­—å…¸ä¸­æ²¡æœ‰é»˜è®¤å€¼ï¼Œåˆ™å¼•å‘å±æ€§é”™è¯¯å¼‚å¸¸
                if "default" not in device_fn_dict:
                    raise AttributeError(
                        f"`{attribute_name}` not found in '{device_spec_path}' and no default fallback function found."
                    ) from e

        # åœ¨æ­¤å¤„ä¸ºæ¯ä¸ª `BACKEND_*` å­—å…¸æ·»åŠ ä¸€ä¸ªæ¡ç›®ï¼Œå¹¶ä»è®¾å¤‡è§„æ ¼æ–‡ä»¶ä¸­æ›´æ–°æ˜ å°„å…³ç³»
        update_mapping_from_spec(BACKEND_MANUAL_SEED, "MANUAL_SEED_FN")
        update_mapping_from_spec(BACKEND_EMPTY_CACHE, "EMPTY_CACHE_FN")
        update_mapping_from_spec(BACKEND_DEVICE_COUNT, "DEVICE_COUNT_FN")
```