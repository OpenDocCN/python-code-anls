# `.\models\maskformer\convert_maskformer_swin_to_pytorch.py`

```py
# è®¾ç½®ç¼–ç æ ¼å¼ä¸º UTF-8
# ç‰ˆæƒå£°æ˜å’Œè®¸å¯è¯ä¿¡æ¯ï¼ŒæŒ‡å®šä»£ç ä½¿ç”¨ Apache License, Version 2.0
# å¯¼å…¥æ‰€éœ€æ¨¡å—å’Œåº“
# è¿™ä¸ªè„šæœ¬ç”¨äºä»åŸå§‹ä»“åº“è½¬æ¢ MaskFormer æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè¯¦ç»†ä¿¡æ¯å‚è§ https://github.com/facebookresearch/MaskFormer

import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import json  # å¯¼å…¥å¤„ç† JSON æ ¼å¼æ•°æ®çš„æ¨¡å—
import pickle  # å¯¼å…¥åºåˆ—åŒ–å’Œååºåˆ—åŒ– Python å¯¹è±¡çš„æ¨¡å—
from pathlib import Path  # å¯¼å…¥å¤„ç†è·¯å¾„æ“ä½œçš„æ¨¡å—

import requests  # å¯¼å…¥å‘é€ HTTP è¯·æ±‚çš„åº“
import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ åº“
from huggingface_hub import hf_hub_download  # å¯¼å…¥ä» Hugging Face Hub ä¸‹è½½èµ„æºçš„å‡½æ•°
from PIL import Image  # å¯¼å…¥ Python Imaging Libraryï¼Œç”¨äºå›¾åƒå¤„ç†

from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, SwinConfig  # å¯¼å…¥ MaskFormer ç›¸å…³ç±»
from transformers.utils import logging  # å¯¼å…¥æ—¥å¿—è®°å½•å·¥å…·

logging.set_verbosity_info()  # è®¾ç½®æ—¥å¿—è®°å½•å™¨çš„è¯¦ç»†ç¨‹åº¦ä¸ºä¿¡æ¯çº§åˆ«
logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


def get_maskformer_config(model_name: str):
    # æ ¹æ®é¢„è®­ç»ƒçš„ Swin æ¨¡å‹é…ç½® MaskFormerConfig
    backbone_config = SwinConfig.from_pretrained(
        "microsoft/swin-tiny-patch4-window7-224", out_features=["stage1", "stage2", "stage3", "stage4"]
    )
    config = MaskFormerConfig(backbone_config=backbone_config)

    repo_id = "huggingface/label-files"
    if "ade20k-full" in model_name:
        # è®¾ç½®é€‚ç”¨äº ade20k-full æ¨¡å‹çš„ç±»åˆ«æ•°å’Œæ ‡ç­¾æ˜ å°„æ–‡ä»¶å
        config.num_labels = 847
        filename = "maskformer-ade20k-full-id2label.json"
    elif "ade" in model_name:
        # è®¾ç½®é€‚ç”¨äº ade æ¨¡å‹çš„ç±»åˆ«æ•°å’Œæ ‡ç­¾æ˜ å°„æ–‡ä»¶å
        config.num_labels = 150
        filename = "ade20k-id2label.json"
    elif "coco-stuff" in model_name:
        # è®¾ç½®é€‚ç”¨äº coco-stuff æ¨¡å‹çš„ç±»åˆ«æ•°å’Œæ ‡ç­¾æ˜ å°„æ–‡ä»¶å
        config.num_labels = 171
        filename = "maskformer-coco-stuff-id2label.json"
    elif "coco" in model_name:
        # TODO
        config.num_labels = 133
        filename = "coco-panoptic-id2label.json"
    elif "cityscapes" in model_name:
        # è®¾ç½®é€‚ç”¨äº cityscapes æ¨¡å‹çš„ç±»åˆ«æ•°å’Œæ ‡ç­¾æ˜ å°„æ–‡ä»¶å
        config.num_labels = 19
        filename = "cityscapes-id2label.json"
    elif "vistas" in model_name:
        # è®¾ç½®é€‚ç”¨äº vistas æ¨¡å‹çš„ç±»åˆ«æ•°å’Œæ ‡ç­¾æ˜ å°„æ–‡ä»¶å
        config.num_labels = 65
        filename = "mapillary-vistas-id2label.json"

    # ä» Hugging Face Hub ä¸‹è½½æŒ‡å®šæ–‡ä»¶å¹¶åŠ è½½ä¸ºå­—å…¸æ ¼å¼
    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
    id2label = {int(k): v for k, v in id2label.items()}

    return config


def create_rename_keys(config):
    rename_keys = []
    # å®šä¹‰éœ€è¦é‡å‘½åçš„é”®åˆ—è¡¨
    # stem
    # fmt: off
    rename_keys.append(("backbone.patch_embed.proj.weight", "model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.weight"))
    rename_keys.append(("backbone.patch_embed.proj.bias", "model.pixel_level_module.encoder.model.embeddings.patch_embeddings.projection.bias"))
    rename_keys.append(("backbone.patch_embed.norm.weight", "model.pixel_level_module.encoder.model.embeddings.norm.weight"))
    # fmt: on
    # å°†é”®å€¼å¯¹("backbone.patch_embed.norm.bias", "model.pixel_level_module.encoder.model.embeddings.norm.bias")æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
    rename_keys.append(("backbone.patch_embed.norm.bias", "model.pixel_level_module.encoder.model.embeddings.norm.bias"))

    # å°†ä»¥ä¸‹é”®å€¼å¯¹ä¾æ¬¡æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­ï¼Œç”¨äºé‡å‘½åæ¨¡å‹ç»“æ„ä¸­çš„å‚æ•°
    rename_keys.append(("sem_seg_head.layer_4.weight", "model.pixel_level_module.decoder.fpn.stem.0.weight"))
    rename_keys.append(("sem_seg_head.layer_4.norm.weight", "model.pixel_level_module.decoder.fpn.stem.1.weight"))
    rename_keys.append(("sem_seg_head.layer_4.norm.bias", "model.pixel_level_module.decoder.fpn.stem.1.bias"))

    # ä½¿ç”¨å¾ªç¯å°†é€ä¸ªsource_indexåˆ°target_indexçš„é€‚é…å™¨å’Œå±‚å‚æ•°é‡å‘½åæ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
    for source_index, target_index in zip(range(3, 0, -1), range(0, 3)):
        rename_keys.append((f"sem_seg_head.adapter_{source_index}.weight", f"model.pixel_level_module.decoder.fpn.layers.{target_index}.proj.0.weight"))
        rename_keys.append((f"sem_seg_head.adapter_{source_index}.norm.weight", f"model.pixel_level_module.decoder.fpn.layers.{target_index}.proj.1.weight"))
        rename_keys.append((f"sem_seg_head.adapter_{source_index}.norm.bias", f"model.pixel_level_module.decoder.fpn.layers.{target_index}.proj.1.bias"))
        rename_keys.append((f"sem_seg_head.layer_{source_index}.weight", f"model.pixel_level_module.decoder.fpn.layers.{target_index}.block.0.weight"))
        rename_keys.append((f"sem_seg_head.layer_{source_index}.norm.weight", f"model.pixel_level_module.decoder.fpn.layers.{target_index}.block.1.weight"))
        rename_keys.append((f"sem_seg_head.layer_{source_index}.norm.bias", f"model.pixel_level_module.decoder.fpn.layers.{target_index}.block.1.bias"))

    # å°†é”®å€¼å¯¹("sem_seg_head.mask_features.weight", "model.pixel_level_module.decoder.mask_projection.weight")æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
    rename_keys.append(("sem_seg_head.mask_features.weight", "model.pixel_level_module.decoder.mask_projection.weight"))
    # å°†é”®å€¼å¯¹("sem_seg_head.mask_features.bias", "model.pixel_level_module.decoder.mask_projection.bias")æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
    rename_keys.append(("sem_seg_head.mask_features.bias", "model.pixel_level_module.decoder.mask_projection.bias"))
    
    # Transformerè§£ç å™¨éƒ¨åˆ†æš‚æ— ä»£ç ï¼Œæœªè¿›è¡Œæ³¨é‡Š
    # éå†ä»é…ç½®ä¸­è·å–çš„è§£ç å™¨å±‚æ•°
    for idx in range(config.decoder_config.decoder_layers):
        # å¤„ç†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºæŠ•å½±å±‚æƒé‡å’Œåç½®
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.self_attn.out_proj.weight", f"model.transformer_module.decoder.layers.{idx}.self_attn.out_proj.weight"))
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.self_attn.out_proj.bias", f"model.transformer_module.decoder.layers.{idx}.self_attn.out_proj.bias"))
        
        # å¤„ç†è·¨æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºæŠ•å½±å±‚æƒé‡å’Œåç½®
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.multihead_attn.out_proj.weight", f"model.transformer_module.decoder.layers.{idx}.encoder_attn.out_proj.weight"))
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.multihead_attn.out_proj.bias", f"model.transformer_module.decoder.layers.{idx}.encoder_attn.out_proj.bias"))
        
        # å¤„ç†MLPç¬¬ä¸€å±‚çš„æƒé‡å’Œåç½®
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.linear1.weight", f"model.transformer_module.decoder.layers.{idx}.fc1.weight"))
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.linear1.bias", f"model.transformer_module.decoder.layers.{idx}.fc1.bias"))
        
        # å¤„ç†MLPç¬¬äºŒå±‚çš„æƒé‡å’Œåç½®
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.linear2.weight", f"model.transformer_module.decoder.layers.{idx}.fc2.weight"))
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.linear2.bias", f"model.transformer_module.decoder.layers.{idx}.fc2.bias"))
        
        # å¤„ç†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„LayerNormå±‚çš„æƒé‡å’Œåç½®
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.norm1.weight", f"model.transformer_module.decoder.layers.{idx}.self_attn_layer_norm.weight"))
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.norm1.bias", f"model.transformer_module.decoder.layers.{idx}.self_attn_layer_norm.bias"))
        
        # å¤„ç†è·¨æ³¨æ„åŠ›æœºåˆ¶çš„LayerNormå±‚çš„æƒé‡å’Œåç½®
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.norm2.weight", f"model.transformer_module.decoder.layers.{idx}.encoder_attn_layer_norm.weight"))
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.norm2.bias", f"model.transformer_module.decoder.layers.{idx}.encoder_attn_layer_norm.bias"))
        
        # å¤„ç†æœ€ç»ˆLayerNormå±‚çš„æƒé‡å’Œåç½®
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.norm3.weight", f"model.transformer_module.decoder.layers.{idx}.final_layer_norm.weight"))
        rename_keys.append((f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.norm3.bias", f"model.transformer_module.decoder.layers.{idx}.final_layer_norm.bias"))

    # å°†æœ€åä¸€ä¸ªæœªå¤„ç†çš„LayerNormå±‚çš„æƒé‡å’Œåç½®æ·»åŠ åˆ°é‡å‘½ååˆ—è¡¨ä¸­
    rename_keys.append(("sem_seg_head.predictor.transformer.decoder.norm.weight", "model.transformer_module.decoder.layernorm.weight"))
    # å°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°æ¨¡å‹å‚æ•°åç§°é…å¯¹å¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("sem_seg_head.predictor.transformer.decoder.norm.bias", "model.transformer_module.decoder.layernorm.bias"))

    # å°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°æ¨¡å‹å‚æ•°åç§°é…å¯¹å¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºé¡¶éƒ¨çš„å¤´éƒ¨æ¨¡å—
    rename_keys.append(("sem_seg_head.predictor.query_embed.weight", "model.transformer_module.queries_embedder.weight"))

    # å°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°æ¨¡å‹å‚æ•°åç§°é…å¯¹å¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºè¾“å…¥æŠ•å½±æƒé‡
    rename_keys.append(("sem_seg_head.predictor.input_proj.weight", "model.transformer_module.input_projection.weight"))
    # å°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°æ¨¡å‹å‚æ•°åç§°é…å¯¹å¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºè¾“å…¥æŠ•å½±åç½®
    rename_keys.append(("sem_seg_head.predictor.input_proj.bias", "model.transformer_module.input_projection.bias"))

    # å°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°æ¨¡å‹å‚æ•°åç§°é…å¯¹å¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºç±»åˆ«é¢„æµ‹æƒé‡
    rename_keys.append(("sem_seg_head.predictor.class_embed.weight", "class_predictor.weight"))
    # å°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°æ¨¡å‹å‚æ•°åç§°é…å¯¹å¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºç±»åˆ«é¢„æµ‹åç½®
    rename_keys.append(("sem_seg_head.predictor.class_embed.bias", "class_predictor.bias"))

    # å¾ªç¯å¤„ç†æ¯ä¸ªæ©ç åµŒå…¥å±‚ï¼Œå°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°æ¨¡å‹å‚æ•°åç§°é…å¯¹å¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    for i in range(3):
        rename_keys.append((f"sem_seg_head.predictor.mask_embed.layers.{i}.weight", f"mask_embedder.{i}.0.weight"))
        rename_keys.append((f"sem_seg_head.predictor.mask_embed.layers.{i}.bias", f"mask_embedder.{i}.0.bias"))
    # fmt: on

    # è¿”å›æœ€ç»ˆçš„é‡å‘½åé”®åˆ—è¡¨
    return rename_keys
# é‡æ–°å‘½åå­—å…¸ `dct` ä¸­é”® `old` ä¸º `new`
def rename_key(dct, old, new):
    val = dct.pop(old)  # å¼¹å‡ºé”®ä¸º `old` çš„å€¼ï¼Œå¹¶ä¿å­˜åˆ°å˜é‡ `val`
    dct[new] = val  # å°†å€¼ `val` ä¸æ–°é”® `new` å…³è”å¹¶æ·»åŠ åˆ°å­—å…¸ä¸­

# we split up the matrix of each encoder layer into queries, keys and values
# å°†æ¯ä¸ªç¼–ç å™¨å±‚çš„çŸ©é˜µæ‹†åˆ†ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼
def read_in_swin_q_k_v(state_dict, backbone_config):
    num_features = [int(backbone_config.embed_dim * 2**i) for i in range(len(backbone_config.depths))]
    for i in range(len(backbone_config.depths)):
        dim = num_features[i]
        for j in range(backbone_config.depths[i]):
            # fmt: off
            # è¯»å–è¾“å…¥æŠ•å½±å±‚ (in_proj) çš„æƒé‡å’Œåç½® (åœ¨åŸå§‹å®ç°ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„çŸ©é˜µåŠ åç½®)
            in_proj_weight = state_dict.pop(f"backbone.layers.{i}.blocks.{j}.attn.qkv.weight")
            in_proj_bias = state_dict.pop(f"backbone.layers.{i}.blocks.{j}.attn.qkv.bias")
            # æ¥ä¸‹æ¥ï¼ŒæŒ‰é¡ºåºæ·»åŠ æŸ¥è¯¢ã€é”®å’Œå€¼åˆ°çŠ¶æ€å­—å…¸
            state_dict[f"model.pixel_level_module.encoder.model.encoder.layers.{i}.blocks.{j}.attention.self.query.weight"] = in_proj_weight[:dim, :]
            state_dict[f"model.pixel_level_module.encoder.model.encoder.layers.{i}.blocks.{j}.attention.self.query.bias"] = in_proj_bias[: dim]
            state_dict[f"model.pixel_level_module.encoder.model.encoder.layers.{i}.blocks.{j}.attention.self.key.weight"] = in_proj_weight[
                dim : dim * 2, :
            ]
            state_dict[f"model.pixel_level_module.encoder.model.encoder.layers.{i}.blocks.{j}.attention.self.key.bias"] = in_proj_bias[
                dim : dim * 2
            ]
            state_dict[f"model.pixel_level_module.encoder.model.encoder.layers.{i}.blocks.{j}.attention.self.value.weight"] = in_proj_weight[
                -dim :, :
            ]
            state_dict[f"model.pixel_level_module.encoder.model.encoder.layers.{i}.blocks.{j}.attention.self.value.bias"] = in_proj_bias[-dim :]
            # fmt: on

# we split up the matrix of each encoder layer into queries, keys and values
# å°†æ¯ä¸ªè§£ç å™¨å±‚çš„çŸ©é˜µæ‹†åˆ†ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼
def read_in_decoder_q_k_v(state_dict, config):
    # fmt: off
    hidden_size = config.decoder_config.hidden_size
    # éå†è§£ç å™¨å±‚æ¬¡çš„æ•°é‡
    for idx in range(config.decoder_config.decoder_layers):
        # è¯»å–è‡ªæ³¨æ„åŠ›è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®ï¼ˆåœ¨åŸå§‹å®ç°ä¸­ï¼Œè¿™æ˜¯å•ç‹¬çš„çŸ©é˜µå’Œåç½®ï¼‰
        in_proj_weight = state_dict.pop(f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.self_attn.in_proj_weight")
        in_proj_bias = state_dict.pop(f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.self_attn.in_proj_bias")
        
        # å°†æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeysï¼‰å’Œå€¼ï¼ˆvaluesï¼‰ä¾æ¬¡æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
        state_dict[f"model.transformer_module.decoder.layers.{idx}.self_attn.q_proj.weight"] = in_proj_weight[: hidden_size, :]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.self_attn.q_proj.bias"] = in_proj_bias[:config.hidden_size]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.self_attn.k_proj.weight"] = in_proj_weight[hidden_size : hidden_size * 2, :]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.self_attn.k_proj.bias"] = in_proj_bias[hidden_size : hidden_size * 2]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.self_attn.v_proj.weight"] = in_proj_weight[-hidden_size :, :]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.self_attn.v_proj.bias"] = in_proj_bias[-hidden_size :]
        
        # è¯»å–äº¤å‰æ³¨æ„åŠ›è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®ï¼ˆåœ¨åŸå§‹å®ç°ä¸­ï¼Œè¿™æ˜¯å•ç‹¬çš„çŸ©é˜µå’Œåç½®ï¼‰
        in_proj_weight = state_dict.pop(f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.multihead_attn.in_proj_weight")
        in_proj_bias = state_dict.pop(f"sem_seg_head.predictor.transformer.decoder.layers.{idx}.multihead_attn.in_proj_bias")
        
        # å°†æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeysï¼‰å’Œå€¼ï¼ˆvaluesï¼‰ä¾æ¬¡æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
        state_dict[f"model.transformer_module.decoder.layers.{idx}.encoder_attn.q_proj.weight"] = in_proj_weight[: hidden_size, :]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.encoder_attn.q_proj.bias"] = in_proj_bias[:config.hidden_size]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.encoder_attn.k_proj.weight"] = in_proj_weight[hidden_size : hidden_size * 2, :]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.encoder_attn.k_proj.bias"] = in_proj_bias[hidden_size : hidden_size * 2]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.encoder_attn.v_proj.weight"] = in_proj_weight[-hidden_size :, :]
        state_dict[f"model.transformer_module.decoder.layers.{idx}.encoder_attn.v_proj.bias"] = in_proj_bias[-hidden_size :]
    
    # æ ¼å¼åŒ–ç»“æŸ
    # fmt: on
# We will verify our results on an image of cute cats
def prepare_img() -> torch.Tensor:
    # å®šä¹‰å›¾åƒçš„ URL
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # é€šè¿‡ HTTP è¯·æ±‚è·å–å›¾åƒçš„åŸå§‹æ•°æ®æµï¼Œå¹¶ç”¨ PIL åº“æ‰“å¼€å›¾åƒ
    im = Image.open(requests.get(url, stream=True).raw)
    return im

@torch.no_grad()
def convert_maskformer_checkpoint(
    model_name: str, checkpoint_path: str, pytorch_dump_folder_path: str, push_to_hub: bool = False
):
    """
    Copy/paste/tweak model's weights to our MaskFormer structure.
    """
    # æ ¹æ®æ¨¡å‹åè·å– MaskFormer çš„é…ç½®ä¿¡æ¯
    config = get_maskformer_config(model_name)

    # åŠ è½½åŸå§‹çš„çŠ¶æ€å­—å…¸
    with open(checkpoint_path, "rb") as f:
        data = pickle.load(f)
    state_dict = data["model"]

    # æ‰“å°çŠ¶æ€å­—å…¸ä¸­æ¯ä¸ªé”®å’Œå¯¹åº”çš„å½¢çŠ¶ï¼ˆæ³¨é‡Šæ‰çš„éƒ¨åˆ†ï¼‰
    # for name, param in state_dict.items():
    #     print(name, param.shape)

    # æ ¹æ®é…ç½®ä¿¡æ¯åˆ›å»ºé‡å‘½åé”®åˆ—è¡¨
    rename_keys = create_rename_keys(config)
    # å¯¹çŠ¶æ€å­—å…¸ä¸­çš„é”®è¿›è¡Œé‡å‘½åæ“ä½œ
    for src, dest in rename_keys:
        rename_key(state_dict, src, dest)
    # ä»çŠ¶æ€å­—å…¸ä¸­è¯»å– Swin Transformer çš„ QKV å‚æ•°
    read_in_swin_q_k_v(state_dict, config.backbone_config)
    # ä»çŠ¶æ€å­—å…¸ä¸­è¯»å–è§£ç å™¨çš„ QKV å‚æ•°
    read_in_decoder_q_k_v(state_dict, config)

    # å°†æ‰€æœ‰å€¼è½¬æ¢ä¸º Torch å¼ é‡
    for key, value in state_dict.items():
        state_dict[key] = torch.from_numpy(value)

    # åŠ è½½ MaskFormer æ¨¡å‹
    model = MaskFormerForInstanceSegmentation(config)
    model.eval()

    # æ‰“å°æ¨¡å‹ä¸­æ¯ä¸ªå‚æ•°çš„åç§°å’Œå½¢çŠ¶
    for name, param in model.named_parameters():
        print(name, param.shape)

    # åŠ è½½çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­ï¼Œå¹¶æ£€æŸ¥ç¼ºå¤±å’Œå¤šä½™çš„é”®
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == [
        "model.pixel_level_module.encoder.model.layernorm.weight",
        "model.pixel_level_module.encoder.model.layernorm.bias",
    ]
    assert len(unexpected_keys) == 0, f"Unexpected keys: {unexpected_keys}"

    # éªŒè¯æ¨¡å‹åœ¨ç»™å®šå›¾åƒä¸Šçš„è¾“å‡ºç»“æœ
    image = prepare_img()
    # æ ¹æ®æ¨¡å‹åè®¾ç½®å¿½ç•¥çš„ç´¢å¼•å€¼
    if "vistas" in model_name:
        ignore_index = 65
    elif "cityscapes" in model_name:
        ignore_index = 65535
    else:
        ignore_index = 255
    # æ ¹æ®æ¨¡å‹åè®¾ç½®æ˜¯å¦å‡å°‘æ ‡ç­¾æ•°
    reduce_labels = True if "ade" in model_name else False
    # åˆ›å»º MaskFormerImageProcessor å®ä¾‹æ¥å¤„ç†å›¾åƒ
    image_processor = MaskFormerImageProcessor(ignore_index=ignore_index, reduce_labels=reduce_labels)

    # å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œè¿”å›æ¨¡å‹æ‰€éœ€çš„è¾“å…¥å¼ é‡
    inputs = image_processor(image, return_tensors="pt")

    # åœ¨æ¨¡å‹ä¸Šæ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–è¾“å‡º
    outputs = model(**inputs)

    # æ‰“å°è¾“å‡ºå¼ é‡çš„ä¸€éƒ¨åˆ†å†…å®¹ï¼ˆLogitsï¼‰
    print("Logits:", outputs.class_queries_logits[0, :3, :3])

    # æ ¹æ®æ¨¡å‹åè®¾ç½®æœŸæœ›çš„ Logits å€¼ï¼Œç”¨äºæ–­è¨€éªŒè¯
    if model_name == "maskformer-swin-tiny-ade":
        expected_logits = torch.tensor(
            [[3.6353, -4.4770, -2.6065], [0.5081, -4.2394, -3.5343], [2.1909, -5.0353, -1.9323]]
        )
    assert torch.allclose(outputs.class_queries_logits[0, :3, :3], expected_logits, atol=1e-4)
    print("Looks ok!")

    # å¦‚æœæŒ‡å®šäº† pytorch_dump_folder_pathï¼Œåˆ™ä¿å­˜æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨
    if pytorch_dump_folder_path is not None:
        print(f"Saving model and image processor to {pytorch_dump_folder_path}")
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        model.save_pretrained(pytorch_dump_folder_path)
        image_processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœ push_to_hub ä¸º Trueï¼Œåˆ™å°†æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨æ¨é€åˆ°æ¨¡å‹ä¸­å¿ƒ
    if push_to_hub:
        print("Pushing model and image processor to the hub...")
        model.push_to_hub(f"nielsr/{model_name}")
        image_processor.push_to_hub(f"nielsr/{model_name}")


if __name__ == "__main__":
    # ä¸»ç¨‹åºå…¥å£ç‚¹ï¼Œæ­¤å¤„ä¸æ·»åŠ ä»»ä½•æ³¨é‡Š
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ¨¡å‹åç§°
    parser.add_argument(
        "--model_name",
        default="maskformer-swin-tiny-ade",
        type=str,
        help=("Name of the MaskFormer model you'd like to convert",),
    )
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ£€æŸ¥ç‚¹è·¯å¾„
    parser.add_argument(
        "--checkpoint_path",
        default="/Users/nielsrogge/Documents/MaskFormer_checkpoints/MaskFormer-Swin-tiny-ADE20k/model.pkl",
        type=str,
        help="Path to the original state dict (.pth file).",
    )
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šPyTorch æ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", 
        default=None, 
        type=str, 
        help="Path to the output PyTorch model directory."
    )
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ˜¯å¦æ¨é€æ¨¡å‹åˆ° ğŸ¤— hub
    parser.add_argument(
        "--push_to_hub", 
        action="store_true", 
        help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå°†ç»“æœå­˜å‚¨åœ¨ args å˜é‡ä¸­
    args = parser.parse_args()
    
    # è°ƒç”¨å‡½æ•°æ¥è½¬æ¢ MaskFormer æ¨¡å‹çš„æ£€æŸ¥ç‚¹
    convert_maskformer_checkpoint(
        args.model_name, args.checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub
    )
```