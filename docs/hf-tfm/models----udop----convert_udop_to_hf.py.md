# `.\models\udop\convert_udop_to_hf.py`

```py
    filepath = hf_hub_download(
        repo_id="hf-internal-testing/fixtures_docvqa", filename="document_2.png", repo_type="dataset"
    )
    # ä½¿ç”¨ Hugging Face Hub ä¸‹è½½æŒ‡å®š repository ä¸­çš„æ–‡ä»¶ 'document_2.png'ï¼Œè¿”å›å…¶æœ¬åœ°æ–‡ä»¶è·¯å¾„
    image = Image.open(filepath).convert("RGB")
    # æ‰“å¼€ä¸‹è½½çš„å›¾åƒæ–‡ä»¶ï¼Œå¹¶è½¬æ¢ä¸º RGB æ¨¡å¼çš„ PIL å›¾åƒå¯¹è±¡

    return image
    # è¿”å›å¤„ç†åçš„å›¾åƒå¯¹è±¡ä½œä¸ºå‡½æ•°çš„è¾“å‡º
    words = ['7', 'ITC', 'Limited', 'REPORT', 'AND', 'ACCOUNTS', '2013', 'ITCâ€™s', 'Brands:', 'An', 'Asset', 'for', 'the', 'Nation', 'The', 'consumer', 'needs', 'and', 'aspirations', 'they', 'fulfil,', 'the', 'benefit', 'they', 'generate', 'for', 'millions', 'across', 'ITCâ€™s', 'value', 'chains,', 'the', 'future-ready', 'capabilities', 'that', 'support', 'them,', 'and', 'the', 'value', 'that', 'they', 'create', 'for', 'the', 'country,', 'have', 'made', 'ITCâ€™s', 'brands', 'national', 'assets,', 'adding', 'to', 'Indiaâ€™s', 'competitiveness.', 'It', 'is', 'ITCâ€™s', 'aspiration', 'to', 'be', 'the', 'No', '1', 'FMCG', 'player', 'in', 'the', 'country,', 'driven', 'by', 'its', 'new', 'FMCG', 'businesses.', 'A', 'recent', 'Nielsen', 'report', 'has', 'highlighted', 'that', "ITC's", 'new', 'FMCG', 'businesses', 'are', 'the', 'fastest', 'growing', 'among', 'the', 'top', 'consumer', 'goods', 'companies', 'operating', 'in', 'India.', 'ITC', 'takes', 'justifiable', 'pride', 'that,', 'along', 'with', 'generating', 'economic', 'value,', 'these', 'celebrated', 'Indian', 'brands', 'also', 'drive', 'the', 'creation', 'of', 'larger', 'societal', 'capital', 'through', 'the', 'virtuous', 'cycle', 'of', 'sustainable', 'and', 'inclusive', 'growth.', 'DI', 'WILLS', '*', ';', 'LOVE', 'DELIGHTFULLY', 'SOFT', 'SKIN?', 'aia', 'Ans', 'Source:', 'https://www.industrydocuments.ucsf.edu/docs/snbx0223']
    # å®šä¹‰ä¸€ä¸ªåŒ…å«æ–‡æœ¬çš„åˆ—è¡¨å’Œè¾¹ç•Œæ¡†çš„ç©ºåˆ—è¡¨
    text_list = []
    bbox_list = []
    # éå†æ¯ä¸ªè¯å’Œå¯¹åº”çš„æ¡†
    for text, box in zip(words, boxes):
        # å¦‚æœæ–‡æœ¬ä¸ºç©ºï¼Œåˆ™è·³è¿‡å½“å‰å¾ªç¯
        if text == "":
            continue
        # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†
        sub_tokens = tokenizer.tokenize(text)
        # éå†æ¯ä¸ªå­è¯å¹¶æ·»åŠ åˆ°æ–‡æœ¬åˆ—è¡¨ä¸­ï¼ŒåŒæ—¶å°†æ¡†æ·»åŠ åˆ°æ¡†åˆ—è¡¨ä¸­
        for sub_token in sub_tokens:
            text_list.append(sub_token)
            bbox_list.append(box)

    # å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºè¾“å…¥ ID åˆ—è¡¨
    input_ids = tokenizer.convert_tokens_to_ids(text_list)

    # å°†å‰é¢çš„æç¤º ID ä¸å½“å‰è¾“å…¥ ID æ‹¼æ¥
    input_ids = prompt_ids + input_ids
    # å°†æ¡†åˆ—è¡¨ä¸ä¸€ä¸ªå…¨é›¶çš„æ¡†åˆ—è¡¨æ‹¼æ¥
    bbox = [[0, 0, 0, 0]] * len(prompt_ids) + bbox_list

    # ä½¿ç”¨å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒå¹¶è·å–åƒç´ å€¼
    pixel_values = image_processor(image, return_tensors="pt").pixel_values
    # ä½¿ç”¨åŸå§‹å˜æ¢å¤„ç†å›¾åƒï¼Œå¹¶å±•å¼€ç»´åº¦
    original_pixel_values = original_transform(image, image_size=image_processor.size["height"]).unsqueeze(0)
    # éªŒè¯åƒç´ å€¼æ˜¯å¦ç›¸ä¼¼
    assert torch.allclose(original_pixel_values, pixel_values)
    # æ‰“å°ä¿¡æ¯ç¡®è®¤åƒç´ å€¼æ­£å¸¸
    print("Pixel values are ok!")

    # è¿”å›è¾“å…¥ ID çš„å¼ é‡ã€è¾¹ç•Œæ¡†çš„å¼ é‡å’Œåƒç´ å€¼
    return torch.tensor(input_ids).unsqueeze(0), torch.tensor(bbox).unsqueeze(0).float(), pixel_values
# å°†ç»™å®šæ¨¡å‹åç§°æ˜ å°„åˆ°å…¶å¯¹åº”çš„æ£€æŸ¥ç‚¹è·¯å¾„
def convert_udop_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):
    # ä¸åŒæ¨¡å‹åç§°åˆ°å…¶å¯¹åº”çš„æ£€æŸ¥ç‚¹è·¯å¾„çš„æ˜ å°„å­—å…¸
    name_to_checkpoint_path = {
        "udop-large": "/Users/nielsrogge/Documents/UDOP/udop-unimodel-large-224/pytorch_model.bin",
        "udop-large-512": "/Users/nielsrogge/Documents/UDOP/udop-unimodel-large-512/pytorch_model.bin",
        "udop-large-512-300k": "/Users/nielsrogge/Documents/UDOP/udop-unimodel-large-512-300k-steps/pytorch_model.bin",
    }

    # æ ¹æ®æ¨¡å‹åç§°è·å–å…¶å¯¹åº”çš„æ£€æŸ¥ç‚¹è·¯å¾„
    checkpoint_path = name_to_checkpoint_path[model_name]
    # ä½¿ç”¨ torch åŠ è½½æ£€æŸ¥ç‚¹ï¼Œå°†å…¶çŠ¶æ€å­—å…¸åŠ è½½åˆ° CPU ä¸Š
    state_dict = torch.load(checkpoint_path, map_location="cpu")

    # æ‰“å°åŠ è½½çš„æ£€æŸ¥ç‚¹è·¯å¾„
    print("Checkpoint path:", checkpoint_path)

    # åˆ›å»º HF æ¨¡å‹å¯¹è±¡
    image_size = 512 if "512" in model_name else 224
    # ä½¿ç”¨æŒ‡å®šçš„é…ç½®åˆ›å»º UDOP æ¨¡å‹é…ç½®å¯¹è±¡
    config = UdopConfig(decoder_start_token_id=0, image_size=image_size)
    # ä½¿ç”¨é…ç½®åˆ›å»ºæ¡ä»¶ç”Ÿæˆçš„ UDOP æ¨¡å‹
    model = UdopForConditionalGeneration(config)
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # é‡å‘½åçŠ¶æ€å­—å…¸çš„é”®åä¸­çš„ç‰¹å®šå­å­—ç¬¦ä¸²
    state_dict = {k.replace("cell2dembedding", "cell_2d_embedding"): v for k, v in state_dict.items()}

    # åŠ è½½æ¨¡å‹çš„æƒé‡ï¼Œå¿½ç•¥ä¸åŒ¹é…çš„é”®
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    # æ‰“å°ç¼ºå¤±çš„é”®å’Œä¸æœŸå¾…çš„é”®
    print("Missing keys:", missing_keys)
    print("Unexpected keys:", unexpected_keys)
    # æ–­è¨€ç¡®ä¿ç¼ºå¤±çš„é”®å’Œä¸æœŸå¾…çš„é”®æ»¡è¶³é¢„æœŸ
    assert missing_keys == ["encoder.embed_patches.proj.weight", "encoder.embed_patches.proj.bias"]
    assert unexpected_keys == ["pos_embed"]

    # å‡†å¤‡è™šæ‹Ÿè¾“å…¥
    # ä»é¢„è®­ç»ƒæ¨¡å‹ "t5-base" åˆ›å»º UDOP åˆ†è¯å™¨
    tokenizer = UdopTokenizer.from_pretrained("t5-base", legacy=True)
    # è®¾ç½®å›¾åƒå¤„ç†å™¨çš„å¤§å°
    size = {"height": image_size, "width": image_size}
    # ä½¿ç”¨ LayoutLMv3 å›¾åƒå¤„ç†å™¨åˆ›å»º UDOP å¤„ç†å™¨
    image_processor = LayoutLMv3ImageProcessor(
        image_mean=IMAGENET_DEFAULT_MEAN, image_std=IMAGENET_DEFAULT_STD, size=size
    )
    # åˆ›å»º UDOP å¤„ç†å™¨
    processor = UdopProcessor(image_processor=image_processor, tokenizer=tokenizer)
    # å‡†å¤‡è™šæ‹Ÿè¾“å…¥æ•°æ®ï¼šåˆ†è¯è¾“å…¥ IDã€è¾¹ç•Œæ¡†å’Œå›¾åƒ
    input_ids, bbox, image = prepare_dummy_inputs(tokenizer, image_processor)
    # æŒ‡å®šæç¤ºæ–‡æœ¬
    prompt = "Question answering. In which year is the report made?"
    # ä½¿ç”¨å¤„ç†å™¨ç¼–ç å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œè¿”å› PyTorch å¼ é‡
    encoding = processor(images=get_image(), text=prompt, return_tensors="pt")

    # è·å–è¾“å…¥æ–‡æœ¬çš„åˆ†è¯ ID
    input_ids = encoding.input_ids
    # å®šä¹‰é¢„æœŸçš„è¾“å…¥ ID å¼ é‡ï¼Œè¿™æ˜¯ä¸€ä¸ª 2D å¼ é‡ï¼Œè¡¨ç¤ºæ¨¡å‹çš„é¢„æœŸè¾“å…¥
    EXPECTED_INPUT_IDS = torch.tensor([[11860, 18243, 5, 86, 84, 215, 19, 8, 934, 263, 58, 1, 489, 27, 3838, 7363, 4083, 14536, 3430, 5686, 5911, 17161, 134, 2038, 27, 3838, 22, 7, 4688, 7, 10, 389, 18202, 21, 8, 11046, 37, 3733, 523, 11, 38, 2388, 1628, 3, 13133, 23334, 6, 8, 1656, 79, 3806, 21, 4040, 640, 27, 3838, 22, 7, 701, 16534, 6, 8, 3, 76, 2693, 18, 23015, 5644, 24, 380, 3, 6015, 6, 11, 8, 701, 24, 79, 482, 21, 3, 88, 684, 6, 43, 263, 27, 3838, 22, 7, 3635, 1157, 4089, 6, 2651, 12, 1547, 22, 7, 3265, 655, 5, 19, 27, 3838, 22, 7, 38, 2388, 257, 12, 36, 8, 465, 209, 13409, 12150, 1959, 16, 8, 684, 6, 6737, 57, 165, 126, 13409, 12150, 1623, 5, 71, 1100, 30298, 934, 65, 12566, 24, 27, 3838, 31, 7, 126, 13409, 12150, 1623, 33, 8, 10391, 1710, 859, 8, 420, 3733, 4968, 688, 2699, 16, 1547, 5, 27, 3838, 1217, 131, 99, 23, 179, 6064, 24, 6, 590, 28, 3, 11600, 1456, 701, 6, 175, 9443, 2557, 3635, 92, 1262, 8, 3409, 13, 2186, 3, 27908, 1784, 190, 8, 3, 5771, 17, 13281, 4005, 13, 5086, 11, 13066, 1170, 5, 10826, 16309, 134, 3, 2, 276, 26, 3, 55, 391, 13570, 5, 10315, 309, 3577, 19114, 371, 4254, 5121, 5055, 6245, 3, 10047, 3162, 58, 3, 9, 61, 1713, 2703, 476, 667, 25158, 301, 6058, 6038, 476, 3765, 9149, 10, 4893, 1303, 1986, 5, 13580, 7, 8224, 28244, 7, 5, 76, 75, 7, 89, 5, 15, 1259, 87, 7171, 7, 87, 7, 29, 115, 226, 4305, 2773, 1]])  # fmt: skip
    # æ£€æŸ¥é¢„æœŸè¾“å…¥ ID æ˜¯å¦ä¸å®é™…è¾“å…¥ ID ç›¸åŒ¹é…ï¼Œä½¿ç”¨ torch.testing.assert_close è¿›è¡Œæ–­è¨€
    torch.testing.assert_close(EXPECTED_INPUT_IDS, input_ids)
    # è·å¾—ç¼–ç ä¸­çš„è¾¹ç•Œæ¡†ï¼Œè½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹
    bbox = encoding.bbox.float()
    # è·å¾—ç¼–ç ä¸­çš„åƒç´ å€¼
    pixel_values = encoding.pixel_values

    # å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯ï¼Œå¹¶å‡†å¤‡ä½¿ç”¨è™šæ‹Ÿè¾“å…¥
    except Exception:
        print("Input_ids don't match, preparing dummy inputs")
        # è°ƒç”¨å‡†å¤‡è™šæ‹Ÿè¾“å…¥çš„å‡½æ•°ï¼Œè·å–è¾“å…¥ IDã€è¾¹ç•Œæ¡†å’Œåƒç´ å€¼
        input_ids, bbox, pixel_values = prepare_dummy_inputs(tokenizer, image_processor)

    # éªŒè¯å•ä¸ªå‰å‘ä¼ æ’­è¿‡ç¨‹
    print("Testing single forward pass..")
    # ç¦ç”¨æ¢¯åº¦è®¡ç®—çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    with torch.no_grad():
        # è®¾ç½®è§£ç å™¨çš„è¾“å…¥ ID
        decoder_input_ids = torch.tensor([[101]])
        # è°ƒç”¨æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–è¾“å‡ºç»“æœ
        outputs = model(input_ids=input_ids, bbox=bbox, pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)
        # æ‰“å°è¾“å‡º logits çš„å½¢çŠ¶
        print("Shape of logits:", outputs.logits.shape)
        # æ‰“å° logits çš„å‰å‡ ä¸ªå€¼
        print("First values of logits:", outputs.logits[0, :3, :3])

    # æ¯”è¾ƒè¾“å‡º logits çš„å‰å‡ ä¸ªå€¼ä¸é¢„æœŸå€¼çš„æ¥è¿‘ç¨‹åº¦ï¼Œè®¾å®šå®¹å·®å€¼ä¸º 1e-4
    # åœ¨ Linux ä¸Šï¼štensor([[-18.5262, 1.5087, -15.7051]])
    # åœ¨ Mac ä¸Šï¼štensor([[-19.4976, 0.8515, -17.1873]])
    try:
        assert torch.allclose(outputs.logits[0, :3, :3], torch.tensor([[-18.5262, 1.5087, -15.7051]]), atol=1e-4)
        print("Looks ok!")
    # å¦‚æœæ¯”è¾ƒä¸é€šè¿‡ï¼Œæ‰“å°æç¤ºä¿¡æ¯
    except Exception:
        print("logits don't match let's try to generate")

    # éªŒè¯è‡ªå›å½’è§£ç è¿‡ç¨‹
    print("Testing generation...")
    # æ„å»ºæ¨¡å‹å‚æ•°å­—å…¸
    model_kwargs = {"bbox": bbox, "pixel_values": pixel_values}
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œç”Ÿæˆï¼ŒæŒ‡å®šæœ€å¤§æ–°å¢ token æ•°é‡ä¸º 20
    outputs = model.generate(input_ids=input_ids, **model_kwargs, max_new_tokens=20)

    # æ‰“å°ç”Ÿæˆçš„ç»“æœæ–‡æœ¬ï¼Œè·³è¿‡ç‰¹æ®Š token åè§£ç æˆå­—ç¬¦ä¸²
    print("Generated:", tokenizer.batch_decode(outputs, skip_special_tokens=True))

    # ä½¿ç”¨åŸå§‹è¾“å…¥æ•°æ®è¿›è¡Œè‡ªå›å½’è§£ç 
    print("Testing generation with original inputs...")
    # ä¸‹è½½æŒ‡å®šçš„æ¨¡å‹è¾“å…¥æ•°æ®æ–‡ä»¶
    filepath = hf_hub_download(repo_id="nielsr/test-image", filename="input_ids_udop.pt", repo_type="dataset")
    # ä»æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„è¾“å…¥æ ‡è¯†ç¬¦
    input_ids = torch.load(filepath)
    # ä½¿ç”¨hf_hub_downloadå‡½æ•°ä¸‹è½½æŒ‡å®šä»“åº“å’Œæ–‡ä»¶åçš„å†…å®¹ï¼Œå¹¶æ›´æ–°filepathå˜é‡
    filepath = hf_hub_download(repo_id="nielsr/test-image", filename="bbox_udop.pt", repo_type="dataset")
    # åŠ è½½ä¿å­˜åœ¨filepathä¸­çš„åŒ…å›´æ¡†æ•°æ®
    bbox = torch.load(filepath)
    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šè¦åŠ è½½çš„åƒç´ å€¼æ–‡ä»¶å
    pixel_values_filename = "pixel_values_udop_512.pt" if "512" in model_name else "pixel_values_udop_224.pt"
    # ä½¿ç”¨hf_hub_downloadå‡½æ•°ä¸‹è½½æŒ‡å®šä»“åº“å’Œæ–‡ä»¶åçš„å†…å®¹ï¼Œå¹¶æ›´æ–°filepathå˜é‡
    filepath = hf_hub_download(repo_id="nielsr/test-image", filename=pixel_values_filename, repo_type="dataset")
    # åŠ è½½ä¿å­˜åœ¨filepathä¸­çš„åƒç´ å€¼æ•°æ®
    pixel_values = torch.load(filepath)

    # æ‰“å°è§£ç åçš„è¾“å…¥æ ‡è¯†ç¬¦ï¼Œè·³è¿‡ç‰¹æ®Šæ ‡è®°
    print("Decoded input ids:", tokenizer.decode(input_ids[0], skip_special_tokens=True))
    # æ‰“å°åŒ…å›´æ¡†çš„å½¢çŠ¶
    print("Bbox shape:", bbox.shape)

    # å‡†å¤‡æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬bboxå’Œpixel_values
    model_kwargs = {"bbox": bbox, "pixel_values": pixel_values}
    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬è¾“å‡ºï¼Œé™åˆ¶æœ€å¤§æ–°ç”Ÿæˆçš„æ ‡è®°æ•°é‡ä¸º20
    outputs = model.generate(input_ids=input_ids, **model_kwargs, max_new_tokens=20)
    # è§£ç æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºæ–‡æœ¬ï¼Œè·³è¿‡ç‰¹æ®Šæ ‡è®°ï¼Œå¹¶å–ç¬¬ä¸€ä¸ªæ–‡æœ¬ç»“æœ
    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    # æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
    print("Generated:", generated_text)

    # å¦‚æœæŒ‡å®šäº†PyTorchæ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„ï¼Œåˆ™ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨çš„é¢„è®­ç»ƒçŠ¶æ€
    if pytorch_dump_folder_path is not None:
        model.save_pretrained(pytorch_dump_folder_path)
        tokenizer.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœè®¾ç½®äº†push_to_hubæ ‡å¿—ï¼Œåˆ™å°†æ¨¡å‹å’Œå¤„ç†å™¨æ¨é€åˆ°Hubä¸ŠæŒ‡å®šçš„ä»“åº“
    if push_to_hub:
        model.push_to_hub(f"microsoft/{model_name}")
        processor.push_to_hub(f"microsoft/{model_name}")
        # é‡è¦æç¤ºï¼šè¦å°†å¿«é€Ÿåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨Hubä¸Šçš„ä»“åº“ä¸­ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
        # å‚è§https://discuss.huggingface.co/t/convert-slow-xlmrobertatokenizer-to-fast-one/20876
if __name__ == "__main__":
    # å¦‚æœè„šæœ¬ç›´æ¥æ‰§è¡Œè€Œéè¢«å¯¼å…¥ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡

    # Required parameters
    parser.add_argument(
        "--model_name",
        default="udop-large",
        type=str,
        choices=["udop-large", "udop-large-512", "udop-large-512-300k"],
        help=("Name of the UDOP model you'd like to convert."),
    )
    # æ·»åŠ ä¸€ä¸ªå¿…éœ€çš„å‚æ•°ï¼Œç”¨äºæŒ‡å®šè¦è½¬æ¢çš„ UDOP æ¨¡å‹çš„åç§°ï¼Œæä¾›äº†é»˜è®¤å€¼å’Œå¯é€‰é¡¹

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼Œç”¨äºæŒ‡å®šè¾“å‡ºçš„ PyTorch æ¨¡å‹ç›®å½•çš„è·¯å¾„

    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ° ğŸ¤— hub

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•°ï¼Œå°†è§£æåçš„å‚æ•°ä¼ é€’ç»™å‡½æ•°è¿›è¡Œ UDOP æ¨¡å‹çš„è½¬æ¢
    convert_udop_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
```