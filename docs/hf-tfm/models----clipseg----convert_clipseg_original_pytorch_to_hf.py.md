# `.\transformers\models\clipseg\convert_clipseg_original_pytorch_to_hf.py`

```
# æŒ‡å®šæ–‡ä»¶ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜ï¼Œå£°æ˜ä½¿ç”¨ Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬
# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
# argparse ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
# requests ç”¨äºå‘é€ HTTP è¯·æ±‚
# torch æ˜¯ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
# PIL æ˜¯ Python Imaging Libraryï¼Œç”¨äºå›¾åƒå¤„ç†
# transformers æ˜¯ Hugging Face æä¾›çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹åº“
# CLIPSegConfig ç”¨äºé…ç½® CLIPSeg æ¨¡å‹
# CLIPSegForImageSegmentation æ˜¯ç”¨äºå›¾åƒåˆ†å‰²çš„ CLIPSeg æ¨¡å‹
# CLIPSegProcessor ç”¨äºå¤„ç† CLIPSeg æ¨¡å‹çš„è¾“å…¥æ•°æ®
# CLIPSegTextConfig ç”¨äºé…ç½® CLIPSeg æ¨¡å‹çš„æ–‡æœ¬éƒ¨åˆ†
# CLIPSegVisionConfig ç”¨äºé…ç½® CLIPSeg æ¨¡å‹çš„è§†è§‰éƒ¨åˆ†
# CLIPTokenizer ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
# ViTImageProcessor ç”¨äºå¤„ç†å›¾åƒæ•°æ®

# å®šä¹‰å‡½æ•° get_clipseg_configï¼Œæ ¹æ®æ¨¡å‹åç§°è·å– CLIPSeg çš„é…ç½®
def get_clipseg_config(model_name):
    # åˆ›å»º CLIPSeg çš„æ–‡æœ¬éƒ¨åˆ†é…ç½®å¯¹è±¡
    text_config = CLIPSegTextConfig()
    # åˆ›å»º CLIPSeg çš„è§†è§‰éƒ¨åˆ†é…ç½®å¯¹è±¡ï¼Œå¹¶è®¾ç½®å›¾åƒå—å¤§å°ä¸º 16
    vision_config = CLIPSegVisionConfig(patch_size=16)

    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šæ˜¯å¦ä½¿ç”¨å¤æ‚çš„è½¬ç½®å·ç§¯
    use_complex_transposed_convolution = True if "refined" in model_name else False
    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šæ˜¯å¦é™ä½ç»´åº¦
    reduce_dim = 16 if "rd16" in model_name else 64

    # æ ¹æ®æ–‡æœ¬å’Œè§†è§‰éƒ¨åˆ†çš„é…ç½®ï¼Œä»¥åŠå…¶ä»–å‚æ•°åˆ›å»º CLIPSeg çš„é…ç½®å¯¹è±¡
    config = CLIPSegConfig.from_text_vision_configs(
        text_config,
        vision_config,
        use_complex_transposed_convolution=use_complex_transposed_convolution,
        reduce_dim=reduce_dim,
    )
    # è¿”å›é…ç½®å¯¹è±¡
    return config

# å®šä¹‰å‡½æ•° rename_keyï¼Œç”¨äºé‡å‘½åæ¨¡å‹å‚æ•°çš„é”®å
def rename_key(name):
    # æ›´æ–°å‰ç¼€
    if "clip_model" in name:
        name = name.replace("clip_model", "clip")
    if "transformer" in name:
        if "visual" in name:
            name = name.replace("visual.transformer", "vision_model")
        else:
            name = name.replace("transformer", "text_model")
    if "resblocks" in name:
        name = name.replace("resblocks", "encoder.layers")
    if "ln_1" in name:
        name = name.replace("ln_1", "layer_norm1")
    if "ln_2" in name:
        name = name.replace("ln_2", "layer_norm2")
    if "c_fc" in name:
        name = name.replace("c_fc", "fc1")
    if "c_proj" in name:
        name = name.replace("c_proj", "fc2")
    if "attn" in name and "self" not in name:
        name = name.replace("attn", "self_attn")
    # æ›´æ–°æ–‡æœ¬ç¼–ç å™¨çš„é”®å
    if "token_embedding" in name:
        name = name.replace("token_embedding", "text_model.embeddings.token_embedding")
    if "positional_embedding" in name and "visual" not in name:
        name = name.replace("positional_embedding", "text_model.embeddings.position_embedding.weight")
    if "ln_final" in name:
        name = name.replace("ln_final", "text_model.final_layer_norm")
    # æ›´æ–°è§†è§‰ç¼–ç å™¨çš„é”®å
    if "visual.class_embedding" in name:
        name = name.replace("visual.class_embedding", "vision_model.embeddings.class_embedding")
    if "visual.conv1" in name:
        name = name.replace("visual.conv1", "vision_model.embeddings.patch_embedding")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "visual.positional_embedding"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.embeddings.position_embedding.weight"
    if "visual.positional_embedding" in name:
        name = name.replace("visual.positional_embedding", "vision_model.embeddings.position_embedding.weight")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "visual.ln_pre"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.pre_layrnorm"
    if "visual.ln_pre" in name:
        name = name.replace("visual.ln_pre", "vision_model.pre_layrnorm")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "visual.ln_post"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.post_layernorm"
    if "visual.ln_post" in name:
        name = name.replace("visual.ln_post", "vision_model.post_layernorm")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "visual.proj"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "visual_projection.weight"
    if "visual.proj" in name:
        name = name.replace("visual.proj", "visual_projection.weight")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "text_projection"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "text_projection.weight"
    if "text_projection" in name:
        name = name.replace("text_projection", "text_projection.weight")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "trans_conv"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "transposed_convolution"
    if "trans_conv" in name:
        name = name.replace("trans_conv", "transposed_convolution")
    # å¦‚æœåŒ…å« "film_mul"ã€"film_add"ã€"reduce" æˆ– "transposed_convolution"ï¼Œåˆ™æ·»åŠ å‰ç¼€ "decoder."
    if "film_mul" in name or "film_add" in name or "reduce" in name or "transposed_convolution" in name:
        name = "decoder." + name
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "blocks"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "decoder.layers"
    if "blocks" in name:
        name = name.replace("blocks", "decoder.layers")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "linear1"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "mlp.fc1"
    if "linear1" in name:
        name = name.replace("linear1", "mlp.fc1")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "linear2"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "mlp.fc2"
    if "linear2" in name:
        name = name.replace("linear2", "mlp.fc2")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "norm1" ä¸”ä¸åŒ…å« "layer_"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "layer_norm1"
    if "norm1" in name and "layer_" not in name:
        name = name.replace("norm1", "layer_norm1")
    # æ£€æŸ¥æ˜¯å¦åŒ…å« "norm2" ä¸”ä¸åŒ…å« "layer_"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ›¿æ¢ä¸º "layer_norm2"
    if "norm2" in name and "layer_" not in name:
        name = name.replace("norm2", "layer_norm2")

    # è¿”å›å¤„ç†åçš„åç§°
    return name
# è½¬æ¢çŠ¶æ€å­—å…¸çš„å‡½æ•°ï¼Œå°†åŸå§‹çš„çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºæ–°çš„æ ¼å¼
def convert_state_dict(orig_state_dict, config):
    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®çš„å‰¯æœ¬
    for key in orig_state_dict.copy().keys():
        # å¼¹å‡ºåŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„é”®ï¼Œå¹¶è·å–å¯¹åº”çš„å€¼
        val = orig_state_dict.pop(key)

        # å¦‚æœé”®ä»¥"clip_model"å¼€å¤´ä¸”åŒ…å«"attn.in_proj"
        if key.startswith("clip_model") and "attn.in_proj" in key:
            # æ ¹æ®é”®çš„ç»“æ„æå–ä¿¡æ¯
            key_split = key.split(".")
            # å¦‚æœé”®ä¸­åŒ…å«"visual"
            if "visual" in key:
                # è·å–å±‚ç¼–å·å’Œç»´åº¦
                layer_num = int(key_split[4])
                dim = config.vision_config.hidden_size
                prefix = "vision_model"
            else:
                # è·å–å±‚ç¼–å·å’Œç»´åº¦
                layer_num = int(key_split[3])
                dim = config.text_config.hidden_size
                prefix = "text_model"

            # å¦‚æœé”®ä¸­åŒ…å«"weight"
            if "weight" in key:
                # æ›´æ–°æ–°çš„é”®å€¼å¯¹ï¼Œé‡å‘½åé”®ä»¥åŒ¹é…æ–°æ¨¡å‹ç»“æ„
                orig_state_dict[f"clip.{prefix}.encoder.layers.{layer_num}.self_attn.q_proj.weight"] = val[:dim, :]
                orig_state_dict[f"clip.{prefix}.encoder.layers.{layer_num}.self_attn.k_proj.weight"] = val[dim : dim * 2, :]
                orig_state_dict[f"clip.{prefix}.encoder.layers.{layer_num}.self_attn.v_proj.weight"] = val[-dim:, :]
            else:
                # æ›´æ–°æ–°çš„é”®å€¼å¯¹ï¼Œé‡å‘½åé”®ä»¥åŒ¹é…æ–°æ¨¡å‹ç»“æ„
                orig_state_dict[f"clip.{prefix}.encoder.layers.{layer_num}.self_attn.q_proj.bias"] = val[:dim]
                orig_state_dict[f"clip.{prefix}.encoder.layers.{layer_num}.self_attn.k_proj.bias"] = val[dim : dim * 2]
                orig_state_dict[f"clip.{prefix}.encoder.layers.{layer_num}.self_attn.v_proj.bias"] = val[-dim:]
        # å¦‚æœé”®ä¸­åŒ…å«"self_attn"ä½†ä¸åŒ…å«"out_proj"
        elif "self_attn" in key and "out_proj" not in key:
            # æ ¹æ®é”®çš„ç»“æ„æå–ä¿¡æ¯
            key_split = key.split(".")
            layer_num = int(key_split[1])
            dim = config.reduce_dim
            # å¦‚æœé”®ä¸­åŒ…å«"weight"
            if "weight" in key:
                # æ›´æ–°æ–°çš„é”®å€¼å¯¹ï¼Œé‡å‘½åé”®ä»¥åŒ¹é…æ–°æ¨¡å‹ç»“æ„
                orig_state_dict[f"decoder.layers.{layer_num}.self_attn.q_proj.weight"] = val[:dim, :]
                orig_state_dict[f"decoder.layers.{layer_num}.self_attn.k_proj.weight"] = val[dim : dim * 2, :]
                orig_state_dict[f"decoder.layers.{layer_num}.self_attn.v_proj.weight"] = val[-dim:, :]
            else:
                # æ›´æ–°æ–°çš„é”®å€¼å¯¹ï¼Œé‡å‘½åé”®ä»¥åŒ¹é…æ–°æ¨¡å‹ç»“æ„
                orig_state_dict[f"decoder.layers.{layer_num}.self_attn.q_proj.bias"] = val[:dim]
                orig_state_dict[f"decoder.layers.{layer_num}.self_attn.k_proj.bias"] = val[dim : dim * 2]
                orig_state_dict[f"decoder.layers.{layer_num}.self_attn.v_proj.bias"] = val[-dim:]
        else:
            # é‡å‘½åé”®ä»¥åŒ¹é…æ–°æ¨¡å‹ç»“æ„
            new_name = rename_key(key)
            # å¦‚æœæ–°é”®ä¸­åŒ…å«"visual_projection"æˆ–"text_projection"ï¼Œåˆ™å¯¹å€¼è¿›è¡Œè½¬ç½®
            if "visual_projection" in new_name or "text_projection" in new_name:
                val = val.T
            orig_state_dict[new_name] = val

    return orig_state_dict


# æˆ‘ä»¬å°†åœ¨ä¸€å¼ å¯çˆ±çš„çŒ«çš„å›¾ç‰‡ä¸ŠéªŒè¯ç»“æœ
def prepare_img():
    # å›¾ç‰‡çš„ URL
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # ä» URL ä¸­è·å–å›¾ç‰‡
    image = Image.open(requests.get(url, stream=True).raw)
    # è¿”å›å›¾ç‰‡å¯¹è±¡
    return image


def convert_clipseg_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub):
    # è·å– CLIPSeg çš„é…ç½®
    config = get_clipseg_config(model_name)
    # åˆ›å»º CLIPSeg æ¨¡å‹å¯¹è±¡
    model = CLIPSegForImageSegmentation(config)
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # åŠ è½½æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    state_dict = torch.load(checkpoint_path, map_location="cpu")

    # ç§»é™¤ä¸€äº›é”®
``` 
    # éå†çŠ¶æ€å­—å…¸çš„æ‹·è´ä¸­çš„é”®
    for key in state_dict.copy().keys():
        # å¦‚æœé”®ä»¥ "model" å¼€å¤´ï¼Œåˆ™ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤è¯¥é”®
        if key.startswith("model"):
            state_dict.pop(key, None)

    # å¯¹ä¸€äº›é”®è¿›è¡Œé‡å‘½å
    state_dict = convert_state_dict(state_dict, config)
    # è½½å…¥æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå…è®¸ç¼ºå¤±é”®
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

    # å¦‚æœç¼ºå¤±çš„é”®ä¸æ˜¯ ["clip.text_model.embeddings.position_ids", "clip.vision_model.embeddings.position_ids"]ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯
    if missing_keys != ["clip.text_model.embeddings.position_ids", "clip.vision_model.embeddings.position_ids"]:
        raise ValueError("Missing keys that are not expected: {}".format(missing_keys))
    # å¦‚æœæ„å¤–çš„é”®ä¸æ˜¯ ["decoder.reduce.weight", "decoder.reduce.bias"]ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯
    if unexpected_keys != ["decoder.reduce.weight", "decoder.reduce.bias"]:
        raise ValueError(f"Unexpected keys: {unexpected_keys}")

    # åˆ›å»º ViTImageProcessor å¯¹è±¡ï¼ŒæŒ‡å®šå¤§å°ä¸º 352
    image_processor = ViTImageProcessor(size=352)
    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ CLIPTokenizer å¯¹è±¡
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    # åˆ›å»º CLIPSegProcessor å¯¹è±¡ï¼ŒæŒ‡å®šå›¾åƒå¤„ç†å™¨å’Œåˆ†è¯å™¨
    processor = CLIPSegProcessor(image_processor=image_processor, tokenizer=tokenizer)

    # å‡†å¤‡å›¾åƒæ•°æ®
    image = prepare_img()
    # å‡†å¤‡æ–‡æœ¬æ•°æ®
    text = ["a glass", "something to fill", "wood", "a jar"]

    # å¤„ç†æ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼Œè¿”å› PyTorch å¼ é‡è¾“å…¥
    inputs = processor(text=text, images=[image] * len(text), padding="max_length", return_tensors="pt")

    # å…³é—­æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡å‹å¤„ç†è¾“å…¥æ•°æ®ï¼Œè·å–è¾“å‡º
        outputs = model(**inputs)

    # éªŒè¯è¾“å‡ºå€¼æ˜¯å¦ç¬¦åˆé¢„æœŸ
    expected_conditional = torch.tensor([0.1110, -0.1882, 0.1645])
    expected_pooled_output = torch.tensor([0.2692, -0.7197, -0.1328])
    if model_name == "clipseg-rd64-refined":
        expected_masks_slice = torch.tensor(
            [[-10.0407, -9.9431, -10.2646], [-9.9751, -9.7064, -9.9586], [-9.6891, -9.5645, -9.9618]]
        )
    elif model_name == "clipseg-rd64":
        expected_masks_slice = torch.tensor(
            [[-7.2877, -7.2711, -7.2463], [-7.2652, -7.2780, -7.2520], [-7.2239, -7.2204, -7.2001]]
        )
    elif model_name == "clipseg-rd16":
        expected_masks_slice = torch.tensor(
            [[-6.3955, -6.4055, -6.4151], [-6.3911, -6.4033, -6.4100], [-6.3474, -6.3702, -6.3762]]
        )
    else:
        # å¦‚æœæ¨¡å‹åç§°ä¸å—æ”¯æŒï¼Œåˆ™å¼•å‘å€¼é”™è¯¯
        raise ValueError(f"Model name {model_name} not supported.")

    # æ–­è¨€è¾“å‡ºçš„ logits çš„éƒ¨åˆ†å€¼æ¥è¿‘é¢„æœŸçš„ masks_sliceï¼Œå®¹å·®ä¸º 1e-3
    assert torch.allclose(outputs.logits[0, :3, :3], expected_masks_slice, atol=1e-3)
    # æ–­è¨€è¾“å‡ºçš„ conditional_embeddings çš„éƒ¨åˆ†å€¼æ¥è¿‘é¢„æœŸçš„ conditionalï¼Œå®¹å·®ä¸º 1e-3
    assert torch.allclose(outputs.conditional_embeddings[0, :3], expected_conditional, atol=1e-3)
    # æ–­è¨€è¾“å‡ºçš„ pooled_output çš„éƒ¨åˆ†å€¼æ¥è¿‘é¢„æœŸçš„ pooled_outputï¼Œå®¹å·®ä¸º 1e-3
    assert torch.allclose(outputs.pooled_output[0, :3], expected_pooled_output, atol=1e-3)
    # æ‰“å°ä¿¡æ¯è¡¨æ˜ä¸€åˆ‡æ­£å¸¸
    print("Looks ok!")

    # å¦‚æœæŒ‡å®šäº† PyTorch è½¬å‚¨æ–‡ä»¶å¤¹è·¯å¾„
    if pytorch_dump_folder_path is not None:
        # æ‰“å°ä¿¡æ¯è¡¨æ˜æ­£åœ¨ä¿å­˜æ¨¡å‹å’Œå¤„ç†å™¨
        print(f"Saving model and processor to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # å°†å¤„ç†å™¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        # æ‰“å°ä¿¡æ¯è¡¨æ˜æ­£åœ¨æ¨é€æ¨¡å‹å’Œå¤„ç†å™¨åˆ° Hub
        print(f"Pushing model and processor for {model_name} to the hub")
        # å°†æ¨¡å‹æ¨é€åˆ° Hub
        model.push_to_hub(f"CIDAS/{model_name}")
        # å°†å¤„ç†å™¨æ¨é€åˆ° Hub
        processor.push_to_hub(f"CIDAS/{model_name}")
# å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€å‚æ•°
    parser.add_argument(
        "--model_name",
        default="clipseg-rd64",
        type=str,
        choices=["clipseg-rd16", "clipseg-rd64", "clipseg-rd64-refined"],
        help=(
            "Name of the model. Supported models are: clipseg-rd64, clipseg-rd16 and clipseg-rd64-refined (rd meaning"
            " reduce dimension)"
        ),
    )
    parser.add_argument(
        "--checkpoint_path",
        default="/Users/nielsrogge/Documents/CLIPSeg/clip_plus_rd64-uni.pth",
        type=str,
        help=(
            "Path to the original checkpoint. Note that the script assumes that the checkpoint includes both CLIP and"
            " the decoder weights."
        ),
    )
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°ï¼Œå°† CLIPSeg æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
    convert_clipseg_checkpoint(args.model_name, args.checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub)
```