# `.\transformers\models\musicgen\convert_musicgen_transformers.py`

```
# è®¾ç½®ç¼–ç æ ¼å¼ä¸º UTF-8
# ç‰ˆæƒå£°æ˜ï¼šç‰ˆæƒå½’ 2023 å¹´ HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰
#
# æ ¹æ® Apache è®¸å¯ 2.0 ç‰ˆæœ¬ï¼ˆ"è®¸å¯è¯"ï¼‰è·å¾—è®¸å¯ï¼›
# é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æœ¬è½¯ä»¶æŒ‰"åŸæ ·"åˆ†å‘ï¼Œ
# ä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶
# æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è®¸å¯è¯
"""ä»åŸå§‹å­˜å‚¨åº“è½¬æ¢ MusicGen æ£€æŸ¥ç‚¹"""
# å¯¼å…¥å¿…è¦çš„åº“
import argparse
from pathlib import Path
from typing import Dict, OrderedDict, Tuple

import torch
# ä» audiocraft åº“ä¸­å¯¼å…¥ MusicGen æ¨¡å‹
from audiocraft.models import MusicGen
# å¯¼å…¥ HuggingFace åº“ä¸­çš„ç›¸å…³æ¨¡å—
from transformers import (
    AutoFeatureExtractor,
    AutoTokenizer,
    EncoderModel,
    MusicgenDecoderConfig,
    MusicgenForConditionalGeneration,
    MusicgenProcessor,
    T5EncoderModel,
)
# å¯¼å…¥ MusicGen æ¨¡å‹çš„ç›¸å…³ç»„ä»¶
from transformers.models.musicgen.modeling_musicgen import MusicgenForCausalLM
# å¯¼å…¥æ—¥å¿—è®°å½•å·¥å…·
from transformers.utils import logging

# è®¾ç½®æ—¥å¿—è®°å½•çš„è¯¦ç»†ç¨‹åº¦ä¸ºä¿¡æ¯çº§åˆ«
logging.set_verbosity_info()
# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# é¢„æœŸä¸¢å¤±çš„é”®åˆ—è¡¨
EXPECTED_MISSING_KEYS = ["model.decoder.embed_positions.weights"]

# å®šä¹‰é‡å‘½åå‡½æ•°
def rename_keys(name):
    # å¦‚æœåç§°ä¸­åŒ…å« "emb"ï¼Œåˆ™æ›¿æ¢ä¸º "model.decoder.embed_tokens"
    if "emb" in name:
        name = name.replace("emb", "model.decoder.embed_tokens")
    # å¦‚æœåç§°ä¸­åŒ…å« "transformer"ï¼Œåˆ™æ›¿æ¢ä¸º "model.decoder"
    if "transformer" in name:
        name = name.replace("transformer", "model.decoder")
    # å¦‚æœåç§°ä¸­åŒ…å« "cross_attention"ï¼Œåˆ™æ›¿æ¢ä¸º "encoder_attn"
    if "cross_attention" in name:
        name = name.replace("cross_attention", "encoder_attn")
    # å¦‚æœåç§°ä¸­åŒ…å« "linear1"ï¼Œåˆ™æ›¿æ¢ä¸º "fc1"
    if "linear1" in name:
        name = name.replace("linear1", "fc1")
    # å¦‚æœåç§°ä¸­åŒ…å« "linear2"ï¼Œåˆ™æ›¿æ¢ä¸º "fc2"
    if "linear2" in name:
        name = name.replace("linear2", "fc2")
    # å¦‚æœåç§°ä¸­åŒ…å« "norm1"ï¼Œåˆ™æ›¿æ¢ä¸º "self_attn_layer_norm"
    if "norm1" in name:
        name = name.replace("norm1", "self_attn_layer_norm")
    # å¦‚æœåç§°ä¸­åŒ…å« "norm_cross"ï¼Œåˆ™æ›¿æ¢ä¸º "encoder_attn_layer_norm"
    if "norm_cross" in name:
        name = name.replace("norm_cross", "encoder_attn_layer_norm")
    # å¦‚æœåç§°ä¸­åŒ…å« "norm2"ï¼Œåˆ™æ›¿æ¢ä¸º "final_layer_norm"
    if "norm2" in name:
        name = name.replace("norm2", "final_layer_norm")
    # å¦‚æœåç§°ä¸­åŒ…å« "out_norm"ï¼Œåˆ™æ›¿æ¢ä¸º "model.decoder.layer_norm"
    if "out_norm" in name:
        name = name.replace("out_norm", "model.decoder.layer_norm")
    # å¦‚æœåç§°ä¸­åŒ…å« "linears"ï¼Œåˆ™æ›¿æ¢ä¸º "lm_heads"
    if "linears" in name:
        name = name.replace("linears", "lm_heads")
    # å¦‚æœåç§°ä¸­åŒ…å«ç‰¹å®šå­—ç¬¦ä¸²ï¼Œè¿›è¡Œæ›¿æ¢
    if "condition_provider.conditioners.description.output_proj" in name:
        name = name.replace("condition_provider.conditioners.description.output_proj", "enc_to_dec_proj")
    # è¿”å›ä¿®æ”¹åçš„åç§°
    return name

# å®šä¹‰å‡½æ•°ï¼Œå¯¹ç»™å®šçš„ state_dict è¿›è¡Œé‡å‘½åå¤„ç†
def rename_state_dict(state_dict: OrderedDict, hidden_size: int) -> Tuple[Dict, Dict]:
    """å‡½æ•°ç”¨äºæ ¹æ® HF æ¨¡å—åç§°å¯¹ fairseq Musicgen state_dict è¿›è¡Œé‡å‘½åå¤„ç†ã€‚
    å®ƒè¿›ä¸€æ­¥å°†çŠ¶æ€å­—å…¸åˆ†ä¸ºè§£ç å™¨ï¼ˆLMï¼‰çŠ¶æ€å­—å…¸å’Œç¼–ç å™¨-è§£ç å™¨æŠ•å½±çš„çŠ¶æ€å­—å…¸ã€‚"""
    # è·å–çŠ¶æ€å­—å…¸çš„é”®åˆ—è¡¨
    keys = list(state_dict.keys())
    # åˆå§‹åŒ–ç¼–ç å™¨-è§£ç å™¨æŠ•å½±çš„çŠ¶æ€å­—å…¸
    enc_dec_proj_state_dict = {}
    # éå†ç»™å®šçš„é”®åˆ—è¡¨
    for key in keys:
        # å¼¹å‡ºå½“å‰é”®å¯¹åº”çš„å€¼
        val = state_dict.pop(key)
        # é‡å‘½åå½“å‰é”®
        key = rename_keys(key)
        # æ£€æŸ¥æ˜¯å¦åŒ…å« "in_proj_weight" å­—ç¬¦ä¸²
        if "in_proj_weight" in key:
            # æ‹†åˆ†èåˆçš„ qkv æŠ•å½±
            state_dict[key.replace("in_proj_weight", "q_proj.weight")] = val[:hidden_size, :]
            state_dict[key.replace("in_proj_weight", "k_proj.weight")] = val[hidden_size : 2 * hidden_size, :]
            state_dict[key.replace("in_proj_weight", "v_proj.weight")] = val[-hidden_size:, :]
        # æ£€æŸ¥æ˜¯å¦åŒ…å« "enc_to_dec_proj" å­—ç¬¦ä¸²
        elif "enc_to_dec_proj" in key:
            # å°†å½“å‰é”®å€¼å¯¹æ·»åŠ åˆ°æ–°å­—å…¸ä¸­
            enc_dec_proj_state_dict[key[len("enc_to_dec_proj.") :]] = val
        else:
            # å¦åˆ™å°†å½“å‰é”®å€¼å¯¹æ·»åŠ åˆ°åŸå­—å…¸ä¸­
            state_dict[key] = val
    # è¿”å›ç»è¿‡å¤„ç†çš„å­—å…¸å’Œé¢å¤–çš„å­—å…¸
    return state_dict, enc_dec_proj_state_dict
# ä»æ£€æŸ¥ç‚¹åç§°ä¸­è§£æå‡º Musicgen è§£ç å™¨çš„é…ç½®ä¿¡æ¯ï¼Œå¹¶è¿”å› MusicgenDecoderConfig å¯¹è±¡
def decoder_config_from_checkpoint(checkpoint: str) -> MusicgenDecoderConfig:
    # æ ¹æ®ä¸åŒçš„æ£€æŸ¥ç‚¹åç§°è®¾ç½®ä¸åŒçš„éšè—å±‚å¤§å°ã€éšè—å±‚æ•°é‡å’Œæ³¨æ„åŠ›å¤´æ•°é‡
    if checkpoint == "small" or checkpoint == "facebook/musicgen-stereo-small":
        hidden_size = 1024
        num_hidden_layers = 24
        num_attention_heads = 16
    elif checkpoint == "medium" or checkpoint == "facebook/musicgen-stereo-medium":
        hidden_size = 1536
        num_hidden_layers = 48
        num_attention_heads = 24
    elif checkpoint == "large" or checkpoint == "facebook/musicgen-stereo-large":
        hidden_size = 2048
        num_hidden_layers = 48
        num_attention_heads = 32
    else:
        raise ValueError(
            "Checkpoint should be one of `['small', 'medium', 'large']` for the mono checkpoints, "
            "or `['facebook/musicgen-stereo-small', 'facebook/musicgen-stereo-medium', 'facebook/musicgen-stereo-large']` "
            f"for the stereo checkpoints, got {checkpoint}."
        )

    # æ ¹æ®æ£€æŸ¥ç‚¹åç§°ä¸­æ˜¯å¦åŒ…å« "stereo" è®¾ç½®éŸ³é¢‘é€šé“å’Œç æœ¬æ•°é‡
    if "stereo" in checkpoint:
        audio_channels = 2
        num_codebooks = 8
    else:
        audio_channels = 1
        num_codebooks = 4

    # åˆ›å»º MusicgenDecoderConfig å¯¹è±¡å¹¶è¿”å›
    config = MusicgenDecoderConfig(
        hidden_size=hidden_size,
        ffn_dim=hidden_size * 4,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads,
        num_codebooks=num_codebooks,
        audio_channels=audio_channels,
    )
    return config


@torch.no_grad()
def convert_musicgen_checkpoint(
    checkpoint, pytorch_dump_folder=None, repo_id=None, device="cpu", safe_serialization=False
):
    # è·å–é¢„è®­ç»ƒçš„ MusicGen æ¨¡å‹
    fairseq_model = MusicGen.get_pretrained(checkpoint, device=device)
    # æ ¹æ®æ£€æŸ¥ç‚¹ä¿¡æ¯è·å– MusicgenDecoderConfig å¯¹è±¡
    decoder_config = decoder_config_from_checkpoint(checkpoint)

    # è·å–è§£ç å™¨çš„çŠ¶æ€å­—å…¸
    decoder_state_dict = fairseq_model.lm.state_dict()
    # é‡å‘½åè§£ç å™¨çš„çŠ¶æ€å­—å…¸ä¸­çš„ç‰¹å®šé”®ï¼Œä¸éšè—å±‚å¤§å°å¯¹åº”
    decoder_state_dict, enc_dec_proj_state_dict = rename_state_dict(
        decoder_state_dict, hidden_size=decoder_config.hidden_size
    )

    # åŠ è½½ T5 æ–‡æœ¬ç¼–ç å™¨å’Œ 32KHz éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹
    text_encoder = T5EncoderModel.from_pretrained("t5-base")
    audio_encoder = EncodecModel.from_pretrained("facebook/encodec_32khz")
    # åˆå§‹åŒ– Musicgen è§£ç å™¨
    decoder = MusicgenForCausalLM(decoder_config).eval()

    # åŠ è½½è§£ç å™¨æƒé‡ï¼Œå¯èƒ½ä¼šç¼ºå°‘åµŒå…¥è¯å’Œç¼–ç å™¨-ï¿½ï¿½ç å™¨æŠ•å½±
    missing_keys, unexpected_keys = decoder.load_state_dict(decoder_state_dict, strict=False)

    # ç§»é™¤ç‰¹å®šé”®åçš„ç¼ºå¤±é”®
    for key in missing_keys.copy():
        if key.startswith(("text_encoder", "audio_encoder")) or key in EXPECTED_MISSING_KEYS:
            missing_keys.remove(key)

    if len(missing_keys) > 0:
        raise ValueError(f"Missing key(s) in state_dict: {missing_keys}")

    if len(unexpected_keys) > 0:
        raise ValueError(f"Unexpected key(s) in state_dict: {unexpected_keys}")

    # åˆå§‹åŒ–ç»¼åˆæ¨¡å‹
    model = MusicgenForConditionalGeneration(text_encoder=text_encoder, audio_encoder=audio_encoder, decoder=decoder)

    # åŠ è½½é¢„è®­ç»ƒçš„ç¼–ç å™¨-è§£ç å™¨æŠ•å½±
    model.enc_to_dec_proj.load_state_dict(enc_dec_proj_state_dict)

    # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›è¡Œæ­£å‘ä¼ æ’­
    # ç”Ÿæˆä¸€ä¸ªé•¿ä¸º2*decoder_config.num_codebooksçš„torché•¿æ•´å‹å¼ é‡ï¼Œå†reshapeä¸º2è¡Œï¼Œ-1åˆ—çš„å½¢çŠ¶
    input_ids = torch.arange(0, 2 * decoder_config.num_codebooks, dtype=torch.long).reshape(2, -1)
    # å°†input_ids reshapeä¸º2*decoder_config.num_codebooksè¡Œï¼Œ-1åˆ—çš„å½¢çŠ¶
    decoder_input_ids = input_ids.reshape(2 * decoder_config.num_codebooks, -1)

    # ä½¿ç”¨torch.no_grad()ä¸Šä¸‹æ–‡ï¼Œè®¡ç®—æ¨¡å‹ç”Ÿæˆçš„logitsï¼Œè¾“å…¥ä¸ºinput_idså’Œdecoder_input_ids
    with torch.no_grad():
        logits = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits

    # å¦‚æœlogitsçš„å½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸï¼ŒæŠ›å‡ºValueErrorå¼‚å¸¸
    if logits.shape != (2 * decoder_config.num_codebooks, 1, 2048):
        raise ValueError("Incorrect shape for logits")

    # ç°åœ¨æ„å»ºå¤„ç†å™¨
    # ä»é¢„è®­ç»ƒæ¨¡å‹"t5-base"åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained("t5-base")
    # ä»é¢„è®­ç»ƒæ¨¡å‹"facebook/encodec_32khz"åŠ è½½ç‰¹å¾æå–å™¨ï¼Œè®¾ç½®padding_sideä¸º"left"ï¼Œfeature_sizeä¸ºdecoder_config.audio_channels
    feature_extractor = AutoFeatureExtractor.from_pretrained(
        "facebook/encodec_32khz", padding_side="left", feature_size=decoder_config.audio_channels
    )

    # è®¾ç½®éŸ³ä¹ç”Ÿæˆå¤„ç†å™¨ï¼Œå…¶ä¸­åŒ…æ‹¬ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨
    processor = MusicgenProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)

    # è®¾ç½®é€‚å½“çš„bos/padä»¤ç‰Œid
    model.generation_config.decoder_start_token_id = 2048
    model.generation_config.pad_token_id = 2048

    # è®¾ç½®å…¶ä»–é»˜è®¤çš„ç”Ÿæˆé…ç½®å‚æ•°
    model.generation_config.max_length = int(30 * audio_encoder.config.frame_rate)
    model.generation_config.do_sample = True
    model.generation_config.guidance_scale = 3.0

    # å¦‚æœpytorch_dump_folderä¸ä¸ºç©º
    if pytorch_dump_folder is not None:
        # å¦‚æœpytorch_dump_folderä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ
        Path(pytorch_dump_folder).mkdir(exist_ok=True)
        # è¾“å‡ºä¿å­˜æ¨¡å‹çš„ä¿¡æ¯ï¼Œå°†æ¨¡å‹ä¿å­˜åˆ°pytorch_dump_folderï¼Œå¹¶ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–æ–¹å¼
        logger.info(f"Saving model {checkpoint} to {pytorch_dump_folder}")
        model.save_pretrained(pytorch_dump_folder, safe_serialization=safe_serialization)
        # å°†å¤„ç†å™¨ä¿å­˜åˆ°pytorch_dump_folder
        processor.save_pretrained(pytorch_dump_folder)

    # å¦‚æœrepo_idä¸ä¸ºç©º
    if repo_id:
        # è¾“å‡ºæ¨é€æ¨¡å‹çš„ä¿¡æ¯ï¼Œå°†æ¨¡å‹æ¨é€åˆ°repo_idï¼Œå¹¶ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–æ–¹å¼
        logger.info(f"Pushing model {checkpoint} to {repo_id}")
        model.push_to_hub(repo_id, safe_serialization=safe_serialization)
        # å°†å¤„ç†å™¨æ¨é€åˆ°repo_id
        processor.push_to_hub(repo_id)
# å¦‚æœå½“å‰è„šæœ¬æ˜¯ä¸»è„šæœ¬å…¥å£
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()

    # å®šä¹‰å¿…éœ€çš„å‚æ•°
    # å‚æ•°åç§°: --checkpoint
    # é»˜è®¤å€¼: "small"
    # å‚æ•°ç±»å‹: å­—ç¬¦ä¸²
    # å¸®åŠ©ä¿¡æ¯: æŒ‡å®šè¦è½¬æ¢çš„ MusicGen æ¨¡å‹çš„æ£€æŸ¥ç‚¹å¤§å°ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€:"['small', 'medium', 'large']" ç”¨äºå•å£°é“æ£€æŸ¥ç‚¹ï¼Œæˆ– "['facebook/musicgen-stereo-small', 'facebook/musicgen-stereo-medium', 'facebook/musicgen-stereo-large']" ç”¨äºç«‹ä½“å£°æ£€æŸ¥ç‚¹ã€‚
    parser.add_argument(
        "--checkpoint",
        default="small",
        type=str,
        help="Checkpoint size of the MusicGen model you'd like to convert. Can be one of: "
        "`['small', 'medium', 'large']` for the mono checkpoints, or "
        "`['facebook/musicgen-stereo-small', 'facebook/musicgen-stereo-medium', 'facebook/musicgen-stereo-large']` "
        "for the stereo checkpoints.",
    )

    # å‚æ•°åç§°: --pytorch_dump_folder
    # æ˜¯å¦å¿…éœ€: æ˜¯
    # é»˜è®¤å€¼: None
    # å‚æ•°ç±»å‹: å­—ç¬¦ä¸²
    # å¸®åŠ©ä¿¡æ¯: è¾“å‡º PyTorch æ¨¡å‹çš„ç›®å½•è·¯å¾„ã€‚
    parser.add_argument(
        "--pytorch_dump_folder",
        required=True,
        default=None,
        type=str,
        help="Path to the output PyTorch model directory.",
    )

    # å‚æ•°åç§°: --push_to_hub
    # é»˜è®¤å€¼: None
    # å‚æ•°ç±»å‹: å­—ç¬¦ä¸²
    # å¸®åŠ©ä¿¡æ¯: åœ¨ ğŸ¤— hub ä¸Šä¸Šä¼ è½¬æ¢åçš„æ¨¡å‹çš„ä½ç½®ã€‚
    parser.add_argument(
        "--push_to_hub", default=None, type=str, help="Where to upload the converted model on the ğŸ¤— hub."
    )

    # å‚æ•°åç§°: --device
    # é»˜è®¤å€¼: "cpu"
    # å‚æ•°ç±»å‹: å­—ç¬¦ä¸²
    # å¸®åŠ©ä¿¡æ¯: ç”¨äºè¿è¡Œè½¬æ¢çš„ Torch è®¾å¤‡, å¯ä»¥æ˜¯ cpu æˆ– cudaã€‚
    parser.add_argument(
        "--device", default="cpu", type=str, help="Torch device to run the conversion, either cpu or cuda."
    )

    # å‚æ•°åç§°: --safe_serialization
    # æ˜¯å¦ä¸ºæ ‡å¿—å‚æ•°: æ˜¯
    # å¸®åŠ©ä¿¡æ¯: æ˜¯å¦ä½¿ç”¨ `safetensors` ä¿å­˜æ¨¡å‹, æˆ–ä½¿ç”¨ä¼ ç»Ÿçš„ PyTorch æ–¹å¼ (ä½¿ç”¨ `pickle`)ã€‚
    parser.add_argument(
        "--safe_serialization",
        action="store_true",
        help="Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).",
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶èµ‹å€¼ç»™ args
    args = parser.parse_args()

    # è°ƒç”¨ convert_musicgen_checkpoint å‡½æ•°, ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ä½œä¸ºå‚æ•°
    convert_musicgen_checkpoint(args.checkpoint, args.pytorch_dump_folder, args.push_to_hub)
```