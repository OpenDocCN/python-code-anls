# `.\models\data2vec\convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8

# ç‰ˆæƒå£°æ˜åŠè®¸å¯è¯ä¿¡æ¯
# Copyright 2021 The HuggingFace Inc. team.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Convert Wav2Vec2 checkpoint."""

# å¯¼å…¥å¿…è¦çš„åº“
import argparse
import os
from functools import reduce

import fairseq  # å¯¼å…¥ fairseq åº“
import torch  # å¯¼å…¥ PyTorch åº“
from datasets import load_dataset  # å¯¼å…¥ load_dataset å‡½æ•°

from transformers import Wav2Vec2Processor, logging  # å¯¼å…¥ Wav2Vec2Processor å’Œ logging
from transformers.models.data2vec.configuration_data2vec_audio import Data2VecAudioConfig  # å¯¼å…¥ Data2VecAudioConfig

# ä» fairseq åº“å¤åˆ¶äº† Data2VecAudioModel åˆ«åä¸º Dummyï¼Œæœªä½¿ç”¨çš„å¯¼å…¥ï¼Œæ•…æ ‡è®°ä¸º F401
# Copied from https://github.com/pytorch/fairseq/blob/main/examples/data2vec/models/data2vec_audio.py
from transformers.models.data2vec.data2vec_audio import Data2VecAudioModel as Dummy  # noqa: F401
from transformers.models.data2vec.modeling_data2vec_audio import Data2VecAudioForCTC, Data2VecAudioModel  # å¯¼å…¥ç›¸å…³çš„æ¨¡å‹å®šä¹‰


logging.set_verbosity_info()  # è®¾ç½® logging çº§åˆ«ä¸º info
logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„ logger å¯¹è±¡

# æ˜ å°„å­—å…¸ï¼Œç”¨äºæ˜ å°„æ¨¡å‹ä¸­çš„å‚æ•°åç§°
MAPPING = {
    "post_extract_proj": "feature_projection.projection",
    "models.0.layer_norm": "feature_projection.layer_norm",
    "self_attn.k_proj": "encoder.layers.*.attention.k_proj",
    "self_attn.v_proj": "encoder.layers.*.attention.v_proj",
    "self_attn.q_proj": "encoder.layers.*.attention.q_proj",
    "self_attn.out_proj": "encoder.layers.*.attention.out_proj",
    "self_attn_layer_norm": "encoder.layers.*.layer_norm",
    "fc1": "encoder.layers.*.feed_forward.intermediate_dense",
    "fc2": "encoder.layers.*.feed_forward.output_dense",
    "final_layer_norm": "encoder.layers.*.final_layer_norm",
    "encoder.layer_norm": "encoder.layer_norm",
    "w2v_model.layer_norm": "feature_projection.layer_norm",
    "w2v_encoder.proj": "lm_head",
    "mask_emb": "masked_spec_embed",
}
TOP_LEVEL_KEYS = [
    "lm_head",
]


def set_recursively(hf_pointer, key, value, full_name, weight_type):
    # é€’å½’è®¾ç½®å‚æ•°å€¼çš„å‡½æ•°
    for attribute in key.split("."):
        hf_pointer = getattr(hf_pointer, attribute)  # è·å–æŒ‡å®šå±æ€§çš„å€¼

    if weight_type is not None:
        hf_shape = getattr(hf_pointer, weight_type).shape  # è·å–æŒ‡å®šæƒé‡ç±»å‹çš„å½¢çŠ¶ä¿¡æ¯
    else:
        hf_shape = hf_pointer.shape  # è·å–å¯¹è±¡çš„å½¢çŠ¶ä¿¡æ¯

    # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…ï¼Œè‹¥ä¸åŒ¹é…åˆ™å¼•å‘ ValueError
    if hf_shape != value.shape:
        raise ValueError(
            f"Shape of hf {key + '.' + weight_type if weight_type is not None else ''} is {hf_shape}, but should be"
            f" {value.shape} for {full_name}"
        )

    # æ ¹æ®æƒé‡ç±»å‹è®¾ç½®å‚æ•°å€¼
    if weight_type == "weight":
        hf_pointer.weight.data = value
    elif weight_type == "weight_g":
        hf_pointer.weight_g.data = value
    elif weight_type == "weight_v":
        hf_pointer.weight_v.data = value
    elif weight_type == "bias":
        hf_pointer.bias.data = value
    else:
        hf_pointer.data = value
    # ä½¿ç”¨æ—¥å¿—è®°å½•å™¨å¯¹è±¡è¾“å‡ºä¿¡æ¯ï¼Œæ ¼å¼åŒ–å­—ç¬¦ä¸²åŒ…å«åŠ¨æ€éƒ¨åˆ†
    logger.info(f"{key + '.' + weight_type if weight_type is not None else ''} was initialized from {full_name}.")
# é€’å½’åŠ è½½ Fairseq æ¨¡å‹çš„æƒé‡åˆ° Hugging Face æ¨¡å‹ä¸­
def recursively_load_weights(fairseq_model, hf_model, is_headless):
    # å­˜å‚¨æœªä½¿ç”¨çš„æƒé‡åˆ—è¡¨
    unused_weights = []
    # è·å– Fairseq æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    fairseq_dict = fairseq_model.state_dict()

    # æ ¹æ®æ˜¯å¦ headless è®¾ç½®ç‰¹å¾æå–å™¨å’Œä½ç½®å·ç§¯åµŒå…¥å™¨
    if not is_headless:
        feature_extractor = hf_model.data2vec_audio.feature_extractor
        pos_conv_embedding = hf_model.data2vec_audio.encoder.pos_conv_embed
    else:
        feature_extractor = hf_model.feature_extractor
        pos_conv_embedding = hf_model.encoder.pos_conv_embed

    # éå† Fairseq æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    for name, value in fairseq_dict.items():
        is_used = False
        # å¦‚æœåç§°ä¸­åŒ…å« "conv_layers"ï¼Œåˆ™åŠ è½½å·ç§¯å±‚æƒé‡
        if "conv_layers" in name:
            load_conv_layer(
                name,
                value,
                feature_extractor,
                unused_weights,
            )
            is_used = True
        # å¦‚æœåç§°ä¸­åŒ…å« "pos_conv"ï¼Œåˆ™åŠ è½½ä½ç½®å·ç§¯å±‚æƒé‡
        elif "pos_conv" in name:
            load_pos_conv_layer(
                name,
                value,
                pos_conv_embedding,
                unused_weights,
            )
            is_used = True
        else:
            # å¦åˆ™ï¼Œæ ¹æ®æ˜ å°„è¡¨ MAPPING åŠ è½½å¯¹åº”çš„æƒé‡
            for key, mapped_key in MAPPING.items():
                if not is_headless:
                    # æ ¹æ®æ¡ä»¶ä¿®æ”¹ mapped_key
                    mapped_key = "data2vec_audio." + mapped_key if mapped_key not in TOP_LEVEL_KEYS else mapped_key
                if key in name or key.split("w2v_model.")[-1] == name.split(".")[0]:
                    is_used = True
                    # å¦‚æœ mapped_key ä¸­åŒ…å« "*", åˆ™æ›¿æ¢ä¸ºå±‚ç´¢å¼•
                    if "*" in mapped_key:
                        layer_index = name.split(key)[0].split(".")[-2]
                        mapped_key = mapped_key.replace("*", layer_index)
                    # æ ¹æ®åç§°ç¡®å®šæƒé‡ç±»å‹
                    if "weight_g" in name:
                        weight_type = "weight_g"
                    elif "weight_v" in name:
                        weight_type = "weight_v"
                    elif "bias" in name:
                        weight_type = "bias"
                    elif "weight" in name:
                        # TODO: ä¸åŒ¹é… quantizer.weight_proj
                        weight_type = "weight"
                    else:
                        weight_type = None
                    # é€’å½’è®¾ç½®æƒé‡åˆ° Hugging Face æ¨¡å‹ä¸­
                    set_recursively(hf_model, mapped_key, value, name, weight_type)
                continue
        # å¦‚æœæœªä½¿ç”¨ï¼Œåˆ™å°†åç§°æ·»åŠ åˆ°æœªä½¿ç”¨æƒé‡åˆ—è¡¨ä¸­
        if not is_used:
            unused_weights.append(name)

    # è®°å½•æœªä½¿ç”¨çš„æƒé‡
    logger.warning(f"Unused weights: {unused_weights}")


# æ ¹æ®å­—ç¬¦ä¸²è·¯å¾„è®¿é—®æ¨¡å—ä¸­çš„å¯¹è±¡
def access_by_string(module, path):
    names = path.split(".")
    return reduce(getattr, names, module)


# è®¾ç½®æƒé‡åˆ°æŒ‡å®šè·¯å¾„çš„å‡½æ•°
def set_weights(full_name, module, fsq_value, hf_weight_path):
    # é€šè¿‡å­—ç¬¦ä¸²è·¯å¾„è·å– Hugging Face æ¨¡å‹ä¸­çš„æƒé‡
    hf_weight = access_by_string(module, hf_weight_path)
    hf_value = hf_weight.data

    # æ£€æŸ¥ Fairseq å’Œ Hugging Face æ¨¡å‹çš„æƒé‡å½¢çŠ¶æ˜¯å¦åŒ¹é…
    if fsq_value.shape != hf_value.shape:
        raise ValueError(f"{full_name} has size {fsq_value.shape}, but {hf_value.shape} was found.")
    # è®¾ç½® Fairseq æ¨¡å‹çš„å€¼åˆ° Hugging Face æ¨¡å‹çš„æƒé‡ä¸­
    hf_weight.data = fsq_value
    # è®°å½•æƒé‡åˆå§‹åŒ–æˆåŠŸçš„ä¿¡æ¯
    logger.info(f"{full_name} was correctly initialized from {hf_weight_path}.")


# åŠ è½½å·ç§¯å±‚æƒé‡çš„å‡½æ•°
def load_conv_layer(full_name, value, feature_extractor, unused_weights):
    # è·å–å·ç§¯å±‚åç§°
    name = full_name.split("conv_layers.")[-1]
    items = name.split(".")
    layer_id = int(items[0])
    type_id = int(items[1])

    weight_type = name.split(".")[-1]
    # å¦‚æœ type_id ç­‰äº 0ï¼Œåˆ™å°† layer_type è®¾ç½®ä¸º "conv"
    if type_id == 0:
        layer_type = "conv"
    # å¦‚æœ type_id ç­‰äº 2ï¼Œåˆ™å°† layer_type è®¾ç½®ä¸º "layer_norm"
    elif type_id == 2:
        layer_type = "layer_norm"
    # å¦‚æœ type_id ä¸æ˜¯ 0 ä¹Ÿä¸æ˜¯ 2ï¼Œåˆ™å°† full_name æ·»åŠ åˆ° unused_weights åˆ—è¡¨ä¸­å¹¶è¿”å›
    else:
        unused_weights.append(full_name)
        return

    # è°ƒç”¨ set_weights å‡½æ•°æ¥è®¾ç½®æƒé‡ï¼Œä½¿ç”¨ç»™å®šçš„ full_nameã€feature_extractorã€value å’Œæ‹¼æ¥çš„è·¯å¾„å­—ç¬¦ä¸²
    set_weights(full_name, feature_extractor, value, f"conv_layers.{layer_id}.{layer_type}.{weight_type}")
def load_pos_conv_layer(full_name, value, pos_conv_embeddings, unused_weights):
    # ä»å®Œæ•´åç§°ä¸­æå–å‡ºlayer_idå’Œtype_id
    name = full_name.split("pos_conv.")[-1]
    items = name.split(".")
    layer_id = int(items[0])
    type_id = int(items[1])

    # æå–æƒé‡ç±»å‹
    weight_type = name.split(".")[-1]
    
    # å¦‚æœtype_idä¸ä¸º0ï¼Œåˆ™å°†full_nameåŠ å…¥unused_weightsåˆ—è¡¨å¹¶è¿”å›
    if type_id != 0:
        unused_weights.append(full_name)
        return
    else:
        layer_type = "conv"

    # è°ƒç”¨set_weightså‡½æ•°ï¼Œè®¾ç½®æƒé‡
    set_weights(full_name, pos_conv_embeddings, value, f"layers.{layer_id}.{layer_type}.{weight_type}")


@torch.no_grad()
def convert_wav2vec2_checkpoint(
    checkpoint_path, pytorch_dump_folder_path, config_path=None, dict_path=None, is_finetuned=True
):
    """
    å°†æ¨¡å‹çš„æƒé‡å¤åˆ¶/ç²˜è´´/è°ƒæ•´ä¸ºtransformersè®¾è®¡ã€‚
    """
    # å¦‚æœæä¾›äº†config_pathï¼Œåˆ™ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½Data2VecAudioConfig
    if config_path is not None:
        config = Data2VecAudioConfig.from_pretrained(config_path)
    else:
        config = Data2VecAudioConfig()

    # å¦‚æœä¸æ˜¯finetunedçŠ¶æ€
    if not is_finetuned:
        # ä¿®æ”¹final_projå±‚çš„åç§°
        hf_wav2vec = Data2VecAudioModel(config)
        data2vec_checkpoint_dir = os.path.dirname(checkpoint_path)

        # åŠ è½½åŸå§‹checkpointçš„çŠ¶æ€å­—å…¸
        state_dict = torch.load(checkpoint_path)
        # è°ƒæ•´final_projå±‚æƒé‡å’Œåç½®çš„å‘½å
        state_dict["model"]["final_proj.weight"] = state_dict["model"].pop("final_proj.0.weight")
        state_dict["model"]["final_proj.bias"] = state_dict["model"].pop("final_proj.0.bias")
        # ä¿å­˜è½¬æ¢åçš„checkpoint
        converted_ckpt = os.path.join(data2vec_checkpoint_dir, "converted.pt")
        torch.save(state_dict, converted_ckpt)
    else:
        # åŠ è½½finetunedçŠ¶æ€çš„æ¨¡å‹
        hf_wav2vec = Data2VecAudioForCTC(config)
        converted_ckpt = checkpoint_path

    # å®šä¹‰å‡½æ•°ï¼Œç”¨äºåŠ è½½fairseqæ¨¡å‹
    def load_data2vec(path):
        model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([path])
        return model[0].eval()

    # åŠ è½½è½¬æ¢åçš„æ¨¡å‹
    model = load_data2vec(converted_ckpt)

    # é€’å½’åŠ è½½æƒé‡åˆ°hf_wav2vecæ¨¡å‹ä¸­
    recursively_load_weights(model, hf_wav2vec, not is_finetuned)

    # ä»é¢„è®­ç»ƒæ¨¡å‹facebook/wav2vec2-large-lv60åŠ è½½processor
    processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-lv60")

    # åŠ è½½LibriSpeech ASRçš„éªŒè¯é›†æ•°æ®é›†
    ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation")
    input_audio = [x["array"] for x in ds[:4]["audio"]]

    # ä½¿ç”¨processorå¯¹è¾“å…¥éŸ³é¢‘è¿›è¡Œå¤„ç†ï¼Œè¿”å›inputså­—å…¸
    inputs = processor(input_audio, return_tensors="pt", padding=True)

    # æå–inputsä¸­çš„input_valueså’Œattention_mask
    input_values = inputs.input_values
    attention_mask = inputs.attention_mask

    # æ‰“å°åŸå§‹çš„input_valueså’Œattention_maskï¼ˆå·²æ³¨é‡Šï¼‰
    # input_values = inputs.input_values[:, :-1]
    # attention_mask = inputs.attention_mask[:, :-1]

    # è®¾ç½®hf_wav2vecå’Œmodelä¸ºevalæ¨¡å¼
    hf_wav2vec.eval()
    model.eval()

    # å¦‚æœæ˜¯finetunedçŠ¶æ€
    if is_finetuned:
        # è·å–æ¨¡å‹é¢„æµ‹çš„è¾“å‡ºå’Œhf_wav2vecçš„è¾“å‡º
        their_output = model(source=input_values, padding_mask=(1 - attention_mask), mask=False, features_only=True)["encoder_out"].transpose(0, 1)
        our_output = hf_wav2vec(input_values, attention_mask=attention_mask)["logits"]

        # è®¡ç®—é¢„æµ‹çš„æ ‡ç­¾idï¼Œå¹¶é€šè¿‡processorè¿›è¡Œæ‰¹é‡è§£ç 
        pred_ids = torch.argmax(our_output, dim=-1)
        output_string = processor.batch_decode(pred_ids)

        # æ‰“å°é¢„æœŸè¾“å‡ºå’Œæ¨¡å‹é¢„æµ‹çš„è¾“å‡ºå­—ç¬¦ä¸²
        print(f"Expected Output: {ds[:4]['text']}, Pred: {output_string}")
    # å¦‚æœæ¡ä»¶ä¸ºå‡ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ
    else:
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè·å–è¾“å‡ºå¼ é‡
        their_output = model(
            source=input_values,  # è¾“å…¥æ•°æ®
            padding_mask=(1 - attention_mask),  # å¡«å……æ©ç 
            mask=False,  # ä¸ä½¿ç”¨é®ç½©
            features_only=True  # ä»…è¿”å›ç‰¹å¾ç»“æœ
        )["layer_results"][-1][0].transpose(0, 1)
        
        # ä½¿ç”¨hf_wav2vecæ¨¡å‹è·å–è¾“å‡ºå¼ é‡
        our_output = hf_wav2vec(input_values, attention_mask=attention_mask)["last_hidden_state"]

    # æ‰“å°æˆ‘ä»¬çš„è¾“å‡ºå’Œä»–ä»¬çš„è¾“å‡ºçš„å½¢çŠ¶
    print(our_output.shape, their_output.shape)
    
    # è®¡ç®—ä¸¤ä¸ªå¼ é‡ä¹‹é—´çš„æœ€å¤§ç»å¯¹å·®å¼‚
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    print(f"max_absolute_diff = {max_absolute_diff}")  # è¾“å‡ºæœ€å¤§ç»å¯¹å·®å¼‚ï¼Œé¢„æœŸåœ¨1e-7å·¦å³
    
    # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºå¼ é‡æ˜¯å¦åœ¨ç»™å®šçš„å®¹å·®èŒƒå›´å†…æ¥è¿‘
    success = torch.allclose(our_output, their_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")  # æ‰“å°ä¸¤ä¸ªæ¨¡å‹æ˜¯å¦è¾“å‡ºç›¸åŒçš„å¼ é‡
    
    # å¦‚æœè¾“å‡ºä¸æ¥è¿‘ï¼ŒæŠ›å‡ºå¼‚å¸¸
    if not success:
        raise Exception("Something went wRoNg")

    # å°†hf_wav2vecæ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    hf_wav2vec.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœæ¨¡å‹å·²ç»å¾®è°ƒï¼Œåˆ™ä¿å­˜processorï¼›å¦åˆ™ï¼Œä¿å­˜ç‰¹å¾æå–å™¨
    if is_finetuned:
        processor.save_pretrained(pytorch_dump_folder_path)
    else:
        processor.feature_extractor.save_pretrained(pytorch_dump_folder_path)
# å¦‚æœè¿™ä¸ªè„šæœ¬è¢«ç›´æ¥æ‰§è¡Œè€Œä¸æ˜¯è¢«å¯¼å…¥ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„è·¯å¾„
    parser.add_argument("--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model.")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼šfairseq æ£€æŸ¥ç‚¹çš„è·¯å¾„
    parser.add_argument("--checkpoint_path", default=None, type=str, help="Path to fairseq checkpoint")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼šå¾®è°ƒæ¨¡å‹çš„å­—å…¸è·¯å¾„
    parser.add_argument("--dict_path", default=None, type=str, help="Path to dict of fine-tuned model")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼šå¾…è½¬æ¢æ¨¡å‹çš„ HFï¼ˆHugging Faceï¼‰é…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument("--config_path", default=None, type=str, help="Path to hf config.json of model to convert")
    # æ·»åŠ ä¸€ä¸ªæ ‡å¿—å‚æ•°ï¼šæŒ‡ç¤ºå¾…è½¬æ¢æ¨¡å‹æ˜¯å¦æ˜¯ç»è¿‡å¾®è°ƒçš„æ¨¡å‹
    parser.add_argument(
        "--not_finetuned", action="store_true", help="Whether the model to convert is a fine-tuned model or not"
    )
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # è°ƒç”¨å‡½æ•° convert_wav2vec2_checkpointï¼Œä¼ é€’å‘½ä»¤è¡Œå‚æ•°ä»¥æ‰§è¡Œæ¨¡å‹è½¬æ¢æ“ä½œ
    convert_wav2vec2_checkpoint(
        args.checkpoint_path, args.pytorch_dump_folder_path, args.config_path, args.dict_path, not args.not_finetuned
    )
```