# `.\models\data2vec\convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py`

```
# æ–‡ä»¶ç¼–ç å£°æ˜
# ç‰ˆæƒå£°æ˜
# åŸºäºApacheè®¸å¯çš„ç‰ˆæƒå£°æ˜
# å¦‚æœé€‚ç”¨çš„è¯ï¼Œæ ¹æ®é€‚ç”¨æ³•å¾‹æˆ–å†™çš„çº¦å®šå‘å¸ƒè½¯ä»¶
#åœ¨æŒ‰ç…§"åŸæ ·"åŸºç¡€ä¸Šåˆ†å‘è½¯ä»¶æ—¶
#æ²¡æœ‰ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–è€…æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºçš„æˆ–è€…é»˜ç¤ºçš„
#æŸ¥çœ‹ç‰¹å®šè¯­è¨€çš„è®¸å¯è¯å’Œ
#è®¸å¯è¯ä¸‹çš„é™åˆ¶
# Wav2Vec2æ£€æŸ¥ç‚¹è½¬æ¢

# å¯¼å…¥å¿…è¦çš„åŒ…
import argparse
import os
from functools import reduce
import fairseq
import torch
from datasets import load_dataset
from transformers import Wav2Vec2Processor, logging
from transformers.models.data2vec.configuration_data2vec_audio import Data2VecAudioConfig

# ä»https://github.com/pytorch/fairseq/blob/main/examples/data2vec/models/data2vec_audio.pyä¸­æ‹·è´çš„
from transformers.models.data2vec.data2vec_audio import Data2VecAudioModel as Dummy  # noqa: F401
from transformers.models.data2vec.modeling_data2vec_audio import Data2VecAudioForCTC, Data2VecAudioModel

# è®¾ç½®æ—¥å¿—çº§åˆ«å’Œè·å–æ—¥å¿—è®°å½•å™¨
logging.set_verbosity_info()
logger = logging.get_logger(__name__)

# æ˜ å°„å…³ç³»å’Œé¡¶çº§é”®
MAPPING = {
    "post_extract_proj": "feature_projection.projection",
    "models.0.layer_norm": "feature_projection.layer_norm",
    "self_attn.k_proj": "encoder.layers.*.attention.k_proj",
    "self_attn.v_proj": "encoder.layers.*.attention.v_proj",
    "self_attn.q_proj": "encoder.layers.*.attention.q_proj",
    "self_attn.out_proj": "encoder.layers.*.attention.out_proj",
    "self_attn_layer_norm": "encoder.layers.*.layer_norm",
    "fc1": "encoder.layers.*.feed_forward.intermediate_dense",
    "fc2": "encoder.layers.*.feed_forward.output_dense",
    "final_layer_norm": "encoder.layers.*.final_layer_norm",
    "encoder.layer_norm": "encoder.layer_norm",
    "w2v_model.layer_norm": "feature_projection.layer_norm",
    "w2v_encoder.proj": "lm_head",
    "mask_emb": "masked_spec_embed",
}
TOP_LEVEL_KEYS = [
    "lm_head",
]

# é€’å½’è®¾ç½®å±æ€§å€¼
def set_recursively(hf_pointer, key, value, full_name, weight_type):
    # é€šè¿‡æ‹†åˆ†é”®åé€çº§è·å–å±æ€§æŒ‡é’ˆ
    for attribute in key.split("."):
        hf_pointer = getattr(hf_pointer, attribute)

    # æ ¹æ®æƒé‡ç±»å‹è®¾ç½®å±æ€§å€¼
    if weight_type is not None:
        hf_shape = getattr(hf_pointer, weight_type).shape
    else:
        hf_shape = hf_pointer.shape

    # æ ¡éªŒå½¢çŠ¶æ˜¯å¦åŒ¹é…
    if hf_shape != value.shape:
        raise ValueError(
            f"Shape of hf {key + '.' + weight_type if weight_type is not None else ''} is {hf_shape}, but should be"
            f" {value.shape} for {full_name}"
        )

    # æ ¹æ®æƒé‡ç±»å‹è®¾ç½®å±æ€§æ•°æ®
    if weight_type == "weight":
        hf_pointer.weight.data = value
    elif weight_type == "weight_g":
        hf_pointer.weight_g.data = value
    elif weight_type == "weight_v":
        hf_pointer.weight_v.data = value
    elif weight_type == "bias":
        hf_pointer.bias.data = value
    else:
        hf_pointer.data = value
    # ä½¿ç”¨æ—¥å¿—è®°å½•å™¨è¾“å‡ºåˆå§‹åŒ–ä¿¡æ¯ï¼Œå¦‚æœweight_typeä¸ä¸ºç©ºï¼Œåˆ™æ‹¼æ¥keyå’Œweight_typeï¼Œå¦åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
    logger.info(f"{key + '.' + weight_type if weight_type is not None else ''} was initialized from {full_name}.")
# é€’å½’åŠ è½½ Fairseq æ¨¡å‹çš„æƒé‡åˆ° Hugging Face æ¨¡å‹ä¸­
def recursively_load_weights(fairseq_model, hf_model, is_headless):
    # è·å– Fairseq æ¨¡å‹çš„å‚æ•°å­—å…¸
    fairseq_dict = fairseq_model.state_dict()

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ headless æ¨¡å¼é€‰æ‹©ç‰¹å¾æå–å™¨å’Œä½ç½®å·ç§¯åµŒå…¥å±‚
    if not is_headless:
        feature_extractor = hf_model.data2vec_audio.feature_extractor
        pos_conv_embedding = hf_model.data2vec_audio.encoder.pos_conv_embed
    else:
        feature_extractor = hf_model.feature_extractor
        pos_conv_embedding = hf_model.encoder.pos_conv_embed

    # éå† Fairseq æ¨¡å‹çš„å‚æ•°å­—å…¸
    for name, value in fairseq_dict.items():
        is_used = False
        # å¦‚æœå‚æ•°åä¸­åŒ…å« "conv_layers"ï¼Œåˆ™åŠ è½½å·ç§¯å±‚å‚æ•°
        if "conv_layers" in name:
            load_conv_layer(
                name,
                value,
                feature_extractor,
                unused_weights,
            )
            is_used = True
        # å¦‚æœå‚æ•°åä¸­åŒ…å« "pos_conv"ï¼Œåˆ™åŠ è½½ä½ç½®å·ç§¯å±‚å‚æ•°
        elif "pos_conv" in name:
            load_pos_conv_layer(
                name,
                value,
                pos_conv_embedding,
                unused_weights,
            )
            is_used = True
        else:
            # éå† MAPPING å­—å…¸ï¼Œå°† Fairseq å‚æ•°åæ˜ å°„åˆ° Hugging Face å‚æ•°å
            for key, mapped_key in MAPPING.items():
                if not is_headless:
                    mapped_key = "data2vec_audio." + mapped_key if mapped_key not in TOP_LEVEL_KEYS else mapped_key
                if key in name or key.split("w2v_model.")[-1] == name.split(".")[0]:
                    is_used = True
                    # å¦‚æœæ˜ å°„çš„ Hugging Face å‚æ•°åä¸­åŒ…å«é€šé…ç¬¦ "*"ï¼Œåˆ™æ›¿æ¢ä¸ºå¯¹åº”çš„å±‚ç´¢å¼•
                    if "*" in mapped_key:
                        layer_index = name.split(key)[0].split(".")[-2]
                        mapped_key = mapped_key.replace("*", layer_index)
                    # æ ¹æ®å‚æ•°åä¸­çš„ä¿¡æ¯ç¡®å®šå‚æ•°ç±»å‹
                    if "weight_g" in name:
                        weight_type = "weight_g"
                    elif "weight_v" in name:
                        weight_type = "weight_v"
                    elif "bias" in name:
                        weight_type = "bias"
                    elif "weight" in name:
                        # TODO: ä¸è¦åŒ¹é… quantizer.weight_proj
                        weight_type = "weight"
                    else:
                        weight_type = None
                    # é€’å½’è®¾ç½® Hugging Face æ¨¡å‹çš„å‚æ•°å€¼
                    set_recursively(hf_model, mapped_key, value, name, weight_type)
                continue
        # å¦‚æœå‚æ•°æœªè¢«ä½¿ç”¨ï¼Œåˆ™å°†å…¶åŠ å…¥æœªä½¿ç”¨å‚æ•°åˆ—è¡¨
        if not is_used:
            unused_weights.append(name)

    # è®°å½•æœªä½¿ç”¨çš„å‚æ•°
    logger.warning(f"Unused weights: {unused_weights}")


# æ ¹æ®å­—ç¬¦ä¸²è·¯å¾„è®¿é—®æ¨¡å—çš„å±æ€§
def access_by_string(module, path):
    names = path.split(".")
    return reduce(getattr, names, module)


# è®¾ç½®æƒé‡å€¼
def set_weights(full_name, module, fsq_value, hf_weight_path):
    # è·å– Hugging Face æ¨¡å‹çš„å‚æ•°å€¼
    hf_weight = access_by_string(module, hf_weight_path)
    hf_value = hf_weight.data

    # æ£€æŸ¥ Fairseq å‚æ•°å€¼å’Œ Hugging Face å‚æ•°å€¼çš„å½¢çŠ¶æ˜¯å¦ä¸€è‡´
    if fsq_value.shape != hf_value.shape:
        raise ValueError(f"{full_name} has size {fsq_value.shape}, but {hf_value.shape} was found.")
    # è®¾ç½® Hugging Face æ¨¡å‹çš„å‚æ•°å€¼
    hf_weight.data = fsq_value
    logger.info(f"{full_name} was correctly initialized from {hf_weight_path}.")


# åŠ è½½å·ç§¯å±‚å‚æ•°
def load_conv_layer(full_name, value, feature_extractor, unused_weights):
    # è·å–å·ç§¯å±‚çš„åç§°å’Œç´¢å¼•
    name = full_name.split("conv_layers.")[-1]
    items = name.split(".")
    layer_id = int(items[0])
    type_id = int(items[1])

    # è·å–å‚æ•°ç±»å‹
    weight_type = name.split(".")[-1]
    # å¦‚æœ type_id ç­‰äº 0ï¼Œåˆ™å°† layer_type è®¾ç½®ä¸º "conv"
    if type_id == 0:
        layer_type = "conv"
    # å¦‚æœ type_id ç­‰äº 2ï¼Œåˆ™å°† layer_type è®¾ç½®ä¸º "layer_norm"
    elif type_id == 2:
        layer_type = "layer_norm"
    # å¦‚æœ type_id ä¸ç­‰äº 0 æˆ– 2ï¼Œåˆ™å°† full_name æ·»åŠ åˆ°æœªä½¿ç”¨æƒé‡åˆ—è¡¨ä¸­ï¼Œç„¶åè¿”å›
    else:
        unused_weights.append(full_name)
        return
    
    # è°ƒç”¨ set_weights å‡½æ•°ï¼Œè®¾ç½®æƒé‡å€¼
    # å‚æ•°åŒ…æ‹¬ full_nameï¼Œfeature_extractorï¼Œvalueï¼Œä»¥åŠä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨äºæè¿°æƒé‡ç±»å‹
    set_weights(full_name, feature_extractor, value, f"conv_layers.{layer_id}.{layer_type}.{weight_type}")
def load_pos_conv_layer(full_name, value, pos_conv_embeddings, unused_weights):
    # ä»å®Œæ•´åç§°ä¸­è·å–å±‚åç§°
    name = full_name.split("pos_conv.")[-1]
    # å°†å±‚åç§°åˆ†å‰²ä¸ºå…ƒç´ åˆ—è¡¨
    items = name.split(".")
    # æå–å±‚ ID å’Œç±»å‹ ID
    layer_id = int(items[0])
    type_id = int(items[1])

    # æå–æƒé‡ç±»å‹
    weight_type = name.split(".")[-1]
    # å¦‚æœç±»å‹ ID ä¸ä¸º0ï¼Œåˆ™å°†å®Œæ•´åç§°æ·»åŠ åˆ°æœªä½¿ç”¨æƒé‡åˆ—è¡¨å¹¶è¿”å›
    if type_id != 0:
        unused_weights.append(full_name)
        return
    else:
        layer_type = "conv"

    # ä½¿ç”¨è®¾ç½®æƒé‡å‡½æ•°è®¾ç½®æƒé‡
    set_weights(full_name, pos_conv_embeddings, value, f"layers.{layer_id}.{layer_type}.{weight_type}")


@torch.no_grad()
def convert_wav2vec2_checkpoint(
    checkpoint_path, pytorch_dump_folder_path, config_path=None, dict_path=None, is_finetuned=True
):
    """
    å°†æ¨¡å‹çš„æƒé‡å¤åˆ¶/ç²˜è´´/è°ƒæ•´åˆ° transformer è®¾è®¡ä¸­ã€‚
    """
    # å¦‚æœå­˜åœ¨é…ç½®è·¯å¾„ï¼Œåˆ™ä»é¢„è®­ç»ƒä¸­åŠ è½½ Data2VecAudioConfig
    if config_path is not None:
        config = Data2VecAudioConfig.from_pretrained(config_path)
    else:
        config = Data2VecAudioConfig()

    # å¦‚æœæ²¡æœ‰è¿›è¡Œå¾®è°ƒ
    if not is_finetuned:
        # ä¿®æ”¹ final_proj å±‚åç§°
        hf_wav2vec = Data2VecAudioModel(config)
        data2vec_checkpoint_dir = os.path.dirname(checkpoint_path)

        state_dict = torch.load(checkpoint_path)
        state_dict["model"]["final_proj.weight"] = state_dict["model"].pop("final_proj.0.weight")
        state_dict["model"]["final_proj.bias"] = state_dict["model"].pop("final_proj.0.bias")
        converted_ckpt = os.path.join(data2vec_checkpoint_dir, "converted.pt")
        torch.save(state_dict, converted_ckpt)
    else:
        hf_wav2vec = Data2VecAudioForCTC(config)
        converted_ckpt = checkpoint_path

    # åŠ è½½ Data2Vec æ¨¡å‹
    def load_data2vec(path):
        model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([path])
        return model[0].eval()

    model = load_data2vec(converted_ckpt)

    # é€’å½’åŠ è½½æƒé‡
    recursively_load_weights(model, hf_wav2vec, not is_finetuned)

    # ä»é¢„è®­ç»ƒä¸­åŠ è½½ Wav2Vec2Processor
    processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-lv60")

    # åŠ è½½æ•°æ®é›†
    ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation")
    input_audio = [x["array"] for x in ds[:4]["audio"]]

    # å¯¹è¾“å…¥è¿›è¡Œé¢„å¤„ç†
    inputs = processor(input_audio, return_tensors="pt", padding=True)

    input_values = inputs.input_values
    attention_mask = inputs.attention_mask
    
    # å°†è¾“å…¥è¿›è¡Œç¼–ç å¹¶è·å–é¢„æµ‹ç»“æœ
    hf_wav2vec.eval()
    model.eval()
    if is_finetuned:
        their_output = model(source=input_values, padding_mask=(1 - attention_mask), mask=False, features_only=True)["encoder_out"].transpose(0, 1)
        our_output = hf_wav2vec(input_values, attention_mask=attention_mask)["logits"]

        pred_ids = torch.argmax(our_output, dim=-1)
        output_string = processor.batch_decode(pred_ids)

        print(f"Expected Output: {ds[:4]['text']}, Pred: {output_string}")
    # å¦‚æœä¸æ˜¯ finetuned æ¨¡å‹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    else:
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè·å–å®ƒä»¬çš„è¾“å‡º
        their_output = model(source=input_values, padding_mask=(1 - attention_mask), mask=False, features_only=True)[
            "layer_results"
        ][-1][0].transpose(0, 1)
        # ä½¿ç”¨ hf_wav2vec å‡½æ•°è·å–è¾“å‡º
        our_output = hf_wav2vec(input_values, attention_mask=attention_mask)["last_hidden_state"]

    # æ‰“å°æˆ‘ä»¬çš„è¾“å‡ºå’Œå®ƒä»¬çš„è¾“å‡ºçš„å½¢çŠ¶
    print(our_output.shape, their_output.shape)
    # è®¡ç®—è¾“å‡ºä¹‹é—´çš„æœ€å¤§ç»å¯¹å·®å¼‚
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    # æ‰“å°æœ€å¤§ç»å¯¹å·®å¼‚çš„å€¼
    print(f"max_absolute_diff = {max_absolute_diff}")  # ~ 1e-7
    # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºæ˜¯å¦éå¸¸æ¥è¿‘
    success = torch.allclose(our_output, their_output, atol=1e-3)
    # æ‰“å°æ˜¯å¦ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºå®Œå…¨ç›¸åŒ
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")
    # å¦‚æœä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºä¸ç›¸åŒï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
    if not success:
        raise Exception("Something went wRoNg")

    # ä¿å­˜ hf_wav2vec æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
    hf_wav2vec.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœæ˜¯ finetuned æ¨¡å‹ï¼Œåˆ™ä¿å­˜ processor åˆ°æŒ‡å®šè·¯å¾„
    if is_finetuned:
        processor.save_pretrained(pytorch_dump_folder_path)
    # å¦‚æœä¸æ˜¯ finetuned æ¨¡å‹ï¼Œåˆ™ä¿å­˜ feature_extractor åˆ°æŒ‡å®šè·¯å¾„
    else:
        processor.feature_extractor.save_pretrained(pytorch_dump_folder_path)
# å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°--pytorch_dump_folder_pathï¼ŒæŒ‡å®šè¾“å‡ºçš„PyTorchæ¨¡å‹è·¯å¾„
    parser.add_argument("--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model.")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°--checkpoint_pathï¼ŒæŒ‡å®šfairseqæ£€æŸ¥ç‚¹è·¯å¾„
    parser.add_argument("--checkpoint_path", default=None, type=str, help="Path to fairseq checkpoint")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°--dict_pathï¼ŒæŒ‡å®šfine-tunedæ¨¡å‹çš„å­—å…¸è·¯å¾„
    parser.add_argument("--dict_path", default=None, type=str, help="Path to dict of fine-tuned model")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°--config_pathï¼ŒæŒ‡å®šè¦è½¬æ¢çš„æ¨¡å‹çš„hf config.jsonè·¯å¾„
    parser.add_argument("--config_path", default=None, type=str, help="Path to hf config.json of model to convert")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°--not_finetunedï¼ŒæŒ‡å®šè¦è½¬æ¢çš„æ¨¡å‹æ˜¯å¦æ˜¯fine-tunedæ¨¡å‹
    parser.add_argument(
        "--not_finetuned", action="store_true", help="Whether the model to convert is a fine-tuned model or not"
    )
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨convert_wav2vec2_checkpointå‡½æ•°ï¼Œå°†fairseqæ£€æŸ¥ç‚¹è½¬æ¢ä¸ºPyTorchæ¨¡å‹
    convert_wav2vec2_checkpoint(
        args.checkpoint_path, args.pytorch_dump_folder_path, args.config_path, args.dict_path, not args.not_finetuned
    )
```