# `.\models\nougat\tokenization_nougat_fast.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜ï¼Œå£°æ˜æ­¤ä»£ç çš„ç‰ˆæƒå½’ HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰ï¼Œä½¿ç”¨ Apache License 2.0 æˆæƒ
# æ ¹æ® Apache License 2.0 è®¸å¯è¯ï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼šhttp://www.apache.org/licenses/LICENSE-2.0
"""
Nougat çš„å¿«é€Ÿåˆ†è¯å™¨ç±»ã€‚
"""
# å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
import re
# å¯¼å…¥ functools æ¨¡å—çš„ partial å‡½æ•°
from functools import partial
# å¯¼å…¥ multiprocessing æ¨¡å—çš„ Pool ç±»
from multiprocessing import Pool
# å¯¼å…¥ List å’Œ Union ç±»å‹æç¤º
from typing import List, Union

# å¯¼å…¥ numpy åº“ï¼Œå¹¶ç”¨ np ä½œä¸ºåˆ«å
import numpy as np

# ä» transformers åº“ä¸­å¯¼å…¥ç›¸å…³çš„å‡½æ•°å’Œç±»
# INIT_TOKENIZER_DOCSTRING æ˜¯æ¥è‡ª tokenization_utils_base æ¨¡å—çš„ä¸€ä¸ªæ–‡æ¡£å­—ç¬¦ä¸²
from transformers.tokenization_utils_base import INIT_TOKENIZER_DOCSTRING
# å¯¼å…¥ PreTrainedTokenizerFast ç±»
from transformers.tokenization_utils_fast import PreTrainedTokenizerFast
# å¯¼å…¥ add_end_docstrings å‡½æ•°
from transformers.utils import add_end_docstrings

# å¯¼å…¥æœ¬åœ°å·¥å…·å‡½æ•°å’Œæ¨¡å—
# is_levenshtein_available å’Œ is_nltk_available æ˜¯æœ¬åœ°å·¥å…·å‡½æ•°
from ...utils import is_levenshtein_available, is_nltk_available, logging, requires_backends

# å¦‚æœ Levenshtein å¯ç”¨ï¼Œåˆ™ä» Levenshtein åº“å¯¼å…¥ ratio å‡½æ•°
if is_levenshtein_available():
    from Levenshtein import ratio

# å¦‚æœ NLTK å¯ç”¨ï¼Œåˆ™å¯¼å…¥ nltk åº“
if is_nltk_available():
    import nltk

# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# åœ¨ INIT_TOKENIZER_DOCSTRING åæ·»åŠ é¢å¤–çš„æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜
INIT_TOKENIZER_DOCSTRING += """
        tokenizer_object ([`tokenizers.Tokenizer`]):
            A [`tokenizers.Tokenizer`] object from ğŸ¤— tokenizers to instantiate from. See [Using tokenizers from ğŸ¤—
            tokenizers](../fast_tokenizers) for more information.
        tokenizer_file ([`str`]):
            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from ğŸ¤—
            tokenizers.
"""

# é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶æ˜ å°„ï¼ŒæŒ‡å®šäº†é¢„è®­ç»ƒæ¨¡å‹çš„ tokenizer_file çš„ URL
PRETRAINED_VOCAB_FILES_MAP = {
    "tokenizer_file": {
        "facebook/nougat-base": "https://huggingface.co/facebook/nougat-base/tokenizer/blob/main/tokenizer.json",
    },
}

# æŒ‡å®šè¯æ±‡æ–‡ä»¶çš„åç§°
VOCAB_FILES_NAMES = {"tokenizer_file": "tokenizer.json"}

# æŒ‡å®šé¢„è®­ç»ƒä½ç½®ç¼–ç çš„å°ºå¯¸
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {"facebook/nougat-base": 3584}


def markdown_compatible(text: str) -> str:
    """
    ä½¿æ–‡æœ¬å…¼å®¹ Markdown æ ¼å¼ã€‚

    æ­¤å‡½æ•°å¯¹æ–‡æœ¬è¿›è¡Œå„ç§æ ¼å¼è°ƒæ•´ï¼Œä»¥ä½¿å…¶ä¸ Markdown å…¼å®¹ã€‚

    Args:
        text (`str`):
            è¦ä½¿å…¶å…¼å®¹ Markdown çš„è¾“å…¥æ–‡æœ¬ã€‚

    Returns:
        `str`: å…¼å®¹ Markdown çš„æ–‡æœ¬ã€‚
    """
    # ç­‰å¼æ ‡ç­¾
    # ç”¨ \[some text\] æ ·å¼çš„æ¨¡å¼æ›¿æ¢ä»¥åè¿›åˆ¶æ•°å­—å¼€å¤´çš„è¡Œï¼Œå°†å…¶è½¬æ¢ä¸º \[[some text] \tag{decimal}\]ã€‚
    text = re.sub(r"^\(([\d.]+[a-zA-Z]?)\) \\\[(.+?)\\\]$", r"\[\2 \\tag{\1}\]", text, flags=re.M)
    # ç”¨ \[some text\] æ ·å¼çš„æ¨¡å¼æ›¿æ¢ä»¥åè¿›åˆ¶æ•°å­—ç»“å°¾çš„è¡Œï¼Œå°†å…¶è½¬æ¢ä¸º \[[some text] \tag{decimal}\]ã€‚
    text = re.sub(r"^\\\[(.+?)\\\] \(([\d.]+[a-zA-Z]?)\)$", r"\[\1 \\tag{\2}\]", text, flags=re.M)
    # ç”¨ \[some text\] æ ·å¼çš„æ¨¡å¼æ›¿æ¢ä»¥æ•°å­—å¼€å¤´ï¼Œä»¥ \[another text\] ç»“å°¾çš„è¡Œï¼Œå°†å…¶è½¬æ¢ä¸º \[[some text] \tag{digits}\] [another text].
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ–‡æœ¬ä¸­ç¬¦åˆç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå°†å…¶è½¬æ¢ä¸ºç‰¹å®šæ ¼å¼çš„æ ‡è®°
    text = re.sub(
        r"^\\\[(.+?)\\\] \(([\d.]+[a-zA-Z]?)\) (\\\[.+?\\\])$",
        r"\[\1 \\tag{\2}\] \3",
        text,
        flags=re.M,
    )
    # æ›¿æ¢æ–‡æœ¬ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²ï¼Œå»é™¤åæ–œæ å’Œå¥ç‚¹ä¹‹é—´çš„ç©ºæ ¼
    text = text.replace(r"\. ", ". ")
    # å°†æ–‡æœ¬ä¸­çš„ç‰¹å®šç²—ä½“æ ¼å¼ç¬¦å·æ›¿æ¢ä¸º LaTeX ä¸­çš„æ•°å­¦ç²—ä½“æ ¼å¼ç¬¦å·
    text = text.replace(r"\bm{", r"\mathbf{").replace(r"{\\bm ", r"\mathbf{")
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ–‡æœ¬ä¸­ç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå°†å…¶è½¬æ¢ä¸ºæ•°å­¦ç²—ä½“æ ¼å¼
    text = re.sub(r"\\mbox{ ?\\boldmath\$(.*?)\$}", r"\\mathbf{\1}", text)
    # å°†æ–‡æœ¬ä¸­çš„ URL æ ¼å¼åŒ–ä¸º Markdown å¯ç‚¹å‡»çš„é“¾æ¥æ ¼å¼
    text = re.sub(
        r"((?:http|ftp|https):\/\/(?:[\w_-]+(?:(?:\.[\w_-]+)+))(?:[\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-]))",
        r"[\1](\1)",
        text,
    )
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é‡æ–°æ ¼å¼åŒ–æ–‡æœ¬ä¸­çš„ç®—æ³•å—ï¼Œç¡®ä¿å…¶åœ¨ Markdown ä¸­æ˜¾ç¤ºä¸ºåˆé€‚çš„ä»£ç å—æ ¼å¼
    text = re.sub(r"```\s*(.+?)\s*```", r"```\n\1\n```", text, flags=re.S)

    # è¿”å›å¤„ç†åçš„æ–‡æœ¬ç»“æœ
    return text
# å°†æ–‡æœ¬ä¸­ç±»ä¼¼åˆ—è¡¨é¡¹çš„è¡Œè§„èŒƒåŒ–ã€‚å‡½æ•°æŸ¥æ‰¾ä»¥'-'æˆ–'*'å¼€å¤´çš„è¡Œï¼Œå¯èƒ½åè·Ÿè¡¨ç¤ºåµŒå¥—çº§åˆ«çš„ç½—é©¬æ•°å­—æˆ–æ•°å­—ã€‚
# å‡½æ•°é‡æ–°æ ¼å¼åŒ–è¿™äº›è¡Œï¼Œä½¿å…¶ç»“æ„æ›´åŠ æ¸…æ™°ã€‚
def normalize_list_like_lines(generation):
    # åŒ¹é…ä»¥'-'æˆ–'*'å¼€å¤´çš„è¡Œï¼Œåé¢ä¸æ˜¯'-'æˆ–'*'ï¼ˆåˆ—è¡¨ï¼‰ï¼Œå¯èƒ½åè·Ÿæ•°å­—\dæˆ–ç½—é©¬æ•°å­—ï¼Œç„¶åæ•è·æ­¤è¡Œçš„å¯é€‰é™„åŠ ç¼–å·ï¼Œ
    # ä¼ é€’ç»™re.finditerä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ã€‚
    pattern = r"(?:^)(-|\*)?(?!-|\*) ?((?:\d|[ixv])+ )?.+? (-|\*) (((?:\d|[ixv])+)\.(\d|[ixv]) )?.*(?:$)"

    # ä»¥é€†åºéå†ç”Ÿæˆçš„åŒ¹é…å¯¹è±¡åˆ—è¡¨
    for match in reversed(list(re.finditer(pattern, generation, flags=re.I | re.M))):
        start, stop = match.span()
        delim = match.group(3) + " "
        splits = match.group(0).split(delim)
        replacement = ""

        # å¦‚æœç¬¬ä¸€ä¸ªæ•è·ç»„åŒ¹é…ä¸æ˜¯Noneï¼Œåˆ™å¿½ç•¥ç¬¬ä¸€ä¸ªæ¡ç›®å¹¶ç»§ç»­
        if match.group(1) is not None:
            splits = splits[1:]
            delim1 = match.group(1) + " "
        else:
            delim1 = ""
            continue  # è·³è¿‡é”™è¯¯çš„æ­£åˆ™è¡¨è¾¾å¼ç»“æœ

        pre, post = generation[:start], generation[stop:]

        # å¤„ç†åˆ†å‰²çš„æ¡ç›®å¹¶ç”Ÿæˆæ›¿æ¢æ–‡æœ¬
        for i, item in enumerate(splits):
            level = 0
            potential_numeral, _, rest = item.strip().partition(" ")
            if not rest:
                continue
            # æ ¹æ®æ£€æµ‹åˆ°çš„ç¼–å·æ¨æ–­å½“å‰åµŒå¥—çº§åˆ«
            if re.match(r"^[\dixv]+((?:\.[\dixv])?)+$", potential_numeral, flags=re.I | re.M):
                level = potential_numeral.count(".")

            replacement += (
                ("\n" if i > 0 else "") + ("\t" * level) + (delim if i > 0 or start == 0 else delim1) + item.strip()
            )

        if post == "":
            post = "\n"

        generation = pre + replacement + post

    return generation


# æ‰¾åˆ°æ–‡æœ¬ä¸­ä¸‹ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·çš„ç´¢å¼•
def find_next_punctuation(text: str, start_idx=0):
    """
    Find the index of the next punctuation mark.

    Args:
        text (`str`):
            String to examine
        start_idx (`int`, *optional*)
            Index where to start
    """
    for i in range(start_idx, len(text)):
        if text[i] in [".", "?", "!", "\n"]:
            return i

    return None


# å°è¯•æˆªæ–­è¾“å…¥å­—ç¬¦ä¸²ä¸­çš„é‡å¤éƒ¨åˆ†
def truncate_repetitions(text: str, min_len: int = 30) -> str:
    """
    Attempt to truncate repeating segments in the input string.

    Args:
        text (str): The input text to process.
        min_len (int, optional): The minimum length of repeating segments to truncate.

    Returns:
        str: The processed text with repeated segments truncated.
    """
    # å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºå°å†™
    text_lower = text.lower()
    # è·å–è¾“å…¥æ–‡æœ¬çš„é•¿åº¦
    text_length = len(text_lower)

    # å¦‚æœè¾“å…¥æ–‡æœ¬é•¿åº¦å°äºæœ€å°é‡å¤æ®µé•¿åº¦çš„ä¸¤å€ï¼Œç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬
    if text_length < 2 * min_len:
        return text

    # å°è¯•æŸ¥æ‰¾å°¾éƒ¨é‡å¤çš„æœ€å¤§é•¿åº¦
    max_repetition_length = None
    for repetition_length in range(min_len, int(text_length / 2)):
        # æ£€æŸ¥å°¾éƒ¨æ˜¯å¦æœ‰é‡å¤
        same = True
        for i in range(0, repetition_length):
            if text_lower[text_length - repetition_length - i - 1] != text_lower[text_length - i - 1]:
                same = False
                break

        if same:
            max_repetition_length = repetition_length

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é‡å¤çš„éƒ¨åˆ†ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
    if max_repetition_length is None:
        return text

    # è·å–æœ€é•¿é‡å¤å­ä¸²
    lcs = text_lower[-max_repetition_length:]

    # ç§»é™¤é™¤æœ€åä¸€ä¸ªé‡å¤å¤–çš„æ‰€æœ‰é‡å¤
    substituted_text = text
    substituted_text_lower = text_lower
    while substituted_text_lower.endswith(lcs):
        substituted_text = substituted_text[:-max_repetition_length]
        substituted_text_lower = substituted_text_lower[:-max_repetition_length]

    # è·å–åŒ…å«é‡å¤çš„å°¾éƒ¨å†…å®¹
    repeating_tail = text_lower[len(substituted_text_lower):]

    # ä»æ–‡æœ¬å¼€å¤´æ·»åŠ å†…å®¹ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡ç‚¹ï¼Œå¹¶ç¡®ä¿æœ€åä¸€å¥ä¸é‡å¤
    substituted_text_lower_out = substituted_text_lower
    while True:
        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·çš„ä½ç½®
        sentence_end = find_next_punctuation(text_lower, len(substituted_text_lower_out))
        sentence_start = find_next_punctuation(text_lower[::-1], len(substituted_text_lower_out))
        if sentence_end and sentence_start:
            # æå–å½“å‰å¥å­
            sentence = text_lower[sentence_start:sentence_end]
            # æ›´æ–°è¾“å‡ºçš„æ–‡æœ¬ä¸ºå½“å‰ä½ç½®å‰çš„å†…å®¹
            substituted_text_lower_out = text_lower[:sentence_end + 1]
            # å¦‚æœå½“å‰å¥å­åœ¨é‡å¤çš„å°¾éƒ¨å‡ºç°ï¼Œç»“æŸå¾ªç¯
            if sentence in repeating_tail:
                break
        else:
            break

    # è·å–æœ€ç»ˆè¾“å‡ºçš„æ–‡æœ¬
    text_out = text[:len(substituted_text_lower_out)]

    return text_out
def remove_numbers(lines):
    def _clean(s):
        return re.sub(r"(?:[\d_]|\*\*)", "", s).strip()  # ç§»é™¤å­—ç¬¦ä¸²ä¸­çš„æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦

    if isinstance(lines, str):
        return _clean(lines)  # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™ç›´æ¥æ¸…ç†å¹¶è¿”å›è¯¥å­—ç¬¦ä¸²
    out = []
    for l in lines:
        out.append(_clean(l))  # å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªå­—ç¬¦ä¸²è¿›è¡Œæ¸…ç†ï¼Œå¹¶åŠ å…¥åˆ°è¾“å‡ºåˆ—è¡¨ä¸­
    return out  # è¿”å›æ¸…ç†åçš„å­—ç¬¦ä¸²åˆ—è¡¨


def get_slices(lines, clean_lines):
    """
    Get slices of text based on specific criteria within the lines.

    This function identifies and returns slices of text from the input lines based on certain conditions.

    These conditions were chosen by the Nougat authors:
    - The slice is less than 200 characters long.
    - The slice is more than 3 characters long.
    - The slice does not start with "[MISSING_PAGE".
    - The slice is either the same as the next slice or the ratio of the two in terms of Levensthein distance is
      greater than 0.9.

    Args:
        lines (`List[str]`):
            The list of lines containing the text.
        clean_lines (`List[str]`):
            A cleaned version of the text (without numbers).

    Returns:
        `List[tuple]`: A list of tuples representing the start and end indices of text slices.
    """
    indices = np.zeros(len(lines))  # åˆ›å»ºä¸€ä¸ªä¸è¾“å…¥è¡Œæ•°ç›¸åŒé•¿åº¦çš„é›¶æ•°ç»„
    for i in range(len(lines) - 1):
        j = i + 1
        while not clean_lines[j] and j < len(lines) - 1:
            j += 1  # è·³è¿‡ç©ºè¡Œæˆ–æ¸…ç†åä¸ºç©ºçš„è¡Œï¼Œç›´åˆ°æ‰¾åˆ°éç©ºè¡Œæˆ–æ¸…ç†åä¸ä¸ºç©ºçš„è¡Œ
        if (
            len(clean_lines[i]) < 200
            and len(clean_lines[i]) > 3
            and len(clean_lines[j]) < 200
            and len(clean_lines[j]) > 3
            and not clean_lines[i].startswith("[MISSING_PAGE")
            and (clean_lines[i] == clean_lines[j] or ratio(clean_lines[i], clean_lines[j]) > 0.9)
        ):
            indices[i:j] = 1  # æ ¹æ®æ¡ä»¶è®¾ç½®ç´¢å¼•æ•°ç»„ä¸­çš„æ ‡è®°ä¸º1
    ids = np.where(indices)[0]  # è·å–æ‰€æœ‰æ ‡è®°ä¸º1çš„ç´¢å¼•ä½ç½®
    slices = []
    if len(ids) == 0:
        return slices  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç´¢å¼•ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
    j0 = 0
    for j, x in enumerate(np.diff(ids) > 3):
        if x:
            slices.append((ids[j0], ids[j] + 2))  # å°†ç¬¦åˆæ¡ä»¶çš„ç‰‡æ®µèµ·å§‹å’Œç»“æŸç´¢å¼•æ·»åŠ åˆ°slicesåˆ—è¡¨ä¸­
            j0 = j + 1
    slices.append((ids[j0], ids[-1] + 2))  # æ·»åŠ æœ€åä¸€ä¸ªç¬¦åˆæ¡ä»¶çš„ç‰‡æ®µèµ·å§‹å’Œç»“æŸç´¢å¼•
    return [sli for sli in slices if sli[1] - sli[0] > 15]  # è¿”å›é•¿åº¦å¤§äº15çš„ç‰‡æ®µåˆ—è¡¨


def remove_slice_from_lines(lines, clean_text, slice) -> str:
    """
    Remove a slice of text from the lines based on specific criteria.

    This function identifies a slice of text within the lines and removes it based on certain conditions.

    Args:
        lines (list of str): The list of lines containing the text.
        clean_text (list of str): A cleaned version of the text (without numbers).
        slice (tuple): A tuple representing the start and end indices of the slice to be removed.

    Returns:
        str: The removed slice of text as a single string.
    """
    base = clean_text[slice[0]]  # è·å–è¦åˆ é™¤çš„ç‰‡æ®µçš„èµ·å§‹ä½ç½®çš„æ¸…ç†åæ–‡æœ¬
    section = list(slice)  # å°†è¦åˆ é™¤çš„ç‰‡æ®µè½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼
    check_start_flag = False  # åˆå§‹åŒ–æ£€æŸ¥å¼€å§‹æ ‡å¿—ä¸ºFalse
    # backwards pass, at most 5 lines
    # å¾ªç¯éå†æ–‡æœ¬çš„æŒ‡å®šèŒƒå›´ï¼Œæœ€å¤šå‘å‰æŸ¥æ‰¾5è¡Œ
    for line_idx in range(max(0, slice[0] - 1), max(0, slice[0] - 5), -1):
        # å¦‚æœå½“å‰è¡Œä¸ºç©ºï¼Œåˆ™è·³è¿‡
        if not lines[line_idx]:
            continue
        # å¦‚æœå½“å‰è¡Œæ˜¯"## References"ï¼Œåˆ™ç¡®å®šè¯¥æ®µè½èµ·å§‹ä½ç½®ä¸ºå½“å‰è¡Œ
        if lines[line_idx] == "## References":
            section[0] = line_idx
            break
        # å¦åˆ™ï¼Œæ¯”è¾ƒå½“å‰è¡Œä¸åŸºå‡†å­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼Œå¦‚æœä½äºé˜ˆå€¼0.9
        elif ratio(base, remove_numbers(lines[line_idx])) < 0.9:
            # å°†æ®µè½èµ·å§‹ä½ç½®è®¾ä¸ºå½“å‰è¡Œçš„ä¸‹ä¸€è¡Œ
            section[0] = line_idx + 1
            # è·å–æ½œåœ¨å¼•ç”¨çš„å†…å®¹ï¼ˆå½“å‰è¡Œçš„ä¸Šä¸€è¡Œä»¥"* ["å¼€å¤´çš„éƒ¨åˆ†ï¼‰
            potential_ref = remove_numbers(lines[max(0, line_idx - 1)].partition("* [")[-1])
            # å¦‚æœæ½œåœ¨å¼•ç”¨çš„é•¿åº¦å¤§äºåŸºå‡†å­—ç¬¦ä¸²é•¿åº¦çš„0.75ï¼Œå¹¶ä¸”ä¸åŸºå‡†å­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ä½äº0.9
            if len(potential_ref) >= 0.75 * len(base) and ratio(base, potential_ref) < 0.9:
                # å°†æ®µè½èµ·å§‹ä½ç½®è®¾ä¸ºå½“å‰è¡Œ
                section[0] = line_idx
            # è®¾ç½®æ£€æŸ¥èµ·å§‹æ ‡å¿—ä¸ºçœŸ
            check_start_flag = True
            break
    
    # forward passï¼Œæœ€å¤šå‘åæŸ¥æ‰¾5è¡Œ
    for line_idx in range(min(len(lines), slice[1]), min(len(lines), slice[1] + 5)):
        # å¦‚æœå½“å‰è¡Œä¸åŸºå‡†å­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼0.9
        if ratio(base, remove_numbers(lines[line_idx])) < 0.9:
            # ç¡®å®šæ®µè½ç»“æŸä½ç½®ä¸ºå½“å‰è¡Œ
            section[1] = line_idx
            break
    
    # å¦‚æœæ–‡æœ¬è¡Œæ•°å°äºç­‰äºæ®µè½ç»“æŸä½ç½®ï¼Œå°†æ®µè½ç»“æŸä½ç½®è®¾ä¸ºæ–‡æœ¬è¡Œæ•°å‡1
    if len(lines) <= section[1]:
        section[1] = len(lines) - 1
    
    # è·å–å¾…åˆ é™¤çš„æ–‡æœ¬æ®µè½ï¼Œä»section[0]åˆ°section[1]è¡Œï¼ˆåŒ…æ‹¬ï¼‰
    to_delete = "\n".join(lines[section[0] : section[1] + 1])
    
    # æˆªå–ä¸‹ä¸€é¡µå†…å®¹
    itera, iterb = enumerate(lines[section[1] - 1]), enumerate(lines[section[1]])
    while True:
        try:
            (ia, a) = next(itera)
            # è·³è¿‡æ•°å­—è¡Œ
            while a.isnumeric():
                (ia, a) = next(itera)
            (ib, b) = next(iterb)
            # è·³è¿‡æ•°å­—è¡Œ
            while b.isnumeric():
                (ib, b) = next(iterb)
            # å¦‚æœé‡åˆ°ä¸ç›¸åŒçš„å­—ç¬¦ï¼Œåˆ™åœæ­¢æˆªå–
            if a != b:
                break
        except StopIteration:
            break
    
    # å¦‚æœæ£€æŸ¥èµ·å§‹æ ‡å¿—ä¸ºçœŸä¸”å¾…åˆ é™¤çš„æ–‡æœ¬åŒ…å«"* ["ï¼Œåˆ™ä¿ç•™"* ["å¼€å¤´çš„å†…å®¹
    if check_start_flag and "* [" in to_delete:
        to_delete = "* [" + to_delete.partition("* [")[-1]
    
    try:
        # è®¡ç®—æˆªå–çš„å°¾éƒ¨å†…å®¹é•¿åº¦
        delta = len(lines[section[1]]) - ib - 1
        # å¦‚æœé•¿åº¦å¤§äº0ï¼Œåˆ™ä»to_deleteä¸­åˆ é™¤ç›¸åº”é•¿åº¦çš„å°¾éƒ¨å†…å®¹
        if delta > 0:
            to_delete = to_delete[:-delta]
    except UnboundLocalError:
        # å¿½ç•¥æœªç»‘å®šå±€éƒ¨å˜é‡çš„å¼‚å¸¸
        pass
    
    # è¿”å›ç»è¿‡å¤„ç†çš„å¾…åˆ é™¤æ–‡æœ¬æ®µè½ï¼ˆå»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦ï¼‰
    return to_delete.strip()
# ä½¿ç”¨è£…é¥°å™¨æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæ–‡æ¡£å­—ç¬¦ä¸²å†…å®¹ä¸ºINIT_TOKENIZER_DOCSTRINGå˜é‡çš„å€¼
@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
# å®šä¹‰ä¸€ä¸ªåä¸ºNougatTokenizerFastçš„ç±»ï¼Œå®ƒç»§æ‰¿è‡ªPreTrainedTokenizerFastç±»
class NougatTokenizerFast(PreTrainedTokenizerFast):
    """
    Fast tokenizer for Nougat (backed by HuggingFace tokenizers library).

    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should
    refer to this superclass for more information regarding those methods. This class mainly adds Nougat-specific
    methods for postprocessing the generated text.

    Args:
        vocab_file (`str`, *optional*):
            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that
            contains the vocabulary necessary to instantiate a tokenizer.
        tokenizer_file (`str`, *optional*):
            [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that
            contains everything needed to load the tokenizer.

        clean_up_tokenization_spaces (`str`, *optional*, defaults to `False`):
            Wether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra
            spaces.

        unk_token (`str`, *optional*, defaults to `"<unk>"`):
            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
            token instead.

        bos_token (`str`, *optional*, defaults to `"<s>"`):
            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.

        eos_token (`str`, *optional*, defaults to `"</s>"`):
            The end of sequence token.

        pad_token (`str`, *optional*, defaults to `"<pad>"`):
            The token used for padding, for example when batching sequences of different lengths.
    """

    # ç±»å±æ€§ï¼šå®šä¹‰è¯æ±‡æ–‡ä»¶ååˆ—è¡¨
    vocab_files_names = VOCAB_FILES_NAMES
    # ç±»å±æ€§ï¼šå®šä¹‰é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶æ˜ å°„
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
    # ç±»å±æ€§ï¼šå®šä¹‰æœ€å¤§æ¨¡å‹è¾“å…¥å°ºå¯¸åˆ—è¡¨
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
    # ç±»å±æ€§ï¼šå®šä¹‰æ¨¡å‹è¾“å…¥åç§°åˆ—è¡¨
    model_input_names = ["input_ids", "attention_mask"]
    # ç±»å±æ€§ï¼šå®šä¹‰æ…¢é€Ÿåˆ†è¯å™¨ç±»åˆ«ï¼Œæ­¤å¤„ä¸ºNone
    slow_tokenizer_class = None

    # åˆå§‹åŒ–æ–¹æ³•
    def __init__(
        self,
        vocab_file=None,
        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token="<unk>",
        bos_token="<s>",
        eos_token="</s>",
        pad_token="<pad>",
        **kwargs,
    ):
        # è°ƒç”¨çˆ¶ç±»PreTrainedTokenizerFastçš„åˆå§‹åŒ–æ–¹æ³•ï¼Œä¼ å…¥å„ç§å‚æ•°
        super().__init__(
            vocab_file=vocab_file,
            tokenizer_file=tokenizer_file,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            unk_token=unk_token,
            bos_token=bos_token,
            eos_token=eos_token,
            pad_token=pad_token,
            **kwargs,
        )
        # è®¾ç½®å®ä¾‹å±æ€§self.vocab_fileä¸ºä¼ å…¥çš„vocab_fileå‚æ•°å€¼
        self.vocab_file = vocab_file
    def remove_hallucinated_references(self, text: str) -> str:
        """
        Remove hallucinated or missing references from the text.

        This function identifies and removes references that are marked as missing or hallucinated from the input text.

        Args:
            text (`str`):
                The input text containing references.

        Returns:
            `str`: The text with hallucinated references removed.
        """
        # å°†æ–‡æœ¬æŒ‰è¡Œåˆ†å‰²æˆåˆ—è¡¨
        lines = text.split("\n")
        # å¦‚æœè¡Œæ•°ä¸º0ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if len(lines) == 0:
            return ""
        # ä»æ–‡æœ¬ä¸­ç§»é™¤æ•°å­—
        clean_lines = remove_numbers(lines)
        # è·å–å¹²å‡€è¡Œå’ŒåŸå§‹è¡Œçš„åˆ‡ç‰‡
        slices = get_slices(lines, clean_lines)
        # å­˜å‚¨å¾…åˆ é™¤çš„åˆ‡ç‰‡
        to_delete = []
        # éå†æ¯ä¸ªåˆ‡ç‰‡
        for slice in slices:
            # ä»åŸå§‹è¡Œå’Œå¹²å‡€è¡Œä¸­ç§»é™¤åˆ‡ç‰‡ï¼Œè¿”å›è¦åˆ é™¤çš„éƒ¨åˆ†
            to_delete.append(remove_slice_from_lines(lines, clean_lines, slice))
        # åå‘éå†å¾…åˆ é™¤çš„éƒ¨åˆ†
        for to_delete in reversed(to_delete):
            # ç”¨æŒ‡å®šæ ‡è®°æ›¿æ¢è¦åˆ é™¤çš„éƒ¨åˆ†ï¼Œç”¨äºæ ‡è®°ä¸¢å¤±é¡µç 
            text = text.replace(to_delete, "\n\n[MISSING_PAGE_POST]\n\n")
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ–‡æœ¬ä¸­çš„ç‰¹å®šæ ¼å¼çš„å¼•ç”¨æ ‡è®°
        text = re.sub(
            r"## References\n+\[MISSING_PAGE_POST(:\d+)?\]",
            "\n\n[MISSING_PAGE_POST\\1]",
            text,
        )
        # è¿”å›å¤„ç†åçš„æ–‡æœ¬
        return text

    def correct_tables(self, generation: str) -> str:
        """
        Takes a generated string and fixes tables/tabulars to make them match the markdown format needed.

        Args:
            generation (str): The generated text to be postprocessed.

        Returns:
            str: The postprocessed text.

        Example:

        ```python
        correct_tables("\\begin{table} \\begin{tabular}{l l} & \\ \\end{tabular} \\end{table}")
        "\\begin{table}\n\\begin{tabular}{l l} & \\ \\end{tabular}\n\\end{table}"
        ```
        """
        # ç§»é™¤æ˜æ˜¾é”™è¯¯çš„è¡¨æ ¼
        for l in generation.split("\n"):
            if l.count("\\begin{tabular}") > 15 or l.count("\\multicolumn") > 60 or l.count("&") > 400:
                generation = generation.replace(l, "")
        # ä¿®æ­£ç©ºç™½å’Œæ ¼å¼
        generation = generation.replace("\\begin{table} \\begin{tabular}", "\\begin{table}\n\\begin{tabular}")
        generation = generation.replace("\\end{tabular} \\end{table}", "\\end{tabular}\n\\end{table}")
        generation = generation.replace("\\end{table} Tab", "\\end{table}\nTab")
        # åœ¨æ¯ä¸ªè¡¨æ ¼å‰åŠ å…¥æ–°è¡Œï¼Œä»¥ç¡®ä¿æ­£ç¡®çš„æ ¼å¼
        generation = re.sub(r"(^.+)\\begin{tab", r"\1\n\\begin{tab", generation, flags=re.M)
        # ç§»é™¤å·¦å¯¹é½çš„ç©º LaTeX è¡¨æ ¼å—
        generation = generation.replace(r"\begin{tabular}{l l}  & \\ \end{tabular}", "")
        # ç§»é™¤åªæœ‰ä¸¤ä¸ªæ¢è¡Œç¬¦çš„è¡¨æ ¼
        generation = generation.replace("\\begin{tabular}{}\n\n\\end{tabular}", "")
        # è¿”å›å¤„ç†åçš„æ–‡æœ¬
        return generation
    ) -> Union[str, List[str]]:
        """
        Postprocess a generated text or a list of generated texts.

        This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.

        Postprocessing is quite slow so it is recommended to use multiprocessing to speed up the process.

        Args:
            generation (Union[str, List[str]]):
                The generated text or a list of generated texts.
            fix_markdown (`bool`, *optional*, defaults to `True`):
                Whether to perform Markdown formatting fixes.
            num_workers (`int`, *optional`):
                Optional number of workers to pass to leverage multiprocessing (postprocessing several texts in
                parallel).

        Returns:
            Union[str, List[str]]: The postprocessed text or list of postprocessed texts.
        """
        # ç¡®ä¿ä¾èµ–çš„åç«¯æ¨¡å—å·²ç»åŠ è½½
        requires_backends(self, ["nltk", "levenshtein"])

        # å¦‚æœç”Ÿæˆç‰©æ˜¯ä¸€ä¸ªåˆ—è¡¨
        if isinstance(generation, list):
            # å¦‚æœæŒ‡å®šäº†å¹¶è¡Œå¤„ç†çš„ worker æ•°é‡
            if num_workers is not None and isinstance(num_workers, int):
                # ä½¿ç”¨ multiprocessing.Pool åˆ›å»ºä¸€ä¸ªè¿›ç¨‹æ± 
                with Pool(num_workers) as p:
                    # ä½¿ç”¨è¿›ç¨‹æ± çš„ map å‡½æ•°å¹¶è¡Œå¤„ç†ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
                    return p.map(partial(self.post_process_single, fix_markdown=fix_markdown), generation)
            else:
                # å¦åˆ™ï¼Œä¸²è¡Œå¤„ç†æ¯ä¸ªç”Ÿæˆçš„æ–‡æœ¬
                return [self.post_process_single(s, fix_markdown=fix_markdown) for s in generation]
        else:
            # å¦‚æœç”Ÿæˆç‰©æ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œåˆ™ç›´æ¥è°ƒç”¨å•æ–‡æœ¬å¤„ç†å‡½æ•°
            return self.post_process_single(generation, fix_markdown=fix_markdown)
```