# `.\transformers\models\nougat\tokenization_nougat_fast.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜åŠè®¸å¯è¯ä¿¡æ¯
"""
Fast tokenizer class for Nougat.
"""

# å¯¼å…¥æ¨¡å—
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
from functools import partial  # å¯¼å…¥ partial å‡½æ•°
from multiprocessing import Pool  # å¯¼å…¥è¿›ç¨‹æ± ç±»
from typing import List, Union  # å¯¼å…¥ç±»å‹æç¤ºç›¸å…³çš„æ¨¡å—

import numpy as np  # å¯¼å…¥ NumPy åº“

# å¯¼å…¥ Hugging Face çš„ tokenizer ç›¸å…³æ¨¡å—
from transformers.tokenization_utils_base import INIT_TOKENIZER_DOCSTRING  # å¯¼å…¥åˆå§‹åŒ– tokenizer çš„æ–‡æ¡£å­—ç¬¦ä¸²
from transformers.tokenization_utils_fast import PreTrainedTokenizerFast  # å¯¼å…¥å¿«é€Ÿ tokenizer åŸºç±»
from transformers.utils import add_end_docstrings  # å¯¼å…¥æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²çš„è¾…åŠ©å‡½æ•°

# å¯¼å…¥è‡ªå®šä¹‰çš„æ¨¡å—
from ...utils import is_levenshtein_available, is_nltk_available, logging, requires_backends  # å¯¼å…¥ä¸€äº›è‡ªå®šä¹‰çš„å·¥å…·å‡½æ•°

# å¦‚æœ Levenshtein å¯ç”¨ï¼Œåˆ™å¯¼å…¥ ratio å‡½æ•°
if is_levenshtein_available():
    from Levenshtein import ratio

# å¦‚æœ NLTK å¯ç”¨ï¼Œåˆ™å¯¼å…¥ NLTK åº“
if is_nltk_available():
    import nltk  # å¯¼å…¥è‡ªç„¶è¯­è¨€å·¥å…·åŒ… NLTK

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# å°† tokenizer çš„åˆå§‹åŒ–æ–‡æ¡£å­—ç¬¦ä¸²æ·»åŠ é¢å¤–å†…å®¹
INIT_TOKENIZER_DOCSTRING += """
        tokenizer_object ([`tokenizers.Tokenizer`]):
            A [`tokenizers.Tokenizer`] object from ğŸ¤— tokenizers to instantiate from. See [Using tokenizers from ğŸ¤—
            tokenizers](../fast_tokenizers) for more information.
        tokenizer_file ([`str`]):
            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from ğŸ¤—
            tokenizers.
"""

# é¢„è®­ç»ƒ tokenizer æ–‡ä»¶æ˜ å°„
PRETRAINED_VOCAB_FILES_MAP = {
    "tokenizer_file": {
        "facebook/nougat-base": "https://huggingface.co/facebook/nougat-base/tokenizer/blob/main/tokenizer.json",
    },
}

# tokenizer æ–‡ä»¶åç§°
VOCAB_FILES_NAMES = {"tokenizer_file": "tokenizer.json"}

# é¢„è®­ç»ƒä½ç½®åµŒå…¥å¤§å°
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {"facebook/nougat-base": 3584}


def markdown_compatible(text: str) -> str:
    """
    Make text compatible with Markdown formatting.

    This function makes various text formatting adjustments to make it compatible with Markdown.

    Args:
        text (`str`):
            The input text to be made Markdown-compatible.

    Returns:
        `str`: The Markdown-compatible text.
    """
    # æ–¹ç¨‹å¼æ ‡ç­¾
    # å°†ä»¥ (åè¿›åˆ¶) [some text] å¼€å¤´çš„è¡Œæ›¿æ¢ä¸º \[[some text] \tag{decimal}\]
    text = re.sub(r"^\(([\d.]+[a-zA-Z]?)\) \\\[(.+?)\\\]$", r"\[\2 \\tag{\1}\]", text, flags=re.M)
    # å°†ä»¥ \[some text\] (decimal)  å¼€å¤´çš„è¡Œæ›¿æ¢ä¸º \[[some text] \tag{decimal}\]
    text = re.sub(r"^\\\[(.+?)\\\] \(([\d.]+[a-zA-Z]?)\)$", r"\[\1 \\tag{\2}\]", text, flags=re.M)
    # å°†ä»¥ \[some text\] (digits) \[another text\]  å¼€å¤´çš„è¡Œæ›¿æ¢ä¸º \[[some text] \tag{digits}\] [another text].
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ–‡æœ¬ä¸­ç¬¦åˆç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå°†å…¶æ ¼å¼åŒ–ä¸ºç‰¹å®šçš„æ ‡è®°å½¢å¼
    text = re.sub(
        r"^\\\[(.+?)\\\] \(([\d.]+[a-zA-Z]?)\) (\\\[.+?\\\])$",
        r"\[\1 \\tag{\2}\] \3",
        text,
        flags=re.M,
    )
    # æ›¿æ¢æ–‡æœ¬ä¸­çš„è½¬ä¹‰å­—ç¬¦ ". " ä¸ºæ™®é€šçš„å¥å·å’Œç©ºæ ¼
    text = text.replace(r"\. ", ". ")
    # æ›¿æ¢æ–‡æœ¬ä¸­çš„åŠ ç²—æ ¼å¼åŒ–å­—ç¬¦ä¸²ä¸º LaTeX çš„æ•°å­¦ç²—ä½“æ ¼å¼
    text = text.replace(r"\bm{", r"\mathbf{").replace(r"{\\bm ", r"\mathbf{")
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ–‡æœ¬ä¸­ç¬¦åˆç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå°†å…¶æ ¼å¼åŒ–ä¸º Markdown é“¾æ¥å½¢å¼
    text = re.sub(r"\\mbox{ ?\\boldmath\$(.*?)\$}", r"\\mathbf{\1}", text)
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ–‡æœ¬ä¸­çš„ URL å­—ç¬¦ä¸²ä¸º Markdown å¯ç‚¹å‡»çš„é“¾æ¥æ ¼å¼
    text = re.sub(
        r"((?:http|ftp|https):\/\/(?:[\w_-]+(?:(?:\.[\w_-]+)+))(?:[\w.,@?^=%&:\/~+#-]*[\w@?^=%&\/~+#-]))",
        r"[\1](\1)",
        text,
    )
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ–‡æœ¬ä¸­çš„ç®—æ³•å—æ ¼å¼ï¼Œå°†å…¶æ ¼å¼åŒ–ä¸º Markdown ä»£ç å—å½¢å¼
    text = re.sub(r"```\s*(.+?)\s*```", r"```\n\1\n```", text, flags=re.S)

    return text
# å®šä¹‰å‡½æ•°ï¼Œç”¨äºè§„èŒƒåŒ–ç±»ä¼¼åˆ—è¡¨é¡¹çš„æ–‡æœ¬è¡Œ
def normalize_list_like_lines(generation):
    """
    Normalize lines in the given text that resemble list items. The function looks for lines that start optionally with
    '-' or '*', possibly followed by Roman numerals or digits indicating nesting levels. The function reformats such
    lines to make them more structured.

    Args:
        generation (str): The input text containing lines that need to be normalized.

    Returns:
        str: The input text with the list-like lines normalized.

    Note:
        The function uses regular expressions to identify and reformat the list-like lines. The patterns capture
        optional bullet points, nesting levels indicated by numerals, and the actual list item content. The
        normalization adjusts the bullet point style and nesting levels based on the captured patterns.
    """
    import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
    # åŒ¹é…ä»¥-æˆ–*å¼€å¤´ä¸”ä¸è·Ÿéš-æˆ–*çš„è¡Œï¼Œåé¢å¯èƒ½è·Ÿç€è¡¨ç¤ºåµŒå¥—çº§åˆ«çš„ç½—é©¬æ•°å­—æˆ–æ•°å­—
    # å†åŠ ä¸Šç”¨\dæˆ–ç½—é©¬æ•°å­—è¡¨ç¤ºçš„è¯¥è¡Œå¯é€‰é™„åŠ ç¼–å·ï¼Œç„¶åé€šè¿‡re.finditerè°ƒç”¨
    pattern = r"(?:^)(-|\*)?(?!-|\*) ?((?:\d|[ixv])+ )?.+? (-|\*) (((?:\d|[ixv])+)\.(\d|[ixv]) )?.*(?:$)"

    # é€†å‘éå†æ‰¾åˆ°çš„åŒ¹é…é¡¹åˆ—è¡¨
    for match in reversed(list(re.finditer(pattern, generation, flags=re.I | re.M))):
        start, stop = match.span()  # è·å–åŒ¹é…é¡¹çš„èµ·å§‹å’Œç»ˆæ­¢ç´¢å¼•
        delim = match.group(3) + " "  # è·å–åŒ¹é…é¡¹ç¬¬ä¸‰ä¸ªåˆ†ç»„ä¸­çš„åˆ†éš”ç¬¦
        splits = match.group(0).split(delim)  # ä½¿ç”¨åˆ†éš”ç¬¦åˆ†å‰²åŒ¹é…é¡¹å†…å®¹
        replacement = ""

        if match.group(1) is not None:
            splits = splits[1:]  # å¦‚æœç¬¬ä¸€ä¸ªåˆ†ç»„ä¸ä¸ºç©ºï¼Œç§»é™¤ç¬¬ä¸€ä¸ªå…ƒç´ 
            delim1 = match.group(1) + " "  # è·å–ç¬¬ä¸€ä¸ªåˆ†ç»„ä¸­çš„åˆ†éš”ç¬¦
        else:
            delim1 = ""
            continue  # è·³è¿‡é”™è¯¯åŒ¹é…

        pre, post = generation[:start], generation[stop:]  # å°†è¾“å…¥æ–‡æœ¬åˆ†æˆå‰éƒ¨åˆ†å’Œåéƒ¨åˆ†

        for i, item in enumerate(splits):
            level = 0  # åˆå§‹åŒ–åµŒå¥—çº§åˆ«ä¸º0
            potential_numeral, _, rest = item.strip().partition(" ")  # ä½¿ç”¨ç©ºæ ¼åˆ†å‰²åŒ¹é…é¡¹çš„å¯èƒ½ç¼–å·
            if not rest:  # å¦‚æœæ²¡æœ‰å‰©ä½™å†…å®¹
                continue
            # æ ¹æ®æ£€æµ‹åˆ°çš„ç¼–å·æ¨æ–­å½“å‰åµŒå¥—çº§åˆ«
            if re.match(r"^[\dixv]+((?:\.[\dixv])?)+$", potential_numeral, flags=re.I | re.M):
                level = potential_numeral.count(".")

            replacement += (
                ("\n" if i > 0 else "") + ("\t" * level) + (delim if i > 0 or start == 0 else delim1) + item.strip()
            )

        if post == "":
            post = "\n"  # å¦‚æœåéƒ¨åˆ†ä¸ºç©ºï¼Œåˆ™è®¾ç½®ä¸ºæ¢è¡Œç¬¦

        generation = pre + replacement + post  # æ›´æ–°æ–‡æœ¬å†…å®¹

    return generation  # è¿”å›å¤„ç†åçš„æ–‡æœ¬å†…å®¹


# å®šä¹‰å‡½æ•°ï¼ŒæŸ¥æ‰¾ä¸‹ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·çš„ç´¢å¼•
def find_next_punctuation(text: str, start_idx=0):
    """
    Find the index of the next punctuation mark.

    Args:
        text (`str`):
            String to examine
        start_idx (`int`, *optional*)
            Index where to start
    """
    # éå†æ–‡æœ¬ï¼ŒæŸ¥æ‰¾ä¸‹ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·çš„ç´¢å¼•
    for i in range(start_idx, len(text)):
        if text[i] in [".", "?", "!", "\n"]:
            return i  # å¦‚æœæ‰¾åˆ°æ ‡ç‚¹ç¬¦å·ï¼Œåˆ™è¿”å›ç´¢å¼•

    return None  # æ²¡æœ‰æ‰¾åˆ°æ ‡ç‚¹ç¬¦å·ï¼Œè¿”å›None


# å®šä¹‰å‡½æ•°ï¼Œå°è¯•åœ¨è¾“å…¥å­—ç¬¦ä¸²ä¸­æˆªæ–­é‡å¤çš„ç‰‡æ®µ
def truncate_repetitions(text: str, min_len: int = 30) -> str:
    """
    Attempt to truncate repeating segments in the input string.
    This function looks for the longest repeating substring at the end of the input string and truncates it to appear
    only once. To be considered for removal, repetitions need to be continuous.

    Args:
        text (`str`):
            The input raw prediction to be truncated.
        min_len (int):
            The minimum length of the repeating segment.

    Returns:
        `str`: The input string with repeated segments truncated.
    """

    # å°†è¾“å…¥çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºå°å†™
    text_lower = text.lower()
    # è·å–æ–‡æœ¬çš„é•¿åº¦
    text_length = len(text_lower)

    if text_length < 2 * min_len:
        return text

    # å°è¯•æ‰¾åˆ°å°¾éƒ¨è¿ç»­é‡å¤çš„é•¿åº¦
    max_repetition_length = None
    for repetition_length in range(min_len, int(text_length / 2)):
        # æ£€æŸ¥æœ«å°¾æ˜¯å¦å­˜åœ¨é‡å¤
        same = True
        for i in range(0, repetition_length):
            if text_lower[text_length - repetition_length - i - 1] != text_lower[text_length - i - 1]:
                same = False
                break

        if same:
            max_repetition_length = repetition_length

    if max_repetition_length is None:
        return text

    # è·å–å°¾éƒ¨é‡å¤çš„å­ä¸²
    lcs = text_lower[-max_repetition_length:]

    # ç§»é™¤é™¤äº†æœ€åä¸€æ¬¡é‡å¤ä¹‹å¤–çš„æ‰€æœ‰é‡å¤
    substituted_text = text
    substituted_text_lower = text_lower
    while substituted_text_lower.endswith(lcs):
        substituted_text = substituted_text[:-max_repetition_length]
        substituted_text_lower = substituted_text_lower[:-max_repetition_length]

    # è¿™æ˜¯å«æœ‰é‡å¤çš„å°¾éƒ¨
    repeating_tail = text_lower[len(substituted_text_lower):]

    # æ·»åŠ ç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·ï¼Œå¹¶ç¡®ä¿æœ€åä¸€å¥ä¸é‡å¤
    substituted_text_lower_out = substituted_text_lower
    while True:
        sentence_end = find_next_punctuation(text_lower, len(substituted_text_lower_out))
        sentence_start = find_next_punctuation(text_lower[::-1], len(substituted_text_lower_out))
        if sentence_end and sentence_start:
            sentence = text_lower[sentence_start:sentence_end]
            substituted_text_lower_out = text_lower[:sentence_end + 1]
            if sentence in repeating_tail:
                break
        else:
            break

    text_out = text[:len(substituted_text_lower_out)]

    return text_out
def remove_numbers(lines):
    # å®šä¹‰å†…éƒ¨å‡½æ•°_cleanï¼Œç”¨äºç§»é™¤æ•°å­—å’Œä¸‹åˆ’çº¿ï¼Œç„¶åå»é™¤é¦–å°¾ç©ºæ ¼
    def _clean(s):
        return re.sub(r"(?:[\d_]|\*\*)", "", s).strip()

    # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™ç›´æ¥ä½¿ç”¨_cleanå‡½æ•°å¤„ç†åè¿”å›
    if isinstance(lines, str):
        return _clean(lines)
    # å¦åˆ™ï¼Œå¯¹æ¯è¡Œåº”ç”¨_cleanå‡½æ•°åå­˜å‚¨åˆ°outåˆ—è¡¨ä¸­å†è¿”å›
    out = []
    for l in lines:
        out.append(_clean(l))
    return out


def get_slices(lines, clean_lines):
    """
    Get slices of text based on specific criteria within the lines.

    This function identifies and returns slices of text from the input lines based on certain conditions.

    These conditions were chosen by the Nougat authors:
    - The slice is less than 200 characters long.
    - The slice is more than 3 characters long.
    - The slice does not start with "[MISSING_PAGE".
    - The slice is either the same as the next slice or the ratio of the two in terms of Levensthein distance is
      greater than 0.9.

    Args:
        lines (`List[str]`):
            The list of lines containing the text.
        clean_lines (`List[str]`):
            A cleaned version of the text (without numbers).

    Returns:
        `List[tuple]`: A list of tuples representing the start and end indices of text slices.
    """
    # åˆå§‹åŒ–ä¸€ä¸ªå…¨é›¶æ•°ç»„ç”¨äºè®°å½•æ˜¯å¦å·²å¤„ç†è¿‡æŸè¡Œ
    indices = np.zeros(len(lines))
    # éå†æ¯è¡Œæ–‡æœ¬ï¼Œç›´åˆ°å€’æ•°ç¬¬äºŒè¡Œ
    for i in range(len(lines) - 1):
        j = i + 1
        # å‘åæ‰¾åˆ°ç¬¬ä¸€ä¸ªéç©ºè¡Œçš„ç´¢å¼•
        while not clean_lines[j] and j < len(lines) - 1:
            j += 1
        # åˆ¤æ–­æ˜¯å¦ç¬¦åˆæ¡ä»¶ï¼Œå¹¶æ ‡è®°è¯¥ç‰‡æ®µå·²å¤„ç†è¿‡
        if (
            len(clean_lines[i]) < 200
            and len(clean_lines[i]) > 3
            and len(clean_lines[j]) < 200
            and len(clean_lines[j]) > 3
            and not clean_lines[i].startswith("[MISSING_PAGE")
            and (clean_lines[i] == clean_lines[j] or ratio(clean_lines[i], clean_lines[j]) > 0.9)
        ):
            indices[i:j] = 1
    # è·å–å·²æ ‡è®°çš„ç´¢å¼•ä½ç½®
    ids = np.where(indices)[0]
    slices = []
    # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ç´¢å¼•ä½ç½®ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
    if len(ids) == 0:
        return slices
    j0 = 0
    # å‘åéå†ç´¢å¼•ä½ç½®å·®ï¼Œæ‰¾å‡ºæ»¡è¶³æ¡ä»¶çš„ç‰‡æ®µï¼Œç»„æˆèµ·å§‹å’Œç»“æŸç´¢å¼•çš„å…ƒç»„ï¼Œå¹¶å­˜å‚¨åˆ°slicesåˆ—è¡¨ä¸­
    for j, x in enumerate(np.diff(ids) > 3):
        if x:
            slices.append((ids[j0], ids[j] + 2))
            j0 = j + 1
    # å¤„ç†æœ€åä¸€ä¸ªç‰‡æ®µï¼Œå°†å…¶åŠ å…¥slicesåˆ—è¡¨
    slices.append((ids[j0], ids[-1] + 2))
    # è¿”å›é•¿åº¦å¤§äº15çš„ç‰‡æ®µåˆ—è¡¨
    return [sli for sli in slices if sli[1] - sli[0] > 15]


def remove_slice_from_lines(lines, clean_text, slice) -> str:
    """
    Remove a slice of text from the lines based on specific criteria.

    This function identifies a slice of text within the lines and removes it based on certain conditions.

    Args:
        lines (list of str): The list of lines containing the text.
        clean_text (list of str): A cleaned version of the text (without numbers).
        slice (tuple): A tuple representing the start and end indices of the slice to be removed.

    Returns:
        str: The removed slice of text as a single string.
    """
    # è·å–è¦ç§»é™¤çš„ç‰‡æ®µçš„èµ·å§‹è¡Œæ–‡æœ¬
    base = clean_text[slice[0]]
    # å°†sliceè½¬æ¢ä¸ºå¯å˜åˆ—è¡¨
    section = list(slice)
    # è®¾ç½®æ ‡å¿—ç”¨äºæ£€æŸ¥å‘å‰è¡Œæ–‡æœ¬çš„èµ·å§‹è¡Œ
    check_start_flag = False
    # å‘å‰éå†ï¼Œæœ€å¤š5è¡Œ
    # ä»ç»™å®šçš„æ–‡æœ¬ç‰‡æ®µä¸­æ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†çš„èµ·å§‹å’Œç»“æŸä½ç½®è¿›è¡Œåˆ‡å‰²
    for line_idx in range(max(0, slice[0] - 1), max(0, slice[0] - 5), -1):
        # å¦‚æœå½“å‰è¡Œä¸ºç©ºï¼Œåˆ™è·³è¿‡
        if not lines[line_idx]:
            continue
        # å¦‚æœå½“å‰è¡Œæ˜¯"## References"ï¼Œåˆ™å°†å‚è€ƒæ–‡çŒ®éƒ¨åˆ†çš„èµ·å§‹ä½ç½®è®¾ä¸ºå½“å‰è¡Œçš„ç´¢å¼•ï¼Œå¹¶è·³å‡ºå¾ªç¯
        if lines[line_idx] == "## References":
            section[0] = line_idx
            break
        # å¦‚æœå½“å‰è¡Œå’ŒåŸºå‡†æ¯”ä¾‹å°äº0.9ï¼Œåˆ™è®¾å®šå‚è€ƒæ–‡çŒ®éƒ¨åˆ†çš„èµ·å§‹ä½ç½®ï¼Œå¹¶æ£€æŸ¥å‰ä¸€è¡Œå¯èƒ½çš„å‚è€ƒæ–‡çŒ®ä¿¡æ¯
        elif ratio(base, remove_numbers(lines[line_idx])) < 0.9:
            section[0] = line_idx + 1
            potential_ref = remove_numbers(lines[max(0, line_idx - 1)].partition("* [")[-1])
            if len(potential_ref) >= 0.75 * len(base) and ratio(base, potential_ref) < 0.9:
                section[0] = line_idx
            check_start_flag = True
            break
    # å‘å‰æŸ¥æ‰¾ï¼Œæœ€å¤šæŸ¥æ‰¾5è¡Œ
    for line_idx in range(min(len(lines), slice[1]), min(len(lines), slice[1] + 5)):
        # å¦‚æœå½“å‰è¡Œå’ŒåŸºå‡†æ¯”ä¾‹å°äº0.9ï¼Œåˆ™å°†å‚è€ƒæ–‡çŒ®éƒ¨åˆ†çš„ç»“æŸä½ç½®è®¾ä¸ºå½“å‰è¡Œçš„ç´¢å¼•ï¼Œå¹¶è·³å‡ºå¾ªç¯
        if ratio(base, remove_numbers(lines[line_idx])) < 0.9:
            section[1] = line_idx
            break
    # å¦‚æœæ–‡æœ¬è¡Œæ•°å°äºç­‰äºå‚è€ƒæ–‡çŒ®éƒ¨åˆ†çš„ç»“æŸä½ç½®ï¼Œåˆ™å°†ç»“æŸä½ç½®è®¾ä¸ºæ–‡æœ¬è¡Œæ•°å‡1
    if len(lines) <= section[1]:
        section[1] = len(lines) - 1
    # è·å–éœ€è¦åˆ é™¤çš„æ–‡æœ¬å†…å®¹
    to_delete = "\n".join(lines[section[0] : section[1] + 1])
    # æˆªæ–­ä¸‹ä¸€é¡µçš„å†…å®¹
    itera, iterb = enumerate(lines[section[1] - 1]), enumerate(lines[section[1]])
    while True:
        try:
            (ia, a) = next(itera)
            while a.isnumeric():
                (ia, a) = next(itera)
            (ib, b) = next(iterb)
            while b.isnumeric():
                (ib, b) = next(iterb)
            if a != b:
                break
        except StopIteration:
            break
    # å¦‚æœéœ€è¦æ£€æŸ¥èµ·å§‹æ ‡å¿—å¹¶ä¸”to_deleteä¸­åŒ…å«"* ["ï¼Œåˆ™è¿›è¡Œå¤„ç†
    if check_start_flag and "* [" in to_delete:
        to_delete = "* [" + to_delete.partition("* [")[-1]
    # å°è¯•è·å–éœ€è¦åˆ é™¤çš„æ–‡æœ¬ä¸å½“å‰æ–‡æœ¬è¡Œé•¿åº¦ä¹‹å·®deltaï¼Œç„¶åæˆªæ–­ç›¸åº”é•¿åº¦çš„æ–‡æœ¬å†…å®¹
    try:
        delta = len(lines[section[1]]) - ib - 1
        if delta > 0:
            to_delete = to_delete[:-delta]
    except UnboundLocalError:
        pass
    # è¿”å›éœ€è¦åˆ é™¤çš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
    return to_delete.strip()
# å¯¼å…¥éœ€è¦çš„åº“å’Œæ¨¡å—
@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class NougatTokenizerFast(PreTrainedTokenizerFast):
    """
    Nougat çš„å¿«é€Ÿåˆ†è¯å™¨ï¼ˆç”± HuggingFace tokenizers åº“æ”¯æŒï¼‰ã€‚

    è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª `PreTrainedTokenizerFast`ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦çš„æ–¹æ³•ã€‚ç”¨æˆ·åº”è¯¥å‚è€ƒè¿™ä¸ªè¶…ç±»ä»¥äº†è§£æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
    è¿™ä¸ªç±»ä¸»è¦æ·»åŠ äº† Nougat ç‰¹å®šçš„æ–¹æ³•ï¼Œç”¨äºå¯¹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œåå¤„ç†ã€‚

    Args:
        vocab_file (`str`, *optional*):
            åŒ…å«å®ä¾‹åŒ–åˆ†è¯å™¨æ‰€éœ€è¯æ±‡è¡¨çš„ [SentencePiece](https://github.com/google/sentencepiece) æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰ .model æ‰©å±•åï¼‰ã€‚
        tokenizer_file (`str`, *optional*):
            åŒ…å«åŠ è½½åˆ†è¯å™¨æ‰€éœ€å†…å®¹çš„ [tokenizers](https://github.com/huggingface/tokenizers) æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰ .json æ‰©å±•åï¼‰ã€‚

        clean_up_tokenization_spaces (`str`, *optional*ï¼Œé»˜è®¤ä¸º `False`):
            æ˜¯å¦åœ¨è§£ç åæ¸…é™¤ç©ºæ ¼ï¼Œæ¸…é™¤åŒ…æ‹¬å»é™¤é¢å¤–çš„ç©ºæ ¼ç­‰æ½œåœ¨è¿åœ¨ä¸€èµ·çš„è¯ã€‚

        unk_token (`str`, *optional*ï¼Œé»˜è®¤ä¸º `"<unk>"`):
            æœªçŸ¥çš„æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œå¹¶ä¼šæ›¿æ¢ä¸ºæ­¤æ ‡è®°ã€‚

        bos_token (`str`, *optional*ï¼Œé»˜è®¤ä¸º `"<s>"`):
            åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºè¡¨ç¤ºåºåˆ—å¼€å§‹çš„æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨çš„æ ‡è®°ã€‚

        eos_token (`str`, *optional*ï¼Œé»˜è®¤ä¸º `"</s>"`):
            ç”¨äºè¡¨ç¤ºåºåˆ—ç»“æŸçš„æ ‡è®°ã€‚

        pad_token (`str`, *optional*ï¼Œé»˜è®¤ä¸º `"<pad>"`):
            ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸ç­‰é•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

    """

    # å®šä¹‰ç±»åˆ«åå­—ã€é¢„è®­ç»ƒè¯æ±‡è¡¨æ–‡ä»¶åå­—ã€ä»¥åŠè¯æ±‡è¡¨å¤§å°ç­‰å¸¸é‡
    vocab_files_names = VOCAB_FILES_NAMES
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
    model_input_names = ["input_ids", "attention_mask"]
    slow_tokenizer_class = None

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œç”¨äºåˆ›å»º NougatTokenizerFast å®ä¾‹
    def __init__(
        self,
        vocab_file=None,
        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token="<unk>",
        bos_token="<s>",
        eos_token="</s>",
        pad_token="<pad>",
        **kwargs,
    ):
        # è°ƒç”¨è¶…ç±» PreTrainedTokenizerFast çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œä¼ å…¥ç›¸åº”çš„å‚æ•°
        super().__init__(
            vocab_file=vocab_file, 
            tokenizer_file=tokenizer_file,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            unk_token=unk_token,
            bos_token=bos_token,
            eos_token=eos_token,
            pad_token=pad_token,
            **kwargs,
        )
        # å°†å‚æ•° vocab_file èµ‹å€¼ç»™å®ä¾‹å±æ€§ vocab_file
        self.vocab_file = vocab_file
    def remove_hallucinated_references(self, text: str) -> str:
        """
        Remove hallucinated or missing references from the text.

        This function identifies and removes references that are marked as missing or hallucinated from the input text.

        Args:
            text (`str`):
                The input text containing references.

        Returns:
            `str`: The text with hallucinated references removed.
        """
        # å°†è¾“å…¥æ–‡æœ¬æŒ‰è¡Œåˆ†å‰²
        lines = text.split("\n")
        # å¦‚æœè¡Œæ•°ä¸º0ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        if len(lines) == 0:
            return ""
        # åˆ é™¤è¡Œä¸­çš„æ•°å­—
        clean_lines = remove_numbers(lines)
        # è·å–å¹²å‡€è¡Œå’ŒåŸå§‹è¡Œçš„åˆ‡ç‰‡
        slices = get_slices(lines, clean_lines)
        # åˆå§‹åŒ–è®°å½•éœ€è¦åˆ é™¤çš„éƒ¨åˆ†åˆ—è¡¨
        to_delete = []
        # éå†åˆ‡ç‰‡ï¼Œå°†éœ€è¦åˆ é™¤çš„éƒ¨åˆ†æ·»åŠ åˆ°to_deleteä¸­
        for slice in slices:
            to_delete.append(remove_slice_from_lines(lines, clean_lines, slice))
        # åå‘éå†to_deleteåˆ—è¡¨ï¼Œå°†éœ€è¦åˆ é™¤çš„éƒ¨åˆ†æ›¿æ¢ä¸ºæŒ‡å®šæ ‡è®°
        for to_delete in reversed(to_delete):
            text = text.replace(to_delete, "\n\n[MISSING_PAGE_POST]\n\n")
        # é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢åŒ¹é…çš„å†…å®¹
        text = re.sub(
            r"## References\n+\[MISSING_PAGE_POST(:\d+)?\]",
            "\n\n[MISSING_PAGE_POST\\1]",
            text,
        )
        # è¿”å›å¤„ç†åçš„æ–‡æœ¬
        return text

    def correct_tables(self, generation: str) -> str:
        """
        Takes a generated string and fixes tables/tabulars to make them match the markdown format needed.

        Args:
            generation (str): The generated text to be postprocessed.

        Returns:
            str: The postprocessed text.

        Example:

        ```python
        correct_tables("\\begin{table} \\begin{tabular}{l l} & \\ \\end{tabular} \\end{table}")
        "\\begin{table}\n\\begin{tabular}{l l} & \\ \\end{tabular}\n\\end{table}"
        ```
        """
        # ç§»é™¤æ˜æ˜¾é”™è¯¯çš„è¡¨
        for l in generation.split("\n"):
            if l.count("\\begin{tabular}") > 15 or l.count("\\multicolumn") > 60 or l.count("&") > 400:
                generation = generation.replace(l, "")
        # ä¿®æ­£ç©ºç™½
        generation = generation.replace("\\begin{table} \\begin{tabular}", "\\begin{table}\n\\begin{tabular}")
        generation = generation.replace("\\end{tabular} \\end{table}", "\\end{tabular}\n\\end{table}")
        generation = generation.replace("\\end{table} Tab", "\\end{table}\nTab")
        generation = re.sub(r"(^.+)\\begin{tab", r"\1\n\\begin{tab", generation, flags=re.M)
        # ç§»é™¤å·¦å¯¹é½çš„ç©ºLaTeX tabularå—
        generation = generation.replace(r"\begin{tabular}{l l}  & \\ \end{tabular}", "")
        # ç§»é™¤åªæœ‰2ä¸ªæ¢è¡Œç¬¦çš„tabulars
        generation = generation.replace("\\begin{tabular}{}\n\n\\end{tabular}", "")
        # è¿”å›å¤„ç†åçš„æ–‡æœ¬
        return generation

    def post_process_generation(
        self,
        generation: Union[str, List[str]],
        fix_markdown: bool = True,
        num_workers: int = None,
    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåå¤„ç†ç”Ÿæˆçš„æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
    def postprocess_generation(self, generation: Union[str, List[str]]) -> Union[str, List[str]]:
        """
        Postprocess a generated text or a list of generated texts.
    
        This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.
    
        Postprocessing is quite slow so it is recommended to use multiprocessing to speed up the process.
    
        Args:
            generation (Union[str, List[str]]):
                The generated text or a list of generated texts.
            fix_markdown (`bool`, *optional*, defaults to `True`):
                Whether to perform Markdown formatting fixes.
            num_workers (`int`, *optional*):
                Optional number of workers to pass to leverage multiprocessing (postprocessing several texts in
                parallel).
    
        Returns:
            Union[str, List[str]]: The postprocessed text or list of postprocessed texts.
        """
        # æ£€æŸ¥æ˜¯å¦å·²å¯¼å…¥æ‰€éœ€çš„åç«¯
        requires_backends(self, ["nltk", "levenshtein"])
    
        # å¦‚æœç”Ÿæˆç‰©æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ—è¡¨
        if isinstance(generation, list):
            # å¦‚æœæŒ‡å®šå¹¶ä¸”æ˜¯æ•´æ•°ç±»å‹ï¼Œåˆ™ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
            if num_workers is not None and isinstance(num_workers, int):
                # ä½¿ç”¨æŒ‡å®šæ•°é‡çš„è¿›ç¨‹æ¥åå¤„ç†ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
                with Pool(num_workers) as p:
                    return p.map(partial(self.post_process_single, fix_markdown=fix_markdown), generation)
            else:
                # å¦åˆ™å¯¹ç”Ÿæˆçš„æ¯ä¸ªæ–‡æœ¬è¿›è¡Œåå¤„ç†
                return [self.post_process_single(s, fix_markdown=fix_markdown) for s in generation]
        else:
            # å¦‚æœç”Ÿæˆç‰©æ˜¯å•ä¸ªæ–‡æœ¬ï¼Œåˆ™ç›´æ¥å¯¹å…¶è¿›è¡Œåå¤„ç†
            return self.post_process_single(generation, fix_markdown=fix_markdown)
```