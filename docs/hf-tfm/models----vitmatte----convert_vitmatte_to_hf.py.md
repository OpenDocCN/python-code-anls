# `.\transformers\models\vitmatte\convert_vitmatte_to_hf.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º utf-8
# ç‰ˆæƒå£°æ˜
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼Œä½¿ç”¨æ­¤æ–‡ä»¶éœ€è¦éµå®ˆè®¸å¯è¯è§„å®š
# å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥è·å–è®¸å¯è¯çš„å‰¯æœ¬
#     http://www.apache.org/licenses/LICENSE-2.0
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäº"AS IS"çš„åŸºç¡€ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶
# è¯·æŸ¥çœ‹è®¸å¯è¯ä»¥è·å–æœ‰å…³æƒé™å’Œé™åˆ¶çš„å…·ä½“è¯­è¨€
# ç”¨äºä»åŸå§‹å­˜å‚¨åº“è½¬æ¢ VitMatte æ£€æŸ¥ç‚¹
# URL: https://github.com/hustvl/ViTMatte

import argparse  # å¯¼å…¥è§£æå‘½ä»¤è¡Œå‚æ•°çš„æ¨¡å—
import requests  # å¯¼å…¥å‘é€ HTTP è¯·æ±‚çš„æ¨¡å—
import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from huggingface_hub import hf_hub_download  # ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹
from PIL import Image  # å¯¼å…¥ Python Imaging Library ç”¨äºå›¾åƒå¤„ç†

from transformers import VitDetConfig, VitMatteConfig, VitMatteForImageMatting, VitMatteImageProcessor  # å¯¼å…¥ VitMatte ç›¸å…³æ¨¡å—

# è·å–é…ç½®ä¿¡æ¯
def get_config(model_name):
    hidden_size = 384 if "small" in model_name else 768
    num_attention_heads = 6 if "small" in model_name else 12

    # è®¾ç½® VitDetConfig é…ç½®ä¿¡æ¯
    backbone_config = VitDetConfig(
        num_channels=4,
        image_size=512,
        pretrain_image_size=224,
        patch_size=16,
        hidden_size=hidden_size,
        num_attention_heads=num_attention_heads,
        use_absolute_position_embeddings=True,
        use_relative_position_embeddings=True,
        window_size=14,
        window_block_indices=[0, 1, 3, 4, 6, 7, 9, 10],
        residual_block_indices=[2, 5, 8, 11],
        out_features=["stage12"],
    )

    return VitMatteConfig(backbone_config=backbone_config, hidden_size=hidden_size)

# åˆ›å»ºéœ€è¦é‡å‘½åçš„é”®å€¼å¯¹åˆ—è¡¨
def create_rename_keys(config):
    rename_keys = []

    # stem
    rename_keys.append(("backbone.pos_embed", "backbone.embeddings.position_embeddings"))
    rename_keys.append(("backbone.patch_embed.proj.weight", "backbone.embeddings.projection.weight"))
    rename_keys.append(("backbone.patch_embed.proj.bias", "backbone.embeddings.projection.bias"))

    return rename_keys

# é‡å‘½åé”®å€¼å¯¹
def rename_key(dct, old, new):
    val = dct.pop(old)
    dct[new] = val

# è½¬æ¢ VitMatte æ£€æŸ¥ç‚¹
def convert_vitmatte_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub):
    config = get_config(model_name)

    # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
    model_name_to_filename = {
        "vitmatte-small-composition-1k": "ViTMatte_S_Com.pth",
        "vitmatte-base-composition-1k": "ViTMatte_B_Com.pth",
        "vitmatte-small-distinctions-646": "ViTMatte_S_DIS.pth",
        "vitmatte-base-distinctions-646": "ViTMatte_B_DIS.pth",
    }

    filename = model_name_to_filename[model_name]
    filepath = hf_hub_download(repo_id="nielsr/vitmatte-checkpoints", filename=filename, repo_type="model")
    state_dict = torch.load(filepath, map_location="cpu")

    # é‡å‘½åé”®
    # éå† state_dict çš„å‰¯æœ¬ä¸­çš„æ‰€æœ‰é”®
    for key in state_dict.copy().keys():
        # å¼¹å‡ºå½“å‰é”®å¯¹åº”çš„å€¼
        val = state_dict.pop(key)
        # å¦‚æœé”®ä¸­åŒ…å«"backbone.blocks"ï¼Œåˆ™æ›¿æ¢ä¸º"backbone.encoder.layer"
        if "backbone.blocks" in key:
            key = key.replace("backbone.blocks", "backbone.encoder.layer")
        # å¦‚æœé”®ä¸­åŒ…å«"attn"ï¼Œåˆ™æ›¿æ¢ä¸º"attention"
        if "attn" in key:
            key = key.replace("attn", "attention")
        # å¦‚æœé”®ä¸­åŒ…å«"fusion_blks"ï¼Œåˆ™æ›¿æ¢ä¸º"fusion_blocks"
        if "fusion_blks" in key:
            key = key.replace("fusion_blks", "fusion_blocks")
        # å¦‚æœé”®ä¸­åŒ…å«"bn"ï¼Œåˆ™æ›¿æ¢ä¸º"batch_norm"
        if "bn" in key:
            key = key.replace("bn", "batch_norm")
        # å°†æ›´æ–°åçš„é”®å€¼å¯¹é‡æ–°åŠ å…¥ state_dict
        state_dict[key] = val

    # ç”Ÿæˆé‡å‘½åé”®çš„åˆ—è¡¨
    rename_keys = create_rename_keys(config)
    # éå†é‡å‘½åé”®åˆ—è¡¨ï¼Œå¯¹ state_dict è¿›è¡Œé”®çš„é‡å‘½å
    for src, dest in rename_keys:
        rename_key(state_dict, src, dest)

    # åˆ›å»º VitMatteImageProcessor å®ä¾‹
    processor = VitMatteImageProcessor()
    # åˆ›å»º VitMatteForImageMatting æ¨¡å‹å®ä¾‹
    model = VitMatteForImageMatting(config)
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # åŠ è½½ state_dict åˆ°æ¨¡å‹ä¸­
    model.load_state_dict(state_dict)

    # ä»ç½‘ç»œåŠ è½½ç¤ºä¾‹å›¾ç‰‡å’Œ trimap
    url = "https://github.com/hustvl/ViTMatte/blob/main/demo/bulb_rgb.png?raw=true"
    image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
    url = "https://github.com/hustvl/ViTMatte/blob/main/demo/bulb_trimap.png?raw=true"
    trimap = Image.open(requests.get(url, stream=True).raw)

    # ä½¿ç”¨ processor å¤„ç†å›¾ç‰‡å’Œ trimapï¼Œè¿”å›åƒç´ å€¼
    pixel_values = processor(images=image, trimaps=trimap.convert("L"), return_tensors="pt").pixel_values

    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹ alpha å€¼
    with torch.no_grad():
        alphas = model(pixel_values).alphas

    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®é¢„æœŸçš„ alpha å€¼åˆ‡ç‰‡
    if model_name == "vitmatte-small-composition-1k":
        expected_slice = torch.tensor([[0.9977, 0.9987, 0.9990], [0.9980, 0.9998, 0.9998], [0.9983, 0.9998, 0.9998]])
    elif model_name == "vitmatte-base-composition-1k":
        expected_slice = torch.tensor([[0.9972, 0.9971, 0.9981], [0.9948, 0.9987, 0.9994], [0.9963, 0.9992, 0.9995]])
    elif model_name == "vitmatte-small-distinctions-646":
        expected_slice = torch.tensor([[0.9880, 0.9970, 0.9972], [0.9960, 0.9996, 0.9997], [0.9963, 0.9996, 0.9997]])
    elif model_name == "vitmatte-base-distinctions-646":
        expected_slice = torch.tensor([[0.9963, 0.9998, 0.9999], [0.9995, 1.0000, 1.0000], [0.9992, 0.9999, 1.0000]])

    # æ–­è¨€æ¨¡å‹é¢„æµ‹çš„ alpha å€¼ä¸é¢„æœŸå€¼æ¥è¿‘
    assert torch.allclose(alphas[0, 0, :3, :3], expected_slice, atol=1e-4)
    # æ‰“å°ç»“æœ
    print("Looks ok!")

    # å¦‚æœæŒ‡å®šäº† pytorch_dump_folder_pathï¼Œåˆ™ä¿å­˜æ¨¡å‹å’Œ processor
    if pytorch_dump_folder_path is not None:
        print(f"Saving model and processor of {model_name} to {pytorch_dump_folder_path}")
        model.save_pretrained(pytorch_dump_folder_path)
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        print(f"Pushing model and processor for {model_name} to hub")
        model.push_to_hub(f"hustvl/{model_name}")
        processor.push_to_hub(f"hustvl/{model_name}")
# å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€å‚æ•°
    parser.add_argument(
        "--model_name",
        default="vitmatte-small-composition-1k",
        type=str,
        choices=[
            "vitmatte-small-composition-1k",
            "vitmatte-base-composition-1k",
            "vitmatte-small-distinctions-646",
            "vitmatte-base-distinctions-646",
        ],
        help="Name of the VitMatte model you'd like to convert.",
    )
    # æ·»åŠ å‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹ç›®å½•çš„è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    # æ·»åŠ å‚æ•°ï¼šæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ° ğŸ¤— hub
    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°ï¼Œå°† VitMatte æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
    convert_vitmatte_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
```