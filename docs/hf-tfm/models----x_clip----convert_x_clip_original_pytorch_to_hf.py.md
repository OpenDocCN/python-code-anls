# `.\models\x_clip\convert_x_clip_original_pytorch_to_hf.py`

```py
# coding=utf-8
# Copyright 2022 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—

import gdown  # å¯¼å…¥ç”¨äºä»Google Driveä¸‹è½½æ–‡ä»¶çš„æ¨¡å—
import numpy as np  # å¯¼å…¥ç”¨äºæ•°å€¼è®¡ç®—çš„numpyåº“
import torch  # å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ åº“
from huggingface_hub import hf_hub_download  # å¯¼å…¥ä»Hugging Face Hubä¸‹è½½æ¨¡å‹çš„å‡½æ•°

from transformers import (  # å¯¼å…¥transformersåº“ä¸­çš„å„ç±»å¯¹è±¡
    CLIPTokenizer,  # CLIPæ¨¡å‹çš„åˆ†è¯å™¨
    CLIPTokenizerFast,  # åŠ é€Ÿç‰ˆæœ¬çš„CLIPåˆ†è¯å™¨
    VideoMAEImageProcessor,  # è§†é¢‘å’Œå›¾åƒå¤„ç†å™¨
    XCLIPConfig,  # XCLIPæ¨¡å‹çš„é…ç½®ç±»
    XCLIPModel,  # XCLIPæ¨¡å‹
    XCLIPProcessor,  # XCLIPå¤„ç†å™¨
    XCLIPTextConfig,  # XCLIPæ–‡æœ¬é…ç½®ç±»
    XCLIPVisionConfig,  # XCLIPè§†è§‰é…ç½®ç±»
)


def get_xclip_config(model_name, num_frames):
    text_config = XCLIPTextConfig()  # åˆ›å»ºä¸€ä¸ªXCLIPæ–‡æœ¬é…ç½®å¯¹è±¡

    # ä»æ¨¡å‹åç§°ä¸­æå–patchå¤§å°
    start_idx = model_name.find("patch")
    patch_size = int(model_name[start_idx + len("patch"): start_idx + len("patch") + 2])
    vision_config = XCLIPVisionConfig(patch_size=patch_size, num_frames=num_frames)  # åˆ›å»ºä¸€ä¸ªXCLIPè§†è§‰é…ç½®å¯¹è±¡

    if "large" in model_name:
        # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"large"ï¼Œè®¾ç½®å¤§æ¨¡å‹çš„æ–‡æœ¬å’Œè§†è§‰é…ç½®
        text_config.hidden_size = 768
        text_config.intermediate_size = 3072
        text_config.num_attention_heads = 12

        vision_config.hidden_size = 1024
        vision_config.intermediate_size = 4096
        vision_config.num_attention_heads = 16
        vision_config.num_hidden_layers = 24
        vision_config.mit_hidden_size = 768
        vision_config.mit_intermediate_size = 3072

    if model_name == "xclip-large-patch14-16-frames":
        # å¦‚æœæ¨¡å‹åç§°æ˜¯"xclip-large-patch14-16-frames"ï¼Œè®¾ç½®ç‰¹å®šçš„å›¾ç‰‡å°ºå¯¸
        vision_config.image_size = 336

    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)  # é€šè¿‡æ–‡æœ¬å’Œè§†è§‰é…ç½®åˆ›å»ºXCLIPæ¨¡å‹çš„é…ç½®å¯¹è±¡

    if "large" in model_name:
        config.projection_dim = 768  # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"large"ï¼Œè®¾ç½®æŠ•å½±ç»´åº¦ä¸º768

    return config  # è¿”å›é…ç½®å¯¹è±¡


def rename_key(name):
    # æ–‡æœ¬ç¼–ç å™¨
    if name == "token_embedding.weight":
        name = name.replace("token_embedding.weight", "text_model.embeddings.token_embedding.weight")
    if name == "positional_embedding":
        name = name.replace("positional_embedding", "text_model.embeddings.position_embedding.weight")
    if "ln_1" in name:
        name = name.replace("ln_1", "layer_norm1")
    if "ln_2" in name:
        name = name.replace("ln_2", "layer_norm2")
    if "c_fc" in name:
        name = name.replace("c_fc", "fc1")
    if "c_proj" in name:
        name = name.replace("c_proj", "fc2")
    if name.startswith("transformer.resblocks"):
        name = name.replace("transformer.resblocks", "text_model.encoder.layers")
    if "attn.out_proj" in name and "message" not in name:
        name = name.replace("attn.out_proj", "self_attn.out_proj")
    if "ln_final" in name:
        name = name.replace("ln_final", "text_model.final_layer_norm")
    # è§†è§‰ç¼–ç å™¨
    # å¦‚æœå˜é‡ name ç­‰äº "visual.class_embedding"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.embeddings.class_embedding"
    if name == "visual.class_embedding":
        name = name.replace("visual.class_embedding", "vision_model.embeddings.class_embedding")
    
    # å¦‚æœå˜é‡ name ç­‰äº "visual.positional_embedding"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.embeddings.position_embedding.weight"
    if name == "visual.positional_embedding":
        name = name.replace("visual.positional_embedding", "vision_model.embeddings.position_embedding.weight")
    
    # å¦‚æœå˜é‡ name ä»¥ "visual.transformer.resblocks" å¼€å¤´ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.encoder.layers"
    if name.startswith("visual.transformer.resblocks"):
        name = name.replace("visual.transformer.resblocks", "vision_model.encoder.layers")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å« "visual.conv1"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.embeddings.patch_embedding"
    if "visual.conv1" in name:
        name = name.replace("visual.conv1", "vision_model.embeddings.patch_embedding")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å« "visual.ln_pre"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.pre_layernorm"
    if "visual.ln_pre" in name:
        name = name.replace("visual.ln_pre", "vision_model.pre_layernorm")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å« "visual.ln_post"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.post_layernorm"
    if "visual.ln_post" in name:
        name = name.replace("visual.ln_post", "vision_model.post_layernorm")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å« "visual.proj"ï¼Œåˆ™æ›¿æ¢ä¸º "visual_projection.weight"
    if "visual.proj" in name:
        name = name.replace("visual.proj", "visual_projection.weight")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å« "text_projection"ï¼Œåˆ™æ›¿æ¢ä¸º "text_projection.weight"
    if "text_projection" in name:
        name = name.replace("text_projection", "text_projection.weight")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å« "prompts_visual_proj"ï¼Œåˆ™æ›¿æ¢ä¸º "prompts_visual_projection"
    if "prompts_visual_proj" in name:
        name = name.replace("prompts_visual_proj", "prompts_visual_projection")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å« "prompts_visual_ln"ï¼Œåˆ™æ›¿æ¢ä¸º "prompts_visual_layernorm"
    if "prompts_visual_ln" in name:
        name = name.replace("prompts_visual_ln", "prompts_visual_layernorm")
    
    # å¦‚æœå˜é‡ name ç­‰äº "mit.positional_embedding"ï¼Œåˆ™æ›¿æ¢ "positional" ä¸º "position"
    if name == "mit.positional_embedding":
        name = name.replace("positional", "position")
    
    # å¦‚æœå˜é‡ name ä»¥ "mit.resblocks" å¼€å¤´ï¼Œåˆ™æ›¿æ¢ä¸º "mit.encoder.layers"
    if name.startswith("mit.resblocks"):
        name = name.replace("mit.resblocks", "mit.encoder.layers")
    
    # å¦‚æœå˜é‡ name ä»¥ "prompts_generator.norm" å¼€å¤´ï¼Œåˆ™æ›¿æ¢ä¸º "prompts_generator.layernorm"
    if name.startswith("prompts_generator.norm"):
        name = name.replace("prompts_generator.norm", "prompts_generator.layernorm")
    
    # è¿”å›å¤„ç†åçš„ name å˜é‡
    return name
# ç®€å•è¿”å›ç»™å®šçš„åŸå§‹çŠ¶æ€å­—å…¸ï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•è½¬æ¢æ“ä½œ
def convert_state_dict(orig_state_dict, config):
    return orig_state_dict



# å‡†å¤‡è§†é¢‘æ•°æ®ï¼Œæ ¹æ®å¸§æ•°é€‰æ‹©å¯¹åº”çš„è§†é¢‘æ–‡ä»¶è¿›è¡Œä¸‹è½½å’ŒåŠ è½½
def prepare_video(num_frames):
    # æ ¹æ®å¸§æ•°é€‰æ‹©å¯¹åº”çš„è§†é¢‘æ–‡ä»¶å
    if num_frames == 8:
        filename = "eating_spaghetti_8_frames.npy"
    elif num_frames == 16:
        filename = "eating_spaghetti.npy"
    elif num_frames == 32:
        filename = "eating_spaghetti_32_frames.npy"
    # ä½¿ç”¨æŒ‡å®šçš„repo_idå’Œæ–‡ä»¶åä»æŒ‡å®šä»“åº“ç±»å‹ï¼ˆdatasetï¼‰ä¸‹è½½æ–‡ä»¶
    file = hf_hub_download(
        repo_id="hf-internal-testing/spaghetti-video",
        filename=filename,
        repo_type="dataset",
    )
    # åŠ è½½numpyæ•°ç»„ä¸­çš„è§†é¢‘æ•°æ®
    video = np.load(file)
    # å°†è§†é¢‘æ•°æ®è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼å¹¶è¿”å›
    return list(video)



# è¿™æ˜¯ä¸€ä¸ªå°šæœªå®ç°çš„å‡½æ•°å£°æ˜ï¼Œç”¨äºå°†XClipæ¨¡å‹çš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºPyTorchæ ¼å¼
def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):
    pass
    model_to_url = {
        # å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œå°†æ¨¡å‹åç§°æ˜ å°„åˆ°å…¶å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½åœ°å€
        # fully supervised kinetics-400 checkpoints
        "xclip-base-patch32": "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_8.pth",
        "xclip-base-patch32-16-frames": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_16.pth"
        ),
        "xclip-base-patch16": "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_8.pth",
        "xclip-base-patch16-16-frames": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_16.pth"
        ),
        "xclip-large-patch14": "https://drive.google.com/u/0/uc?id=1NUOImq0o5DlQTST17iIP3vG7DgmHQuCx&amp;export=download&amp;confirm=t&amp;uuid=b26caedc-88e2-473e-830a-9d158b653cdb",
        "xclip-large-patch14-16-frames": "https://drive.google.com/u/0/uc?id=1FOYgnJc097OJ4lGwtRCCydQyVPJEOH7d&amp;export=download&amp;confirm=t&amp;uuid=538fa810-e671-4050-b385-9a623f89804f",
        # fully supervised kinetics-600 checkpoints
        "xclip-base-patch16-kinetics-600": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_8.pth"
        ),
        "xclip-base-patch16-kinetics-600-16-frames": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_16.pth"
        ),
        "xclip-large-patch14-kinetics-600": "https://drive.google.com/u/0/uc?id=1FV8C1INuM91sLAN4ImjzePLIlpMSihwV&amp;export=download&amp;confirm=t&amp;uuid=141d4977-4a65-44ae-864f-4b0c19f838be",
        # few shot
        "xclip-base-patch16-hmdb-2-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_2.pth"
        ),
        "xclip-base-patch16-hmdb-4-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_4.pth"
        ),
        "xclip-base-patch16-hmdb-8-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_8.pth"
        ),
        "xclip-base-patch16-hmdb-16-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_16.pth"
        ),
        "xclip-base-patch16-ucf-2-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_2.pth"
        ),
        "xclip-base-patch16-ucf-4-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_4.pth"
        ),
        "xclip-base-patch16-ucf-8-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_8.pth"
        ),
        "xclip-base-patch16-ucf-16-shot": (
            "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_16.pth"
        ),
        # zero shot
        "xclip-base-patch16-zero-shot": "https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/zero.pth",
    }

    # æ ¹æ®ç»™å®šçš„æ¨¡å‹åç§°è·å–ç›¸åº”çš„é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½åœ°å€
    checkpoint_url = model_to_url[model_name]
    
    # é»˜è®¤å¸§æ•°è®¾ç½®ä¸º8å¸§ï¼Œå¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"16-frames"ï¼Œåˆ™è®¾ç½®ä¸º16å¸§
    num_frames = 8
    if "16-frames" in model_name:
        num_frames = 16
    # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å« "shot"ï¼Œè®¾å®šå¸§æ•°ä¸º32
    elif "shot" in model_name:
        num_frames = 32

    # æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„é…ç½®ä¿¡æ¯
    config = get_xclip_config(model_name, num_frames)
    # åˆ›å»º XCLIPModel æ¨¡å‹å¯¹è±¡
    model = XCLIPModel(config)
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # å¦‚æœ checkpoint_url ä¸­åŒ…å« "drive"
    if "drive" in checkpoint_url:
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶åä¸º "pytorch_model.bin"
        output = "pytorch_model.bin"
        # ä½¿ç”¨ gdown ä¸‹è½½ checkpoint_url å¯¹åº”çš„æ–‡ä»¶åˆ° output
        gdown.cached_download(checkpoint_url, output, quiet=False)
        # ä»ä¸‹è½½çš„æ–‡ä»¶ä¸­åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸åˆ° state_dictï¼Œå¹¶æŒ‡å®šåœ¨ CPU ä¸ŠåŠ è½½
        state_dict = torch.load(output, map_location="cpu")["model"]
    else:
        # ä» checkpoint_url åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çŠ¶æ€å­—å…¸åˆ° state_dict
        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)["model"]

    # è½¬æ¢åŠ è½½çš„çŠ¶æ€å­—å…¸ä»¥åŒ¹é…å½“å‰é…ç½®
    state_dict = convert_state_dict(state_dict, config)

    # åˆ›å»º XCLIPModel æ¨¡å‹å¯¹è±¡
    model = XCLIPModel(config)
    # æ ¹æ®åŠ è½½çš„çŠ¶æ€å­—å…¸åŠ è½½æ¨¡å‹å‚æ•°ï¼Œå…è®¸ç¼ºå°‘é”®ï¼Œä¸¥æ ¼æ€§è®¾ç½®ä¸º False
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    # æ–­è¨€ç¡®ä¿ç¼ºå°‘çš„é”®ä¸ºæŒ‡å®šçš„åˆ—è¡¨
    assert missing_keys == ["text_model.embeddings.position_ids", "vision_model.embeddings.position_ids"]
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©å›¾ç‰‡å¤„ç†çš„å°ºå¯¸
    size = 336 if model_name == "xclip-large-patch14-16-frames" else 224
    # åˆ›å»ºè§†é¢‘å¤šæ¨¡æ€è‡ªåŠ¨ç¼–ç å™¨å›¾åƒå¤„ç†å™¨å¯¹è±¡ï¼ŒæŒ‡å®šå›¾ç‰‡å°ºå¯¸
    image_processor = VideoMAEImageProcessor(size=size)
    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ CLIPTokenizer å¯¹è±¡
    slow_tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ CLIPTokenizerFast å¯¹è±¡
    fast_tokenizer = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")
    # åˆ›å»º XCLIPProcessor å¤„ç†å™¨å¯¹è±¡ï¼ŒæŒ‡å®šå›¾åƒå¤„ç†å™¨å’Œå¿«é€Ÿåˆ†è¯å™¨
    processor = XCLIPProcessor(image_processor=image_processor, tokenizer=fast_tokenizer)

    # å‡†å¤‡è§†é¢‘æ•°æ®ï¼Œè·å–è¾“å…¥å‚æ•°
    video = prepare_video(num_frames)
    # ä½¿ç”¨å¤„ç†å™¨å¤„ç†æ–‡æœ¬å’Œè§†é¢‘è¾“å…¥æ•°æ®ï¼Œè¿”å› PyTorch å¼ é‡æ ¼å¼ï¼Œè¿›è¡Œå¡«å……
    inputs = processor(
        text=["playing sports", "eating spaghetti", "go shopping"], videos=video, return_tensors="pt", padding=True
    )

    # æ‰“å°åƒç´ å€¼çš„å½¢çŠ¶ä¿¡æ¯
    print("Shape of pixel values:", inputs.pixel_values.shape)

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œè·å–è¾“å‡º
        outputs = model(**inputs)

    # éªŒè¯è¾“å‡ºç»“æœ
    logits_per_video = outputs.logits_per_video
    # å¯¹ logits è¿›è¡Œ softmax å¤„ç†å¾—åˆ°æ¦‚ç‡
    probs = logits_per_video.softmax(dim=1)
    # æ‰“å°æ¦‚ç‡å€¼
    print("Probs:", probs)

    # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©é¢„æœŸçš„æ¦‚ç‡å¼ é‡
    if model_name == "xclip-base-patch32":
        expected_probs = torch.tensor([[0.0019, 0.9951, 0.0030]])
    elif model_name == "xclip-base-patch32-16-frames":
        expected_probs = torch.tensor([[7.0999e-04, 9.9883e-01, 4.5580e-04]])
    elif model_name == "xclip-base-patch16":
        expected_probs = torch.tensor([[0.0083, 0.9681, 0.0236]])
    elif model_name == "xclip-base-patch16-16-frames":
        expected_probs = torch.tensor([[7.6937e-04, 9.9728e-01, 1.9473e-03]])
    elif model_name == "xclip-large-patch14":
        expected_probs = torch.tensor([[0.0062, 0.9864, 0.0075]])
    elif model_name == "xclip-large-patch14-16-frames":
        expected_probs = torch.tensor([[3.3877e-04, 9.9937e-01, 2.8888e-04]])
    elif model_name == "xclip-base-patch16-kinetics-600":
        expected_probs = torch.tensor([[0.0555, 0.8914, 0.0531]])
    elif model_name == "xclip-base-patch16-kinetics-600-16-frames":
        expected_probs = torch.tensor([[3.8554e-04, 9.9929e-01, 3.2754e-04]])
    elif model_name == "xclip-large-patch14-kinetics-600":
        expected_probs = torch.tensor([[0.0036, 0.9920, 0.0045]])
    elif model_name == "xclip-base-patch16-hmdb-2-shot":
        expected_probs = torch.tensor([[7.1890e-06, 9.9994e-01, 5.6559e-05]])
    # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©é¢„æœŸçš„æ¦‚ç‡å¼ é‡
    elif model_name == "xclip-base-patch16-hmdb-4-shot":
        expected_probs = torch.tensor([[1.0320e-05, 9.9993e-01, 6.2435e-05]])
    elif model_name == "xclip-base-patch16-hmdb-8-shot":
        expected_probs = torch.tensor([[4.1377e-06, 9.9990e-01, 9.8386e-05]])
    elif model_name == "xclip-base-patch16-hmdb-16-shot":
        expected_probs = torch.tensor([[4.1347e-05, 9.9962e-01, 3.3411e-04]])
    elif model_name == "xclip-base-patch16-ucf-2-shot":
        expected_probs = torch.tensor([[8.5857e-05, 9.9928e-01, 6.3291e-04]])
    elif model_name == "xclip-base-patch16-ucf-4-shot":
        expected_probs = torch.tensor([[8.5857e-05, 9.9928e-01, 6.3291e-04]])
    elif model_name == "xclip-base-patch16-ucf-8-shot":
        expected_probs = torch.tensor([[0.0027, 0.9904, 0.0070]])
    elif model_name == "xclip-base-patch16-ucf-16-shot":
        expected_probs = torch.tensor([[9.8219e-04, 9.9593e-01, 3.0863e-03]])
    # zero shot
    elif model_name == "xclip-base-patch16-zero-shot":
        expected_probs = torch.tensor([[3.5082e-04, 9.9785e-01, 1.7966e-03]])
    else:
        raise ValueError(f"Model name {model_name} not supported")

    # ä½¿ç”¨assertè¯­å¥æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡å€¼ä¸é¢„æœŸæ¦‚ç‡å¼ é‡çš„æ¥è¿‘ç¨‹åº¦
    assert torch.allclose(probs, expected_probs, atol=1e-3)
    # è¾“å‡ºç¡®è®¤ä¿¡æ¯
    print("Looks ok!")

    # å¦‚æœæŒ‡å®šäº†PyTorchæ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œåˆ™ä¿å­˜æ¨¡å‹
    if pytorch_dump_folder_path is not None:
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        model.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ°hubï¼Œåˆ™æ¨é€æ¨¡å‹ã€processorå’Œslow tokenizeræ–‡ä»¶åˆ°æŒ‡å®šçš„ç»„ç»‡
    if push_to_hub:
        print("Pushing model, processor and slow tokenizer files to the hub...")
        model.push_to_hub(model_name, organization="nielsr")
        processor.push_to_hub(model_name, organization="nielsr")
        slow_tokenizer.push_to_hub(model_name, organization="nielsr")
if __name__ == "__main__":
    # å¦‚æœä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œï¼Œè¿›å…¥ä¸»ç¨‹åºé€»è¾‘

    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡

    # Required parameters
    parser.add_argument(
        "--model_name",
        default="xclip-base-patch32",
        type=str,
        help="Name of the model.",
    )
    # æ·»åŠ æ¨¡å‹åç§°å‚æ•°ï¼Œé»˜è®¤ä¸º"xclip-base-patch32"ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å®šæ¨¡å‹åç§°

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    # æ·»åŠ PyTorchæ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å®šPyTorchæ¨¡å‹çš„è¾“å‡ºç›®å½•è·¯å¾„

    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    # æ·»åŠ æ˜¯å¦æ¨é€åˆ°ğŸ¤— hubçš„å‚æ•°ï¼Œä½¿ç”¨store_trueæ¥æ ‡è®°æ˜¯å¦æ¨é€æ¨¡å‹åˆ°hub

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•°è¿›è¡Œæ¨¡å‹è½¬æ¢ï¼Œä¼ å…¥å‘½ä»¤è¡Œè§£æåçš„å‚æ•°
    convert_xclip_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
```