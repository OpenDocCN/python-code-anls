# `.\transformers\data\datasets\language_modeling.py`

```
# å¯¼å…¥æ‰€éœ€çš„æ¨¡å—å’Œåº“
import json  # å¯¼å…¥ç”¨äº JSON æ•°æ®å¤„ç†çš„æ¨¡å—
import os  # å¯¼å…¥ç”¨äºæ“ä½œæ“ä½œç³»ç»ŸåŠŸèƒ½çš„æ¨¡å—
import pickle  # å¯¼å…¥ç”¨äºåºåˆ—åŒ–å’Œååºåˆ—åŒ– Python å¯¹è±¡çš„æ¨¡å—
import random  # å¯¼å…¥ç”¨äºç”Ÿæˆéšæœºæ•°çš„æ¨¡å—
import time  # å¯¼å…¥ç”¨äºæ—¶é—´ç›¸å…³æ“ä½œçš„æ¨¡å—
import warnings  # å¯¼å…¥ç”¨äºè­¦å‘Šå¤„ç†çš„æ¨¡å—
from typing import Dict, List, Optional  # å¯¼å…¥ç”¨äºç±»å‹æç¤ºçš„æ¨¡å—

import torch  # å¯¼å…¥ PyTorch åº“
from filelock import FileLock  # å¯¼å…¥ç”¨äºæ–‡ä»¶é”çš„æ¨¡å—
from torch.utils.data import Dataset  # å¯¼å…¥ PyTorch æ•°æ®é›†ç±»

from ...tokenization_utils import PreTrainedTokenizer  # å¯¼å…¥é¢„è®­ç»ƒåˆ†è¯å™¨ç±»
from ...utils import logging  # å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—

# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# å¼ƒç”¨è­¦å‘Šæ¶ˆæ¯
DEPRECATION_WARNING = (
    "This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets "
    "library. You can have a look at this example script for pointers: {0}"
)

# æ–‡æœ¬æ•°æ®é›†ç±»
class TextDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,  # é¢„è®­ç»ƒåˆ†è¯å™¨
        file_path: str,  # æ–‡ä»¶è·¯å¾„
        block_size: int,  # æ•°æ®å—å¤§å°
        overwrite_cache=False,  # æ˜¯å¦è¦†ç›–ç¼“å­˜
        cache_dir: Optional[str] = None,  # ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºNone
    ):
        # å‘å‡ºå³å°†å¼ƒç”¨çš„è­¦å‘Šï¼Œæä¾›ç›¸å…³é“¾æ¥ä»¥è·å–æ›´å¤šä¿¡æ¯
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
            ),
            FutureWarning,
        )
        # å¦‚æœæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œå¼•å‘æ•°å€¼é”™è¯¯
        if os.path.isfile(file_path) is False:
            raise ValueError(f"Input file path {file_path} not found")

        # å‡å»é…å¯¹æ¨¡å¼ä¸‹ç‰¹æ®Šæ ‡è®°çš„æ•°é‡ä»¥è®¡ç®—å—å¤§å°
        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)

        # æ‹†åˆ†æ–‡ä»¶è·¯å¾„ï¼Œè·å–ç›®å½•å’Œæ–‡ä»¶å
        directory, filename = os.path.split(file_path)
        # åˆ›å»ºç¼“å­˜ç‰¹å¾æ–‡ä»¶è·¯å¾„
        cached_features_file = os.path.join(
            cache_dir if cache_dir is not None else directory,
            f"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}",
        )

        # ç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åªæœ‰ç¬¬ä¸€ä¸ªè¿›ç¨‹å¤„ç†æ•°æ®é›†ï¼Œå…¶ä»–è¿›ç¨‹ä½¿ç”¨ç¼“å­˜
        lock_path = cached_features_file + ".lock"
        with FileLock(lock_path):
            # å¦‚æœç¼“å­˜ç‰¹å¾æ–‡ä»¶å­˜åœ¨ä¸”ä¸è¦†ç›–ç¼“å­˜ï¼Œåˆ™åŠ è½½ç¼“å­˜æ•°æ®
            if os.path.exists(cached_features_file) and not overwrite_cache:
                start = time.time()
                with open(cached_features_file, "rb") as handle:
                    self.examples = pickle.load(handle)
                logger.info(
                    f"Loading features from cached file {cached_features_file} [took %.3f s]", time.time() - start
                )

            else:
                # å¦åˆ™ï¼Œä»æ•°æ®é›†æ–‡ä»¶ä¸­åˆ›å»ºç‰¹å¾
                logger.info(f"Creating features from dataset file at {directory}")

                self.examples = []
                with open(file_path, encoding="utf-8") as f:
                    text = f.read()

                # ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°çš„ ID
                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))

                # æŒ‰å—å¤§å°æˆªæ–­æ–‡æœ¬
                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size
                    self.examples.append(
                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])
                    )
                # æ³¨æ„ï¼šä¸ºç®€åŒ–èµ·è§ï¼ˆæ— å¡«å……ï¼‰ï¼Œè¿™é‡Œä¸¢å¼ƒäº†æœ€åä¸€ä¸ªæˆªæ–­çš„ç¤ºä¾‹
                # å¦‚æœä½ çš„æ•°æ®é›†è¾ƒå°ï¼Œé¦–å…ˆä½ åº”è¯¥å¯»æ‰¾ä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›† :-) å…¶æ¬¡ï¼Œ
                # ä½ å¯ä»¥é€šè¿‡æ·»åŠ ï¼ˆç‰¹å®šäºæ¨¡å‹çš„ï¼‰å¡«å……æ¥æ”¹å˜è¿™ç§è¡Œä¸ºã€‚

                start = time.time()
                # å°†ç‰¹å¾ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ä¸­
                with open(cached_features_file, "wb") as handle:
                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)
                logger.info(
                    f"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]"
                )

    def __len__(self):
        # è¿”å›ç¤ºä¾‹åˆ—è¡¨çš„é•¿åº¦
        return len(self.examples)

    def __getitem__(self, i) -> torch.Tensor:
        # è¿”å›æŒ‡å®šç´¢å¼•å¤„çš„ç¤ºä¾‹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º Torch å¼ é‡
        return torch.tensor(self.examples[i], dtype=torch.long)
# åˆ›å»ºä¸€ä¸ªç”¨äºé€è¡Œè¯»å–æ–‡æœ¬æ•°æ®çš„æ•°æ®é›†ç±»ï¼Œå‡†å¤‡é€è¡Œè¯»å–æ–‡æœ¬æ•°æ®çš„æ•°æ®é›†ç±»ä»¥åä¼šè¢«ä¸€ä¸ªä¸æ¡†æ¶æ— å…³çš„æ–¹æ³•å–ä»£
class LineByLineTextDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—åˆ†è¯å™¨ã€æ–‡ä»¶è·¯å¾„å’Œå—å¤§å°ä½œä¸ºå‚æ•°
    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):
        # å‘å‡ºè­¦å‘Šï¼Œæç¤ºæ­¤æ–¹æ³•å³å°†è¢«ä¸€ä¸ªä¸æ¡†æ¶æ— å…³çš„æ–¹æ³•å–ä»£ï¼ŒåŒæ—¶æä¾›äº†ä¸€ä¸ªé“¾æ¥ä»¥ä¾›æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
            ),
            FutureWarning,
        )
        # å¦‚æœæŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¼‚å¸¸
        if os.path.isfile(file_path) is False:
            raise ValueError(f"Input file path {file_path} not found")
        # ä¸ç¼“å­˜ç‰¹å¾ï¼ŒåŸºäºä»¥ä¸‹å‡è®¾ï¼šæˆ‘ä»¬å¾ˆå¿«å°†åœ¨æ‰€æœ‰åœ°æ–¹ä½¿ç”¨`tokenizers`ä»“åº“ä¸­çš„å¿«é€Ÿå¤šçº¿ç¨‹åˆ†è¯å™¨
        logger.info(f"Creating features from dataset file at {file_path}")
        
        # ä½¿ç”¨UTF-8ç¼–ç æ‰“å¼€æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ï¼Œå¹¶è¯»å–æ–‡ä»¶ä¸­çš„éç©ºè¡Œ
        with open(file_path, encoding="utf-8") as f:
            # å°†éç©ºè¡Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œå»é™¤äº†è¡Œå°¾çš„æ¢è¡Œç¬¦
            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]
        
        # ä½¿ç”¨åˆ†è¯å™¨å¯¹è¯»å–çš„æ–‡æœ¬è¡Œè¿›è¡Œåˆ†è¯ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°å¹¶æˆªæ–­åˆ°æŒ‡å®šçš„å—å¤§å°
        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)
        # å°†åˆ†è¯åçš„è¾“å…¥ ID å­˜å‚¨ä¸ºç¤ºä¾‹ï¼Œä½œä¸ºæ•°æ®é›†çš„ä¸€éƒ¨åˆ†
        self.examples = batch_encoding["input_ids"]
        # å°†ç¤ºä¾‹è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œé”®ä¸º"input_ids"ï¼Œå€¼ä¸ºå¯¹åº”çš„å¼ é‡
        self.examples = [{"input_ids": torch.tensor(e, dtype=torch.long)} for e in self.examples]

    # è¿”å›æ•°æ®é›†ä¸­ç¤ºä¾‹çš„æ•°é‡
    def __len__(self):
        return len(self.examples)

    # æ ¹æ®ç´¢å¼•è¿”å›å¯¹åº”çš„ç¤ºä¾‹ï¼Œä»¥å­—å…¸æ ¼å¼åŒ…å«é”®"input_ids"å’Œå¯¹åº”çš„å¼ é‡å€¼
    def __getitem__(self, i) -> Dict[str, torch.tensor]:
        return self.examples[i]


# å®šä¹‰å¦ä¸€ä¸ªæ•°æ®é›†ç±»ï¼Œç”¨äºé€è¡Œè¯»å–å…·æœ‰å‚è€ƒæ•°æ®çš„æ–‡æœ¬
class LineByLineWithRefDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """
    # åˆå§‹åŒ–å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªåˆ†è¯å™¨å¯¹è±¡ã€æ–‡ä»¶è·¯å¾„ã€å—å¤§å°å’Œå‚è€ƒæ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°
    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int, ref_path: str):
        # å‘å‡ºå…³äºå³å°†å¼ƒç”¨çš„è­¦å‘Šï¼ŒåŒ…å«ä¸€ä¸ªé“¾æ¥æŒ‡å‘ç¤ºä¾‹ä»£ç çš„åœ°å€
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm_wwm.py"
            ),
            FutureWarning,
        )
        # å¦‚æœæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡º ValueError å¼‚å¸¸
        if os.path.isfile(file_path) is False:
            raise ValueError(f"Input file path {file_path} not found")
        # å¦‚æœå‚è€ƒæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡º ValueError å¼‚å¸¸
        if os.path.isfile(ref_path) is False:
            raise ValueError(f"Ref file path {file_path} not found")
        # è®°å½•æ—¥å¿—ï¼ŒæŒ‡ç¤ºæ­£åœ¨ä»æ•°æ®é›†æ–‡ä»¶åˆ›å»ºç‰¹å¾
        logger.info(f"Creating features from dataset file at {file_path}")
        # è®°å½•æ—¥å¿—ï¼ŒæŒ‡ç¤ºæ­£åœ¨ä½¿ç”¨å‚è€ƒæ®µè½ç»“æœ
        logger.info(f"Use ref segment results at {ref_path}")
        # æ‰“å¼€æ•°æ®é›†æ–‡ä»¶ï¼Œè¯»å–æ‰€æœ‰è¡Œ
        with open(file_path, encoding="utf-8") as f:
            # ä½¿ç”¨ readlines() æ–¹æ³•è¯»å–æ–‡ä»¶çš„æ‰€æœ‰è¡Œï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨åˆ—è¡¨ä¸­
            data = f.readlines()  # use this method to avoid delimiter '\u2029' to split a line
        # æ¸…ç†æ•°æ®ï¼šå»é™¤æ¯è¡Œä¸¤è¾¹çš„ç©ºç™½å­—ç¬¦ï¼Œå¹¶ä¸”ç¡®ä¿è¡Œä¸ä¸ºç©º
        data = [line.strip() for line in data if len(line) > 0 and not line.isspace()]
        # ä»æ–‡ä»¶ä¸­è·å–å‚è€ƒä¿¡æ¯
        with open(ref_path, encoding="utf-8") as f:
            # ä½¿ç”¨ read().splitlines() æ–¹æ³•è¯»å–æ–‡ä»¶çš„æ‰€æœ‰è¡Œï¼Œå¹¶å°†å…¶æ‹†åˆ†æˆè¡Œçš„åˆ—è¡¨
            ref = [json.loads(line) for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]
        # æ£€æŸ¥æ•°æ®å’Œå‚è€ƒçš„é•¿åº¦æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™æŠ›å‡º ValueError å¼‚å¸¸
        if len(data) != len(ref):
            raise ValueError(
                f"Length of Input file should be equal to Ref file. But the length of {file_path} is {len(data)} "
                f"while length of {ref_path} is {len(ref)}"
            )

        # ä½¿ç”¨åˆ†è¯å™¨å¯¹æ•°æ®è¿›è¡Œç¼–ç ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ã€æˆªæ–­å’Œé™åˆ¶æœ€å¤§é•¿åº¦ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ batch_encoding ä¸­
        batch_encoding = tokenizer(data, add_special_tokens=True, truncation=True, max_length=block_size)
        # å°†ç¼–ç åçš„æ•°æ®å­˜å‚¨åœ¨ self.examples ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬ä½¿ç”¨å­—å…¸å­˜å‚¨
        self.examples = batch_encoding["input_ids"]
        # å°†æ¯ä¸ªæ ·æœ¬è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œå¹¶å­˜å‚¨åœ¨ self.examples ä¸­
        self.examples = [{"input_ids": torch.tensor(e, dtype=torch.long)} for e in self.examples]

        # è®¡ç®—æ ·æœ¬æ•°é‡
        n = len(self.examples)
        # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å¯¹åº”çš„å‚è€ƒä¿¡æ¯
        for i in range(n):
            # ä½¿ç”¨ torch.tensor åˆ›å»ºå‚è€ƒä¿¡æ¯çš„å¼ é‡ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ self.examples[i]["chinese_ref"] ä¸­
            self.examples[i]["chinese_ref"] = torch.tensor(ref[i], dtype=torch.long)

    # è·å–æ•°æ®é›†çš„é•¿åº¦
    def __len__(self):
        return len(self.examples)

    # è·å–æŒ‡å®šç´¢å¼•çš„æ ·æœ¬
    def __getitem__(self, i) -> Dict[str, torch.tensor]:
        return self.examples[i]
class LineByLineWithSOPTextDataset(Dataset):
    """
    Dataset for sentence order prediction task, prepare sentence pairs for SOP task
    """

    def __init__(self, tokenizer: PreTrainedTokenizer, file_dir: str, block_size: int):
        # å‘å‡ºè­¦å‘Šï¼Œæé†’ä½¿ç”¨è€…æ­¤åŠŸèƒ½å³å°†è¢«å¼ƒç”¨
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
            ),
            FutureWarning,
        )
        # æ£€æŸ¥æ–‡ä»¶ç›®å½•æ˜¯å¦å­˜åœ¨
        if os.path.isdir(file_dir) is False:
            raise ValueError(f"{file_dir} is not a directory")
        logger.info(f"Creating features from dataset file folder at {file_dir}")
        self.examples = []
        # TODO: randomness could apply a random seed, ex. rng = random.Random(random_seed)
        # æ–‡ä»¶è·¯å¾„ç±»ä¼¼äº ./dataset/wiki_1, ./dataset/wiki_2
        for file_name in os.listdir(file_dir):
            file_path = os.path.join(file_dir, file_name)
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.isfile(file_path) is False:
                raise ValueError(f"{file_path} is not a file")
            article_open = False
            with open(file_path, encoding="utf-8") as f:
                original_lines = f.readlines()
                article_lines = []
                for line in original_lines:
                    if "<doc id=" in line:
                        article_open = True
                    elif "</doc>" in line:
                        article_open = False
                        document = [
                            tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line))
                            for line in article_lines[1:]
                            if (len(line) > 0 and not line.isspace())
                        ]

                        examples = self.create_examples_from_document(document, block_size, tokenizer)
                        self.examples.extend(examples)
                        article_lines = []
                    else:
                        if article_open:
                            article_lines.append(line)

        logger.info("Dataset parse finished.")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i) -> Dict[str, torch.tensor]:
        return self.examples[i]


class TextDatasetForNextSentencePrediction(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        overwrite_cache=False,
        short_seq_probability=0.1,
        nsp_probability=0.5,
    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return self.examples[i]
```