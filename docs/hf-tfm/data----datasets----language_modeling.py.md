# `.\data\datasets\language_modeling.py`

```py
# å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œåº“
import json
import os
import pickle
import random
import time
import warnings
from typing import Dict, List, Optional

import torch
from filelock import FileLock
from torch.utils.data import Dataset

# å¯¼å…¥ç›¸å¯¹è·¯å¾„çš„æ¨¡å—å’Œåº“
from ...tokenization_utils import PreTrainedTokenizer
from ...utils import logging

# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# å¼ƒç”¨è­¦å‘Šä¿¡æ¯
DEPRECATION_WARNING = (
    "This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets "
    "library. You can have a look at this example script for pointers: {0}"
)

class TextDataset(Dataset):
    """
    è¿™ä¸ªç±»å°†å¾ˆå¿«è¢«ä¸€ä¸ªä¸æ¡†æ¶æ— å…³çš„æ–¹æ³•æ‰€å–ä»£ã€‚
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        overwrite_cache=False,
        cache_dir: Optional[str] = None,
    ):
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
            ),
            FutureWarning,
        )
        # æ£€æŸ¥è¾“å…¥çš„æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æŠ›å‡ºå¼‚å¸¸
        if os.path.isfile(file_path) is False:
            raise ValueError(f"Input file path {file_path} not found")

        # æ ¹æ®tokenizerçš„ç‰¹æ®Štokenæ•°ç›®ï¼Œè°ƒæ•´block_sizeçš„å¤§å°
        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)

        # å°†æ–‡ä»¶è·¯å¾„æ‹†åˆ†ä¸ºç›®å½•å’Œæ–‡ä»¶å
        directory, filename = os.path.split(file_path)
        # è®¾ç½®ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«æ¨¡å‹åç§°ã€block_sizeå’Œæ–‡ä»¶åç­‰ä¿¡æ¯
        cached_features_file = os.path.join(
            cache_dir if cache_dir is not None else directory,
            f"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}",
        )

        # ç¡®ä¿åªæœ‰åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„ç¬¬ä¸€ä¸ªè¿›ç¨‹å¤„ç†æ•°æ®é›†ï¼Œå…¶ä»–è¿›ç¨‹ä½¿ç”¨ç¼“å­˜
        lock_path = cached_features_file + ".lock"
        # ä½¿ç”¨æ–‡ä»¶é”å®šç¡®ä¿å¹¶å‘å®‰å…¨æ€§
        with FileLock(lock_path):
            # å¦‚æœç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸éœ€è¦è¦†ç›–ï¼Œåˆ™åŠ è½½ç¼“å­˜ä¸­çš„ç‰¹å¾
            if os.path.exists(cached_features_file) and not overwrite_cache:
                start = time.time()
                with open(cached_features_file, "rb") as handle:
                    self.examples = pickle.load(handle)
                logger.info(
                    f"Loading features from cached file {cached_features_file} [took %.3f s]", time.time() - start
                )

            else:
                logger.info(f"Creating features from dataset file at {directory}")

                # åˆå§‹åŒ–self.examplesä¸ºç©ºåˆ—è¡¨
                self.examples = []
                # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å–æ–‡æœ¬å†…å®¹
                with open(file_path, encoding="utf-8") as f:
                    text = f.read()

                # ä½¿ç”¨tokenizerå°†æ–‡æœ¬åˆ†è¯å¹¶è½¬æ¢ä¸ºå¯¹åº”çš„token IDs
                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))

                # æ ¹æ®block_sizeå°†tokenized_textåˆ†å‰²æˆç‰‡æ®µï¼Œå¹¶æ„å»ºç‰¹å¾åˆ—è¡¨self.examples
                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size
                    self.examples.append(
                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])
                    )
                # æ³¨æ„ï¼Œè¿™é‡Œä¸ºç®€åŒ–èµ·è§ï¼Œæœ€åä¸€ä¸ªè¢«æˆªæ–­çš„ç¤ºä¾‹è¢«ä¸¢å¼ƒäº†ï¼ˆæ²¡æœ‰è¿›è¡Œå¡«å……ï¼‰
                # å¦‚æœä½ çš„æ•°æ®é›†å¾ˆå°ï¼Œé¦–å…ˆåº”è¯¥å¯»æ‰¾æ›´å¤§çš„æ•°æ®é›†ï¼Œå¹¶ä¸”ä½ å¯ä»¥é€šè¿‡æ·»åŠ ï¼ˆç‰¹å®šäºæ¨¡å‹çš„ï¼‰å¡«å……æ¥æ›´æ”¹æ­¤è¡Œä¸ºã€‚

                start = time.time()
                # å°†self.examplesä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ä¸­
                with open(cached_features_file, "wb") as handle:
                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)
                logger.info(
                    f"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]"
                )

    # è¿”å›self.examplesçš„é•¿åº¦ä½œä¸ºæ•°æ®é›†çš„é•¿åº¦
    def __len__(self):
        return len(self.examples)

    # æ ¹æ®ç´¢å¼•è¿”å›å¯¹åº”çš„torch.Tensorå¯¹è±¡ï¼ŒåŒ…å«åœ¨self.examplesä¸­çš„æ•°æ®
    def __getitem__(self, i) -> torch.Tensor:
        return torch.tensor(self.examples[i], dtype=torch.long)
class LineByLineTextDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """

    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):
        # å‘å‡ºè­¦å‘Šï¼ŒæŒ‡å‡ºæ­¤æ–¹æ³•å³å°†è¢«ä¸ä¾èµ–æ¡†æ¶çš„æ–¹æ³•å–ä»£
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
            ),
            FutureWarning,
        )
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
        if os.path.isfile(file_path) is False:
            raise ValueError(f"Input file path {file_path} not found")
        # è®°å½•æ¶ˆæ¯åˆ°æ—¥å¿—ï¼ŒæŒ‡ç¤ºæ­£åœ¨ä»æ–‡ä»¶è·¯å¾„åˆ›å»ºæ•°æ®é›†ç‰¹å¾
        logger.info(f"Creating features from dataset file at {file_path}")

        # ä½¿ç”¨ utf-8 ç¼–ç æ‰“å¼€æ–‡ä»¶ï¼Œè¯»å–æ‰€æœ‰éç©ºè¡Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
        with open(file_path, encoding="utf-8") as f:
            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]

        # ä½¿ç”¨ç»™å®šçš„åˆ†è¯å™¨å¯¹è¡Œè¿›è¡Œç¼–ç ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°å¹¶æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)
        # å°†ç¼–ç åçš„è¾“å…¥ IDs å­˜å‚¨åœ¨ç¤ºä¾‹ä¸­
        self.examples = batch_encoding["input_ids"]
        # å°†æ¯ä¸ªç¤ºä¾‹å°è£…ä¸ºåŒ…å«è¾“å…¥ IDs çš„å­—å…¸ï¼Œå¹¶ä½¿ç”¨é•¿æ•´å‹å¼ é‡è¿›è¡Œå­˜å‚¨
        self.examples = [{"input_ids": torch.tensor(e, dtype=torch.long)} for e in self.examples]

    def __len__(self):
        # è¿”å›ç¤ºä¾‹åˆ—è¡¨çš„é•¿åº¦ï¼Œå³æ•°æ®é›†ä¸­ç¤ºä¾‹çš„æ•°é‡
        return len(self.examples)

    def __getitem__(self, i) -> Dict[str, torch.tensor]:
        # è¿”å›ç´¢å¼•ä¸º i çš„ç¤ºä¾‹ï¼Œè¯¥ç¤ºä¾‹æ˜¯åŒ…å«è¾“å…¥ IDs çš„å­—å…¸
        return self.examples[i]


class LineByLineWithRefDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """
    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int, ref_path: str):
        # å‘å‡ºè­¦å‘Šï¼ŒæŒ‡ç¤ºä»£ç çš„æŸäº›åŠŸèƒ½å°†æ¥ä¼šè¢«å¼ƒç”¨ï¼Œå¹¶æä¾›äº†æ›´å¤šä¿¡æ¯çš„é“¾æ¥
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm_wwm.py"
            ),
            FutureWarning,
        )
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å¼•å‘ ValueError å¼‚å¸¸
        if os.path.isfile(file_path) is False:
            raise ValueError(f"Input file path {file_path} not found")
        # æ£€æŸ¥å‚è€ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å¼•å‘ ValueError å¼‚å¸¸
        if os.path.isfile(ref_path) is False:
            raise ValueError(f"Ref file path {file_path} not found")
        
        # ä¸ç¼“å­˜ç‰¹å¾ï¼Œå‡è®¾å¾ˆå¿«å°†åœ¨æ‰€æœ‰åœ°æ–¹ä½¿ç”¨æ¥è‡ª `tokenizers` ä»“åº“çš„å¿«é€Ÿå¤šçº¿ç¨‹åˆ†è¯å™¨
        logger.info(f"Creating features from dataset file at {file_path}")
        logger.info(f"Use ref segment results at {ref_path}")
        
        # ä½¿ç”¨ UTF-8 ç¼–ç æ‰“å¼€æ•°æ®æ–‡ä»¶ï¼Œå¹¶è¯»å–æ‰€æœ‰è¡Œåˆ°å˜é‡ data ä¸­
        with open(file_path, encoding="utf-8") as f:
            data = f.readlines()  # ä½¿ç”¨è¿™ç§æ–¹æ³•é¿å…ä½¿ç”¨åˆ†éš”ç¬¦ '\u2029' æ¥åˆ†å‰²è¡Œ
        
        # å»é™¤æ¯è¡Œä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦ï¼Œå¹¶æ’é™¤ç©ºè¡Œï¼Œç”Ÿæˆæœ€ç»ˆçš„æ•°æ®åˆ—è¡¨
        data = [line.strip() for line in data if len(line) > 0 and not line.isspace()]
        
        # ä½¿ç”¨ UTF-8 ç¼–ç æ‰“å¼€å‚è€ƒæ–‡ä»¶ï¼Œå¹¶æŒ‰è¡Œè§£ææ¯è¡Œä¸º JSON å¯¹è±¡ï¼Œç”Ÿæˆ ref åˆ—è¡¨
        with open(ref_path, encoding="utf-8") as f:
            ref = [json.loads(line) for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]
        
        # æ£€æŸ¥æ•°æ®åˆ—è¡¨å’Œå‚è€ƒåˆ—è¡¨çš„é•¿åº¦æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™å¼•å‘ ValueError å¼‚å¸¸
        if len(data) != len(ref):
            raise ValueError(
                f"Length of Input file should be equal to Ref file. But the length of {file_path} is {len(data)} "
                f"while length of {ref_path} is {len(ref)}"
            )

        # ä½¿ç”¨ tokenizer å¯¹æ•°æ®è¿›è¡Œç¼–ç å¤„ç†ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°å¹¶æˆªæ–­åˆ°æŒ‡å®šçš„ block_size
        batch_encoding = tokenizer(data, add_special_tokens=True, truncation=True, max_length=block_size)
        
        # å°†æ¯ä¸ªç¼–ç åçš„ç¤ºä¾‹çš„ "input_ids" å­˜å‚¨ä¸ºåˆ—è¡¨çš„å½¢å¼ï¼Œå­˜å‚¨åœ¨ self.examples ä¸­
        self.examples = batch_encoding["input_ids"]
        
        # å°†æ¯ä¸ª "input_ids" è½¬æ¢ä¸ºåŒ…å« torch.tensor çš„å­—å…¸å½¢å¼ï¼Œå­˜å‚¨åœ¨ self.examples ä¸­
        self.examples = [{"input_ids": torch.tensor(e, dtype=torch.long)} for e in self.examples]

        # ä¸ºæ¯ä¸ªç¤ºä¾‹æ·»åŠ  "chinese_ref" å­—æ®µï¼Œå€¼ä¸ºå‚è€ƒæ•°æ®çš„ torch.tensor å½¢å¼
        n = len(self.examples)
        for i in range(n):
            self.examples[i]["chinese_ref"] = torch.tensor(ref[i], dtype=torch.long)

    def __len__(self):
        # è¿”å›ç¤ºä¾‹åˆ—è¡¨çš„é•¿åº¦ï¼Œç”¨äºç¡®å®šæ•°æ®é›†çš„å¤§å°
        return len(self.examples)

    def __getitem__(self, i) -> Dict[str, torch.tensor]:
        # æ ¹æ®ç´¢å¼• i è¿”å›å¯¹åº”çš„ç¤ºä¾‹ï¼Œä¸ºå­—å…¸å½¢å¼ï¼ŒåŒ…å« "input_ids" å’Œ "chinese_ref"
        return self.examples[i]
class LineByLineWithSOPTextDataset(Dataset):
    """
    Dataset for sentence order prediction task, prepare sentence pairs for SOP task
    """

    def __init__(self, tokenizer: PreTrainedTokenizer, file_dir: str, block_size: int):
        # å‘å‡ºè­¦å‘Šï¼Œæé†’æ­¤åŠŸèƒ½å³å°†è¢«å¼ƒç”¨ï¼Œå¹¶æä¾›ç›¸å…³é“¾æ¥
        warnings.warn(
            DEPRECATION_WARNING.format(
                "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
            ),
            FutureWarning,
        )
        # å¦‚æœæä¾›çš„æ–‡ä»¶ç›®å½•ä¸æ˜¯ä¸€ä¸ªç›®å½•ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
        if os.path.isdir(file_dir) is False:
            raise ValueError(f"{file_dir} is not a directory")
        # è®°å½•ä¿¡æ¯ï¼ŒæŒ‡å‡ºæ­£åœ¨ä»æŒ‡å®šæ–‡ä»¶å¤¹åˆ›å»ºæ•°æ®é›†ç‰¹å¾
        logger.info(f"Creating features from dataset file folder at {file_dir}")
        # åˆå§‹åŒ–ç©ºçš„ç¤ºä¾‹åˆ—è¡¨
        self.examples = []
        # éå†æ–‡ä»¶ç›®å½•ä¸‹çš„æ¯ä¸ªæ–‡ä»¶å
        for file_name in os.listdir(file_dir):
            file_path = os.path.join(file_dir, file_name)
            # å¦‚æœæ–‡ä»¶è·¯å¾„ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
            if os.path.isfile(file_path) is False:
                raise ValueError(f"{file_path} is not a file")
            # åˆå§‹åŒ–æ–‡ç« æ‰“å¼€æ ‡å¿—ä¸ºå‡
            article_open = False
            # æ‰“å¼€æ–‡ä»¶ï¼Œä½¿ç”¨UTF-8ç¼–ç 
            with open(file_path, encoding="utf-8") as f:
                # è¯»å–åŸå§‹è¡Œ
                original_lines = f.readlines()
                # åˆå§‹åŒ–æ–‡ç« è¡Œåˆ—è¡¨
                article_lines = []
                # éå†åŸå§‹è¡Œ
                for line in original_lines:
                    # å¦‚æœå½“å‰è¡ŒåŒ…å«"<doc id="ï¼Œè¡¨ç¤ºæ–‡ç« å¼€å§‹
                    if "<doc id=" in line:
                        article_open = True
                    # å¦‚æœå½“å‰è¡ŒåŒ…å«"</doc>"ï¼Œè¡¨ç¤ºæ–‡ç« ç»“æŸ
                    elif "</doc>" in line:
                        article_open = False
                        # å°†æ–‡ç« è¡Œåˆ—è¡¨ä¸­ç¬¬äºŒè¡Œå¼€å§‹ï¼ˆæ’é™¤ç¬¬ä¸€è¡Œæ ‡é¢˜ï¼‰çš„æ¯ä¸€è¡Œè½¬æ¢ä¸ºtoken IDs
                        document = [
                            tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line))
                            for line in article_lines[1:]
                            if (len(line) > 0 and not line.isspace())
                        ]
                        # æ ¹æ®æ–‡æ¡£åˆ›å»ºç¤ºä¾‹ï¼Œå°†å…¶æ‰©å±•åˆ°self.examplesåˆ—è¡¨ä¸­
                        examples = self.create_examples_from_document(document, block_size, tokenizer)
                        self.examples.extend(examples)
                        # æ¸…ç©ºæ–‡ç« è¡Œåˆ—è¡¨
                        article_lines = []
                    else:
                        # å¦‚æœæ–‡ç« æ­£åœ¨æ‰“å¼€ï¼Œåˆ™å°†å½“å‰è¡Œæ·»åŠ åˆ°æ–‡ç« è¡Œåˆ—è¡¨ä¸­
                        if article_open:
                            article_lines.append(line)

        # è®°å½•ä¿¡æ¯ï¼ŒæŒ‡å‡ºæ•°æ®é›†è§£æå®Œæˆ
        logger.info("Dataset parse finished.")

    def __len__(self):
        # è¿”å›ç¤ºä¾‹åˆ—è¡¨çš„é•¿åº¦
        return len(self.examples)

    def __getitem__(self, i) -> Dict[str, torch.tensor]:
        # è¿”å›æŒ‡å®šç´¢å¼•å¤„çš„ç¤ºä¾‹
        return self.examples[i]


class TextDatasetForNextSentencePrediction(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        overwrite_cache=False,
        short_seq_probability=0.1,
        nsp_probability=0.5,
    ):
        # åˆå§‹åŒ–ç¤ºä¾‹åˆ—è¡¨ä¸ºç©º
        self.examples = []

    def __len__(self):
        # è¿”å›ç¤ºä¾‹åˆ—è¡¨çš„é•¿åº¦
        return len(self.examples)

    def __getitem__(self, i):
        # è¿”å›æŒ‡å®šç´¢å¼•å¤„çš„ç¤ºä¾‹
        return self.examples[i]
```