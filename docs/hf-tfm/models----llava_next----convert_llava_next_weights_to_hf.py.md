# `.\models\llava_next\convert_llava_next_weights_to_hf.py`

```
# å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œåº“
import argparse  # è§£æå‘½ä»¤è¡Œå‚æ•°çš„åº“
import glob  # åŒ¹é…æ–‡ä»¶è·¯å¾„åçš„æ¨¡å¼æ‰©å±•åº“
import json  # å¤„ç† JSON æ ¼å¼æ•°æ®çš„åº“
from pathlib import Path  # å¤„ç†æ–‡ä»¶è·¯å¾„çš„å¯¹è±¡æ¨¡å—

import requests  # å‘é€ HTTP è¯·æ±‚çš„åº“
import torch  # PyTorch æ·±åº¦å­¦ä¹ åº“
from accelerate import init_empty_weights  # åˆå§‹åŒ–ç©ºçš„æ¨¡å‹æƒé‡çš„åŠ é€Ÿåº“å‡½æ•°
from huggingface_hub import hf_hub_download, snapshot_download  # ä»Hugging Face Hubä¸‹è½½æ¨¡å‹å’Œå¿«ç…§çš„å‡½æ•°
from PIL import Image  # Python Imaging Libraryï¼Œå¤„ç†å›¾åƒçš„åº“
from safetensors import safe_open  # å®‰å…¨åœ°æ‰“å¼€å¼ é‡æ•°æ®çš„åº“å‡½æ•°

from transformers import (  # å¯¼å…¥ Hugging Face Transformers åº“ä¸­çš„ç›¸å…³æ¨¡å—å’Œç±»
    AddedToken,
    AutoConfig,
    AutoTokenizer,
    LlavaNextConfig,
    LlavaNextForConditionalGeneration,
    LlavaNextImageProcessor,
    LlavaNextProcessor,
)

# å°†éœ€è¦ä¿®æ”¹çš„é”®å€¼æ˜ å°„å…³ç³»å®šä¹‰ä¸ºå¸¸é‡
KEYS_TO_MODIFY_MAPPING = {
    "model.vision_tower.": "",  # æ›¿æ¢æ¨¡å‹è§†è§‰å¡”ç›¸å…³çš„é”®
    "model.mm_projector": "multi_modal_projector",  # æ›¿æ¢å¤šæ¨¡æ€æŠ•å½±å™¨çš„é”®
    "model": "model.model",  # æ›¿æ¢æ¨¡å‹çš„é”®
    "vision_model.model": "vision_model",  # æ›¿æ¢è§†è§‰æ¨¡å‹çš„é”®
    "lm_head": "language_model.lm_head",  # æ›¿æ¢è¯­è¨€æ¨¡å‹å¤´éƒ¨çš„é”®
    "model.model": "language_model.model",  # æ›¿æ¢æ¨¡å‹çš„é”®
    "multi_modal_projector.0": "multi_modal_projector.linear_1",  # æ›¿æ¢å¤šæ¨¡æ€æŠ•å½±å™¨çš„ç¬¬ä¸€å±‚çº¿æ€§å±‚é”®
    "multi_modal_projector.2": "multi_modal_projector.linear_2",  # æ›¿æ¢å¤šæ¨¡æ€æŠ•å½±å™¨çš„ç¬¬äºŒå±‚çº¿æ€§å±‚é”®
    "language_model.model.image_newline": "image_newline",  # æ›¿æ¢è¯­è¨€æ¨¡å‹ä¸­çš„å›¾åƒæ¢è¡Œé”®
}


# åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸çš„å‡½æ•°
def load_original_state_dict(model_id):
    # ä»æŒ‡å®šçš„æ¨¡å‹ ID ä¸‹è½½å¹¶è§£å‹å¿«ç…§ï¼Œåªå…è®¸å®‰å…¨å¼ é‡æ–‡ä»¶æ ¼å¼
    directory_path = snapshot_download(repo_id=model_id, allow_patterns=["*.safetensors"])

    # åˆ›å»ºä¸€ä¸ªç©ºçš„åŸå§‹çŠ¶æ€å­—å…¸
    original_state_dict = {}
    # éå†æ‰€æœ‰è§£å‹åçš„æ–‡ä»¶
    for path in glob.glob(f"{directory_path}/*"):
        # å¦‚æœæ–‡ä»¶æ˜¯å®‰å…¨å¼ é‡æ–‡ä»¶
        if path.endswith(".safetensors"):
            # å®‰å…¨åœ°æ‰“å¼€æ–‡ä»¶å¹¶ä½¿ç”¨ PyTorch æ¡†æ¶è¯»å–
            with safe_open(path, framework="pt", device="cpu") as f:
                # éå†æ–‡ä»¶ä¸­çš„æ¯ä¸ªé”®å’Œå¯¹åº”çš„å¼ é‡
                for key in f.keys():
                    original_state_dict[key] = f.get_tensor(key)

    # è¿”å›å®Œæ•´çš„åŸå§‹çŠ¶æ€å­—å…¸
    return original_state_dict


# å°†çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºé€‚åˆ Hugging Face çš„æ ¼å¼çš„å‡½æ•°
def convert_state_dict_to_hf(state_dict):
    # åˆ›å»ºä¸€ä¸ªæ–°çš„çŠ¶æ€å­—å…¸
    new_state_dict = {}
    # éå†åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹
    for key, value in state_dict.items():
        # å¦‚æœé”®ä»¥ ".inv_freq" ç»“å°¾ï¼Œåˆ™è·³è¿‡
        if key.endswith(".inv_freq"):
            continue
        # éå†é¢„å®šä¹‰çš„éœ€è¦ä¿®æ”¹çš„é”®å€¼æ˜ å°„å…³ç³»
        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():
            # å¦‚æœéœ€è¦ä¿®æ”¹çš„é”®å€¼æ˜ å°„å…³ç³»å­˜åœ¨äºå½“å‰é”®ä¸­
            if key_to_modify in key:
                # æ›¿æ¢å½“å‰é”®ä¸­çš„ç›¸åº”éƒ¨åˆ†ä¸ºæ–°çš„é”®
                key = key.replace(key_to_modify, new_key)

        # å°†å½“å‰å¤„ç†åçš„é”®å€¼å¯¹åŠ å…¥æ–°çš„çŠ¶æ€å­—å…¸ï¼Œå¹¶å°†å€¼è½¬æ¢ä¸º float16 ç±»å‹
        new_state_dict[key] = value.to(torch.float16)

    # è¿”å›è½¬æ¢åçš„æ–°çŠ¶æ€å­—å…¸
    return new_state_dict


# åŠ è½½å›¾åƒçš„å‡½æ•°
def load_image():
    # å›¾åƒçš„ URL åœ°å€
    url = "https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true"
    # ä»æŒ‡å®šçš„ URL è·å–å›¾åƒæ•°æ®ï¼Œä»¥æµçš„æ–¹å¼è¯»å–
    image = Image.open(requests.get(url, stream=True).raw)
    # è¿”å›è¯»å–çš„å›¾åƒæ•°æ®
    return image
def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):
    # ä½¿ç”¨æŒ‡å®šçš„ model_id ä» HF Hub ä¸‹è½½æ¨¡å‹é…ç½®æ–‡ä»¶ config.json
    filepath = hf_hub_download(repo_id=model_id, filename="config.json", repo_type="model")
    
    # æ‰“å¼€å¹¶è¯»å– JSON æ–‡ä»¶å†…å®¹
    with open(filepath) as f:
        data = json.load(f)
        print(data)

    # æ ¹æ® model_id ä¸åŒè®¾ç½®å¯¹åº”çš„ text_model_id å’Œ image_token_index
    if model_id == "liuhaotian/llava-v1.6-mistral-7b":
        text_model_id = "mistralai/Mistral-7B-Instruct-v0.2"
        image_token_index = 32000
    elif model_id == "liuhaotian/llava-v1.6-vicuna-7b":
        text_model_id = "lmsys/vicuna-7b-v1.5"
        image_token_index = 32000
    elif model_id == "liuhaotian/llava-v1.6-vicuna-13b":
        text_model_id = "lmsys/vicuna-13b-v1.5"
        image_token_index = 32000
    elif model_id == "liuhaotian/llava-v1.6-34b":
        text_model_id = "NousResearch/Nous-Hermes-2-Yi-34B"
        image_token_index = 64000
    
    # ä»æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­è·å– vision_model_id
    vision_model_id = data["mm_vision_tower"]

    # è®¾ç½®é»˜è®¤çš„ torch æ•°æ®ç±»å‹ä¸º torch.float16
    torch.set_default_dtype(torch.float16)
    
    # ä½¿ç”¨ text_model_id åˆ›å»º AutoConfig å¯¹è±¡
    text_config = AutoConfig.from_pretrained(text_model_id)

    # æ ¹æ® model_id ç¡®å®šæ˜¯å¦ä½¿ç”¨ fast tokenizer
    use_fast = False if model_id == "liuhaotian/llava-v1.6-34b" else True
    tokenizer = AutoTokenizer.from_pretrained(text_model_id, use_fast=use_fast)
    
    # æ·»åŠ ç‰¹æ®Šçš„ "<image>" token åˆ° tokenizer
    tokenizer.add_tokens(AddedToken("<image>", special=True, normalized=False), special_tokens=True)

    if model_id == "liuhaotian/llava-v1.6-mistral-7b":
        # å¯¹äº Mistral-7B æ¨¡å‹ï¼Œæ·»åŠ  "<pad>" ä½œä¸º padding token
        tokenizer.add_special_tokens({"pad_token": "<pad>"})

    # ä½¿ç”¨ vision_model_id åˆ›å»º LlavaNextImageProcessor å¯¹è±¡
    image_processor = LlavaNextImageProcessor.from_pretrained(vision_model_id)
    
    # åˆ›å»º LlavaNextProcessor å¯¹è±¡ï¼Œä¼ å…¥ tokenizer å’Œ image_processor
    processor = LlavaNextProcessor(tokenizer=tokenizer, image_processor=image_processor)

    # æ„å»º LlavaNextConfig å¯¹è±¡ï¼ŒåŒ…æ‹¬ text_configã€image_grid_pinpoints ç­‰å‚æ•°
    config = LlavaNextConfig(
        text_config=text_config.to_dict(),
        image_grid_pinpoints=image_processor.image_grid_pinpoints,
        use_image_newline_parameter=True,
        image_token_index=image_token_index,
    )

    # åˆå§‹åŒ–ç©ºçš„æƒé‡ï¼Œå¹¶åˆ›å»º LlavaNextForConditionalGeneration æ¨¡å‹
    with init_empty_weights():
        model = LlavaNextForConditionalGeneration(config)

    # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
    state_dict = load_original_state_dict(model_id)
    state_dict = convert_state_dict_to_hf(state_dict)
    
    # åŠ è½½è½¬æ¢åçš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­
    model.load_state_dict(state_dict, assign=True)
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # è·å–æ¨¡å‹ä¸­é¢„æ‰©å±•çš„ embeddings
    pre_expansion_embeddings = model.language_model.model.embed_tokens.weight.data
    
    # è®¡ç®— embeddings çš„å‡å€¼ mu
    mu = torch.mean(pre_expansion_embeddings, dim=0).float()
    n = pre_expansion_embeddings.size()[0]
    
    # è®¡ç®— embeddings çš„åæ–¹å·®çŸ©é˜µ sigma
    sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n
    
    # åˆ›å»ºå¤šå˜é‡æ­£æ€åˆ†å¸ƒå¯¹è±¡ dist
    dist = torch.distributions.multivariate_normal.MultivariateNormal(mu, covariance_matrix=1e-5 * sigma)

    # æ·»åŠ ä¸€ä¸ª "<image>" token ä»¥è°ƒæ•´æ¨¡å‹å¤§å°
    # ä¸ºäº†æ€§èƒ½åŸå› ï¼Œå°†æ¨¡å‹çš„å¡«å……å½¢çŠ¶è®¾ä¸º 64
    pad_shape = 64
    vocab_size = config.text_config.vocab_size
    
    if model_id == "liuhaotian/llava-v1.6-34b":
        # å¯¹äºè¯¥æ¨¡å‹ï¼Œæœ‰ 3 ä¸ªé¢å¤–çš„ tokenï¼Œå³ "<|startoftext|>", "<|endoftext|>" å’Œ "<image>"
        num_tokens = vocab_size + 3
    else:
        # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œæœ‰ 2 ä¸ªé¢å¤–çš„ tokenï¼Œå³ "<image>" å’Œ "<pad>"
        num_tokens = vocab_size + 2
    # è°ƒæ•´æ¨¡å‹çš„è¯åµŒå…¥å¤§å°ï¼Œä½¿å…¶èƒ½å®¹çº³ç»™å®šçš„è¯æ±‡é‡ï¼Œå¹¶ä¸”å°†å…¶å¡«å……åˆ°æŒ‡å®šçš„å½¢çŠ¶
    model.resize_token_embeddings(num_tokens, pad_to_multiple_of=pad_shape)

    # ä½¿ç”¨åˆ†å¸ƒé‡‡æ ·å¡«å……è¯åµŒå…¥æƒé‡çš„æœªåˆå§‹åŒ–éƒ¨åˆ†
    model.language_model.model.embed_tokens.weight.data[vocab_size:] = torch.stack(
        tuple(
            (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0]))
        ),
        dim=0,
    )

    # ä½¿ç”¨åˆ†å¸ƒé‡‡æ ·å¡«å……è¯­è¨€æ¨¡å‹å¤´éƒ¨çš„æœªåˆå§‹åŒ–éƒ¨åˆ†
    model.language_model.lm_head.weight.data[vocab_size:] = torch.stack(
        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0]))),
        dim=0,
    )

    # è®¾ç½®æ¨¡å‹è®¡ç®—è®¾å¤‡ä¸º CUDA ç¬¬äºŒå—GPU
    device = "cuda:2"
    model.to(device)

    # å‡†å¤‡è¾“å…¥æ•°æ®
    image = load_image()
    if model_id == "liuhaotian/llava-v1.6-mistral-7b":
        # æ ¹æ®æ¨¡å‹IDé€‰æ‹©ç›¸åº”çš„æç¤ºæ–‡æœ¬
        prompt = "[INST] <image>\nWhat is shown in this image? [/INST]"
    elif model_id in ["liuhaotian/llava-v1.6-vicuna-7b", "liuhaotian/llava-v1.6-vicuna-13b"]:
        prompt = "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\nWhat is shown in this image? ASSISTANT:"
    elif model_id == "liuhaotian/llava-v1.6-34b":
        prompt = "<|im_start|>system\nAnswer the questions.<|im_end|><|im_start|>user\n<image>\nWhat is shown in this image?<|im_end|><|im_start|>assistant\n"
    # ä½¿ç”¨å¤„ç†å™¨å¯¹å›¾åƒå’Œæç¤ºæ–‡æœ¬è¿›è¡Œå¤„ç†ï¼Œè¿”å›PyTorchå¼ é‡æ ¼å¼çš„è¾“å…¥æ•°æ®
    inputs = processor(images=image, text=prompt, return_tensors="pt")

    # éªŒè¯è¾“å…¥æ•°æ®
    # ä¸‹è½½å¹¶åŠ è½½åŸå§‹åƒç´ å€¼æ•°æ®æ–‡ä»¶
    filepath = hf_hub_download(repo_id="nielsr/test-image", filename="llava_1_6_pixel_values.pt", repo_type="dataset")
    original_pixel_values = torch.load(filepath, map_location="cpu")
    # æ–­è¨€åŸå§‹åƒç´ å€¼ä¸è¾“å…¥æ•°æ®ä¸­çš„åƒç´ å€¼ç›¸è¿‘ï¼ˆä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°æ¯”è¾ƒï¼‰
    assert torch.allclose(original_pixel_values, inputs.pixel_values.half())

    if model_id == "liuhaotian/llava-v1.6-mistral-7b":
        # ä¸‹è½½å¹¶åŠ è½½åŸå§‹è¾“å…¥IDæ•°æ®æ–‡ä»¶
        filepath = hf_hub_download(repo_id="nielsr/test-image", filename="llava_1_6_input_ids.pt", repo_type="dataset")
        original_input_ids = torch.load(filepath, map_location="cpu")
        # å°†åŸå§‹è¾“å…¥ä¸­çš„ç‰¹æ®Šæ ‡è®° -200 æ›¿æ¢ä¸ºå›¾åƒæ ‡è®°ç´¢å¼•
        original_input_ids[original_input_ids == -200] = image_token_index
        # è§£ç å¹¶æ‰“å°å¤„ç†åçš„è¾“å…¥IDæ•°æ®ï¼ˆæ’é™¤ç‰¹æ®Šæ ‡è®° -200ï¼‰
        print(tokenizer.decode([id for id in original_input_ids.tolist()[0] if id != -200]))
        # æ–­è¨€å¤„ç†åçš„è¾“å…¥IDä¸æ¨¡å‹è¾“å…¥IDç›¸åŒ
        assert original_input_ids[0].tolist() == inputs.input_ids[0].tolist()

    elif model_id == "liuhaotian/llava-v1.6-34b":
        # ä¸‹è½½å¹¶åŠ è½½ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬çš„åŸå§‹è¾“å…¥IDæ•°æ®æ–‡ä»¶
        filepath = hf_hub_download(
            repo_id="nielsr/test-image", filename="llava_1_6_34b_input_ids.pt", repo_type="dataset"
        )
        original_input_ids = torch.load(filepath, map_location="cpu")
        # å°†åŸå§‹è¾“å…¥ä¸­çš„ç‰¹æ®Šæ ‡è®° -200 æ›¿æ¢ä¸ºå›¾åƒæ ‡è®°ç´¢å¼•
        original_input_ids[original_input_ids == -200] = image_token_index
        # æ–­è¨€å¤„ç†åçš„è¾“å…¥IDä¸æ¨¡å‹è¾“å…¥IDç›¸åŒ
        assert original_input_ids[0].tolist() == inputs.input_ids[0].tolist()

    # æ–­è¨€å›¾åƒå°ºå¯¸ä¸è¾“å…¥æ•°æ®ä¸­çš„å›¾åƒå°ºå¯¸ç›¸åŒ
    image_sizes = torch.tensor([[899, 1024]])
    assert image_sizes[0].tolist() == inputs.image_sizes[0].tolist()

    # æ‰§è¡Œå•æ¬¡å‰å‘ä¼ æ’­éªŒè¯
    print("Single forward pass")
    # è¿›å…¥æ¨æ–­æ¨¡å¼ï¼Œæ­¤æ¨¡å¼ä¸‹ä¸ä¼šè¿›è¡Œæ¢¯åº¦è®¡ç®—
    with torch.inference_mode():
        # å°†è¾“å…¥æ•°æ®ç§»åˆ°æŒ‡å®šè®¾å¤‡ä¸Š
        inputs = inputs.to(device)
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œè·å–è¾“å‡ºç»“æœ
        outputs = model(**inputs)
        # æ‰“å°è¾“å‡º logits çš„å½¢çŠ¶
        print("Shape of logits:", outputs.logits.shape)
        # æ‰“å° logits çš„å‰å‡ ä¸ªå€¼
        print("First values of logits:", outputs.logits[0, :3, :3])

        # æ ¹æ®ä¸åŒçš„æ¨¡å‹ ID è®¾ç½®é¢„æœŸçš„è¾“å‡ºåˆ‡ç‰‡
        if model_id == "liuhaotian/llava-v1.6-mistral-7b":
            expected_slice = torch.tensor(
                [[-4.8555, -4.6992, -0.1996], [-10.5703, -10.7344, -2.7246], [-7.0391, -7.3672, -0.2634]],
                dtype=torch.float32,
                device=device,
            )
        elif model_id == "liuhaotian/llava-v1.6-vicuna-7b":
            expected_slice = torch.tensor(
                [[1.4883, 0.9976, -0.6992], [-9.7031, -5.7031, -1.5557], [-5.1328, -5.5586, 8.8281]],
                dtype=torch.float32,
                device=device,
            )
        elif model_id == "liuhaotian/llava-v1.6-vicuna-13b":
            expected_slice = torch.tensor(
                [[-0.9614, 7.3125, 0.2106], [-7.2695, -8.5469, 3.6211], [-6.3750, -8.1875, 5.4688]],
                dtype=torch.float32,
                device=device,
            )
        elif model_id == "liuhaotian/llava-v1.6-34b":
            expected_slice = torch.tensor(
                [[-9.0859, -9.1406, 5.9453], [-5.9570, -5.9766, 2.2754], [-5.7305, -5.7539, 4.0000]],
                dtype=torch.float32,
                device=device,
            )
        else:
            # å¦‚æœæ¨¡å‹ ID ä¸åœ¨é¢„æœŸèŒƒå›´å†…ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"Model {model_id} not supported")

        # æ–­è¨€å®é™…è¾“å‡ºçš„ logits åˆ‡ç‰‡ä¸é¢„æœŸçš„éå¸¸æ¥è¿‘
        assert torch.allclose(outputs.logits[0, :3, :3], expected_slice, atol=1e-4)
        # æ‰“å°ç¡®è®¤ logits æ­£ç¡®
        print("Logits are ok!")

    # éªŒè¯ç”Ÿæˆè¿‡ç¨‹
    output_ids = model.generate(
        **inputs,
        max_new_tokens=100,
        use_cache=True,
    )

    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬å¹¶å»é™¤ç‰¹æ®Šæ ‡è®°
    generated_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()

    # æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
    print("Generated text:", repr(generated_text))

    # æ ¹æ®æ¨¡å‹ ID éªŒè¯ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ç¬¦åˆé¢„æœŸ
    if model_id == "liuhaotian/llava-v1.6-mistral-7b":
        expected_text = '[INST]  \nWhat is shown in this image? [/INST] The image appears to be a radar chart, which is a type of multi-dimensional plot that displays data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point.\n\nIn this particular radar chart, there are several axes labeled with different metrics or benchmarks, such as "MMM-Vet," "MMM-Bench," "LLaVA-Bench," "SLED-Bench," "'
    elif model_id == "liuhaotian/llava-v1.6-vicuna-7b":
        expected_text = """A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human\'s questions. USER:  \nWhat is shown in this image? ASSISTANT: The image appears to be a graphical representation of a benchmarking study comparing the performance of various models or systems. It\'s a scatter plot with a circular layout, where each point represents a different model or system, and the axes represent different metrics or dimensions of comparison.\n\nThe metrics are likely related to machine learning or artificial intelligence performance, as indicated by the terms like "BLIP-2," "Instruct BLIP," "POE," "QWA," "V"""
    elif model_id == "liuhaotian/llava-v1.6-vicuna-13b":
        expected_text = "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER:  \nWhat is shown in this image? ASSISTANT: The image appears to be a radar chart, also known as a spider chart or star chart, which is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point.\n\nIn this particular radar chart, there are several variables represented:\n\n- MM-Vet\n- LLa-Va-Bench\n- SEED-Bench\n- MM"
    elif model_id == "liuhaotian/llava-v1.6-34b":
        expected_text = "<|im_start|> system\nAnswer the questions. <|im_start|> user\n\nWhat is shown in this image? <|im_start|> assistant\nThe image appears to be a radar chart, also known as a spider chart, which is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point.\n\nIn this particular chart, there are several datasets represented by different colors and labeled with various acronyms such as MM-Vet, LLaVA-Bench, SEED-Bench, MM-Bench-CN, MM-"
    else:
        raise ValueError(f"Model {model_id} not supported")

    # ç¡®ä¿ç”Ÿæˆçš„æ–‡æœ¬ä¸é¢„æœŸæ–‡æœ¬ä¸€è‡´
    assert generated_text == expected_text
    # æ‰“å°ç¡®è®¤ä¿¡æ¯
    print("Generated text is ok!")

    # éªŒè¯æ‰¹é‡ç”Ÿæˆ
    print("Batched generation...")
    # æŒ‡å®šå›¾åƒ URL
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # ä¸‹è½½å¹¶æ‰“å¼€å›¾åƒ
    cats_image = Image.open(requests.get(url, stream=True).raw)

    # å¤„ç†å™¨æ¥æ”¶å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶è¿›è¡Œå¡«å……å’Œå¼ é‡åŒ–å¤„ç†
    inputs = processor(
        images=[image, cats_image],  # å›¾åƒåˆ—è¡¨
        text=[prompt, "[INST] <image>\nHow many cats are there? [/INST]"],  # æ–‡æœ¬åˆ—è¡¨
        padding=True,  # æ˜¯å¦å¡«å……
        return_tensors="pt",  # è¿”å› PyTorch å¼ é‡
    ).to(device)

    # æ‰“å°æ¯ä¸ªè¾“å…¥é¡¹çš„å½¢çŠ¶
    for k, v in inputs.items():
        print(k, v.shape)

    # æ‰“å°å›¾åƒå°ºå¯¸ä¿¡æ¯
    print("Image sizes:", inputs.image_sizes)

    # ç¡®ä¿å›¾åƒå°ºå¯¸ç›¸åŒï¼Œä»¥ç¡®ä¿æ‰¹é‡ç”Ÿæˆæ­£å¸¸å·¥ä½œ
    inputs.image_sizes[1] = inputs.image_sizes[0]

    # å†æ¬¡ç¡®è®¤æ‰¹é‡ç”Ÿæˆæ­£åœ¨è¿›è¡Œ
    print("Batched generation...")
    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼Œæ¥æ”¶è¾“å…¥å‚æ•°ï¼Œå¹¶æŒ‡å®šæœ€å¤§æ–°å¢æ ‡è®°æ•°ä¸º20ï¼Œå¯ç”¨ç¼“å­˜
    output_ids = model.generate(
        **inputs,
        max_new_tokens=20,
        use_cache=True,
    )

    # ä½¿ç”¨åˆ†è¯å™¨æ‰¹é‡è§£ç ç”Ÿæˆçš„è¾“å‡ºæ ‡è¯†ç¬¦åºåˆ—ï¼Œè·³è¿‡ç‰¹æ®Šæ ‡è®°å¹¶è¿”å›æ–‡æœ¬è¾“å‡ºåˆ—è¡¨
    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    # æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬è¾“å‡ºåˆ—è¡¨
    print(outputs)

    # å¦‚æœæŒ‡å®šäº† PyTorch æ¨¡å‹å¯¼å‡ºè·¯å¾„
    if pytorch_dump_folder_path is not None:
        # æ‰“å°ä¿å­˜æ¨¡å‹å’Œå¤„ç†å™¨çš„æ¶ˆæ¯ï¼Œå¹¶åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        print(f"Saving model and processor for {model_id} to {pytorch_dump_folder_path}")
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # å°†å¤„ç†å™¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        # ä»æ¨¡å‹ ID ä¸­æå–ä»“åº“ ID
        repo_id = model_id.split("/")[-1]
        # æ¨é€æ¨¡å‹åˆ° Hubï¼Œå‘½åè§„åˆ™ä¸º llava-hf/{repo_id}-hf
        model.push_to_hub(f"llava-hf/{repo_id}-hf")
        # æ¨é€å¤„ç†å™¨åˆ° Hubï¼Œå‘½åè§„åˆ™ä¸º llava-hf/{repo_id}-hf
        processor.push_to_hub(f"llava-hf/{repo_id}-hf")
# å¦‚æœè¿™ä¸ªè„šæœ¬è¢«ç›´æ¥è¿è¡Œï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šæ¨¡å‹çš„Hubä½ç½®ä»¥è¿›è¡Œè½¬æ¢
    parser.add_argument(
        "--model_id",
        help="Hub location of the model to convert",
        default="liuhaotian/llava-v1.6-mistral-7b",
        choices=[
            "liuhaotian/llava-v1.6-mistral-7b",
            "liuhaotian/llava-v1.6-vicuna-7b",
            "liuhaotian/llava-v1.6-vicuna-13b",
            "liuhaotian/llava-v1.6-34b",
        ],
        required=False,
    )
    
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šè¾“å‡ºçš„PyTorchæ¨¡å‹ç›®å½•çš„è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®ä¸ºTrueè¡¨ç¤ºæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ°ğŸ¤— hub
    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•° convert_llava_to_hfï¼Œä¼ é€’è§£æåçš„å‘½ä»¤è¡Œå‚æ•°ä½œä¸ºå‚æ•°
    convert_llava_to_hf(args.model_id, args.pytorch_dump_folder_path, args.push_to_hub)
```