# `.\models\sam\modeling_tf_sam.py`

```py
# coding=utf-8
# 指定文件编码为 UTF-8

# Copyright 2023 The Meta AI Authors and The HuggingFace Team. All rights reserved.
# 版权声明，声明文件版权归 Meta AI 作者和 HuggingFace 团队所有

# Licensed under the Apache License, Version 2.0 (the "License");
# 依据 Apache 许可证 2.0 版本授权

# you may not use this file except in compliance with the License.
# 除非符合许可证规定，否则不得使用本文件

# You may obtain a copy of the License at
# 可以从以下网址获取许可证的副本
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# 除非适用法律要求或书面同意，否则本软件按“原样”分发，不附任何明示或暗示的担保或条件

# See the License for the specific language governing permissions and
# limitations under the License.
# 请参阅许可证，了解权限和限制的具体条款

"""
TensorFlow SAM model. This file was mostly generated by auto-translation from the PyTorch original. In the event of a
discrepancy, the original file should be regarded as the 'reference' version.
"""
# TensorFlow SAM 模型文件，大部分由 PyTorch 原始文件自动翻译生成，如有不一致，请以原始文件为参考版本

# 导入必要的库和模块
from __future__ import annotations

import collections  # 导入 collections 库
from dataclasses import dataclass  # 从 dataclasses 模块导入 dataclass 装饰器
from typing import Optional, Tuple, Union  # 导入类型提示所需的类和联合类型

import numpy as np  # 导入 NumPy 库并简写为 np
import tensorflow as tf  # 导入 TensorFlow 库并简写为 tf

# 导入相对路径的模块
from ...activations_tf import ACT2FN  # 从活化函数模块导入 ACT2FN
from ...modeling_tf_outputs import TFBaseModelOutput  # 从 TensorFlow 输出模块导入 TFBaseModelOutput
from ...modeling_tf_utils import (  # 从 TensorFlow 实用工具模块导入以下功能：
    TFModelInputType, TFPreTrainedModel,  # TFModelInputType、TFPreTrainedModel 类
    keras, shape_list, unpack_inputs  # keras、shape_list、unpack_inputs 函数
)
from ...tf_utils import flatten, functional_layernorm  # 从 TensorFlow 实用工具模块导入 flatten、functional_layernorm 函数
from ...utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward, logging  # 从工具模块导入 ModelOutput 类、若干函数、logging 模块
from .configuration_sam import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig  # 从 sam 配置模块导入若干配置类

logger = logging.get_logger(__name__)  # 获取当前模块的 logger 对象

_CONFIG_FOR_DOC = "SamConfig"  # 用于文档的配置信息
_CHECKPOINT_FOR_DOC = "facebook/sam-vit-huge"  # 用于文档的检查点信息

TF_SAM_PRETRAINED_MODEL_ARCHIVE_LIST = [  # TensorFlow SAM 预训练模型存档列表
    "facebook/sam-vit-huge",  # Facebook SAM-ViT Huge 模型
    "facebook/sam-vit-large",  # Facebook SAM-ViT Large 模型
    "facebook/sam-vit-base",  # Facebook SAM-ViT Base 模型
    # 查看所有 SAM 模型，请访问 https://huggingface.co/models?filter=sam
]

@dataclass
class TFSamVisionEncoderOutput(ModelOutput):
    """
    Base class for sam vision model's outputs that also contains image embeddings obtained by applying the projection
    layer to the pooler_output.
    """
    # TFSamVisionEncoderOutput 类，用作 SAM 视觉模型输出的基类，还包含通过将投影层应用于 pooler_output 获得的图像嵌入
    """
    Args:
        image_embeds (`tf.Tensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):
            The image embeddings obtained by applying the projection layer to the pooler_output.
        
        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        
        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for
            the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        
        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
    """

    # 可选参数：图像嵌入，形状为 `(batch_size, output_dim)`，当模型以 `with_projection=True` 初始化时返回
    image_embeds: tf.Tensor | None = None

    # 必需参数：最后一个隐藏层的隐藏状态，形状为 `(batch_size, sequence_length, hidden_size)`
    last_hidden_state: tf.Tensor = None

    # 可选参数：元组，包含隐藏状态的序列，形状为 `(batch_size, sequence_length, hidden_size)`
    # 当 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回
    # 包括模型每一层的输出以及可选的初始嵌入输出
    hidden_states: Tuple[tf.Tensor, ...] | None = None

    # 可选参数：元组，包含注意力权重的序列，形状为 `(batch_size, num_heads, sequence_length, sequence_length)`
    # 当 `output_attentions=True` 或 `config.output_attentions=True` 时返回
    # 用于计算自注意力头中加权平均值的注意力权重
    attentions: Tuple[tf.Tensor, ...] | None = None
# 定义一个数据类，表示Segment-Anything模型的输出结果，继承自ModelOutput类
@dataclass
class TFSamImageSegmentationOutput(ModelOutput):
    """
    Base class for Segment-Anything model's output

    Args:
        iou_scores (`tf.Tensor` of shape `(batch_size, num_masks)`):
            The iou scores of the predicted masks.
            预测掩膜的IoU分数。

        pred_masks (`tf.Tensor` of shape `(batch_size, num_masks, height, width)`):
            The predicted low resolutions masks. Needs to be post-processed by the processor.
            预测的低分辨率掩膜，需要由处理器进行后处理。

        vision_hidden_states  (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `tf.Tensor` (one for the output of the embeddings, if the model has an embedding layer, + one for
            the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the vision model at the output of each layer plus the optional initial embedding outputs.
            视觉模型在每一层输出的隐藏状态，以及可选的初始嵌入输出。

        vision_attentions  (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
            注意力权重，在注意力softmax后计算的，用于计算自注意力头中的加权平均值。

        mask_decoder_attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
            注意力权重，在注意力softmax后计算的，用于计算自注意力头中的加权平均值。
    """

    # IoU分数的张量，形状为(batch_size, num_masks)
    iou_scores: tf.Tensor = None

    # 预测掩膜的张量，形状为(batch_size, num_masks, height, width)
    pred_masks: tf.Tensor = None

    # 视觉隐藏状态的元组，每个元素是一个形状为(batch_size, sequence_length, hidden_size)的张量
    vision_hidden_states: Tuple[tf.Tensor, ...] | None = None

    # 视觉注意力的元组，每个元素是一个形状为(batch_size, num_heads, sequence_length, sequence_length)的张量
    vision_attentions: Tuple[tf.Tensor, ...] | None = None

    # 掩膜解码器注意力的元组，每个元素是一个形状为(batch_size, num_heads, sequence_length, sequence_length)的张量
    mask_decoder_attentions: Tuple[tf.Tensor, ...] | None = None


class TFSamPatchEmbeddings(keras.layers.Layer):
    """
    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial
    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a
    Transformer.
    """
    # 初始化方法，用于初始化类实例
    def __init__(self, config, **kwargs):
        # 调用父类的初始化方法
        super().__init__(**kwargs)
        # 从配置中获取图像大小和补丁大小
        image_size, patch_size = config.image_size, config.patch_size
        # 从配置中获取通道数和隐藏层大小
        num_channels, hidden_size = config.num_channels, config.hidden_size
        # 如果图像大小和补丁大小不是可迭代对象，则转换为元组
        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)
        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)
        # 计算图像中的补丁数量
        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])
        # 将计算得到的各种参数保存在类实例中
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_channels = num_channels
        self.num_patches = num_patches

        # 创建投影层，使用 Conv2D 卷积层
        self.projection = keras.layers.Conv2D(
            hidden_size, kernel_size=patch_size, strides=patch_size, name="projection"
        )

    # 调用方法，用于执行前向传播
    def call(self, pixel_values):
        # 获取输入张量的形状信息
        batch_size, num_channels, height, width = shape_list(pixel_values)
        # 如果输入张量的通道数与配置中的不匹配，则引发值错误
        if num_channels != self.num_channels:
            raise ValueError(
                "Make sure that the channel dimension of the pixel values match with the one set in the configuration."
            )
        # 如果输入图像的高度或宽度与配置中的不匹配，则引发值错误
        if height != self.image_size[0] or width != self.image_size[1]:
            raise ValueError(
                f"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]})."
            )
        # 对输入张量进行转置，然后通过投影层进行嵌入处理
        embeddings = self.projection(tf.transpose(pixel_values, perm=[0, 2, 3, 1]))
        # 返回嵌入结果
        return embeddings

    # 构建方法，用于构建模型
    def build(self, input_shape=None):
        # 如果模型已经构建，则直接返回
        if self.built:
            return
        # 标记模型为已构建
        self.built = True
        # 如果投影层已经存在，则使用 TensorFlow 的 name_scope 来构建投影层
        if getattr(self, "projection", None) is not None:
            with tf.name_scope(self.projection.name):
                self.projection.build([None, None, None, self.num_channels])
class TFSamMLPBlock(keras.layers.Layer):
    # 初始化方法，用于创建 TFSamMLPBlock 实例
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        # 创建第一个全连接层，设置输出维度为 config.mlp_dim，命名为 "lin1"
        self.lin1 = keras.layers.Dense(config.mlp_dim, name="lin1")
        # 创建第二个全连接层，设置输出维度为 config.hidden_size，命名为 "lin2"
        self.lin2 = keras.layers.Dense(config.hidden_size, name="lin2")
        # 获取激活函数，根据配置从全局变量 ACT2FN 中选择对应的函数
        self.act = ACT2FN[config.hidden_act]
        # 保存配置信息到实例变量中
        self.config = config

    # 前向传播方法，用于定义层的计算逻辑
    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:
        # 使用第一个全连接层进行计算
        hidden_states = self.lin1(hidden_states)
        # 应用激活函数
        hidden_states = self.act(hidden_states)
        # 使用第二个全连接层进行计算
        hidden_states = self.lin2(hidden_states)
        # 返回计算结果
        return hidden_states

    # 构建方法，用于构建层的权重
    def build(self, input_shape=None):
        if self.built:
            return
        self.built = True
        # 如果 lin1 层存在，则为其构建权重
        if getattr(self, "lin1", None) is not None:
            with tf.name_scope(self.lin1.name):
                self.lin1.build([None, None, self.config.hidden_size])
        # 如果 lin2 层存在，则为其构建权重
        if getattr(self, "lin2", None) is not None:
            with tf.name_scope(self.lin2.name):
                self.lin2.build([None, None, self.config.mlp_dim])


class TFSamLayerNorm(keras.layers.Layer):
    # LayerNorm 层支持 channels_last 或 channels_first 两种数据格式
    # 默认使用 channels_last
    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last", **kwargs):
        super().__init__(**kwargs)
        self.eps = eps
        self.data_format = data_format
        self.normalized_shape = normalized_shape
        # 如果数据格式不在支持的列表中，则抛出异常
        if self.data_format not in ["channels_last", "channels_first"]:
            raise NotImplementedError(f"Unsupported data format: {self.data_format}")

    # 构建方法，用于构建层的权重
    def build(self, input_shape):
        # 添加权重：标准化尺寸对应的权重，初始化为全1；偏置初始化为全0
        self.weight = self.add_weight(shape=self.normalized_shape, initializer="ones", name="weight")
        self.bias = self.add_weight(shape=self.normalized_shape, initializer="zeros", name="bias")
        super().build(input_shape)

    # 前向传播方法，用于定义层的计算逻辑
    def call(self, x: tf.Tensor) -> tf.Tensor:
        # 根据数据格式选择不同的 LayerNorm 函数进行计算
        if self.data_format == "channels_last":
            x = functional_layernorm(x, weight=self.weight, bias=self.bias, epsilon=self.eps, axis=-1)
        elif self.data_format == "channels_first":
            x = functional_layernorm(x, weight=self.weight, bias=self.bias, epsilon=self.eps, axis=1)
        return x


class TFSamAttention(keras.layers.Layer):
    """
    SAM's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and
    values.
    """
    # 初始化方法，接受配置和可选的下采样率作为参数
    def __init__(self, config, downsample_rate=None, **kwargs):
        # 调用父类初始化方法
        super().__init__(**kwargs)
        # 设置隐藏层大小
        self.hidden_size = config.hidden_size

        # 如果未提供下采样率，则使用配置中的注意力下采样率
        downsample_rate = config.attention_downsample_rate if downsample_rate is None else downsample_rate

        # 计算内部维度，即隐藏层大小除以下采样率
        self.internal_dim = config.hidden_size // downsample_rate
        # 设置注意力头的数量
        self.num_attention_heads = config.num_attention_heads
        # 检查内部维度是否可以被注意力头数量整除
        if self.internal_dim % config.num_attention_heads != 0:
            raise ValueError("num_attention_heads must divide hidden_size.")

        # 初始化查询投影层
        self.q_proj = keras.layers.Dense(self.internal_dim, name="q_proj")
        # 初始化键投影层
        self.k_proj = keras.layers.Dense(self.internal_dim, name="k_proj")
        # 初始化值投影层
        self.v_proj = keras.layers.Dense(self.internal_dim, name="v_proj")
        # 初始化输出投影层
        self.out_proj = keras.layers.Dense(self.hidden_size, name="out_proj")

    # 将隐藏状态张量分离为多个注意力头
    def _separate_heads(self, hidden_states: tf.Tensor, num_attention_heads: int) -> tf.Tensor:
        batch, point_batch_size, n_tokens, channel = shape_list(hidden_states)
        # 计算每个注意力头的通道数
        c_per_head = channel // num_attention_heads
        # 重塑张量形状以分离头
        hidden_states = tf.reshape(
            hidden_states, (batch * point_batch_size, n_tokens, num_attention_heads, c_per_head)
        )
        return tf.transpose(hidden_states, perm=[0, 2, 1, 3])

    # 将分离的注意力头重新组合为隐藏状态张量
    def _recombine_heads(self, hidden_states: tf.Tensor, point_batch_size: int) -> tf.Tensor:
        batch, n_heads, n_tokens, c_per_head = shape_list(hidden_states)
        # 调换张量的维度顺序
        hidden_states = tf.transpose(hidden_states, perm=[0, 2, 1, 3])
        # 重塑张量形状以重新组合头
        return tf.reshape(
            hidden_states,
            (batch // tf.reduce_max([1, point_batch_size]), point_batch_size, n_tokens, n_heads * c_per_head),
        )

    # 模型的调用方法，接受查询、键和值张量，并返回输出张量
    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor) -> tf.Tensor:
        # 对输入进行投影
        query = self.q_proj(query)
        key = self.k_proj(key)
        value = self.v_proj(value)

        # 获取点批处理大小
        point_batch_size = shape_list(query)[1]
        
        # 将投影后的张量分离为多个注意力头
        query = self._separate_heads(query, self.num_attention_heads)
        key = self._separate_heads(key, self.num_attention_heads)
        value = self._separate_heads(value, self.num_attention_heads)

        # 计算自注意力
        _, _, _, c_per_head = shape_list(query)
        attn = tf.matmul(
            query, tf.transpose(key, perm=[0, 1, 3, 2])
        )  # batch_size * point_batch_size  x N_heads x N_tokens x N_tokens
        # 缩放注意力权重
        attn = attn / tf.math.sqrt(float(c_per_head))
        # 应用 softmax 函数获得归一化的注意力权重
        attn = tf.nn.softmax(attn, axis=-1)

        # 计算输出张量
        out = tf.matmul(attn, value)
        # 将重新组合后的注意力头合并
        out = self._recombine_heads(out, point_batch_size)
        # 对输出应用输出投影
        out = self.out_proj(out)

        return out
    # 构建函数，用于构建模型的输入形状
    def build(self, input_shape=None):
        # 如果已经构建过，直接返回，避免重复构建
        if self.built:
            return
        # 设置标记为已构建
        self.built = True
        
        # 如果存在查询投影层，执行以下操作
        if getattr(self, "q_proj", None) is not None:
            # 在 TensorFlow 中创建名称作用域，命名为 self.q_proj.name
            with tf.name_scope(self.q_proj.name):
                # 使用 [None, None, self.hidden_size] 的形状构建查询投影层
                self.q_proj.build([None, None, self.hidden_size])
        
        # 如果存在键投影层，执行以下操作
        if getattr(self, "k_proj", None) is not None:
            # 在 TensorFlow 中创建名称作用域，命名为 self.k_proj.name
            with tf.name_scope(self.k_proj.name):
                # 使用 [None, None, self.hidden_size] 的形状构建键投影层
                self.k_proj.build([None, None, self.hidden_size])
        
        # 如果存在值投影层，执行以下操作
        if getattr(self, "v_proj", None) is not None:
            # 在 TensorFlow 中创建名称作用域，命名为 self.v_proj.name
            with tf.name_scope(self.v_proj.name):
                # 使用 [None, None, self.hidden_size] 的形状构建值投影层
                self.v_proj.build([None, None, self.hidden_size])
        
        # 如果存在输出投影层，执行以下操作
        if getattr(self, "out_proj", None) is not None:
            # 在 TensorFlow 中创建名称作用域，命名为 self.out_proj.name
            with tf.name_scope(self.out_proj.name):
                # 使用 [None, None, self.internal_dim] 的形状构建输出投影层
                self.out_proj.build([None, None, self.internal_dim])
# 定义了一个基于 Transformer 的自定义层，用于处理两种不同数据类型之间的注意力机制和多层感知机操作

class TFSamTwoWayAttentionBlock(keras.layers.Layer):
    def __init__(self, config, attention_downsample_rate: int = 2, skip_first_layer_pe: bool = False, **kwargs):
        """
        初始化函数，配置了四个层次的 Transformer 模块：
            (1) 自注意力层，处理稀疏输入
            (2) 从稀疏输入到密集输入的交叉注意力层
            (3) 在稀疏输入上的多层感知机块
            (4) 从密集输入到稀疏输入的交叉注意力层

        Arguments:
            config (`SamMaskDecoderConfig`):
                用于实例化该块的配置文件
            attention_downsample_rate (*optionalk*, int, defaults to 2):
                用于减少注意力内部维度的下采样比率
            skip_first_layer_pe (*optional*, bool, defaults to `False`):
                是否跳过在第一层添加 query_point_embedding 的步骤
        """
        super().__init__(**kwargs)

        # 从配置中获取隐藏大小和层归一化的 epsilon 值
        self.hidden_size = config.hidden_size
        self.layer_norm_eps = config.layer_norm_eps

        # 定义自注意力层和归一化层
        self.self_attn = TFSamAttention(config, downsample_rate=1, name="self_attn")
        self.layer_norm1 = keras.layers.LayerNormalization(epsilon=self.layer_norm_eps, name="layer_norm1")

        # 定义从标记到图像的交叉注意力层和归一化层
        self.cross_attn_token_to_image = TFSamAttention(
            config, downsample_rate=attention_downsample_rate, name="cross_attn_token_to_image"
        )
        self.layer_norm2 = keras.layers.LayerNormalization(epsilon=self.layer_norm_eps, name="layer_norm2")

        # 定义多层感知机块和归一化层
        self.mlp = TFSamMLPBlock(config, name="mlp")
        self.layer_norm3 = keras.layers.LayerNormalization(epsilon=self.layer_norm_eps, name="layer_norm3")

        # 定义从图像到标记的交叉注意力层的归一化层
        self.layer_norm4 = keras.layers.LayerNormalization(epsilon=self.layer_norm_eps, name="layer_norm4")
        self.cross_attn_image_to_token = TFSamAttention(
            config, downsample_rate=attention_downsample_rate, name="cross_attn_image_to_token"
        )

        # 是否跳过第一层添加 query_point_embedding
        self.skip_first_layer_pe = skip_first_layer_pe

    def call(
        self,
        queries: tf.Tensor,
        keys: tf.Tensor,
        query_point_embedding: tf.Tensor,
        key_point_embedding: tf.Tensor,
        output_attentions: bool = False,
    ):
        # Self attention block
        # 如果设置了跳过第一层的位置编码，则使用自注意力机制
        if self.skip_first_layer_pe:
            queries = self.self_attn(query=queries, key=queries, value=queries)
        else:
            # 否则，将位置编码加到查询中，然后进行自注意力计算
            query = queries + query_point_embedding
            attn_out = self.self_attn(query=query, key=query, value=queries)
            queries = queries + attn_out
        queries = self.layer_norm1(queries)

        # Cross attention block, tokens attending to image embedding
        # 将位置编码加到查询中，图像的键和位置编码相加后进行交叉注意力计算
        query = queries + query_point_embedding
        key = keys + key_point_embedding
        attn_out = self.cross_attn_token_to_image(query=query, key=key, value=keys)
        queries = queries + attn_out
        queries = self.layer_norm2(queries)

        # MLP block
        # 使用多层感知机（MLP）处理查询
        mlp_out = self.mlp(queries)
        queries = queries + mlp_out
        queries = self.layer_norm3(queries)

        # Cross attention block, image embedding attending to tokens
        # 将位置编码加到查询中，图像的查询和位置编码相加后进行交叉注意力计算
        query = queries + query_point_embedding
        key = keys + key_point_embedding
        attn_out = self.cross_attn_image_to_token(query=key, key=query, value=queries)
        keys = keys + attn_out
        keys = self.layer_norm4(keys)

        # 构建输出元组
        outputs = (queries, keys)

        # 如果需要输出注意力权重，则添加到输出元组中
        if output_attentions:
            outputs = outputs + (attn_out,)
        else:
            outputs = outputs + (None,)

        return outputs

    def build(self, input_shape=None):
        # 如果模型已经构建，则直接返回
        if self.built:
            return
        self.built = True
        
        # 构建自注意力层（如果存在）
        if getattr(self, "self_attn", None) is not None:
            with tf.name_scope(self.self_attn.name):
                self.self_attn.build(None)
        
        # 构建层归一化1（如果存在）
        if getattr(self, "layer_norm1", None) is not None:
            with tf.name_scope(self.layer_norm1.name):
                self.layer_norm1.build([None, None, None, self.hidden_size])
        
        # 构建图像到标记的交叉注意力层（如果存在）
        if getattr(self, "cross_attn_token_to_image", None) is not None:
            with tf.name_scope(self.cross_attn_token_to_image.name):
                self.cross_attn_token_to_image.build(None)
        
        # 构建层归一化2（如果存在）
        if getattr(self, "layer_norm2", None) is not None:
            with tf.name_scope(self.layer_norm2.name):
                self.layer_norm2.build([None, None, None, self.hidden_size])
        
        # 构建多层感知机（MLP）层（如果存在）
        if getattr(self, "mlp", None) is not None:
            with tf.name_scope(self.mlp.name):
                self.mlp.build(None)
        
        # 构建层归一化3（如果存在）
        if getattr(self, "layer_norm3", None) is not None:
            with tf.name_scope(self.layer_norm3.name):
                self.layer_norm3.build([None, None, None, self.hidden_size])
        
        # 构建图像到标记的交叉注意力层（如果存在）
        if getattr(self, "cross_attn_image_to_token", None) is not None:
            with tf.name_scope(self.cross_attn_image_to_token.name):
                self.cross_attn_image_to_token.build(None)
# 定义一个名为 TFSamTwoWayTransformer 的自定义层，继承自 keras.layers.Layer
class TFSamTwoWayTransformer(keras.layers.Layer):
    # 初始化函数，接受一个 config 对象和其他关键字参数
    def __init__(self, config: SamMaskDecoderConfig, **kwargs):
        # 调用父类的初始化函数
        super().__init__(**kwargs)
        # 将传入的 config 对象保存为属性
        self.config = config

        # 从 config 中获取隐藏层数目并保存为属性
        self.num_hidden_layers = config.num_hidden_layers
        # 初始化一个空列表用于保存多个自定义注意力块
        self.layers = []

        # 循环创建指定数量的自定义注意力块，并添加到 self.layers 中
        for i in range(self.num_hidden_layers):
            self.layers.append(TFSamTwoWayAttentionBlock(config, skip_first_layer_pe=(i == 0), name=f"layers_._{i}"))

        # 创建一个用于最终注意力层的对象，并命名为 final_attn_token_to_image
        self.final_attn_token_to_image = TFSamAttention(config, name="final_attn_token_to_image")
        # 创建一个 LayerNormalization 层，并使用 config 中的 epsilon 参数，命名为 layer_norm_final_attn
        self.layer_norm_final_attn = keras.layers.LayerNormalization(
            epsilon=config.layer_norm_eps, name="layer_norm_final_attn"
        )

    # 定义 call 方法，处理输入张量并执行前向传播
    def call(
        self,
        point_embeddings: tf.Tensor,
        image_embeddings: tf.Tensor,
        image_positional_embeddings: tf.Tensor,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, TFBaseModelOutput]:
        # 确定是否输出注意力权重，默认从 self.config 中获取
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        # 确定是否输出隐藏状态，默认从 self.config 中获取
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        # 确定是否使用返回字典，默认从 self.config 中获取
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # 初始化一个空元组，用于存储所有注意力权重
        all_attentions = ()

        # 如果 image_embeddings 为 None，则抛出 ValueError
        if image_embeddings is None:
            raise ValueError("You have to specify an image_embedding")

        # 对 image_embeddings 进行转置和扁平化处理，保持一致性，并添加一个维度
        image_embeddings = tf.transpose(flatten(image_embeddings, 2), perm=(0, 2, 1))[:, None]
        # 对 image_positional_embeddings 进行转置和扁平化处理，保持一致性，并添加一个维度
        image_positional_embeddings = tf.transpose(flatten(image_positional_embeddings, 2), (0, 2, 1))[:, None]

        # 准备查询向量，使用 point_embeddings
        queries = point_embeddings
        # 准备键向量，使用 image_embeddings

        keys = image_embeddings

        # 遍历 self.layers 中的每个注意力块，并应用于查询和键向量
        for layer in self.layers:
            queries, keys, attention_outputs = layer(
                queries=queries,
                keys=keys,
                query_point_embedding=point_embeddings,
                key_point_embedding=image_positional_embeddings,
                output_attentions=output_attentions,
            )

            # 如果设置了 output_attentions，则将注意力权重输出存储到 all_attentions 中
            if output_attentions:
                all_attentions = all_attentions + (attention_outputs,)

        # 应用从点到图像的最终注意力层
        query = queries + point_embeddings
        key = keys + image_positional_embeddings

        # 调用 self.final_attn_token_to_image 执行注意力操作，输出结果存储在 attn_out 中
        attn_out = self.final_attn_token_to_image(query=query, key=key, value=keys)

        # 将 attn_out 加到 queries 中，并通过 layer_norm_final_attn 进行规范化处理
        queries = queries + attn_out
        queries = self.layer_norm_final_attn(queries)

        # 返回处理后的 queries、keys 和 all_attentions（如果有）
        return queries, keys, all_attentions
    # 构建方法，用于构建模型结构
    def build(self, input_shape=None):
        # 如果已经构建过，直接返回，避免重复构建
        if self.built:
            return
        # 设置标志为已构建
        self.built = True
        
        # 如果存在最终注意力机制的映射到图像的部分，构建这部分
        if getattr(self, "final_attn_token_to_image", None) is not None:
            # 在命名空间下构建最终注意力机制映射到图像的层
            with tf.name_scope(self.final_attn_token_to_image.name):
                self.final_attn_token_to_image.build(None)
        
        # 如果存在最终注意力层归一化部分，构建这部分
        if getattr(self, "layer_norm_final_attn", None) is not None:
            # 在命名空间下构建最终注意力层归一化层
            with tf.name_scope(self.layer_norm_final_attn.name):
                self.layer_norm_final_attn.build([None, None, None, self.config.hidden_size])
        
        # 遍历所有层，分别在其命名空间下构建每一层
        for layer in self.layers:
            with tf.name_scope(layer.name):
                layer.build(None)
class TFSamFeedForward(keras.layers.Layer):
    # 定义一个自定义层 TFSamFeedForward，继承自 keras.layers.Layer
    def __init__(
        self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int, sigmoid_output: bool = False, **kwargs
    ):
        super().__init__(**kwargs)
        # 调用父类的初始化方法
        self.num_layers = num_layers
        # 设置层的数量
        self.activation = keras.layers.ReLU()
        # 设置激活函数为 ReLU
        self.proj_in = keras.layers.Dense(hidden_dim, input_shape=(input_dim,), name="proj_in")
        # 定义输入投影层，将输入维度映射到隐藏维度
        self.proj_out = keras.layers.Dense(output_dim, input_shape=(hidden_dim,), name="proj_out")
        # 定义输出投影层，将隐藏维度映射到输出维度
        self.layers = [
            keras.layers.Dense(hidden_dim, input_shape=(hidden_dim,), name=f"layers_._{i}")
            for i in range(num_layers - 2)
        ]
        # 定义多个隐藏层，除去输入和输出层
        self.sigmoid_output = sigmoid_output
        # 设置是否使用 sigmoid 输出
        self.hidden_dim = hidden_dim
        # 存储隐藏层维度
        self.input_dim = input_dim
        # 存储输入维度

    def call(self, hidden_states):
        # 定义层的前向传播过程
        hidden_states = self.proj_in(hidden_states)
        # 输入投影层处理输入数据
        hidden_states = self.activation(hidden_states)
        # 使用激活函数处理投影后的数据
        for layer in self.layers:
            hidden_states = self.activation(layer(hidden_states))
            # 遍历并使用激活函数处理每个隐藏层的数据

        hidden_states = self.proj_out(hidden_states)
        # 输出投影层处理隐藏层输出数据
        if self.sigmoid_output:
            hidden_states = tf.sigmoid(hidden_states)
            # 如果需要 sigmoid 输出，则应用 sigmoid 函数
        return hidden_states
        # 返回最终输出数据

    def build(self, input_shape=None):
        # 定义层的构建方法
        if self.built:
            return
        self.built = True
        # 标记层已构建
        if getattr(self, "proj_in", None) is not None:
            with tf.name_scope(self.proj_in.name):
                self.proj_in.build([None, None, self.input_dim])
        # 如果输入投影层存在，使用其名称作用域构建
        if getattr(self, "proj_out", None) is not None:
            with tf.name_scope(self.proj_out.name):
                self.proj_out.build([None, None, self.hidden_dim])
        # 如果输出投影层存在，使用其名称作用域构建
        if getattr(self, "layers", None) is not None:
            for layer in self.layers:
                with tf.name_scope(layer.name):
                    layer.build([None, None, self.hidden_dim])
        # 遍历每个隐藏层，使用其名称作用域构建
    def __init__(self, config: SamMaskDecoderConfig, **kwargs):
        super().__init__(**kwargs)  # 调用父类的初始化方法

        self.hidden_size = config.hidden_size  # 设置隐藏层大小

        self.num_multimask_outputs = config.num_multimask_outputs  # 多掩模输出数量
        self.num_mask_tokens = config.num_multimask_outputs + 1  # 掩模令牌数量

        self.transformer = TFSamTwoWayTransformer(config, name="transformer")  # 创建一个双向变换器对象

        self.upscale_conv1 = keras.layers.Conv2DTranspose(
            self.hidden_size // 4, kernel_size=2, strides=2, name="upscale_conv1", data_format="channels_first"
        )  # 第一个上采样卷积层，将隐藏层大小的四分之一作为输出，步长为2，使用通道优先的数据格式

        self.upscale_conv2 = keras.layers.Conv2DTranspose(
            self.hidden_size // 8, kernel_size=2, strides=2, name="upscale_conv2", data_format="channels_first"
        )  # 第二个上采样卷积层，将隐藏层大小的八分之一作为输出，步长为2，使用通道优先的数据格式

        self.upscale_layer_norm = TFSamLayerNorm(
            self.hidden_size // 4, data_format="channels_first", name="upscale_layer_norm"
        )  # 上采样层的归一化层，以隐藏层大小的四分之一作为输入，使用通道优先的数据格式

        self.activation = tf.nn.gelu  # 激活函数设置为 GELU 函数

        mlps_list = []
        for i in range(self.num_mask_tokens):
            mlps_list += [
                TFSamFeedForward(
                    self.hidden_size,
                    self.hidden_size,
                    self.hidden_size // 8,
                    3,
                    name=f"output_hypernetworks_mlps_._{i}",
                )
            ]  # 构建多个前馈网络，并添加到列表中作为超网络的输出

        self.output_hypernetworks_mlps = mlps_list  # 超网络的输出层列表

        self.iou_prediction_head = TFSamFeedForward(
            self.hidden_size,
            config.iou_head_hidden_dim,
            self.num_mask_tokens,
            config.iou_head_depth,
            name="iou_prediction_head",
        )  # IOU 预测头部的前馈网络

    def build(self, input_shape=None):
        if self.built:
            return  # 如果已经构建过，直接返回

        self.built = True  # 标记为已构建

        self.iou_token = self.add_weight(shape=(1, self.hidden_size), name="iou_token.weight", trainable=True)  # 添加 IOU 令牌权重参数

        self.mask_tokens = self.add_weight(
            shape=(self.num_mask_tokens, self.hidden_size), name="mask_tokens.weight", trainable=True
        )  # 添加掩模令牌权重参数

        if getattr(self, "transformer", None) is not None:
            with tf.name_scope(self.transformer.name):
                self.transformer.build(None)  # 构建变换器对象

        if getattr(self, "upscale_conv1", None) is not None:
            with tf.name_scope(self.upscale_conv1.name):
                self.upscale_conv1.build([None, self.hidden_size, None, None])  # 构建第一个上采样卷积层

        if getattr(self, "upscale_conv2", None) is not None:
            with tf.name_scope(self.upscale_conv2.name):
                self.upscale_conv2.build([None, self.hidden_size // 4, None, None])  # 构建第二个上采样卷积层

        if getattr(self, "upscale_layer_norm", None) is not None:
            with tf.name_scope(self.upscale_layer_norm.name):
                self.upscale_layer_norm.build(None)  # 构建上采样层的归一化层

        if getattr(self, "iou_prediction_head", None) is not None:
            with tf.name_scope(self.iou_prediction_head.name):
                self.iou_prediction_head.build(None)  # 构建 IOU 预测头部的前馈网络

        for mlp in self.output_hypernetworks_mlps:
            with tf.name_scope(mlp.name):
                mlp.build(None)  # 构建超网络的输出层列表中的每个前馈网络
    # 定义一个方法 `call`，接受多个参数作为输入
    def call(
        self,
        # 图像嵌入向量，使用 TensorFlow 的张量表示
        image_embeddings: tf.Tensor,
        # 图像位置嵌入向量，使用 TensorFlow 的张量表示
        image_positional_embeddings: tf.Tensor,
        # 稀疏提示嵌入向量，使用 TensorFlow 的张量表示
        sparse_prompt_embeddings: tf.Tensor,
        # 密集提示嵌入向量，使用 TensorFlow 的张量表示
        dense_prompt_embeddings: tf.Tensor,
        # 是否输出多掩码的结果，布尔类型
        multimask_output: bool,
        # 是否输出注意力信息，可选的布尔类型参数，默认为 None
        output_attentions: Optional[bool] = None,
class TFSamPositionalEmbedding(keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.scale = config.hidden_size // 2  # 计算缩放因子，用于位置编码
        self.config = config

    def build(self, input_shape):
        # 构建层时，创建一个不可训练的权重矩阵作为位置编码的基础
        self.positional_embedding = self.add_weight(
            name="positional_embedding",
            shape=(2, self.config.num_pos_feats),  # 设置权重矩阵的形状
            initializer=keras.initializers.RandomNormal(mean=0.0, stddev=self.scale),  # 使用随机正态分布初始化权重
            trainable=False,  # 设置权重为不可训练
        )
        super().build(input_shape)

    def call(self, input_coords, input_shape=None):
        """Positionally encode points that are normalized to [0,1]."""
        coordinates = tf.identity(input_coords)

        if input_shape is not None:
            coordinates = tf.stack(
                [
                    tf.cast(coordinates[:, :, :, 0], tf.float32) / input_shape[1],  # 将 x 坐标归一化到 [0,1]
                    tf.cast(coordinates[:, :, :, 1], tf.float32) / input_shape[0],  # 将 y 坐标归一化到 [0,1]
                ],
                axis=-1,
            )

        # 将归一化后的坐标转换到 [-1, 1] 区间
        coordinates = 2 * coordinates - 1
        coordinates = tf.cast(coordinates, self.positional_embedding.dtype)  # 转换坐标数据类型以匹配位置编码的数据类型
        coordinates = tf.matmul(coordinates, self.positional_embedding)  # 计算坐标与位置编码的乘积
        coordinates = 2 * np.pi * coordinates  # 缩放乘积以增加周期性
        # 输出正弦和余弦函数的组合，用于位置编码
        return tf.concat([tf.sin(coordinates), tf.cos(coordinates)], axis=-1)


class TFSamMaskEmbedding(keras.layers.Layer):
    def __init__(self, config: SamPromptEncoderConfig, **kwargs):
        super().__init__(**kwargs)
        self.mask_input_channels = config.mask_input_channels // 4  # 计算掩码输入通道数的四分之一
        self.activation = ACT2FN[config.hidden_act]  # 激活函数由配置决定
        self.conv1 = keras.layers.Conv2D(self.mask_input_channels, kernel_size=2, strides=2, name="conv1")  # 第一个卷积层
        self.conv2 = keras.layers.Conv2D(config.mask_input_channels, kernel_size=2, strides=2, name="conv2")  # 第二个卷积层
        self.conv3 = keras.layers.Conv2D(config.hidden_size, kernel_size=1, name="conv3")  # 第三个卷积层
        self.layer_norm1 = TFSamLayerNorm(self.mask_input_channels, config.layer_norm_eps, name="layer_norm1")  # 第一个层归一化层
        self.layer_norm2 = TFSamLayerNorm(self.mask_input_channels * 4, config.layer_norm_eps, name="layer_norm2")  # 第二个层归一化层
        self.config = config  # 保存配置信息
    def call(self, masks):
        # 转置输入张量，将通道维度移到最后一个维度
        masks = tf.transpose(masks, perm=(0, 2, 3, 1))  # Convert to channels-last
        # 第一层卷积操作
        hidden_states = self.conv1(masks)
        # 第一层层归一化
        hidden_states = self.layer_norm1(hidden_states)
        # 激活函数处理
        hidden_states = self.activation(hidden_states)

        # 第二层卷积操作
        hidden_states = self.conv2(hidden_states)
        # 第二层层归一化
        hidden_states = self.layer_norm2(hidden_states)
        # 激活函数处理
        hidden_states = self.activation(hidden_states)
        # 第三层卷积操作
        dense_embeddings = self.conv3(hidden_states)
        # 转置张量，将通道维度移到第二个位置，回到 channels-first 格式
        dense_embeddings = tf.transpose(dense_embeddings, perm=(0, 3, 1, 2))  # Convert back to channels-first
        return dense_embeddings

    def build(self, input_shape=None):
        # 由于此类不会使用标准的虚拟输入，因此需要显式的 build 方法
        if self.built:
            return
        self.built = True
        # 使用 tf.name_scope 确定每个层的输入形状
        with tf.name_scope("conv1"):
            self.conv1.build([None, None, None, 1])
        with tf.name_scope("conv2"):
            self.conv2.build([None, None, None, self.mask_input_channels])
        with tf.name_scope("conv3"):
            self.conv3.build([None, None, None, self.mask_input_channels * 4])
        with tf.name_scope("layer_norm1"):
            self.layer_norm1.build([None, None, None, self.mask_input_channels])
        with tf.name_scope("layer_norm2"):
            self.layer_norm2.build([None, None, None, self.mask_input_channels * 4])
class TFSamPromptEncoder(keras.layers.Layer):
    def __init__(self, config: SamPromptEncoderConfig, shared_patch_embedding, **kwargs):
        super().__init__(**kwargs)
        self.shared_embedding = shared_patch_embedding  # 设置共享的补丁嵌入对象
        self.mask_embed = TFSamMaskEmbedding(config, name="mask_embed")  # 创建一个 TFSamMaskEmbedding 实例作为 mask_embed
        self.no_mask_embed = None  # 初始化为 None，在 build 方法中将被赋值为权重
        self.image_embedding_size = (config.image_embedding_size, config.image_embedding_size)  # 图像嵌入的尺寸
        self.input_image_size = config.image_size  # 输入图像的尺寸

        self.point_embed = []  # 初始化为空列表，用于存储点嵌入的权重
        self.hidden_size = config.hidden_size  # 隐藏层的大小
        self.not_a_point_embed = None  # 初始化为 None，在 build 方法中将被赋值为权重
        self.config = config  # 保存配置对象

    def build(self, input_shape=None):
        self.no_mask_embed = self.add_weight(
            name="no_mask_embed.weight",
            shape=(1, self.hidden_size),
            initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02),
            trainable=True,
        )  # 添加一个权重变量，用于表示没有 mask 的嵌入

        self.point_embed = [
            self.add_weight(
                name=f"point_embed_._{i}.weight",
                shape=(1, self.hidden_size),
                initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02),
                trainable=True,
            )
            for i in range(self.config.num_point_embeddings)
        ]  # 添加多个权重变量，用于表示点嵌入

        self.not_a_point_embed = self.add_weight(
            name="not_a_point_embed.weight",
            shape=(1, self.hidden_size),
            initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02),
            trainable=True,
        )  # 添加一个权重变量，用于表示非点嵌入

        with tf.name_scope("mask_embed"):
            # 显式构建 mask_embed，因为它不会被标准的虚拟输入所触及
            self.mask_embed.build(
                (None, self.config.mask_input_channels, self.config.image_size, self.config.image_size)
            )  # 构建 mask_embed 的结构

        if self.built:
            return
        self.built = True

        if getattr(self, "mask_embed", None) is not None:
            with tf.name_scope(self.mask_embed.name):
                self.mask_embed.build(None)  # 如果 mask_embed 存在，则进一步构建它的结构
    def _embed_points(self, points: tf.Tensor, labels: tf.Tensor, pad: bool) -> tf.Tensor:
        """Embeds point prompts."""
        # 将点坐标加上0.5以将其移动到像素中心
        points = points + 0.5  # Shift to center of pixel
        if pad:
            # 构建目标点的形状，用于填充
            target_point_shape = (shape_list(points)[0], shape_list(points)[1], 1, shape_list(points)[-1])
            target_labels_shape = (shape_list(points)[0], shape_list(points)[1], 1)
            # 创建零填充的点和标签
            padding_point = tf.zeros(target_point_shape, dtype=points.dtype)
            padding_label = -tf.ones(target_labels_shape, dtype=labels.dtype)
            # 在点和标签的第三个维度上连接填充内容
            points = tf.concat([points, padding_point], axis=2)
            labels = tf.concat([labels, padding_label], axis=2)
        input_shape = (self.input_image_size, self.input_image_size)
        # 使用共享的嵌入层嵌入点坐标
        point_embedding = self.shared_embedding(points, input_shape)

        # 根据标签值进行条件选择和嵌入
        point_embedding = tf.where(labels[..., None] == -1, self.not_a_point_embed[0], point_embedding)

        point_embedding = tf.where(
            labels[..., None] != -10,
            point_embedding,
            tf.zeros_like(point_embedding),
        )
        point_embedding = tf.where(
            (labels == 0)[:, :, :, None], point_embedding + self.point_embed[0], point_embedding
        )
        point_embedding = tf.where(
            (labels == 1)[:, :, :, None], point_embedding + self.point_embed[1], point_embedding
        )
        return point_embedding

    def _embed_boxes(self, boxes: tf.Tensor) -> tf.Tensor:
        """Embeds box prompts."""
        # 将框坐标加上0.5以将其移动到像素中心
        boxes = boxes + 0.5  # Shift to center of pixel
        batch_size, nb_boxes = shape_list(boxes)[:2]
        # 重塑框的坐标形状以适应嵌入层
        coords = tf.reshape(boxes, (batch_size, nb_boxes, 2, 2))
        input_shape = (self.input_image_size, self.input_image_size)
        # 使用共享的嵌入层嵌入角点坐标
        corner_embedding = self.shared_embedding(coords, input_shape)
        # 根据条件在角点嵌入上添加偏移量
        corner_embedding += tf.where(
            tf.range(shape_list(corner_embedding)[2])[None, None, :, None] == 0,
            self.point_embed[2][0],
            self.point_embed[3][0],
        )
        return corner_embedding

    def call(
        self,
        batch_size: Optional[int],
        input_points: Optional[Tuple[tf.Tensor, tf.Tensor]],
        input_labels: tf.Tensor | None,
        input_boxes: tf.Tensor | None,
        input_masks: tf.Tensor | None,
        """
        Embeds different types of prompts, returning both sparse and dense embeddings.

        Args:
            points (`tf.Tensor`, *optional*):
                point coordinates and labels to embed.
            boxes (`tf.Tensor`, *optional*):
                boxes to embed
            masks (`tf.Tensor`, *optional`):
                masks to embed
        """
        # 初始化稀疏和密集嵌入为 None
        sparse_embeddings = None

        # 如果输入的点不为空，则进行点嵌入
        if input_points is not None:
            # 获取批量大小和点批次大小
            batch_size, point_batch_size = shape_list(input_points)[:2]
            # 如果输入的标签为空，则抛出数值错误
            if input_labels is None:
                raise ValueError("If points are provided, labels must also be provided.")
            # 使用内部方法 _embed_points 进行点的嵌入
            point_embeddings = self._embed_points(input_points, input_labels, pad=(input_boxes is None))
            # 创建全零的稀疏嵌入张量
            sparse_embeddings = tf.zeros(
                (batch_size, point_batch_size, 0, self.hidden_size), dtype=point_embeddings.dtype
            )
            # 将点嵌入拼接到稀疏嵌入张量中
            sparse_embeddings = tf.concat([sparse_embeddings, point_embeddings], axis=2)

        # 如果输入的盒子不为空，则进行盒子的嵌入
        if input_boxes is not None:
            # 获取批量大小
            batch_size = shape_list(input_boxes)[0]
            # 使用内部方法 _embed_boxes 进行盒子的嵌入
            box_embeddings = self._embed_boxes(input_boxes)
            # 如果稀疏嵌入张量为空，则将盒子嵌入设为稀疏嵌入张量
            if sparse_embeddings is None:
                sparse_embeddings = box_embeddings
            else:
                # 否则将盒子嵌入拼接到稀疏嵌入张量中
                sparse_embeddings = tf.concat([sparse_embeddings, box_embeddings], axis=2)

        # 如果输入的掩码不为空，则进行掩码的嵌入
        if input_masks is not None:
            # 使用 mask_embed 方法进行掩码的嵌入
            dense_embeddings = self.mask_embed(input_masks)
        else:
            # 否则使用无掩码嵌入的第一个元素作为密集嵌入
            dense_embeddings = self.no_mask_embed[0]
            # 调整密集嵌入的形状
            dense_embeddings = tf.reshape(dense_embeddings, (1, -1, 1, 1))
            # 在指定维度上复制密集嵌入
            dense_embeddings = tf.tile(
                dense_embeddings, (batch_size, 1, self.image_embedding_size[0], self.image_embedding_size[1])
            )

        # 如果稀疏嵌入张量仍为空，则创建全零的稀疏嵌入张量
        if sparse_embeddings is None:
            sparse_embeddings = tf.zeros((batch_size, 0, 1, self.hidden_size), dtype=dense_embeddings.dtype)

        # 返回稀疏嵌入张量和密集嵌入张量
        return sparse_embeddings, dense_embeddings
# 定义一个自定义的注意力层，用于处理多头注意力机制和相对位置编码
class TFSamVisionAttention(keras.layers.Layer):
    """Multi-head Attention block with relative position embeddings."""

    def __init__(self, config, window_size, **kwargs):
        super().__init__(**kwargs)
        # 计算输入大小，根据配置和窗口大小决定
        input_size = (
            (config.image_size // config.patch_size, config.image_size // config.patch_size)
            if window_size == 0
            else (window_size, window_size)
        )
        self.input_size = input_size

        # 注意力头的数量
        self.num_attention_heads = config.num_attention_heads
        # 每个注意力头的维度
        head_dim = config.hidden_size // config.num_attention_heads
        self.head_dim = head_dim
        # 缩放因子，用于缩放注意力分数
        self.scale = head_dim**-0.5
        # 注意力层的 dropout 概率
        self.dropout = config.attention_dropout

        # QKV 查询键值对应的全连接层
        self.qkv = keras.layers.Dense(config.hidden_size * 3, use_bias=config.qkv_bias, name="qkv")
        # 投影层，用于最终的输出
        self.proj = keras.layers.Dense(config.hidden_size, name="proj")

        # 是否使用相对位置编码
        self.use_rel_pos = config.use_rel_pos
        if self.use_rel_pos:
            # 如果使用相对位置编码，确保输入大小已提供
            if input_size is None:
                raise ValueError("Input size must be provided if using relative positional encoding.")
        self.config = config

    def build(self, input_shape=None):
        # 如果输入大小不为 None，则初始化相对位置编码
        if self.input_size is not None:
            # 水平方向的相对位置编码权重矩阵
            self.rel_pos_h = self.add_weight(
                shape=(2 * self.input_size[0] - 1, self.head_dim), initializer="zeros", name="rel_pos_h"
            )
            # 垂直方向的相对位置编码权重矩阵
            self.rel_pos_w = self.add_weight(
                shape=(2 * self.input_size[1] - 1, self.head_dim), initializer="zeros", name="rel_pos_w"
            )

        if self.built:
            return
        self.built = True
        # 构建 QKV 全连接层
        if getattr(self, "qkv", None) is not None:
            with tf.name_scope(self.qkv.name):
                self.qkv.build([None, None, self.config.hidden_size])
        # 构建投影层
        if getattr(self, "proj", None) is not None:
            with tf.name_scope(self.proj.name):
                self.proj.build([None, None, self.config.hidden_size])
    def get_rel_pos(self, q_size: int, k_size: int, rel_pos: tf.Tensor) -> tf.Tensor:
        """
        Get relative positional embeddings according to the relative positions of
            query and key sizes.

        Args:
            q_size (int):
                size of the query.
            k_size (int):
                size of key k.
            rel_pos (`tf.Tensor`):
                relative position embeddings (L, channel).

        Returns:
            Extracted positional embeddings according to relative positions.
        """
        # Calculate the maximum relative distance based on query and key sizes
        max_rel_dist = int(2 * max(q_size, k_size) - 1)
        
        # Interpolate rel_pos if its length does not match max_rel_dist
        if rel_pos.shape[0] != max_rel_dist:
            # Resize rel_pos using bilinear interpolation to match max_rel_dist
            rel_pos_resized = tf.image.resize(
                tf.reshape(rel_pos, (1, rel_pos.shape[0], -1)),
                size=(max_rel_dist, rel_pos.shape[1]),
                method="bilinear",
            )
            # Reshape the interpolated rel_pos to match the expected shape
            rel_pos_resized = tf.reshape(rel_pos_resized, (-1, max_rel_dist))
        else:
            # Use rel_pos directly if its length matches max_rel_dist
            rel_pos_resized = rel_pos

        # Calculate relative coordinates scaled according to the sizes of q and k
        q_coords = tf.expand_dims(tf.range(q_size, dtype=tf.float32), 1) * max(k_size / q_size, 1.0)
        k_coords = tf.expand_dims(tf.range(k_size, dtype=tf.float32), 0) * max(q_size / k_size, 1.0)
        relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)

        # Gather positional embeddings based on the calculated relative coordinates
        return tf.gather(rel_pos_resized, tf.cast(relative_coords, tf.int32))

    def add_decomposed_rel_pos(
        self,
        attn: tf.Tensor,
        query: tf.Tensor,
        rel_pos_h: tf.Tensor,
        rel_pos_w: tf.Tensor,
        q_size: Tuple[int, int],
        k_size: Tuple[int, int],
        ...
    ) -> tf.Tensor:
        """
        Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.
        https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py

        Args:
            attn (`tf.Tensor`):
                attention map.
            query (`tf.Tensor`):
                query q in the attention layer with shape (batch_size, query_height * query_width, channel).
            rel_pos_h (`tf.Tensor`):
                relative position embeddings (Lh, channel) for height axis.
            rel_pos_w (`tf.Tensor`):
                relative position embeddings (Lw, channel) for width axis.
            q_size (tuple):
                spatial sequence size of query q with (query_height, query_width).
            k_size (tuple):
                spatial sequence size of key k with (key_height, key_width).

        Returns:
            attn (`tf.Tensor`):
                attention map with added relative positional embeddings.
        """
        # 解包查询和键的空间尺寸
        query_height, query_width = q_size
        key_height, key_width = k_size
        
        # 获取相对位置嵌入的高度和宽度分量
        relative_position_height = self.get_rel_pos(query_height, key_height, rel_pos_h)
        relative_position_width = self.get_rel_pos(query_width, key_width, rel_pos_w)
        
        # 获取查询张量的形状信息
        batch_size, _, dim = shape_list(query)
        
        # 将查询张量重塑为四维张量，以便进行相对位置计算
        reshaped_query = tf.reshape(query, (batch_size, query_height, query_width, dim))
        
        # 使用 Einstein Summation 计算高度和宽度方向上的相对位置加权
        rel_h = tf.einsum("bhwc,hkc->bhwk", reshaped_query, relative_position_height)
        rel_w = tf.einsum("bhwc,wkc->bhwk", reshaped_query, relative_position_width)
        
        # 将注意力张量重塑为五维张量，以便应用相对位置加权
        attn = tf.reshape(attn, (batch_size, query_height, query_width, key_height, key_width))
        
        # 将相对位置加权应用到注意力张量上，并将其重塑为二维张量
        attn = attn + tf.expand_dims(rel_h, axis=-1) + tf.expand_dims(rel_w, axis=-2)
        attn = tf.reshape(attn, (batch_size, query_height * query_width, key_height * key_width))
        
        # 返回加入相对位置嵌入后的注意力张量
        return attn
    # 定义一个方法，输入为 hidden_states (TensorFlow 张量)，output_attentions 和 training 的布尔参数
    def call(self, hidden_states: tf.Tensor, output_attentions=False, training=False) -> tf.Tensor:
        # 获取 batch_size, height, width 和通道数
        batch_size, height, width, _ = shape_list(hidden_states)
        # 通过 qkv 层处理 hidden_states，得到一个形状为 (batch_size, height * width, 3, num_attention_heads, -1) 的张量
        qkv = tf.reshape(self.qkv(hidden_states), (batch_size, height * width, 3, self.num_attention_heads, -1))
        # 调整维度，将 qkv 张量从 (batch_size, height * width, 3, num_attention_heads, -1) 转换为 (3, batch_size, num_attention_heads, height * width, -1)
        qkv = tf.transpose(qkv, perm=(2, 0, 3, 1, 4))
        # 将 qkv 张量重新调整为 (batch_size * num_attention_heads, height * width, channel)，并拆分为 query, key, value
        query, key, value = tf.unstack(
            tf.reshape(qkv, (3, batch_size * self.num_attention_heads, height * width, -1)), axis=0
        )
        # 计算注意力权重，query 与 key 的乘积，缩放后进行矩阵乘法
        attn_weights = tf.matmul(query * self.scale, key, transpose_b=True)

        # 如果使用相对位置编码，调用 add_decomposed_rel_pos 方法添加相对位置编码
        if self.use_rel_pos:
            attn_weights = self.add_decomposed_rel_pos(
                attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)
            )

        # 对注意力权重进行 softmax 操作，归一化权重
        attn_weights = tf.nn.softmax(attn_weights, axis=-1)

        # 根据是否训练，应用 dropout
        if training:
            attn_probs = tf.nn.dropout(attn_weights, rate=self.dropout)
        else:
            attn_probs = attn_weights

        # 计算注意力输出，使用注意力权重与 value 的矩阵乘法，重塑形状为 (batch_size, num_attention_heads, height, width, -1)
        attn_output = tf.reshape(attn_probs @ value, (batch_size, self.num_attention_heads, height, width, -1))
        # 调整输出维度，将其从 (batch_size, num_attention_heads, height, width, -1) 转换为 (batch_size, height, width, num_attention_heads, -1)
        attn_output = tf.transpose(attn_output, perm=(0, 2, 3, 1, 4))
        # 将注意力输出重新调整形状为 (batch_size, height, width, hidden_size)
        attn_output = tf.reshape(attn_output, (batch_size, height, width, self.config.hidden_size))

        # 通过 proj 层对注意力输出进行投影
        attn_output = self.proj(attn_output)

        # 如果需要输出注意力权重，返回包含注意力输出和注意力权重的元组，否则只返回注意力输出
        if output_attentions:
            outputs = (attn_output, attn_weights)
        else:
            outputs = (attn_output, None)

        return outputs
# 定义自定义层 TFSamVisionLayer，继承自 keras.layers.Layer
class TFSamVisionLayer(keras.layers.Layer):
    
    # 初始化方法，接受配置 config 和窗口大小 window_size 作为参数
    def __init__(self, config, window_size, **kwargs):
        super().__init__(**kwargs)
        
        # 创建 LayerNormalization 层，用于归一化数据，设定 epsilon 为 config 中的 layer_norm_eps，命名为 "layer_norm1"
        self.layer_norm1 = keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="layer_norm1")
        
        # 创建 TFSamVisionAttention 层，用于处理注意力机制，传入配置 config 和窗口大小 window_size，命名为 "attn"
        self.attn = TFSamVisionAttention(config, window_size, name="attn")
        
        # 创建第二个 LayerNormalization 层，设定 epsilon 为 config 中的 layer_norm_eps，命名为 "layer_norm2"
        self.layer_norm2 = keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="layer_norm2")
        
        # 创建 TFSamMLPBlock 层，用于多层感知机处理，传入配置 config，命名为 "mlp"
        self.mlp = TFSamMLPBlock(config, name="mlp")
        
        # 设置窗口大小和配置参数为实例变量
        self.window_size = window_size
        self.config = config

    # 定义窗口划分方法，接受 hidden_states 和 window_size 作为输入，返回划分后的窗口和填充后的高度和宽度元组
    def window_partition(self, hidden_states: tf.Tensor, window_size: int) -> Tuple[tf.Tensor, Tuple[int, int]]:
        # 获取 hidden_states 的形状信息
        batch_size, height, width, channel = shape_list(hidden_states)

        # 计算高度和宽度的填充量，使其能够被 window_size 整除
        pad_h = (window_size - height % window_size) % window_size
        pad_w = (window_size - width % window_size) % window_size
        
        # 如果存在填充，则在高度和宽度上进行填充
        if pad_h > 0 or pad_w > 0:
            hidden_states = tf.pad(hidden_states, [[0, 0], [0, pad_h], [0, pad_w], [0, 0]])
        
        # 计算填充后的高度和宽度
        pad_height, pad_width = height + pad_h, width + pad_w
        
        # 将 hidden_states 重新形状为 batch_size, pad_height // window_size, window_size, pad_width // window_size, window_size, channel
        hidden_states = tf.reshape(
            hidden_states,
            [batch_size, pad_height // window_size, window_size, pad_width // window_size, window_size, channel],
        )
        
        # 将形状变换后的 hidden_states 进行转置和重塑，得到 windows，形状为 [-1, window_size, window_size, channel]
        windows = tf.reshape(
            tf.transpose(hidden_states, perm=[0, 1, 3, 2, 4, 5]), [-1, window_size, window_size, channel]
        )
        
        # 返回 windows 和填充后的高度和宽度元组
        return windows, (pad_height, pad_width)

    # 定义窗口反划分方法，接受 windows、window_size、padding_shape 和 original_shape 作为输入，返回反划分后的 hidden_states
    def window_unpartition(
        self, windows: tf.Tensor, window_size: int, padding_shape: Tuple[int, int], original_shape: Tuple[int, int]
    ) -> tf.Tensor:
        # 获取填充后的高度和宽度
        pad_height, pad_width = padding_shape
        
        # 获取原始的高度和宽度
        height, width = original_shape
        
        # 计算 batch_size
        batch_size = shape_list(windows)[0] // (pad_height * pad_width // window_size // window_size)
        
        # 将 windows 重新形状为 batch_size, pad_height // window_size, pad_width // window_size, window_size, window_size, -1
        hidden_states = tf.reshape(
            windows, [batch_size, pad_height // window_size, pad_width // window_size, window_size, window_size, -1]
        )
        
        # 进行转置和重塑，得到 hidden_states，形状为 batch_size, pad_height, pad_width, -1
        hidden_states = tf.reshape(
            tf.transpose(hidden_states, perm=[0, 1, 3, 2, 4, 5]), [batch_size, pad_height, pad_width, -1]
        )
        
        # 如果填充后的高度或宽度大于原始高度或宽度，则截取对应部分
        if pad_height > height or pad_width > width:
            hidden_states = hidden_states[:, :height, :width, :]
        
        # 返回反划分后的 hidden_states
        return hidden_states

    # 定义调用方法 call，接受 hidden_states、output_attentions 和 training 作为输入
    def call(
        self,
        hidden_states: tf.Tensor,
        output_attentions: Optional[bool] = False,
        training: Optional[bool] = False,
    ) -> Tuple[tf.Tensor]:
        # 保留原始隐藏状态作为残差连接的基础
        residual = hidden_states

        # 应用 Layer Normalization 到隐藏状态
        hidden_states = self.layer_norm1(hidden_states)
        
        # 如果窗口大小大于0，则进行窗口划分
        if self.window_size > 0:
            # 获取隐藏状态的高度和宽度
            height, width = hidden_states.shape[1], hidden_states.shape[2]
            # 对隐藏状态进行窗口划分，同时获取填充形状
            hidden_states, padding_shape = self.window_partition(hidden_states, self.window_size)

        # 进行自注意力机制操作，并获取注意力权重
        hidden_states, attn_weights = self.attn(
            hidden_states=hidden_states,
            output_attentions=output_attentions,
            training=training,
        )
        
        # 如果窗口大小大于0，则进行窗口反划分
        if self.window_size > 0:
            hidden_states = self.window_unpartition(hidden_states, self.window_size, padding_shape, (height, width))

        # 添加残差连接到经过注意力操作后的隐藏状态
        hidden_states = residual + hidden_states
        
        # 应用 Layer Normalization 到加和后的隐藏状态
        layernorm_output = self.layer_norm2(hidden_states)
        
        # 应用 MLP（多层感知机）到 Layer Normalization 后的输出
        hidden_states = hidden_states + self.mlp(layernorm_output)

        # 准备输出，包含最终的隐藏状态
        outputs = (hidden_states,)
        
        # 如果需要输出注意力权重，则将它们加入到输出元组中
        if output_attentions:
            outputs += (attn_weights,)

        return outputs

    def build(self, input_shape=None):
        # 如果已经构建过，则直接返回
        if self.built:
            return
        # 标记为已构建
        self.built = True
        
        # 构建 layer_norm1 层
        if getattr(self, "layer_norm1", None) is not None:
            with tf.name_scope(self.layer_norm1.name):
                self.layer_norm1.build([None, None, None, self.config.hidden_size])
        
        # 构建 attention 层
        if getattr(self, "attn", None) is not None:
            with tf.name_scope(self.attn.name):
                self.attn.build(None)
        
        # 构建 layer_norm2 层
        if getattr(self, "layer_norm2", None) is not None:
            with tf.name_scope(self.layer_norm2.name):
                self.layer_norm2.build([None, None, None, self.config.hidden_size])
        
        # 构建 MLP 层
        if getattr(self, "mlp", None) is not None:
            with tf.name_scope(self.mlp.name):
                self.mlp.build(None)
# 定义自定义的视觉编码器层，继承自 Keras 的 Layer 类
class TFSamVisionEncoder(keras.layers.Layer):
    # 初始化方法，接收配置对象和其他关键字参数
    def __init__(self, config: SamVisionConfig, **kwargs):
        super().__init__(**kwargs)
        # 将配置对象保存到实例变量中
        self.config = config
        # 设置图像大小属性
        self.image_size = config.image_size

        # 创建图像块嵌入层对象，并命名为 "patch_embed"
        self.patch_embed = TFSamPatchEmbeddings(config, name="patch_embed")

        # 初始化位置嵌入变量，暂时设为 None
        self.pos_embed = None

        # 初始化层列表，用于保存多个视觉层对象
        self.layers = []
        # 循环创建指定数量的视觉层对象
        for i in range(config.num_hidden_layers):
            # 创建单个视觉层对象，名称中包括层索引 i
            layer = TFSamVisionLayer(
                config,
                window_size=config.window_size if i not in config.global_attn_indexes else 0,
                name=f"layers_._{i}",
            )
            # 将创建的视觉层对象添加到层列表中
            self.layers.append(layer)

        # 创建视觉颈部对象，并命名为 "neck"
        self.neck = TFSamVisionNeck(config, name="neck")
    # 构建模型，初始化模型的权重和结构
    def build(self, input_shape=None):
        # 如果模型已经构建过，则直接返回
        if self.built:
            return
        # 标记模型为已构建状态
        self.built = True
        
        # 如果配置要求使用绝对位置编码
        if self.config.use_abs_pos:
            # 初始化绝对位置嵌入，其形状与预训练图像大小相关
            self.pos_embed = self.add_weight(
                shape=[
                    1,
                    self.config.image_size // self.config.patch_size,
                    self.config.image_size // self.config.patch_size,
                    self.config.hidden_size,
                ],
                initializer="zeros",
                trainable=True,
                name="pos_embed",
            )

        # 如果已定义 patch_embed 属性，则构建其内部结构
        if getattr(self, "patch_embed", None) is not None:
            with tf.name_scope(self.patch_embed.name):
                self.patch_embed.build(None)
        
        # 如果已定义 neck 属性，则构建其内部结构
        if getattr(self, "neck", None) is not None:
            with tf.name_scope(self.neck.name):
                self.neck.build(None)
        
        # 遍历模型的所有层，并构建每一层的结构
        for layer in self.layers:
            with tf.name_scope(layer.name):
                layer.build(None)

    # 返回输入嵌入
    def get_input_embeddings(self):
        return self.patch_embed

    # 模型调用函数，用于执行前向传播
    def call(
        self,
        pixel_values: tf.Tensor | None = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        training: Optional[bool] = False,
        # 更多参数，用于控制模型行为

        # 函数参数说明：
        # pixel_values: 输入的像素张量，可以为 None
        # output_attentions: 是否输出注意力信息，可选布尔值
        # output_hidden_states: 是否输出隐藏状态信息，可选布尔值
        # return_dict: 是否返回字典格式的结果，可选布尔值
        # training: 是否处于训练模式，可选布尔值，默认为 False
        ) -> Union[Tuple, TFSamVisionEncoderOutput]:
        # 确定是否输出注意力权重
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        # 确定是否输出隐藏层状态
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        # 确定是否使用返回字典形式
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # 如果像素值为 None，则抛出数值错误异常
        if pixel_values is None:
            raise ValueError("You have to specify pixel_values")

        # 使用 patch_embed 方法处理像素值，得到隐藏状态
        hidden_states = self.patch_embed(pixel_values)
        # 如果存在位置编码，则加上位置编码
        if self.pos_embed is not None:
            hidden_states = hidden_states + self.pos_embed

        # 如果输出隐藏状态为真，则初始化存储所有隐藏状态的元组
        all_hidden_states = () if output_hidden_states else None
        # 如果输出注意力权重为真，则初始化存储所有注意力权重的元组
        all_self_attentions = () if output_attentions else None

        # 遍历所有层并逐层处理
        for i, layer_module in enumerate(self.layers):
            # 如果输出隐藏状态为真，则将当前隐藏状态添加到所有隐藏状态中
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            # 调用当前层的处理方法，获取当前层的输出
            layer_outputs = layer_module(hidden_states, output_attentions=output_attentions, training=training)

            # 更新隐藏状态为当前层的输出的第一个元素
            hidden_states = layer_outputs[0]

            # 如果输出注意力权重为真，则将当前层的注意力权重添加到所有注意力权重中
            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)

        # 如果输出隐藏状态为真，则将最终隐藏状态添加到所有隐藏状态中
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        # 使用 neck 方法处理最终的隐藏状态
        hidden_states = self.neck(hidden_states)

        # 如果不使用返回字典形式，则按顺序返回隐藏状态、所有隐藏状态、所有注意力权重
        if not return_dict:
            outputs = (hidden_states,)
            if output_hidden_states:
                outputs = outputs + (all_hidden_states,)
            if output_attentions:
                outputs = outputs + (all_self_attentions,)
            return outputs

        # 如果使用返回字典形式，则返回 TFSamVisionEncoderOutput 类的实例
        return TFSamVisionEncoderOutput(
            last_hidden_state=hidden_states,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
        )
@add_start_docstrings(
    "Segment Anything Model (SAM) for generating segmentation masks, given an input image and ",
    " optional 2D location and bounding boxes.",
    SAM_START_DOCSTRING,
)
class TFSamModel(TFSamPreTrainedModel):
    _keys_to_ignore_on_load_missing = [r"prompt_encoder.shared_embedding.positional_embedding"]

    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)
        # 初始化共享的图像嵌入层，使用配置中的视觉配置
        self.shared_image_embedding = TFSamPositionalEmbedding(config.vision_config, name="shared_image_embedding")

        # 初始化视觉编码器，使用配置中的视觉配置
        self.vision_encoder = TFSamVisionEncoder(config.vision_config, name="vision_encoder")
        
        # 初始化提示编码器，使用配置中的提示编码器配置和共享的图像嵌入层
        self.prompt_encoder = TFSamPromptEncoder(
            config.prompt_encoder_config, self.shared_image_embedding, name="prompt_encoder"
        )
        
        # 初始化掩码解码器，使用配置中的掩码解码器配置
        self.mask_decoder = TFSamMaskDecoder(config.mask_decoder_config, name="mask_decoder")
        
        # 保存配置以供后续调用使用
        self.config = config

    def get_input_embeddings(self):
        # 获取视觉编码器的输入嵌入
        return self.vision_encoder.get_input_embeddings()

    def get_image_wide_positional_embeddings(self):
        # 获取图像广域位置嵌入

        # 图像嵌入尺寸
        size = self.config.prompt_encoder_config.image_embedding_size
        
        # 创建尺寸为（size, size）的全1张量
        grid = tf.ones((size, size))
        
        # 沿着垂直方向累积求和，并进行中心化处理
        y_embed = tf.math.cumsum(grid, axis=0) - 0.5
        
        # 沿着水平方向累积求和，并进行中心化处理
        x_embed = tf.math.cumsum(grid, axis=1) - 0.5
        
        # 将嵌入位置坐标归一化
        y_embed = y_embed / size
        x_embed = x_embed / size
        
        # 使用共享的图像嵌入层获取位置嵌入张量
        positional_embedding = self.shared_image_embedding(tf.stack([x_embed, y_embed], axis=-1))
        
        # 调整维度顺序为 channel x height x width，并扩展维度为 (1, channel, height, width)
        return tf.expand_dims(tf.transpose(positional_embedding, perm=[2, 0, 1]), axis=0)

    def get_image_embeddings(
        self,
        pixel_values,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ):
        # 获取图像嵌入
        # 在实际使用中，此方法将会进一步实现
        pass
    ):
        r"""
        Returns the image embeddings by passing the pixel values through the vision encoder.

        Args:
            pixel_values (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`):
                Input pixel values
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.TFModelOutput`] instead of a plain tuple.

        """
        # 使用视觉编码器处理像素值，返回图像嵌入向量
        vision_output = self.vision_encoder(
            pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        # 提取图像嵌入向量
        image_embeddings = vision_output[0]
        return image_embeddings

    def get_prompt_embeddings(
        self,
        input_points: tf.Tensor | None = None,
        input_labels: tf.Tensor | None = None,
        input_boxes: tf.Tensor | None = None,
        input_masks: tf.Tensor | None = None,
    ):
        r"""
        Returns the prompt embeddings by passing the input points, labels, boxes and masks through the prompt encoder.

        Args:
            input_points (`tf.Tensor` of shape `(batch_size, point_batch_size, num_points_per_image, 2)`):
                Optional input points for the prompt encoder. The padding of the point is automatically done by the
                processor. `point_batch_size` refers to the number of masks that we want the model to predict per
                point. The model will output `point_batch_size` times 3 masks in total.
            input_labels (`tf.Tensor` of shape `(batch_size, point_batch_size, num_points_per_image)`):
                Optional input labels for the prompt encoder. The padding of the labels is automatically done by the
                processor, or can be fed by the user.
            input_boxes (`tf.Tensor` of shape `(batch_size, num_boxes_per_image, 4)`):
                Optional input boxes for the prompt encoder. The padding of the boxes is automatically done by the
                processor. users can also pass manually the input boxes.
            input_masks (`tf.Tensor` of shape `(batch_size, image_size, image_size)`):
                Optional input masks for the prompt encoder.
        """
        # 使用提示编码器处理输入的点、标签、框和掩码，返回提示嵌入向量
        prompt_output = self.prompt_encoder(
            input_points=input_points,
            input_labels=input_labels,
            input_boxes=input_boxes,
            input_masks=input_masks,
        )
        return prompt_output

    @unpack_inputs
    @add_start_docstrings_to_model_forward(SAM_INPUTS_DOCSTRING)
    # 定义一个方法 `call`，用于执行模型推断或训练过程中的前向传播
    def call(
        self,
        pixel_values: TFModelInputType | None = None,
        input_points: tf.Tensor | None = None,
        input_labels: tf.Tensor | None = None,
        input_boxes: tf.Tensor | None = None,
        input_masks: tf.Tensor | None = None,
        image_embeddings: tf.Tensor | None = None,
        multimask_output: bool = True,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
        training: bool = False,
        **kwargs,
    ):
        # 如果 `serving_output` 方法定义了输出值 `output: TFSamImageSegmentationOutput`
        def serving_output(self, output: TFSamImageSegmentationOutput) -> TFSamImageSegmentationOutput:
            # 如果模型配置要求输出隐藏状态，则将视觉隐藏状态转换为 TensorFlow 张量，否则设为 None
            hs = tf.convert_to_tensor(output.vision_hidden_states) if self.config.output_hidden_states else None
            # 如果模型配置要求输出注意力权重，则将视觉注意力转换为 TensorFlow 张量，否则设为 None
            attns = tf.convert_to_tensor(output.vision_attentions) if self.config.output_attentions else None

            # 返回一个 `TFSamImageSegmentationOutput` 对象，根据模型配置决定是否包含隐藏状态和注意力权重
            return TFSamImageSegmentationOutput(
                iou_scores=output.iou_scores,
                pred_masks=output.pred_masks,
                vision_hidden_states=hs if self.config.output_hidden_states else None,
                vision_attentions=attns if self.config.output_attentions else None,
                mask_decoder_attentions=output.mask_decoder_attentions if self.config.output_attentions else None,
            )

        # 定义一个方法 `build`，用于构建模型
        def build(self, input_shape=None):
            # 如果模型已经构建完成，则直接返回
            if self.built:
                return
            # 将模型状态标记为已构建
            self.built = True
            # 如果模型具有 `shared_image_embedding` 属性，则构建共享图像嵌入
            if getattr(self, "shared_image_embedding", None) is not None:
                with tf.name_scope(self.shared_image_embedding.name):
                    self.shared_image_embedding.build(None)
            # 如果模型具有 `vision_encoder` 属性，则构建视觉编码器
            if getattr(self, "vision_encoder", None) is not None:
                with tf.name_scope(self.vision_encoder.name):
                    self.vision_encoder.build(None)
            # 如果模型具有 `prompt_encoder` 属性，则构建提示编码器
            if getattr(self, "prompt_encoder", None) is not None:
                with tf.name_scope(self.prompt_encoder.name):
                    self.prompt_encoder.build(None)
            # 如果模型具有 `mask_decoder` 属性，则构建掩码解码器
            if getattr(self, "mask_decoder", None) is not None:
                with tf.name_scope(self.mask_decoder.name):
                    self.mask_decoder.build(None)
```