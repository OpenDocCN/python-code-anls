# `.\transformers\tools\base.py`

```
#!/usr/bin/env python
# coding=utf-8

# ç‰ˆæƒå£°æ˜

# å¯¼å…¥æ¨¡å—å’Œåº“
import base64
import importlib
import inspect
import io
import json
import os
import tempfile
from typing import Any, Dict, List, Optional, Union

# ä» huggingface_hub æ¨¡å—ä¸­å¯¼å…¥å‡½æ•°
from huggingface_hub import create_repo, hf_hub_download, metadata_update, upload_folder
from huggingface_hub.utils import RepositoryNotFoundError, build_hf_headers, get_session

# ä»è‡ªå®šä¹‰çš„æ¨¡å—ä¸­å¯¼å…¥å‡½æ•°å’Œç±»
from ..dynamic_module_utils import custom_object_save, get_class_from_dynamic_module, get_imports
from ..image_utils import is_pil_image
from ..models.auto import AutoProcessor
from ..utils import (
    CONFIG_NAME,
    cached_file,
    is_accelerate_available,
    is_torch_available,
    is_vision_available,
    logging,
)

# ä» gradio æ¨¡å—ä¸­å¯¼å…¥å‡½æ•°
from .agent_types import handle_agent_inputs, handle_agent_outputs


# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# å¦‚æœ Torch å¯ç”¨åˆ™å¯¼å…¥ Torch
if is_torch_available():
    import torch

# å¦‚æœ Accelerate å¯ç”¨åˆ™å¯¼å…¥ send_to_device å‡½æ•°
if is_accelerate_available():
    from accelerate.utils import send_to_device

# å·¥å…·é…ç½®æ–‡ä»¶å
TOOL_CONFIG_FILE = "tool_config.json"

# è·å– repo ç±»å‹çš„å‡½æ•°
def get_repo_type(repo_id, repo_type=None, **hub_kwargs):
    if repo_type is not None:
        return repo_type
    try:
        # å°è¯•ä¸‹è½½ space ç±»å‹çš„ä»“åº“ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› model ç±»å‹
        hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type="space", **hub_kwargs)
        return "space"
    except RepositoryNotFoundError:
        try:
            # å°è¯•ä¸‹è½½ model ç±»å‹çš„ä»“åº“ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æŠ›å‡ºé”™è¯¯
            hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type="model", **hub_kwargs)
            return "model"
        except RepositoryNotFoundError:
            raise EnvironmentError(f"`{repo_id}` does not seem to be a valid repo identifier on the Hub.")
        except Exception:
            return "model"
    except Exception:
        return "space"


# æ ¹æ®æ¨¡æ¿ç”Ÿæˆåº”ç”¨æ–‡ä»¶å†…å®¹çš„å‡½æ•°
APP_FILE_TEMPLATE = """from transformers import launch_gradio_demo
from {module_name} import {class_name}

launch_gradio_demo({class_name})
"""


# å·¥å…·ç±»
class Tool:
    """
    A base class for the functions used by the agent. Subclass this and implement the `__call__` method as well as the
    following class attributes:

    - **description** (`str`) -- A short description of what your tool does, the inputs it expects and the output(s) it
      will return. For instance 'This is a tool that downloads a file from a `url`. It takes the `url` as input, and
      returns the text contained in the file'.
    """
    # å®šä¹‰ Tool ç±»ï¼Œç”¨äºåˆ›å»ºå·¥å…·
    class Tool:
        """
        Tool ç±»æ˜¯ç”¨äºåˆ›å»ºå·¥å…·çš„åŸºç±»ï¼Œå¯ä»¥ç»§æ‰¿å¹¶å®šåˆ¶åŒ–è‡ªå·±çš„å·¥å…·ã€‚
    
        Args:
            name (str): å·¥å…·çš„åç§°ï¼Œåœ¨æç¤ºä¸­å‘ä»£ç†å±•ç¤ºçš„åç§°ï¼Œä¾‹å¦‚ "text-classifier" æˆ– "image_generator"ã€‚
            inputs (List[str]): è¾“å…¥æ•°æ®çš„æ¨¡æ€åˆ—è¡¨ï¼ˆæŒ‰ç…§è°ƒç”¨ä¸­çš„é¡ºåºï¼‰ã€‚æ¨¡æ€åº”ä¸º "text"ã€"image" æˆ– "audio"ã€‚ä»…ç”¨äº `launch_gradio_demo` æˆ–ä½¿æ‚¨çš„å·¥å…·æœ‰è‰¯å¥½çš„æ’ç‰ˆã€‚
            outputs (List[str]): å·¥å…·è¿”å›çš„æ¨¡æ€åˆ—è¡¨ï¼ˆä¸è°ƒç”¨æ–¹æ³•çš„è¿”å›é¡ºåºç›¸åŒï¼‰ã€‚æ¨¡æ€åº”ä¸º "text"ã€"image" æˆ– "audio"ã€‚ä»…ç”¨äº `launch_gradio_demo` æˆ–ä½¿æ‚¨çš„å·¥å…·æœ‰è‰¯å¥½çš„æ’ç‰ˆã€‚
    
        æ‚¨è¿˜å¯ä»¥é‡å†™æ–¹æ³• [`~Tool.setup`]ï¼Œå¦‚æœæ‚¨çš„å·¥å…·åœ¨å¯ç”¨ä¹‹å‰æœ‰æ˜‚è´µçš„æ“ä½œè¦æ‰§è¡Œï¼ˆä¾‹å¦‚åŠ è½½æ¨¡å‹ï¼‰ã€‚[`~Tool.setup`] å°†åœ¨é¦–æ¬¡ä½¿ç”¨å·¥å…·æ—¶è°ƒç”¨ï¼Œä½†ä¸ä¼šåœ¨å®ä¾‹åŒ–æ—¶è°ƒç”¨ã€‚
        """
    
        # æè¿°å·¥å…·çš„å±æ€§
        description: str = "This is a tool that ..."
        # å·¥å…·çš„åç§°å±æ€§
        name: str = ""
    
        # è¾“å…¥æ•°æ®æ¨¡æ€åˆ—è¡¨å±æ€§
        inputs: List[str]
        # è¾“å‡ºæ•°æ®æ¨¡æ€åˆ—è¡¨å±æ€§
        outputs: List[str]
    
        # åˆå§‹åŒ–æ–¹æ³•
        def __init__(self, *args, **kwargs):
            # æ ‡è®°å·¥å…·æ˜¯å¦å·²åˆå§‹åŒ–
            self.is_initialized = False
    
        # è°ƒç”¨æ–¹æ³•ï¼Œéœ€åœ¨ Tool çš„å­ç±»ä¸­å®ç°
        def __call__(self, *args, **kwargs):
            # è¿”å›æœªå®ç°é”™è¯¯ï¼Œæç¤ºåœ¨ Tool çš„å­ç±»ä¸­å®ç°è¯¥æ–¹æ³•
            return NotImplemented("Write this method in your subclass of `Tool`.")
    
        # è®¾ç½®æ–¹æ³•ï¼Œç”¨äºæ‰§è¡Œæ˜‚è´µæ“ä½œï¼Œéœ€åœ¨ Tool çš„å­ç±»ä¸­é‡å†™
        def setup(self):
            """
            åœ¨è¿™é‡Œé‡å†™æ­¤æ–¹æ³•ï¼Œç”¨äºæ‰§è¡Œåœ¨å¼€å§‹ä½¿ç”¨å·¥å…·ä¹‹å‰éœ€è¦æ‰§è¡Œçš„ä»»ä½•æ˜‚è´µæ“ä½œã€‚ä¾‹å¦‚åŠ è½½å¤§å‹æ¨¡å‹ã€‚
            """
            # æ ‡è®°å·¥å…·å·²åˆå§‹åŒ–
            self.is_initialized = True
    def save(self, output_dir):
        """
        Saves the relevant code files for your tool so it can be pushed to the Hub. This will copy the code of your
        tool in `output_dir` as well as autogenerate:

        - a config file named `tool_config.json`
        - an `app.py` file so that your tool can be converted to a space
        - a `requirements.txt` containing the names of the module used by your tool (as detected when inspecting its
          code)

        You should only use this method to save tools that are defined in a separate module (not `__main__`).

        Args:
            output_dir (`str`): The folder in which you want to save your tool.
        """
        # Create the output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Save module file
        if self.__module__ == "__main__":
            # Raise an error if the tool is defined in __main__ module
            raise ValueError(
                f"We can't save the code defining {self} in {output_dir} as it's been defined in __main__. You "
                "have to put this code in a separate module so we can include it in the saved folder."
            )
        # Save the custom object to the output directory
        module_files = custom_object_save(self, output_dir)

        # Get the module name and class name
        module_name = self.__class__.__module__
        last_module = module_name.split(".")[-1]
        full_name = f"{last_module}.{self.__class__.__name__}"

        # Save config file
        config_file = os.path.join(output_dir, "tool_config.json")
        if os.path.isfile(config_file):
            with open(config_file, "r", encoding="utf-8") as f:
                tool_config = json.load(f)
        else:
            tool_config = {}

        # Update the tool config with class information
        tool_config = {"tool_class": full_name, "description": self.description, "name": self.name}
        with open(config_file, "w", encoding="utf-8") as f:
            f.write(json.dumps(tool_config, indent=2, sort_keys=True) + "\n")

        # Save app file
        app_file = os.path.join(output_dir, "app.py")
        with open(app_file, "w", encoding="utf-8") as f:
            f.write(APP_FILE_TEMPLATE.format(module_name=last_module, class_name=self.__class__.__name__))

        # Save requirements file
        requirements_file = os.path.join(output_dir, "requirements.txt")
        imports = []
        for module in module_files:
            imports.extend(get_imports(module))
        imports = list(set(imports))
        with open(requirements_file, "w", encoding="utf-8") as f:
            f.write("\n".join(imports) + "\n")

    @classmethod
    def from_hub(
        cls,
        repo_id: str,
        model_repo_id: Optional[str] = None,
        token: Optional[str] = None,
        remote: bool = False,
        **kwargs,
    def push_to_hub(
        self,
        repo_id: str,
        commit_message: str = "Upload tool",
        private: Optional[bool] = None,
        token: Optional[Union[bool, str]] = None,
        create_pr: bool = False,
    ) -> str:
        """
        Upload the tool to the Hub.

        Parameters:
            repo_id (`str`):
                The name of the repository you want to push your tool to. It should contain your organization name when
                pushing to a given organization.
            commit_message (`str`, *optional*, defaults to `"Upload tool"`):
                Message to commit while pushing.
            private (`bool`, *optional`):
                Whether or not the repository created should be private.
            token (`bool` or `str`, *optional`):
                The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated
                when running `huggingface-cli login` (stored in `~/.huggingface`).
            create_pr (`bool`, *optional`, defaults to `False`):
                Whether or not to create a PR with the uploaded files or directly commit.
        """
        # åˆ›å»ºä»“åº“å¹¶è¿”å›ä»“åº“ URL
        repo_url = create_repo(
            repo_id=repo_id, token=token, private=private, exist_ok=True, repo_type="space", space_sdk="gradio"
        )
        # æ›´æ–°ä»“åº“å…ƒæ•°æ®
        repo_id = repo_url.repo_id
        metadata_update(repo_id, {"tags": ["tool"]}, repo_type="space")

        # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¿å­˜æ‰€æœ‰æ–‡ä»¶
        with tempfile.TemporaryDirectory() as work_dir:
            # ä¿å­˜æ‰€æœ‰æ–‡ä»¶
            self.save(work_dir)
            logger.info(f"Uploading the following files to {repo_id}: {','.join(os.listdir(work_dir))}")
            # ä¸Šä¼ æ–‡ä»¶å¤¹åˆ°ä»“åº“
            return upload_folder(
                repo_id=repo_id,
                commit_message=commit_message,
                folder_path=work_dir,
                token=token,
                create_pr=create_pr,
                repo_type="space",
            )

    @staticmethod
    def from_gradio(gradio_tool):
        """
        Creates a [`Tool`] from a gradio tool.
        """

        class GradioToolWrapper(Tool):
            def __init__(self, _gradio_tool):
                super().__init__()
                self.name = _gradio_tool.name
                self.description = _gradio_tool.description

        # å°† Gradio å·¥å…·åŒ…è£…ä¸º Tool ç±»
        GradioToolWrapper.__call__ = gradio_tool.run
        return GradioToolWrapper(gradio_tool)
class RemoteTool(Tool):
    """
    A [`Tool`] that will make requests to an inference endpoint.

    Args:
        endpoint_url (`str`, *optional*):
            The url of the endpoint to use.
        token (`str`, *optional*):
            The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when
            running `huggingface-cli login` (stored in `~/.huggingface`).
        tool_class (`type`, *optional*):
            The corresponding `tool_class` if this is a remote version of an existing tool. Will help determine when
            the output should be converted to another type (like images).
    """

    def __init__(self, endpoint_url=None, token=None, tool_class=None):
        # åˆå§‹åŒ– RemoteTool ç±»ï¼Œè®¾ç½®å±æ€§å€¼
        self.endpoint_url = endpoint_url
        # åˆ›å»º EndpointClient å¯¹è±¡ï¼Œç”¨äºä¸æŒ‡å®šçš„ç«¯ç‚¹è¿›è¡Œé€šä¿¡
        self.client = EndpointClient(endpoint_url, token=token)
        # è®¾ç½® tool_class å±æ€§
        self.tool_class = tool_class

    def prepare_inputs(self, *args, **kwargs):
        """
        Prepare the inputs received for the HTTP client sending data to the endpoint. Positional arguments will be
        matched with the signature of the `tool_class` if it was provided at instantation. Images will be encoded into
        bytes.

        You can override this method in your custom class of [`RemoteTool`].
        """
        # å¤åˆ¶å…³é”®å­—å‚æ•°
        inputs = kwargs.copy()
        # å¤„ç†ä½ç½®å‚æ•°
        if len(args) > 0:
            if self.tool_class is not None:
                # åŒ¹é…å‚æ•°ä¸ç­¾å
                if issubclass(self.tool_class, PipelineTool):
                    call_method = self.tool_class.encode
                else:
                    call_method = self.tool_class.__call__
                signature = inspect.signature(call_method).parameters
                parameters = [
                    k
                    for k, p in signature.items()
                    if p.kind not in [inspect._ParameterKind.VAR_POSITIONAL, inspect._ParameterKind.VAR_KEYWORD]
                ]
                if parameters[0] == "self":
                    parameters = parameters[1:]
                if len(args) > len(parameters):
                    raise ValueError(
                        f"{self.tool_class} only accepts {len(parameters)} arguments but {len(args)} were given."
                    )
                for arg, name in zip(args, parameters):
                    inputs[name] = arg
            elif len(args) > 1:
                raise ValueError("A `RemoteTool` can only accept one positional input.")
            elif len(args) == 1:
                if is_pil_image(args[0]):
                    return {"inputs": self.client.encode_image(args[0])}
                return {"inputs": args[0]}

        # å¤„ç†è¾“å…¥ä¸­çš„å›¾åƒæ•°æ®
        for key, value in inputs.items():
            if is_pil_image(value):
                inputs[key] = self.client.encode_image(value)

        return {"inputs": inputs}
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºæå–è¾“å‡ºç»“æœ
    def extract_outputs(self, outputs):
        """
        You can override this method in your custom class of [`RemoteTool`] to apply some custom post-processing of the
        outputs of the endpoint.
        """
        # è¿”å›åŸå§‹è¾“å‡ºç»“æœ
        return outputs

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºè°ƒç”¨è¿œç¨‹å·¥å…·
    def __call__(self, *args, **kwargs):
        # å¤„ç†è¾“å…¥å‚æ•°
        args, kwargs = handle_agent_inputs(*args, **kwargs)

        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸ºå›¾åƒ
        output_image = self.tool_class is not None and self.tool_class.outputs == ["image"]
        # å‡†å¤‡è¾“å…¥æ•°æ®
        inputs = self.prepare_inputs(*args, **kwargs)
        # æ ¹æ®è¾“å…¥æ•°æ®è°ƒç”¨å®¢æˆ·ç«¯
        if isinstance(inputs, dict):
            outputs = self.client(**inputs, output_image=output_image)
        else:
            outputs = self.client(inputs, output_image=output_image)
        # å¤„ç†å¤šå±‚åµŒå¥—çš„è¾“å‡ºç»“æœ
        if isinstance(outputs, list) and len(outputs) == 1 and isinstance(outputs[0], list):
            outputs = outputs[0]

        # å¤„ç†è¾“å‡ºç»“æœ
        outputs = handle_agent_outputs(outputs, self.tool_class.outputs if self.tool_class is not None else None)

        # æå–è¾“å‡ºç»“æœ
        return self.extract_outputs(outputs)
class PipelineTool(Tool):
    """
    A [`Tool`] tailored towards Transformer models. On top of the class attributes of the base class [`Tool`], you will
    need to specify:

    - **model_class** (`type`) -- The class to use to load the model in this tool.
    - **default_checkpoint** (`str`) -- The default checkpoint that should be used when the user doesn't specify one.
    - **pre_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the
      pre-processor
    - **post_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the
      post-processor (when different from the pre-processor).

    Args:
        model (`str` or [`PreTrainedModel`], *optional*):
            The name of the checkpoint to use for the model, or the instantiated model. If unset, will default to the
            value of the class attribute `default_checkpoint`.
        pre_processor (`str` or `Any`, *optional*):
            The name of the checkpoint to use for the pre-processor, or the instantiated pre-processor (can be a
            tokenizer, an image processor, a feature extractor or a processor). Will default to the value of `model` if
            unset.
        post_processor (`str` or `Any`, *optional*):
            The name of the checkpoint to use for the post-processor, or the instantiated pre-processor (can be a
            tokenizer, an image processor, a feature extractor or a processor). Will default to the `pre_processor` if
            unset.
        device (`int`, `str` or `torch.device`, *optional*):
            The device on which to execute the model. Will default to any accelerator available (GPU, MPS etc...), the
            CPU otherwise.
        device_map (`str` or `dict`, *optional*):
            If passed along, will be used to instantiate the model.
        model_kwargs (`dict`, *optional*):
            Any keyword argument to send to the model instantiation.
        token (`str`, *optional*):
            The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when
            running `huggingface-cli login` (stored in `~/.huggingface`).
        hub_kwargs (additional keyword arguments, *optional*):
            Any additional keyword argument to send to the methods that will load the data from the Hub.
    """

    # é»˜è®¤ä½¿ç”¨ AutoProcessor ç±»ä½œä¸ºé¢„å¤„ç†å™¨
    pre_processor_class = AutoProcessor
    # æ¨¡å‹ç±»éœ€è¦åœ¨å­ç±»ä¸­æŒ‡å®š
    model_class = None
    # é»˜è®¤æ£€æŸ¥ç‚¹ä¸ºç©º
    post_processor_class = AutoProcessor
    # é»˜è®¤ï¿½ï¿½æŸ¥ç‚¹ä¸ºç©º

    def __init__(
        self,
        model=None,
        pre_processor=None,
        post_processor=None,
        device=None,
        device_map=None,
        model_kwargs=None,
        token=None,
        **hub_kwargs,
        ):
        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† torch åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™æŠ›å‡º ImportError å¼‚å¸¸
        if not is_torch_available():
            raise ImportError("Please install torch in order to use this tool.")

        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† accelerate åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™æŠ›å‡º ImportError å¼‚å¸¸
        if not is_accelerate_available():
            raise ImportError("Please install accelerate in order to use this tool.")

        # å¦‚æœæœªä¼ å…¥æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ£€æŸ¥ç‚¹ï¼Œå¦‚æœé»˜è®¤æ£€æŸ¥ç‚¹ä¹Ÿæœªè®¾ç½®ï¼Œåˆ™æŠ›å‡º ValueError å¼‚å¸¸
        if model is None:
            if self.default_checkpoint is None:
                raise ValueError("This tool does not implement a default checkpoint, you need to pass one.")
            model = self.default_checkpoint
        # å¦‚æœæœªä¼ å…¥é¢„å¤„ç†å™¨ï¼Œåˆ™ä½¿ç”¨æ¨¡å‹ä½œä¸ºé¢„å¤„ç†å™¨
        if pre_processor is None:
            pre_processor = model

        # åˆå§‹åŒ–æ¨¡å‹ã€é¢„å¤„ç†å™¨ã€åå¤„ç†å™¨ã€è®¾å¤‡ã€è®¾å¤‡æ˜ å°„ã€æ¨¡å‹å‚æ•°ç­‰å±æ€§
        self.model = model
        self.pre_processor = pre_processor
        self.post_processor = post_processor
        self.device = device
        self.device_map = device_map
        self.model_kwargs = {} if model_kwargs is None else model_kwargs
        # å¦‚æœè®¾å¤‡æ˜ å°„ä¸ä¸ºç©ºï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°æ¨¡å‹å‚æ•°ä¸­
        if device_map is not None:
            self.model_kwargs["device_map"] = device_map
        self.hub_kwargs = hub_kwargs
        # å°† token æ·»åŠ åˆ° hub_kwargs ä¸­
        self.hub_kwargs["token"] = token

        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()

    def setup(self):
        """
        Instantiates the `pre_processor`, `model` and `post_processor` if necessary.
        """
        # å¦‚æœé¢„å¤„ç†å™¨æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™æ ¹æ®å­—ç¬¦ä¸²å®ä¾‹åŒ–é¢„å¤„ç†å™¨ç±»
        if isinstance(self.pre_processor, str):
            self.pre_processor = self.pre_processor_class.from_pretrained(self.pre_processor, **self.hub_kwargs)

        # å¦‚æœæ¨¡å‹æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™æ ¹æ®å­—ç¬¦ä¸²å®ä¾‹åŒ–æ¨¡å‹ç±»
        if isinstance(self.model, str):
            self.model = self.model_class.from_pretrained(self.model, **self.model_kwargs, **self.hub_kwargs)

        # å¦‚æœåå¤„ç†å™¨ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨é¢„å¤„ç†å™¨ä½œä¸ºåå¤„ç†å™¨ï¼›å¦‚æœåå¤„ç†å™¨æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™æ ¹æ®å­—ç¬¦ä¸²å®ä¾‹åŒ–åå¤„ç†å™¨ç±»
        if self.post_processor is None:
            self.post_processor = self.pre_processor
        elif isinstance(self.post_processor, str):
            self.post_processor = self.post_processor_class.from_pretrained(self.post_processor, **self.hub_kwargs)

        # å¦‚æœè®¾å¤‡ä¸ºç©ºï¼Œåˆ™æ ¹æ®è®¾å¤‡æ˜ å°„è®¾ç½®è®¾å¤‡ï¼›å¦‚æœè®¾å¤‡æ˜ å°„ä¸ºç©ºï¼Œåˆ™è·å–é»˜è®¤è®¾å¤‡
        if self.device is None:
            if self.device_map is not None:
                self.device = list(self.model.hf_device_map.values())[0]
            else:
                self.device = get_default_device()

        # å¦‚æœè®¾å¤‡æ˜ å°„ä¸ºç©ºï¼Œåˆ™å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device_map is None:
            self.model.to(self.device)

        # è°ƒç”¨çˆ¶ç±»çš„è®¾ç½®æ–¹æ³•
        super().setup()

    def encode(self, raw_inputs):
        """
        Uses the `pre_processor` to prepare the inputs for the `model`.
        """
        # ä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†åŸå§‹è¾“å…¥æ•°æ®
        return self.pre_processor(raw_inputs)

    def forward(self, inputs):
        """
        Sends the inputs through the `model`.
        """
        # ä½¿ç”¨æ¨¡å‹å¤„ç†è¾“å…¥æ•°æ®
        with torch.no_grad():
            return self.model(**inputs)

    def decode(self, outputs):
        """
        Uses the `post_processor` to decode the model output.
        """
        # ä½¿ç”¨åå¤„ç†å™¨è§£ç æ¨¡å‹è¾“å‡º
        return self.post_processor(outputs)
    # å®šä¹‰ä¸€ä¸ªç‰¹æ®Šæ–¹æ³•ï¼Œç”¨äºå®ä¾‹å¯¹è±¡çš„è°ƒç”¨
    def __call__(self, *args, **kwargs):
        # å¤„ç†è¾“å…¥å‚æ•°ï¼Œç¡®ä¿å‚æ•°æ ¼å¼æ­£ç¡®
        args, kwargs = handle_agent_inputs(*args, **kwargs)

        # å¦‚æœAgentå¯¹è±¡è¿˜æœªåˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–
        if not self.is_initialized:
            self.setup()

        # å¯¹è¾“å…¥å‚æ•°è¿›è¡Œç¼–ç å¤„ç†
        encoded_inputs = self.encode(*args, **kwargs)
        # å°†ç¼–ç åçš„è¾“å…¥å‘é€åˆ°æŒ‡å®šè®¾å¤‡
        encoded_inputs = send_to_device(encoded_inputs, self.device)
        # å¯¹ç¼–ç åçš„è¾“å…¥è¿›è¡Œå‰å‘ä¼ æ’­
        outputs = self.forward(encoded_inputs)
        # å°†è¾“å‡ºå‘é€å›CPUè®¾å¤‡
        outputs = send_to_device(outputs, "cpu")
        # å¯¹è¾“å‡ºè¿›è¡Œè§£ç å¤„ç†
        decoded_outputs = self.decode(outputs)

        # å¤„ç†Agentå¯¹è±¡çš„è¾“å‡ºç»“æœï¼Œå¹¶è¿”å›
        return handle_agent_outputs(decoded_outputs, self.outputs)
# å¯åŠ¨ä¸€ä¸ª gradio æ¼”ç¤ºå·¥å…·ï¼Œéœ€è¦ä¼ å…¥ä¸€ä¸ªå·¥å…·ç±»ã€‚å·¥å…·ç±»éœ€è¦æ­£ç¡®å®ç°ç±»å±æ€§ `inputs` å’Œ `outputs`ã€‚
def launch_gradio_demo(tool_class: Tool):
    try:
        # å°è¯•å¯¼å…¥ gradio åº“
        import gradio as gr
    except ImportError:
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡º ImportError
        raise ImportError("Gradio should be installed in order to launch a gradio demo.")

    # åˆ›å»ºæŒ‡å®šå·¥å…·ç±»çš„å®ä¾‹
    tool = tool_class()

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè°ƒç”¨å·¥å…·ç±»çš„å®ä¾‹
    def fn(*args, **kwargs):
        return tool(*args, **kwargs)

    # åˆ›å»º gr.Interface å¯¹è±¡ï¼Œä¼ å…¥å‡½æ•°ã€è¾“å…¥ã€è¾“å‡ºã€æ ‡é¢˜å’Œæè¿°ï¼Œç„¶åå¯åŠ¨æ¼”ç¤º
    gr.Interface(
        fn=fn,
        inputs=tool_class.inputs,
        outputs=tool_class.outputs,
        title=tool_class.__name__,
        article=tool.description,
    ).launch()


# TODO: Migrate to Accelerate for this once `PartialState.default_device` makes its way into a release.
# è·å–é»˜è®¤è®¾å¤‡çš„å‡½æ•°ï¼Œå³è¿”å›å½“å‰å¯ç”¨çš„è®¾å¤‡
def get_default_device():
    logger.warning(
        "`get_default_device` is deprecated and will be replaced with `accelerate`'s `PartialState().default_device` "
        "in version 4.38 of ğŸ¤— Transformers. "
    )
    # å¦‚æœæ²¡æœ‰å®‰è£… torch åº“ï¼ŒæŠ›å‡º ImportError
    if not is_torch_available():
        raise ImportError("Please install torch in order to use this tool.")

    # æ£€æŸ¥æ˜¯å¦æ”¯æŒ MPS æˆ– CUDAï¼Œè¿”å›ç›¸åº”çš„è®¾å¤‡
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")


# æ˜ å°„ä»»åŠ¡åç§°åˆ°å·¥å…·ç±»çš„å­—å…¸
TASK_MAPPING = {
    "document-question-answering": "DocumentQuestionAnsweringTool",
    "image-captioning": "ImageCaptioningTool",
    "image-question-answering": "ImageQuestionAnsweringTool",
    "image-segmentation": "ImageSegmentationTool",
    "speech-to-text": "SpeechToTextTool",
    "summarization": "TextSummarizationTool",
    "text-classification": "TextClassificationTool",
    "text-question-answering": "TextQuestionAnsweringTool",
    "text-to-speech": "TextToSpeechTool",
    "translation": "TranslationTool",
}


# è·å–é»˜è®¤çš„ç«¯ç‚¹é…ç½®
def get_default_endpoints():
    # ä»ç¼“å­˜æ–‡ä»¶ä¸­è¯»å–é»˜è®¤ç«¯ç‚¹é…ç½®
    endpoints_file = cached_file("huggingface-tools/default-endpoints", "default_endpoints.json", repo_type="dataset")
    with open(endpoints_file, "r", encoding="utf-8") as f:
        endpoints = json.load(f)
    return endpoints


# æ£€æŸ¥ä»»åŠ¡æˆ–ä»“åº“ ID æ˜¯å¦æ”¯æŒè¿œç¨‹åŠ è½½
def supports_remote(task_or_repo_id):
    # è·å–é»˜è®¤ç«¯ç‚¹é…ç½®
    endpoints = get_default_endpoints()
    return task_or_repo_id in endpoints


# åŠ è½½å·¥å…·çš„ä¸»è¦å‡½æ•°ï¼Œå¯ä»¥åœ¨ Hub æˆ– Transformers åº“ä¸­å¿«é€ŸåŠ è½½å·¥å…·
def load_tool(task_or_repo_id, model_repo_id=None, remote=False, token=None, **kwargs):
    Args:
        task_or_repo_id (`str`):
            è¦åŠ è½½å·¥å…·çš„ä»»åŠ¡æˆ– Hub ä¸Šå·¥å…·çš„å­˜å‚¨åº“ IDã€‚åœ¨ Transformers ä¸­å®ç°çš„ä»»åŠ¡æœ‰ï¼š

            - `"document-question-answering"`
            - `"image-captioning"`
            - `"image-question-answering"`
            - `"image-segmentation"`
            - `"speech-to-text"`
            - `"summarization"`
            - `"text-classification"`
            - `"text-question-answering"`
            - `"text-to-speech"`
            - `"translation"`

        model_repo_id (`str`, *å¯é€‰*):
            ä½¿ç”¨æ­¤å‚æ•°å¯ä»¥ä½¿ç”¨ä¸æ‰€é€‰å·¥å…·çš„é»˜è®¤æ¨¡å‹ä¸åŒçš„æ¨¡å‹ã€‚
        remote (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`):
            æ˜¯å¦é€šè¿‡ä¸‹è½½æ¨¡å‹æˆ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰ä½¿ç”¨æ¨ç†ç«¯ç‚¹æ¥ä½¿ç”¨æ‚¨çš„å·¥å…·ã€‚
        token (`str`, *å¯é€‰*):
            ç”¨äºåœ¨ hf.co ä¸Šè¯†åˆ«æ‚¨çš„ä»¤ç‰Œã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä½¿ç”¨è¿è¡Œ `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚
        kwargs (å…¶ä»–å…³é”®å­—å‚æ•°, *å¯é€‰*):
            å°†è¢«æ‹†åˆ†ä¸ºä¸¤éƒ¨åˆ†çš„å…¶ä»–å…³é”®å­—å‚æ•°ï¼šæ‰€æœ‰ä¸ Hub ç›¸å…³çš„å‚æ•°ï¼ˆå¦‚ `cache_dir`ã€`revision`ã€`subfolder`ï¼‰å°†åœ¨ä¸‹è½½å·¥å…·æ–‡ä»¶æ—¶ä½¿ç”¨ï¼Œå…¶ä»–å‚æ•°å°†ä¼ é€’ç»™å…¶åˆå§‹åŒ–ã€‚
    """
    if task_or_repo_id in TASK_MAPPING:
        tool_class_name = TASK_MAPPING[task_or_repo_id]
        main_module = importlib.import_module("transformers")
        tools_module = main_module.tools
        tool_class = getattr(tools_module, tool_class_name)

        if remote:
            if model_repo_id is None:
                endpoints = get_default_endpoints()
                if task_or_repo_id not in endpoints:
                    raise ValueError(
                        f"Could not infer a default endpoint for {task_or_repo_id}, you need to pass one using the "
                        "`model_repo_id` argument."
                    )
                model_repo_id = endpoints[task_or_repo_id]
            return RemoteTool(model_repo_id, token=token, tool_class=tool_class)
        else:
            return tool_class(model_repo_id, token=token, **kwargs)
    else:
        return Tool.from_hub(task_or_repo_id, model_repo_id=model_repo_id, token=token, remote=remote, **kwargs)
# å®šä¹‰ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºä¸ºå‡½æ•°æ·»åŠ æè¿°
def add_description(description):
    """
    A decorator that adds a description to a function.
    """

    def inner(func):
        # å°†æè¿°ä¿¡æ¯æ·»åŠ åˆ°å‡½æ•°å¯¹è±¡çš„å±æ€§ä¸­
        func.description = description
        func.name = func.__name__
        return func

    return inner


## Will move to the Hub
# å®šä¹‰ä¸€ä¸ª EndpointClient ç±»
class EndpointClient:
    def __init__(self, endpoint_url: str, token: Optional[str] = None):
        # æ„å»ºè¯·æ±‚å¤´ä¿¡æ¯
        self.headers = {**build_hf_headers(token=token), "Content-Type": "application/json"}
        self.endpoint_url = endpoint_url

    @staticmethod
    # å°†å›¾åƒç¼–ç ä¸º base64 æ ¼å¼
    def encode_image(image):
        _bytes = io.BytesIO()
        image.save(_bytes, format="PNG")
        b64 = base64.b64encode(_bytes.getvalue())
        return b64.decode("utf-8")

    @staticmethod
    # å°† base64 æ ¼å¼çš„å›¾åƒè§£ç ä¸ºå›¾åƒå¯¹è±¡
    def decode_image(raw_image):
        if not is_vision_available():
            raise ImportError(
                "This tool returned an image but Pillow is not installed. Please install it (`pip install Pillow`)."
            )

        from PIL import Image

        b64 = base64.b64decode(raw_image)
        _bytes = io.BytesIO(b64)
        return Image.open(_bytes)

    def __call__(
        self,
        inputs: Optional[Union[str, Dict, List[str], List[List[str]]] = None,
        params: Optional[Dict] = None,
        data: Optional[bytes] = None,
        output_image: bool = False,
    ) -> Any:
        # æ„å»ºè¯·æ±‚çš„ payload
        payload = {}
        if inputs:
            payload["inputs"] = inputs
        if params:
            payload["parameters"] = params

        # å‘èµ· API è°ƒç”¨
        response = get_session().post(self.endpoint_url, headers=self.headers, json=payload, data=data)

        # é»˜è®¤æƒ…å†µä¸‹ï¼Œè§£æå“åº”å¹¶è¿”å›ç»™ç”¨æˆ·
        if output_image:
            return self.decode_image(response.content)
        else:
            return response.json()
```