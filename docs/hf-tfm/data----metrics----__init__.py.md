# `.\transformers\data\metrics\__init__.py`

```
# å¯¼å…¥è­¦å‘Šæ¨¡å—
import warnings
# å¯¼å…¥æ£€æŸ¥Scikit-learnæ˜¯å¦å¯ç”¨çš„å‡½æ•°å’Œåç«¯éœ€æ±‚æ£€æŸ¥å‡½æ•°
from ...utils import is_sklearn_available, requires_backends

# å¦‚æœScikit-learnå¯ç”¨ï¼Œåˆ™å¯¼å…¥ç›¸å…³å‡½æ•°
if is_sklearn_available():
    from scipy.stats import pearsonr, spearmanr
    from sklearn.metrics import f1_score, matthews_corrcoef

# è­¦å‘Šæ¶ˆæ¯ï¼Œæç¤ºå³å°†ç§»é™¤è¯¥åº¦é‡æ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨ğŸ¤— Evaluateåº“
DEPRECATION_WARNING = (
    "This metric will be removed from the library soon, metrics should be handled with the ğŸ¤— Evaluate "
    "library. You can have a look at this example script for pointers: "
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py"
)

# ç®€å•å‡†ç¡®åº¦åº¦é‡å‡½æ•°
def simple_accuracy(preds, labels):
    # å‘å‡ºå³å°†å¼ƒç”¨è­¦å‘Š
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    # æ£€æŸ¥å¹¶è¦æ±‚Scikit-learnå¯ç”¨
    requires_backends(simple_accuracy, "sklearn")
    # è®¡ç®—ç®€å•å‡†ç¡®åº¦
    return (preds == labels).mean()

# å‡†ç¡®åº¦å’ŒF1åº¦é‡å‡½æ•°
def acc_and_f1(preds, labels):
    # å‘å‡ºå³å°†å¼ƒç”¨è­¦å‘Š
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    # æ£€æŸ¥å¹¶è¦æ±‚Scikit-learnå¯ç”¨
    requires_backends(acc_and_f1, "sklearn")
    # è®¡ç®—å‡†ç¡®åº¦
    acc = simple_accuracy(preds, labels)
    # è®¡ç®—F1åˆ†æ•°
    f1 = f1_score(y_true=labels, y_pred=preds)
    # è¿”å›å‡†ç¡®åº¦ã€F1åˆ†æ•°å’ŒäºŒè€…å¹³å‡å€¼
    return {
        "acc": acc,
        "f1": f1,
        "acc_and_f1": (acc + f1) / 2,
    }

# çš®å°”é€Šç›¸å…³ç³»æ•°å’Œæ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°åº¦é‡å‡½æ•°
def pearson_and_spearman(preds, labels):
    # å‘å‡ºå³å°†å¼ƒç”¨è­¦å‘Š
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    # æ£€æŸ¥å¹¶è¦æ±‚Scikit-learnå¯ç”¨
    requires_backends(pearson_and_spearman, "sklearn")
    # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
    pearson_corr = pearsonr(preds, labels)[0]
    # è®¡ç®—æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°
    spearman_corr = spearmanr(preds, labels)[0]
    # è¿”å›çš®å°”é€Šç›¸å…³ç³»æ•°ã€æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°å’ŒäºŒè€…å¹³å‡å€¼
    return {
        "pearson": pearson_corr,
        "spearmanr": spearman_corr,
        "corr": (pearson_corr + spearman_corr) / 2,
    }

# GLUEä»»åŠ¡çš„åº¦é‡å‡½æ•°
def glue_compute_metrics(task_name, preds, labels):
    # å‘å‡ºå³å°†å¼ƒç”¨è­¦å‘Š
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    # æ£€æŸ¥å¹¶è¦æ±‚Scikit-learnå¯ç”¨
    requires_backends(glue_compute_metrics, "sklearn")
    # æ£€æŸ¥é¢„æµ‹å€¼å’Œæ ‡ç­¾å€¼é•¿åº¦æ˜¯å¦ä¸€è‡´
    assert len(preds) == len(labels), f"Predictions and labels have mismatched lengths {len(preds)} and {len(labels)}"
    # æ ¹æ®ä»»åŠ¡åç§°æ‰§è¡Œä¸åŒçš„åº¦é‡
    if task_name == "cola":
        # å¯¹äºCoLAä»»åŠ¡ï¼Œè¿”å›é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°
        return {"mcc": matthews_corrcoef(labels, preds)}
    elif task_name == "sst-2":
        # å¯¹äºSST-2ä»»åŠ¡ï¼Œè¿”å›ç®€å•å‡†ç¡®åº¦
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "mrpc":
        # å¯¹äºMRPCä»»åŠ¡ï¼Œè¿”å›å‡†ç¡®åº¦å’ŒF1åˆ†æ•°
        return acc_and_f1(preds, labels)
    elif task_name == "sts-b":
        # å¯¹äºSTS-Bä»»åŠ¡ï¼Œè¿”å›çš®å°”é€Šç›¸å…³ç³»æ•°å’Œæ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°
        return pearson_and_spearman(preds, labels)
    elif task_name == "qqp":
        # å¯¹äºQQPä»»åŠ¡ï¼Œè¿”å›å‡†ç¡®åº¦å’ŒF1åˆ†æ•°
        return acc_and_f1(preds, labels)
    elif task_name == "mnli":
        # å¯¹äºMNLIä»»åŠ¡ï¼Œè¿”å›MNLIå‡†ç¡®åº¦
        return {"mnli/acc": simple_accuracy(preds, labels)}
    elif task_name == "mnli-mm":
        # å¯¹äºMNLI-MMä»»åŠ¡ï¼Œè¿”å›MNLI-MMå‡†ç¡®åº¦
        return {"mnli-mm/acc": simple_accuracy(preds, labels)}
    elif task_name == "qnli":
        # å¯¹äºQNLIä»»åŠ¡ï¼Œè¿”å›å‡†ç¡®åº¦
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "rte":
        # å¯¹äºRTEä»»åŠ¡ï¼Œè¿”å›å‡†ç¡®åº¦
        return {"acc": simple_accuracy(preds, labels)}
```  
    # å¦‚æœä»»åŠ¡åç§°ä¸º "wnli"ï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«å‡†ç¡®ç‡çš„å­—å…¸
    elif task_name == "wnli":
        return {"acc": simple_accuracy(preds, labels)}
    # å¦‚æœä»»åŠ¡åç§°ä¸º "hans"ï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«å‡†ç¡®ç‡çš„å­—å…¸
    elif task_name == "hans":
        return {"acc": simple_accuracy(preds, labels)}
    # å¦‚æœä»»åŠ¡åç§°æ—¢ä¸æ˜¯ "wnli" ä¹Ÿä¸æ˜¯ "hans"ï¼Œåˆ™å¼•å‘ä¸€ä¸ª KeyError å¼‚å¸¸
    else:
        raise KeyError(task_name)
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè®¡ç®— xnli ä»»åŠ¡çš„æŒ‡æ ‡
def xnli_compute_metrics(task_name, preds, labels):
    # å‘å‡ºè­¦å‘Šï¼Œæç¤ºè¯¥å‡½æ•°å³å°†è¢«å¼ƒç”¨
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æ‰€éœ€çš„åç«¯åº“ "sklearn"
    requires_backends(xnli_compute_metrics, "sklearn")
    # æ£€æŸ¥é¢„æµ‹å€¼å’Œæ ‡ç­¾å€¼çš„é•¿åº¦æ˜¯å¦ä¸€è‡´
    if len(preds) != len(labels):
        # å¦‚æœé•¿åº¦ä¸ä¸€è‡´ï¼Œåˆ™æŠ›å‡ºæ•°å€¼é”™è¯¯å¼‚å¸¸
        raise ValueError(f"Predictions and labels have mismatched lengths {len(preds)} and {len(labels)}")
    # å¦‚æœä»»åŠ¡åä¸º "xnli"ï¼Œåˆ™è¿”å›å‡†ç¡®ç‡æŒ‡æ ‡
    if task_name == "xnli":
        return {"acc": simple_accuracy(preds, labels)}
    else:
        # å¦‚æœä»»åŠ¡åä¸æ˜¯ "xnli"ï¼Œåˆ™æŠ›å‡ºé”®é”™è¯¯å¼‚å¸¸
        raise KeyError(task_name)
```