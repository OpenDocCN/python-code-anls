# `.\data\metrics\__init__.py`

```
# å¯¼å…¥è­¦å‘Šæ¨¡å—ï¼Œç”¨äºå‘å‡ºå…³äºæœªæ¥ç‰ˆæœ¬ä¸æ¨èä½¿ç”¨çš„è­¦å‘Šä¿¡æ¯
import warnings

# ä»å·¥å…·åŒ…ä¸­å¯¼å…¥æ£€æŸ¥å‡½æ•°å’Œåç«¯ä¾èµ–çš„å‡½æ•°
from ...utils import is_sklearn_available, requires_backends

# å¦‚æœæ£€æµ‹åˆ° sklearn å¯ç”¨ï¼Œåˆ™å¯¼å…¥ç›¸å…³çš„æŒ‡æ ‡å‡½æ•°
if is_sklearn_available():
    from scipy.stats import pearsonr, spearmanr
    from sklearn.metrics import f1_score, matthews_corrcoef

# è­¦å‘Šä¿¡æ¯ï¼ŒæŒ‡å‡ºå½“å‰æŒ‡æ ‡å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­ç§»é™¤ï¼Œæ¨èä½¿ç”¨ Evaluate åº“å¤„ç†æŒ‡æ ‡
DEPRECATION_WARNING = (
    "This metric will be removed from the library soon, metrics should be handled with the ğŸ¤— Evaluate "
    "library. You can have a look at this example script for pointers: "
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py"
)

# è®¡ç®—ç®€å•å‡†ç¡®ç‡çš„å‡½æ•°å®šä¹‰ï¼Œå‘å‡ºæœªæ¥ç‰ˆæœ¬è­¦å‘Šï¼Œå¹¶æ£€æŸ¥ sklearn åç«¯ä¾èµ–
def simple_accuracy(preds, labels):
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    requires_backends(simple_accuracy, "sklearn")
    return (preds == labels).mean()

# è®¡ç®—å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°çš„å‡½æ•°å®šä¹‰ï¼Œå‘å‡ºæœªæ¥ç‰ˆæœ¬è­¦å‘Šï¼Œå¹¶æ£€æŸ¥ sklearn åç«¯ä¾èµ–
def acc_and_f1(preds, labels):
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    requires_backends(acc_and_f1, "sklearn")
    acc = simple_accuracy(preds, labels)
    f1 = f1_score(y_true=labels, y_pred=preds)
    return {
        "acc": acc,
        "f1": f1,
        "acc_and_f1": (acc + f1) / 2,
    }

# è®¡ç®— Pearson ç›¸å…³ç³»æ•°å’Œ Spearman ç§©ç›¸å…³ç³»æ•°çš„å‡½æ•°å®šä¹‰ï¼Œå‘å‡ºæœªæ¥ç‰ˆæœ¬è­¦å‘Šï¼Œå¹¶æ£€æŸ¥ sklearn åç«¯ä¾èµ–
def pearson_and_spearman(preds, labels):
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    requires_backends(pearson_and_spearman, "sklearn")
    pearson_corr = pearsonr(preds, labels)[0]
    spearman_corr = spearmanr(preds, labels)[0]
    return {
        "pearson": pearson_corr,
        "spearmanr": spearman_corr,
        "corr": (pearson_corr + spearman_corr) / 2,
    }

# è®¡ç®— GLUE ä»»åŠ¡ä¸­æŒ‡æ ‡çš„å‡½æ•°å®šä¹‰ï¼Œå‘å‡ºæœªæ¥ç‰ˆæœ¬è­¦å‘Šï¼Œå¹¶æ£€æŸ¥ sklearn åç«¯ä¾èµ–
def glue_compute_metrics(task_name, preds, labels):
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    requires_backends(glue_compute_metrics, "sklearn")
    assert len(preds) == len(labels), f"Predictions and labels have mismatched lengths {len(preds)} and {len(labels)}"
    if task_name == "cola":
        return {"mcc": matthews_corrcoef(labels, preds)}
    elif task_name == "sst-2":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "mrpc":
        return acc_and_f1(preds, labels)
    elif task_name == "sts-b":
        return pearson_and_spearman(preds, labels)
    elif task_name == "qqp":
        return acc_and_f1(preds, labels)
    elif task_name == "mnli":
        return {"mnli/acc": simple_accuracy(preds, labels)}
    elif task_name == "mnli-mm":
        return {"mnli-mm/acc": simple_accuracy(preds, labels)}
    elif task_name == "qnli":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "rte":
        return {"acc": simple_accuracy(preds, labels)}
    # å¦‚æœä»»åŠ¡åä¸º "wnli"ï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«å‡†ç¡®ç‡çš„å­—å…¸ï¼Œä½¿ç”¨ simple_accuracy å‡½æ•°è®¡ç®—
    elif task_name == "wnli":
        return {"acc": simple_accuracy(preds, labels)}
    # å¦‚æœä»»åŠ¡åä¸º "hans"ï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«å‡†ç¡®ç‡çš„å­—å…¸ï¼Œä½¿ç”¨ simple_accuracy å‡½æ•°è®¡ç®—
    elif task_name == "hans":
        return {"acc": simple_accuracy(preds, labels)}
    # å¦‚æœä»»åŠ¡åæ—¢ä¸æ˜¯ "wnli" ä¹Ÿä¸æ˜¯ "hans"ï¼Œåˆ™æŠ›å‡º KeyError å¼‚å¸¸
    else:
        raise KeyError(task_name)
# è®¡ç®— xnli ä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡
def xnli_compute_metrics(task_name, preds, labels):
    # å‘å‡ºè­¦å‘Šï¼ŒæŒ‡ç¤ºæ­¤å‡½æ•°å³å°†å¼ƒç”¨
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    # ç¡®ä¿éœ€è¦çš„åç«¯åº“è¢«åŠ è½½ï¼Œè¿™é‡Œæ˜¯ sklearn
    requires_backends(xnli_compute_metrics, "sklearn")
    # æ£€æŸ¥é¢„æµ‹å€¼å’Œæ ‡ç­¾çš„é•¿åº¦æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™å¼•å‘æ•°å€¼é”™è¯¯
    if len(preds) != len(labels):
        raise ValueError(f"Predictions and labels have mismatched lengths {len(preds)} and {len(labels)}")
    # å¦‚æœä»»åŠ¡åä¸º "xnli"ï¼Œè¿”å›ç²¾åº¦ï¼ˆaccuracyï¼‰æŒ‡æ ‡çš„å­—å…¸
    if task_name == "xnli":
        return {"acc": simple_accuracy(preds, labels)}
    else:
        # å¦åˆ™å¼•å‘ä»»åŠ¡åé”™è¯¯
        raise KeyError(task_name)
```