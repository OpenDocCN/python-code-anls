# `.\models\siglip\convert_siglip_to_hf.py`

```
# è·å– SigLIP æ¨¡å‹é…ç½®ä¿¡æ¯çš„å‡½æ•°
def get_siglip_config(model_name):
    # åˆ›å»º SigLIPConfig å¯¹è±¡
    config = SiglipConfig()

    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šè¯æ±‡è¡¨å¤§å°
    vocab_size = 250000 if "i18n" in model_name else 32000
    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šå›¾åƒå¤§å°
    image_size = model_name_to_image_size[model_name]
    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šè¡¥ä¸å¤§å°
    patch_size = 16 if "patch16" in model_name else 14

    # è®¾ç½®è§†è§‰é…ç½®çš„å›¾åƒå¤§å°å’Œè¡¥ä¸å¤§å°
    config.vision_config.image_size = image_size
    config.vision_config.patch_size = patch_size
    # è®¾ç½®æ–‡æœ¬é…ç½®çš„è¯æ±‡è¡¨å¤§å°
    config.text_config.vocab_size = vocab_size

    # å¦‚æœæ¨¡å‹åç§°åŒ…å« "base"ï¼Œåˆ™æ— éœ€é¢å¤–æ“ä½œ
    if "base" in model_name:
        pass
    # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"large"
    elif "large" in model_name:
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„éšè—å±‚å¤§å°ä¸º1024
        config.text_config.hidden_size = 1024
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„ä¸­é—´å±‚å¤§å°ä¸º4096
        config.text_config.intermediate_size = 4096
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„éšè—å±‚æ•°é‡ä¸º24
        config.text_config.num_hidden_layers = 24
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„æ³¨æ„åŠ›å¤´æ•°ä¸º16
        config.text_config.num_attention_heads = 16
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„éšè—å±‚å¤§å°ä¸º1024
        config.vision_config.hidden_size = 1024
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„ä¸­é—´å±‚å¤§å°ä¸º4096
        config.vision_config.intermediate_size = 4096
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„éšè—å±‚æ•°é‡ä¸º24
        config.vision_config.num_hidden_layers = 24
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„æ³¨æ„åŠ›å¤´æ•°ä¸º16
        config.vision_config.num_attention_heads = 16
    # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"so400m"
    elif "so400m" in model_name:
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„éšè—å±‚å¤§å°ä¸º1152
        config.text_config.hidden_size = 1152
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„ä¸­é—´å±‚å¤§å°ä¸º4304
        config.text_config.intermediate_size = 4304
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„éšè—å±‚æ•°é‡ä¸º27
        config.text_config.num_hidden_layers = 27
        # è®¾ç½®æ–‡æœ¬æ¨¡å‹çš„æ³¨æ„åŠ›å¤´æ•°ä¸º16
        config.text_config.num_attention_heads = 16
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„éšè—å±‚å¤§å°ä¸º1152
        config.vision_config.hidden_size = 1152
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„ä¸­é—´å±‚å¤§å°ä¸º4304
        config.vision_config.intermediate_size = 4304
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„éšè—å±‚æ•°é‡ä¸º27
        config.vision_config.num_hidden_layers = 27
        # è®¾ç½®è§†è§‰æ¨¡å‹çš„æ³¨æ„åŠ›å¤´æ•°ä¸º16
        config.vision_config.num_attention_heads = 16
    else:
        # è‹¥æ¨¡å‹åç§°ä¸ç¬¦åˆå·²çŸ¥æ¨¡å‹ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
        raise ValueError("Model not supported")

    # è¿”å›é…ç½®å¯¹è±¡config
    return config
def create_rename_keys(config):
    rename_keys = []
    # fmt: off  # å…³é—­ä»£ç æ ¼å¼åŒ–ï¼Œä»¥ä¾¿åç»­æ‰‹åŠ¨æŒ‡å®šæ ¼å¼

    # vision encoder  # ä»¥ä¸‹æ˜¯å…³äºè§†è§‰ç¼–ç å™¨çš„é‡å‘½åé”®è®¾ç½®

    # å°†æ—§é”® "params/img/embedding/kernel" æ˜ å°„åˆ°æ–°é”® "vision_model.embeddings.patch_embedding.weight"ï¼Œå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("params/img/embedding/kernel", "vision_model.embeddings.patch_embedding.weight"))

    # å°†æ—§é”® "params/img/embedding/bias" æ˜ å°„åˆ°æ–°é”® "vision_model.embeddings.patch_embedding.bias"ï¼Œå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("params/img/embedding/bias", "vision_model.embeddings.patch_embedding.bias"))

    # å°†æ—§é”® "params/img/pos_embedding" æ˜ å°„åˆ°æ–°é”® "vision_model.embeddings.position_embedding.weight"ï¼Œå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("params/img/pos_embedding", "vision_model.embeddings.position_embedding.weight"))
    # éå†ä»é…ç½®ä¸­è·å–çš„è§†è§‰æ¨¡å‹çš„éšè—å±‚æ•°é‡æ¬¡æ•°ï¼Œè¿›è¡Œé‡å‘½åé”®å€¼å¯¹çš„æ·»åŠ 
    for i in range(config.vision_config.num_hidden_layers):
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_0å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_0/scale", f"vision_model.encoder.layers.{i}.layer_norm1.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_0å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_0/bias", f"vision_model.encoder.layers.{i}.layer_norm1.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_1å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_1/scale", f"vision_model.encoder.layers.{i}.layer_norm2.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_1å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_1/bias", f"vision_model.encoder.layers.{i}.layer_norm2.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0å±‚çš„ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_0/kernel", f"vision_model.encoder.layers.{i}.mlp.fc1.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0å±‚çš„ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_0/bias", f"vision_model.encoder.layers.{i}.mlp.fc1.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0å±‚çš„ç¬¬äºŒå±‚å…¨è¿æ¥å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_1/kernel", f"vision_model.encoder.layers.{i}.mlp.fc2.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0å±‚çš„ç¬¬äºŒå±‚å…¨è¿æ¥å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_1/bias", f"vision_model.encoder.layers.{i}.mlp.fc2.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„keyæŠ•å½±å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/key/kernel", f"vision_model.encoder.layers.{i}.self_attn.k_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„keyæŠ•å½±å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/key/bias", f"vision_model.encoder.layers.{i}.self_attn.k_proj.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„valueæŠ•å½±å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/value/kernel", f"vision_model.encoder.layers.{i}.self_attn.v_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„valueæŠ•å½±å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/value/bias", f"vision_model.encoder.layers.{i}.self_attn.v_proj.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„queryæŠ•å½±å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/query/kernel", f"vision_model.encoder.layers.{i}.self_attn.q_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„queryæŠ•å½±å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/query/bias", f"vision_model.encoder.layers.{i}.self_attn.q_proj.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„è¾“å‡ºæŠ•å½±å±‚çš„æƒé‡å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/out/kernel", f"vision_model.encoder.layers.{i}.self_attn.out_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0å±‚çš„è¾“å‡ºæŠ•å½±å±‚çš„åç½®å‚æ•°è·¯å¾„
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/out/bias", f"vision_model.encoder.layers.{i}.self_attn.out_proj.bias"))
    
    # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹çš„ç¼–ç å™¨å±‚ä¹‹åçš„LayerNormå±‚çš„æƒé‡å‚æ•°è·¯å¾„
    rename_keys.append(("params/img/Transformer/encoder_norm/scale", "vision_model.post_layernorm.weight"))
    # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹çš„ç¼–ç å™¨å±‚ä¹‹åçš„LayerNormå±‚çš„åç½®å‚æ•°è·¯å¾„
    rename_keys.append(("params/img/Transformer/encoder_norm/bias", "vision_model.post_layernorm.bias"))
    
    # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†æ—§å‚æ•°è·¯å¾„æ˜ å°„åˆ°æ–°çš„è§†è§‰æ¨¡å‹çš„å¤´éƒ¨æ¨¡å—çš„æ¢æµ‹å‚æ•°è·¯å¾„
    rename_keys.append(("params/img/MAPHead_0/probe", "vision_model.head.probe"))
    # å°†é”®å€¼å¯¹æ·»åŠ åˆ° `rename_keys` åˆ—è¡¨ï¼Œç”¨äºæŒ‡å®šæºé”®å’Œç›®æ ‡é”®çš„æ˜ å°„å…³ç³»ï¼Œç”¨äºé‡å‘½åæ¨¡å‹å‚æ•°
    
    rename_keys.append(("params/img/MAPHead_0/LayerNorm_0/scale", "vision_model.head.layernorm.weight"))
    rename_keys.append(("params/img/MAPHead_0/LayerNorm_0/bias", "vision_model.head.layernorm.bias"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_0/kernel", "vision_model.head.mlp.fc1.weight"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_0/bias", "vision_model.head.mlp.fc1.bias"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_1/kernel", "vision_model.head.mlp.fc2.weight"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_1/bias", "vision_model.head.mlp.fc2.bias"))
    rename_keys.append(("params/img/MAPHead_0/MultiHeadDotProductAttention_0/out/kernel", "vision_model.head.attention.out_proj.weight"))
    rename_keys.append(("params/img/MAPHead_0/MultiHeadDotProductAttention_0/out/bias", "vision_model.head.attention.out_proj.bias"))
    
    # text encoder
    
    # æ·»åŠ ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„é”®å€¼å¯¹æ˜ å°„ï¼Œé‡å‘½åæ¨¡å‹å‚æ•°
    rename_keys.append(("params/txt/Embed_0/embedding", "text_model.embeddings.token_embedding.weight"))
    rename_keys.append(("params/txt/pos_embedding", "text_model.embeddings.position_embedding.weight"))
    # éå†é…ç½®ä¸­æŒ‡å®šçš„æ–‡æœ¬æ¨¡å‹éšè—å±‚æ•°é‡æ¬¡æ•°
    for i in range(config.text_config.num_hidden_layers):
        # å°†å‚æ•°é‡å‘½åå¹¶æ·»åŠ åˆ° rename_keys åˆ—è¡¨ä¸­ï¼Œæ˜ å°„åˆ°æ–‡æœ¬æ¨¡å‹ç¼–ç å™¨æ¯ä¸€å±‚çš„ LayerNorm å±‚çš„æƒé‡å’Œåç½®
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_0/scale", f"text_model.encoder.layers.{i}.layer_norm1.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_0/bias", f"text_model.encoder.layers.{i}.layer_norm1.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_1/scale", f"text_model.encoder.layers.{i}.layer_norm2.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_1/bias", f"text_model.encoder.layers.{i}.layer_norm2.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_0/kernel", f"text_model.encoder.layers.{i}.mlp.fc1.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_0/bias", f"text_model.encoder.layers.{i}.mlp.fc1.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_1/kernel", f"text_model.encoder.layers.{i}.mlp.fc2.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_1/bias", f"text_model.encoder.layers.{i}.mlp.fc2.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/key/kernel", f"text_model.encoder.layers.{i}.self_attn.k_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/key/bias", f"text_model.encoder.layers.{i}.self_attn.k_proj.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/value/kernel", f"text_model.encoder.layers.{i}.self_attn.v_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/value/bias", f"text_model.encoder.layers.{i}.self_attn.v_proj.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/query/kernel", f"text_model.encoder.layers.{i}.self_attn.q_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/query/bias", f"text_model.encoder.layers.{i}.self_attn.q_proj.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/out/kernel", f"text_model.encoder.layers.{i}.self_attn.out_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/out/bias", f"text_model.encoder.layers.{i}.self_attn.out_proj.bias"))
    
    # å°†æœ€åå‡ ä¸ªå‚æ•°é‡å‘½åå¹¶æ·»åŠ åˆ° rename_keys åˆ—è¡¨ä¸­ï¼Œæ˜ å°„åˆ°æ–‡æœ¬æ¨¡å‹çš„æœ€ç»ˆå½’ä¸€åŒ–å±‚ã€è¾“å‡ºå±‚æƒé‡å’Œåç½®
    rename_keys.append(("params/txt/Encoder_0/encoder_norm/scale", "text_model.final_layer_norm.weight"))
    rename_keys.append(("params/txt/Encoder_0/encoder_norm/bias", "text_model.final_layer_norm.bias"))
    rename_keys.append(("params/txt/head/kernel", "text_model.head.weight"))
    rename_keys.append(("params/txt/head/bias", "text_model.head.bias"))
    
    # å­¦ä¹ åˆ°çš„æ¸©åº¦å’Œåç½®ï¼ˆæ­¤å¤„çš„æ³¨é‡Šå¹¶æ²¡æœ‰æä¾›ä»£ç ç»†èŠ‚ï¼Œå¯èƒ½è¡¨ç¤ºè¿™éƒ¨åˆ†ä¿¡æ¯æ˜¯ä»æ•°æ®ä¸­å­¦ä¹ åˆ°çš„é¢å¤–å‚æ•°ï¼‰
    # å°†å…ƒç»„ ("params/t", "logit_scale") æ·»åŠ åˆ° rename_keys åˆ—è¡¨ä¸­
    rename_keys.append(("params/t", "logit_scale"))
    # å°†å…ƒç»„ ("params/b", "logit_bias") æ·»åŠ åˆ° rename_keys åˆ—è¡¨ä¸­
    rename_keys.append(("params/b", "logit_bias"))

    # è¿”å› rename_keys åˆ—è¡¨ä½œä¸ºå‡½æ•°çš„ç»“æœ
    return rename_keys
# é‡å‘½åå­—å…¸ä¸­çš„é”®ï¼Œå¹¶æ ¹æ®é…ç½®ä¿®æ”¹å€¼çš„å½¢çŠ¶
def rename_key(dct, old, new, config):
    # å¼¹å‡ºæ—§é”®å¯¹åº”çš„å€¼
    val = dct.pop(old)

    # æ ¹æ®æ–°é”®ä¸­çš„æ ‡è¯†å’Œé…ç½®è°ƒæ•´å€¼çš„å½¢çŠ¶
    if ("out_proj" in new or "v_proj" in new or "k_proj" in new or "q_proj" in new) and "vision" in new:
        val = val.reshape(-1, config.vision_config.hidden_size)
    if ("out_proj" in new or "v_proj" in new or "k_proj" in new or "q_proj" in new) and "text" in new:
        val = val.reshape(-1, config.text_config.hidden_size)

    # å¦‚æœæ–°é”®æŒ‡å®šäº†ç‰¹å®šçš„æƒé‡çŸ©é˜µï¼Œè¿›è¡Œè½¬ç½®æ“ä½œ
    if "patch_embedding.weight" in new:
        val = val.transpose(3, 2, 0, 1)
    elif new.endswith("weight") and "position_embedding" not in new and "token_embedding" not in new:
        val = val.T

    # æ ¹æ®æ–°é”®ä¸­çš„æ ‡è¯†å’Œé…ç½®å†æ¬¡è°ƒæ•´å€¼çš„å½¢çŠ¶
    if "position_embedding" in new and "vision" in new:
        val = val.reshape(-1, config.vision_config.hidden_size)
    if "position_embedding" in new and "text" in new:
        val = val.reshape(-1, config.text_config.hidden_size)

    # å¦‚æœæ–°é”®æ˜¯åç½®é¡¹ï¼Œå°†å€¼è°ƒæ•´ä¸ºä¸€ç»´æ•°ç»„
    if new.endswith("bias"):
        val = val.reshape(-1)

    # å°†å¤„ç†åçš„å€¼è½¬æ¢ä¸º Torch å¼ é‡ï¼Œå¹¶å­˜å…¥å­—å…¸ä¸­
    dct[new] = torch.from_numpy(val)


# ä»çŠ¶æ€å­—å…¸ä¸­è¯»å–æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥æŠ•å½±å±‚å‚æ•°
def read_in_q_k_v_head(state_dict, config):
    # å¼¹å‡ºå¹¶é‡å¡‘é”®ä¸º"key/kernel"çš„å‚æ•°
    key_proj_weight = (
        state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/key/kernel")
        .reshape(-1, config.vision_config.hidden_size)
        .T
    )
    # å¼¹å‡ºå¹¶é‡å¡‘é”®ä¸º"key/bias"çš„å‚æ•°
    key_proj_bias = state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/key/bias").reshape(-1)
    # å¼¹å‡ºå¹¶é‡å¡‘é”®ä¸º"value/kernel"çš„å‚æ•°
    value_proj_weight = (
        state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/value/kernel")
        .reshape(-1, config.vision_config.hidden_size)
        .T
    )
    # å¼¹å‡ºå¹¶é‡å¡‘é”®ä¸º"value/bias"çš„å‚æ•°
    value_proj_bias = state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/value/bias").reshape(-1)
    # å¼¹å‡ºå¹¶é‡å¡‘é”®ä¸º"query/kernel"çš„å‚æ•°
    query_proj_weight = (
        state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/query/kernel")
        .reshape(-1, config.vision_config.hidden_size)
        .T
    )
    # å¼¹å‡ºå¹¶é‡å¡‘é”®ä¸º"query/bias"çš„å‚æ•°
    query_proj_bias = state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/query/bias").reshape(-1)

    # å°†é‡å¡‘åçš„å‚æ•°æ‹¼æ¥æˆä¸€ä¸ªå•ä¸€çš„çŸ©é˜µå’Œå‘é‡ï¼Œå¹¶åŠ å…¥çŠ¶æ€å­—å…¸ä¸­
    state_dict["vision_model.head.attention.in_proj_weight"] = torch.from_numpy(
        np.concatenate([query_proj_weight, key_proj_weight, value_proj_weight], axis=0)
    )
    state_dict["vision_model.head.attention.in_proj_bias"] = torch.from_numpy(
        np.concatenate([query_proj_bias, key_proj_bias, value_proj_bias], axis=0)
    )
# å®šä¹‰å‡½æ•°ï¼Œç”¨äºå°†æ¨¡å‹çš„æƒé‡è½¬æ¢åˆ° SigLIP ç»“æ„
def convert_siglip_checkpoint(model_name, pytorch_dump_folder_path, verify_logits=True, push_to_hub=False):
    """
    Copy/paste/tweak model's weights to our SigLIP structure.
    """

    # è·å–é»˜è®¤çš„ SigLIP é…ç½®
    config = get_siglip_config(model_name)

    # è·å–æ¨¡å‹åç§°å¯¹åº”çš„æ£€æŸ¥ç‚¹
    checkpoint = model_name_to_checkpoint[model_name]

    # è·å–è¯æ±‡æ–‡ä»¶è·¯å¾„
    if "i18n" in model_name:
        vocab_file = "/Users/nielsrogge/Documents/SigLIP/multilingual_vocab/sentencepiece.model"
    else:
        vocab_file = "/Users/nielsrogge/Documents/SigLIP/english_vocab/sentencepiece.model"

    # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
    data = load(checkpoint)
    state_dict = flatten_nested_dict(data)

    # ç§»é™¤å¹¶é‡å‘½åä¸€äº›é”®
    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:
        rename_key(state_dict, src, dest, config)

    # å¯¹æ³¨æ„åŠ›æ± åŒ–å¤´çš„ qkv çŸ©é˜µéœ€è¦ç‰¹æ®Šå¤„ç†
    read_in_q_k_v_head(state_dict, config)

    # åŠ è½½ HuggingFace æ¨¡å‹
    model = SiglipModel(config).eval()
    model.load_state_dict(state_dict)

    # åˆ›å»ºå¤„ç†å™¨
    # æ³¨æ„: ä½¿å¾—åˆ†è¯å™¨ä¸è¿”å› attention_maskï¼Œå› ä¸ºåŸå§‹æ¨¡å‹ä¸éœ€è¦å®ƒ
    image_size = config.vision_config.image_size
    size = {"height": image_size, "width": image_size}
    image_processor = SiglipImageProcessor(size=size)
    tokenizer = SiglipTokenizer(vocab_file=vocab_file, model_input_names=["input_ids"])
    processor = SiglipProcessor(image_processor=image_processor, tokenizer=tokenizer)

    # åœ¨è™šæ‹Ÿå›¾ç‰‡å’Œæ–‡æœ¬ä¸Šè¿›è¡ŒéªŒè¯
    url_1 = "https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg"
    image_1 = Image.open(requests.get(url_1, stream=True).raw).convert("RGB")
    url_2 = "https://cdn.openai.com/multimodal-neurons/assets/apple/apple-blank.jpg"
    image_2 = Image.open(requests.get(url_2, stream=True).raw).convert("RGB")
    texts = ["an apple", "a picture of an apple"]

    inputs = processor(images=[image_1, image_2], text=texts, return_tensors="pt", padding="max_length")

    # é’ˆå¯¹è¾“å…¥çš„ input_ids è¿›è¡ŒéªŒè¯
    if image_size == 224:
        filename = "siglip_pixel_values.pt"
    elif image_size == 256:
        filename = "siglip_pixel_values_256.pt"
    elif image_size == 384:
        filename = "siglip_pixel_values_384.pt"
    elif image_size == 512:
        filename = "siglip_pixel_values_512.pt"
    else:
        raise ValueError("Image size not supported")

    # ä¸‹è½½å¹¶åŠ è½½åŸå§‹åƒç´ æ•°å€¼
    filepath = hf_hub_download(repo_id="nielsr/test-image", filename=filename, repo_type="dataset")
    original_pixel_values = torch.load(filepath)

    # ä¸‹è½½å¹¶åŠ è½½åŸå§‹ input_ids
    filepath = hf_hub_download(repo_id="nielsr/test-image", filename="siglip_input_ids.pt", repo_type="dataset")
    original_input_ids = torch.load(filepath)

    # å¦‚æœæ¨¡å‹åç§°ä¸åŒ…å« "i18n"ï¼Œåˆ™æ–­è¨€ inputs.input_ids ä¸ original_input_ids ç›¸ç­‰
    if "i18n" not in model_name:
        assert inputs.input_ids.tolist() == original_input_ids.tolist()

    # æ‰“å°åŸå§‹åƒç´ å€¼çš„å¹³å‡å€¼
    print("Mean of original pixel values:", original_pixel_values.mean())
    # è¾“å‡ºæ–°åƒç´ å€¼çš„å‡å€¼
    print("Mean of new pixel values:", inputs.pixel_values.mean())

    # ä½¿ç”¨åŸå§‹åƒç´ å€¼è¿›è¡Œæµ‹è¯•ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å‡†ç¡®çš„åƒç´ å€¼
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œè¾“å…¥åŒ…æ‹¬è¾“å…¥çš„ ID å’ŒåŸå§‹åƒç´ å€¼
        outputs = model(input_ids=inputs.input_ids, pixel_values=original_pixel_values)

    # è¾“å‡ºå‰ä¸‰è¡Œä¸‰åˆ—çš„ logits_per_image
    print(outputs.logits_per_image[:3, :3])

    # è®¡ç®—è¾“å‡ºçš„ logits_per_image çš„ sigmoid å‡½æ•°ï¼Œå¾—åˆ°æ¦‚ç‡å€¼
    probs = torch.sigmoid(outputs.logits_per_image)
    # æ‰“å°ç¬¬ä¸€å¼ å›¾åƒæ˜¯ texts[0] çš„æ¦‚ç‡
    print(f"{probs[0][0]:.1%} that image 0 is '{texts[0]}'")
    # æ‰“å°ç¬¬ä¸€å¼ å›¾åƒæ˜¯ texts[1] çš„æ¦‚ç‡
    print(f"{probs[0][1]:.1%} that image 0 is '{texts[1]}'")

    # å¦‚æœéœ€è¦éªŒè¯ logits
    if verify_logits:
        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©é¢„æœŸçš„ slice
        if model_name == "siglip-base-patch16-224":
            expected_slice = torch.tensor(
                [[-2.9621, -2.1672], [-0.2713, 0.2910]],
            )
        elif model_name == "siglip-base-patch16-256":
            expected_slice = torch.tensor(
                [[-3.1146, -1.9894], [-0.7312, 0.6387]],
            )
        elif model_name == "siglip-base-patch16-384":
            expected_slice = torch.tensor(
                [[-2.8098, -2.1891], [-0.4242, 0.4102]],
            )
        elif model_name == "siglip-base-patch16-512":
            expected_slice = torch.tensor(
                [[-2.7899, -2.2668], [-0.4295, -0.0735]],
            )
        elif model_name == "siglip-large-patch16-256":
            expected_slice = torch.tensor(
                [[-1.5827, -0.5801], [-0.9153, 0.1363]],
            )
        elif model_name == "siglip-large-patch16-384":
            expected_slice = torch.tensor(
                [[-2.1523, -0.2899], [-0.2959, 0.7884]],
            )
        elif model_name == "siglip-so400m-patch14-384":
            expected_slice = torch.tensor([[-1.2441, -0.6649], [-0.7060, 0.7374]])
        elif model_name == "siglip-base-patch16-256-i18n":
            expected_slice = torch.tensor(
                [[-0.9064, 0.1073], [-0.0299, 0.5304]],
            )

        # æ–­è¨€å‰ä¸‰è¡Œä¸‰åˆ—çš„ logits_per_image ä¸é¢„æœŸçš„ slice ç›¸ä¼¼
        assert torch.allclose(outputs.logits_per_image[:3, :3], expected_slice, atol=1e-4)
        print("Looks ok!")

    # å¦‚æœæœ‰æŒ‡å®šçš„ pytorch_dump_folder_path
    if pytorch_dump_folder_path is not None:
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        # æ‰“å°ä¿å­˜æ¨¡å‹å’Œå¤„ç†å™¨çš„ä¿¡æ¯
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # æ‰“å°ä¿å­˜å¤„ç†å™¨çš„ä¿¡æ¯
        print(f"Saving processor to {pytorch_dump_folder_path}")
        # å°†å¤„ç†å™¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        # æ¨é€æ¨¡å‹åˆ° Hub
        model.push_to_hub(f"nielsr/{model_name}")
        # æ¨é€å¤„ç†å™¨åˆ° Hub
        processor.push_to_hub(f"nielsr/{model_name}")
if __name__ == "__main__":
    # å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç 
    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨å¯¹è±¡

    # Required parameters
    parser.add_argument(
        "--model_name",
        default="siglip-base-patch16-224",
        type=str,
        choices=model_name_to_checkpoint.keys(),
        help="Name of the model you'd like to convert.",
    )
    # æ·»åŠ ä¸€ä¸ªå¿…é€‰çš„å‚æ•° `--model_name`ï¼Œé»˜è®¤ä¸º "siglip-base-patch16-224"ï¼Œ
    # ç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œå¯ä»¥ä» `model_name_to_checkpoint` å­—å…¸çš„é”®ä¸­é€‰æ‹©ï¼Œ
    # ç”¨äºæŒ‡å®šè¦è½¬æ¢çš„æ¨¡å‹åç§°

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    # æ·»åŠ ä¸€ä¸ªå¯é€‰çš„å‚æ•° `--pytorch_dump_folder_path`ï¼Œé»˜è®¤ä¸º Noneï¼Œ
    # ç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å®šè¾“å‡º PyTorch æ¨¡å‹çš„ç›®å½•è·¯å¾„

    parser.add_argument(
        "--verify_logits",
        action="store_false",
        help="Whether to verify logits against the original implementation.",
    )
    # æ·»åŠ ä¸€ä¸ªå¯é€‰çš„å¼€å…³å‚æ•° `--verify_logits`ï¼Œ
    # å½“å­˜åœ¨æ—¶å°†å…¶è®¾ç½®ä¸º Falseï¼Œç”¨äºæŒ‡ç¤ºæ˜¯å¦å¯¹ logits è¿›è¡Œä¸åŸå§‹å®ç°çš„éªŒè¯

    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    # æ·»åŠ ä¸€ä¸ªå¯é€‰çš„å¼€å…³å‚æ•° `--push_to_hub`ï¼Œ
    # å½“å­˜åœ¨æ—¶è®¾ç½®ä¸º Trueï¼Œç”¨äºæŒ‡ç¤ºæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ° ğŸ¤— hub

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨è½¬æ¢å‡½æ•°ï¼Œä¼ å…¥è§£æåçš„å‚æ•°
    convert_siglip_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.verify_logits, args.push_to_hub)
```