# `.\transformers\models\siglip\convert_siglip_to_hf.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜ï¼ŒæŒ‡æ˜ç‰ˆæƒå½’å±äº HuggingFace Inc. å›¢é˜Ÿ
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼Œåªæœ‰åœ¨éµå®ˆè®¸å¯è¯çš„æƒ…å†µä¸‹æ‰èƒ½ä½¿ç”¨æ­¤æ–‡ä»¶
# å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼šhttp://www.apache.org/licenses/LICENSE-2.0
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäº"åŸæ ·"çš„ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶
# è¯·æŸ¥çœ‹è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶
# è½¬æ¢æ¥è‡ªåŸå§‹å­˜å‚¨åº“çš„ SigLIP æ£€æŸ¥ç‚¹
# URL: https://github.com/google-research/big_vision/tree/main

import argparse  # å¯¼å…¥è§£æå‘½ä»¤è¡Œå‚æ•°çš„æ¨¡å—
import collections  # å¯¼å…¥ collections æ¨¡å—
from pathlib import Path  # ä» pathlib æ¨¡å—å¯¼å…¥ Path ç±»

import numpy as np  # å¯¼å…¥ NumPy åº“å¹¶é‡å‘½åä¸º np
import requests  # å¯¼å…¥ requests åº“
import torch  # å¯¼å…¥ PyTorch åº“
from huggingface_hub import hf_hub_download  # ä» huggingface_hub æ¨¡å—å¯¼å…¥ hf_hub_download å‡½æ•°
from numpy import load  # ä» NumPy å¯¼å…¥ load å‡½æ•°
from PIL import Image  # ä» PIL åº“å¯¼å…¥ Image ç±»

from transformers import SiglipConfig, SiglipImageProcessor, SiglipModel, SiglipProcessor, SiglipTokenizer  # ä» transformers æ¨¡å—å¯¼å…¥å¤šä¸ªç±»
from transformers.utils import logging  # ä» transformers.utils æ¨¡å—å¯¼å…¥ logging æ¨¡å—

logging.set_verbosity_info()  # è®¾ç½®æ—¥å¿—è®°å½•çº§åˆ«ä¸º info
logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨

model_name_to_checkpoint = {  # å®šä¹‰æ¨¡å‹åç§°åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
    # åŸºç¡€æ£€æŸ¥ç‚¹
    "siglip-base-patch16-224": "/Users/nielsrogge/Documents/SigLIP/webli_en_b16_224_63724782.npz",
    "siglip-base-patch16-256": "/Users/nielsrogge/Documents/SigLIP/webli_en_b16_256_60500360.npz",
    "siglip-base-patch16-384": "/Users/nielsrogge/Documents/SigLIP/webli_en_b16_384_68578854.npz",
    "siglip-base-patch16-512": "/Users/nielsrogge/Documents/SigLIP/webli_en_b16_512_68580893.npz",
    # å¤§å‹æ£€æŸ¥ç‚¹
    "siglip-large-patch16-256": "/Users/nielsrogge/Documents/SigLIP/webli_en_l16_256_60552751.npz",
    "siglip-large-patch16-384": "/Users/nielsrogge/Documents/SigLIP/webli_en_l16_384_63634585.npz",
    # å¤šè¯­è¨€æ£€æŸ¥ç‚¹
    "siglip-base-patch16-256-i18n": "/Users/nielsrogge/Documents/SigLIP/webli_i18n_b16_256_66117334.npz",
    # so400m æ£€æŸ¥ç‚¹
    "siglip-so400m-patch14-384": "/Users/nielsrogge/Documents/SigLIP/webli_en_so400m_384_58765454.npz",
}

model_name_to_image_size = {  # å®šä¹‰æ¨¡å‹åç§°åˆ°å›¾åƒå¤§å°çš„æ˜ å°„
    "siglip-base-patch16-224": 224,
    "siglip-base-patch16-256": 256,
    "siglip-base-patch16-384": 384,
    "siglip-base-patch16-512": 512,
    "siglip-large-patch16-256": 256,
    "siglip-large-patch16-384": 384,
    "siglip-base-patch16-256-i18n": 256,
    "siglip-so400m-patch14-384": 384,
}

def get_siglip_config(model_name):  # å®šä¹‰å‡½æ•°ï¼Œæ ¹æ®æ¨¡å‹åç§°è·å– SigLIP é…ç½®
    config = SiglipConfig()  # åˆ›å»º SigLIP é…ç½®å¯¹è±¡

    vocab_size = 250000 if "i18n" in model_name else 32000  # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"i18n"ï¼Œåˆ™è¯æ±‡è¡¨å¤§å°ä¸º 250000ï¼Œå¦åˆ™ä¸º 32000
    image_size = model_name_to_image_size[model_name]  # è·å–æ¨¡å‹åç§°å¯¹åº”çš„å›¾åƒå¤§å°
    patch_size = 16 if "patch16" in model_name else 14  # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"patch16"ï¼Œåˆ™å—å¤§å°ä¸º 16ï¼Œå¦åˆ™ä¸º 14

    # è®¾ç½®æ¶æ„çš„å¤§å°
    config.vision_config.image_size = image_size  # è®¾ç½®å›¾åƒå¤§å°
    config.vision_config.patch_size = patch_size  # è®¾ç½®å—å¤§å°
    config.text_config.vocab_size = vocab_size  # è®¾ç½®è¯æ±‡è¡¨å¤§å°

    if "base" in model_name:  # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"base"
        pass  # ä»€ä¹ˆä¹Ÿä¸åš
    # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"large"ï¼Œåˆ™è®¾ç½®æ–‡æœ¬é…ç½®å’Œè§†è§‰é…ç½®çš„å‚æ•°ä¸ºå¤§å‹æ¨¡å‹çš„æ•°å€¼
    elif "large" in model_name:
        config.text_config.hidden_size = 1024
        config.text_config.intermediate_size = 4096
        config.text_config.num_hidden_layers = 24
        config.text_config.num_attention_heads = 16
        config.vision_config.hidden_size = 1024
        config.vision_config.intermediate_size = 4096
        config.vision_config.num_hidden_layers = 24
        config.vision_config.num_attention_heads = 16
    # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"so400m"ï¼Œåˆ™è®¾ç½®æ–‡æœ¬é…ç½®å’Œè§†è§‰é…ç½®çš„å‚æ•°ä¸ºso400mæ¨¡å‹çš„æ•°å€¼
    elif "so400m" in model_name:
        config.text_config.hidden_size = 1152
        config.text_config.intermediate_size = 4304
        config.text_config.num_hidden_layers = 27
        config.text_config.num_attention_heads = 16
        config.vision_config.hidden_size = 1152
        config.vision_config.intermediate_size = 4304
        config.vision_config.num_hidden_layers = 27
        config.vision_config.num_attention_heads = 16
    # å¦‚æœæ¨¡å‹åç§°ä¸ç¬¦åˆä»¥ä¸Šæ¡ä»¶ï¼Œåˆ™æŠ›å‡ºæ•°å€¼é”™è¯¯
    else:
        raise ValueError("Model not supported")

    # è¿”å›é…ç½®å¯¹è±¡
    return config
def create_rename_keys(config):
    rename_keys = []
    # fmt: off

    # vision encoder

    # å°†æŒ‡å®šçš„å‚æ•°é‡å‘½åä¸ºæ–°çš„é”®ï¼Œå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("params/img/embedding/kernel", "vision_model.embeddings.patch_embedding.weight"))
    rename_keys.append(("params/img/embedding/bias", "vision_model.embeddings.patch_embedding.bias"))
    rename_keys.append(("params/img/pos_embedding", "vision_model.embeddings.position_embedding.weight"))
    # éå†è§†è§‰æ¨¡å‹çš„éšè—å±‚ï¼Œä¸ºæ¯ä¸€å±‚çš„å‚æ•°æ·»åŠ é‡å‘½åé”®å€¼å¯¹
    for i in range(config.vision_config.num_hidden_layers):
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_0çš„scaleå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_0/scale", f"vision_model.encoder.layers.{i}.layer_norm1.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_0çš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_0/bias", f"vision_model.encoder.layers.{i}.layer_norm1.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_1çš„scaleå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_1/scale", f"vision_model.encoder.layers.{i}.layer_norm2.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„LayerNorm_1çš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/LayerNorm_1/bias", f"vision_model.encoder.layers.{i}.layer_norm2.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0çš„Dense_0çš„kernelå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_0/kernel", f"vision_model.encoder.layers.{i}.mlp.fc1.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0çš„Dense_0çš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_0/bias", f"vision_model.encoder.layers.{i}.mlp.fc1.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0çš„Dense_1çš„kernelå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_1/kernel", f"vision_model.encoder.layers.{i}.mlp.fc2.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MlpBlock_0çš„Dense_1çš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MlpBlock_0/Dense_1/bias", f"vision_model.encoder.layers.{i}.mlp.fc2.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0çš„keyçš„kernelå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/key/kernel", f"vision_model.encoder.layers.{i}.self_attn.k_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚ï¿½ï¿½ï¿½MultiHeadDotProductAttention_0çš„keyçš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/key/bias", f"vision_model.encoder.layers.{i}.self_attn.k_proj.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0çš„valueçš„kernelå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/value/kernel", f"vision_model.encoder.layers.{i}.self_attn.v_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0çš„valueçš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/value/bias", f"vision_model.encoder.layers.{i}.self_attn.v_proj.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0çš„queryçš„kernelå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/query/kernel", f"vision_model.encoder.layers.{i}.self_attn.q_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0çš„queryçš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/query/bias", f"vision_model.encoder.layers.{i}.self_attn.q_proj.bias"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0çš„outçš„kernelå‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/out/kernel", f"vision_model.encoder.layers.{i}.self_attn.out_proj.weight"))
        # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„ç¬¬iå±‚çš„MultiHeadDotProductAttention_0çš„outçš„biaså‚æ•°
        rename_keys.append((f"params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/out/bias", f"vision_model.encoder.layers.{i}.self_attn.out_proj.bias"))

    # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„encoder_normçš„scaleå‚æ•°
    rename_keys.append(("params/img/Transformer/encoder_norm/scale", "vision_model.post_layernorm.weight"))
    # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹ç¼–ç å™¨çš„encoder_normçš„biaså‚æ•°
    rename_keys.append(("params/img/Transformer/encoder_norm/bias", "vision_model.post_layernorm.bias"))

    # æ·»åŠ é‡å‘½åé”®å€¼å¯¹ï¼Œå°†å‚æ•°è·¯å¾„æ˜ å°„åˆ°è§†è§‰æ¨¡å‹çš„MAPHead_0çš„probeå‚æ•°
    rename_keys.append(("params/img/MAPHead_0/probe", "vision_model.head.probe"))
    # å°†å‚æ•°é‡å‘½åå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºæ¨¡å‹å‚æ•°åŠ è½½
    rename_keys.append(("params/img/MAPHead_0/LayerNorm_0/scale", "vision_model.head.layernorm.weight"))
    rename_keys.append(("params/img/MAPHead_0/LayerNorm_0/bias", "vision_model.head.layernorm.bias"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_0/kernel", "vision_model.head.mlp.fc1.weight"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_0/bias", "vision_model.head.mlp.fc1.bias"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_1/kernel", "vision_model.head.mlp.fc2.weight"))
    rename_keys.append(("params/img/MAPHead_0/MlpBlock_0/Dense_1/bias", "vision_model.head.mlp.fc2.bias"))
    rename_keys.append(("params/img/MAPHead_0/MultiHeadDotProductAttention_0/out/kernel", "vision_model.head.attention.out_proj.weight"))
    rename_keys.append(("params/img/MAPHead_0/MultiHeadDotProductAttention_0/out/bias", "vision_model.head.attention.out_proj.bias"))

    # æ–‡æœ¬ç¼–ç å™¨

    # å°†å‚æ•°é‡å‘½åå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºæ¨¡å‹å‚æ•°åŠ è½½
    rename_keys.append(("params/txt/Embed_0/embedding", "text_model.embeddings.token_embedding.weight"))
    rename_keys.append(("params/txt/pos_embedding", "text_model.embeddings.position_embedding.weight"))
    # éå†æ–‡æœ¬æ¨¡å‹çš„éšè—å±‚ï¼Œå°†å‚æ•°é‡å‘½åå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    for i in range(config.text_config.num_hidden_layers):
        # å°†å‚æ•°é‡å‘½åå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_0/scale", f"text_model.encoder.layers.{i}.layer_norm1.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_0/bias", f"text_model.encoder.layers.{i}.layer_norm1.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_1/scale", f"text_model.encoder.layers.{i}.layer_norm2.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/LayerNorm_1/bias", f"text_model.encoder.layers.{i}.layer_norm2.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_0/kernel", f"text_model.encoder.layers.{i}.mlp.fc1.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_0/bias", f"text_model.encoder.layers.{i}.mlp.fc1.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_1/kernel", f"text_model.encoder.layers.{i}.mlp.fc2.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MlpBlock_0/Dense_1/bias", f"text_model.encoder.layers.{i}.mlp.fc2.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/key/kernel", f"text_model.encoder.layers.{i}.self_attn.k_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/key/bias", f"text_model.encoder.layers.{i}.self_attn.k_proj.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/value/kernel", f"text_model.encoder.layers.{i}.self_attn.v_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/value/bias", f"text_model.encoder.layers.{i}.self_attn.v_proj.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/query/kernel", f"text_model.encoder.layers.{i}.self_attn.q_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/query/bias", f"text_model.encoder.layers.{i}.self_attn.q_proj.bias"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/out/kernel", f"text_model.encoder.layers.{i}.self_attn.out_proj.weight"))
        rename_keys.append((f"params/txt/Encoder_0/encoderblock_{i}/MultiHeadDotProductAttention_0/out/bias", f"text_model.encoder.layers.{i}.self_attn.out_proj.bias"))

    # å°†æœ€ç»ˆå±‚çš„å‚æ•°é‡å‘½åå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("params/txt/Encoder_0/encoder_norm/scale", "text_model.final_layer_norm.weight"))
    rename_keys.append(("params/txt/Encoder_0/encoder_norm/bias", "text_model.final_layer_norm.bias"))
    rename_keys.append(("params/txt/head/kernel", "text_model.head.weight"))
    rename_keys.append(("params/txt/head/bias", "text_model.head.bias"))

    # å­¦ä¹ çš„æ¸©åº¦å’Œåå·®
    # å°†("params/t", "logit_scale")æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("params/t", "logit_scale"))
    # å°†("params/b", "logit_bias")æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
    rename_keys.append(("params/b", "logit_bias"))

    # æ ¼å¼åŒ–ä»£ç ï¼Œç»“æŸfmt: offåŒºå—
    # fmt: on
    # è¿”å›é‡å‘½åé”®åˆ—è¡¨
    return rename_keys
# é‡å‘½åå­—å…¸ä¸­çš„é”®ï¼Œå¹¶æ ¹æ®é…ç½®å¯¹å€¼è¿›è¡Œç›¸åº”çš„å¤„ç†
def rename_key(dct, old, new, config):
    # å¼¹å‡ºæ—§é”®å¯¹åº”çš„å€¼
    val = dct.pop(old)

    # æ ¹æ®æ–°é”®çš„ç‰¹å¾å’Œé…ç½®å¯¹å€¼è¿›è¡Œä¸åŒçš„reshapeæ“ä½œ
    if ("out_proj" in new or "v_proj" in new or "k_proj" in new or "q_proj" in new) and "vision" in new:
        val = val.reshape(-1, config.vision_config.hidden_size)
    if ("out_proj" in new or "v_proj" in new or "k_proj" in new or "q_proj" in new) and "text" in new:
        val = val.reshape(-1, config.text_config.hidden_size)

    # æ ¹æ®æ–°é”®çš„ç‰¹å¾å¯¹å€¼è¿›è¡Œä¸åŒçš„transposeæ“ä½œ
    if "patch_embedding.weight" in new:
        val = val.transpose(3, 2, 0, 1)
    elif new.endswith("weight") and "position_embedding" not in new and "token_embedding" not in new:
        val = val.T

    # æ ¹æ®æ–°é”®çš„ç‰¹å¾å’Œé…ç½®å¯¹å€¼è¿›è¡Œä¸åŒçš„reshapeæ“ä½œ
    if "position_embedding" in new and "vision" in new:
        val = val.reshape(-1, config.vision_config.hidden_size)
    if "position_embedding" in new and "text" in new:
        val = val.reshape(-1, config.text_config.hidden_size)

    # æ ¹æ®æ–°é”®çš„ç‰¹å¾å¯¹å€¼è¿›è¡Œreshapeæ“ä½œ
    if new.endswith("bias"):
        val = val.reshape(-1)

    # å°†å¤„ç†åçš„å€¼æ·»åŠ åˆ°å­—å…¸ä¸­
    dct[new] = torch.from_numpy(val)


# è¯»å–è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
def read_in_q_k_v_head(state_dict, config):
    # è¯»å–å¹¶å¤„ç†é”®ã€å€¼ã€æŸ¥è¯¢çš„æŠ•å½±å±‚æƒé‡å’Œåç½®
    key_proj_weight = (
        state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/key/kernel")
        .reshape(-1, config.vision_config.hidden_size)
        .T
    )
    key_proj_bias = state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/key/bias").reshape(-1)
    value_proj_weight = (
        state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/value/kernel")
        .reshape(-1, config.vision_config.hidden_size)
        .T
    )
    value_proj_bias = state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/value/bias").reshape(-1)
    query_proj_weight = (
        state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/query/kernel")
        .reshape(-1, config.vision_config.hidden_size)
        .T
    )
    query_proj_bias = state_dict.pop("params/img/MAPHead_0/MultiHeadDotProductAttention_0/query/bias").reshape(-1)

    # å°†å¤„ç†åçš„æƒé‡å’Œåç½®åˆå¹¶ï¼Œå¹¶æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
    state_dict["vision_model.head.attention.in_proj_weight"] = torch.from_numpy(
        np.concatenate([query_proj_weight, key_proj_weight, value_proj_weight], axis=0)
    )
    state_dict["vision_model.head.attention.in_proj_bias"] = torch.from_numpy(
        np.concatenate([query_proj_bias, key_proj_bias, value_proj_bias], axis=0)
    )


# å‡†å¤‡ä¸€å¼ å¯çˆ±çŒ«å’ªçš„å›¾ç‰‡
def prepare_img():
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # ä»URLè·å–å›¾ç‰‡å¹¶è¿”å›
    image = Image.open(requests.get(url, stream=True).raw)
    return image


# å°†åµŒå¥—å­—å…¸å±•å¹³ä¸ºä¸€çº§å­—å…¸
def flatten_nested_dict(params, parent_key="", sep="/"):
    items = []

    # éå†å­—å…¸ä¸­çš„é”®å€¼å¯¹
    for k, v in params.items():
        new_key = parent_key + sep + k if parent_key else k

        # å¦‚æœå€¼æ˜¯å­—å…¸ï¼Œåˆ™é€’å½’å±•å¹³
        if isinstance(v, collections.abc.MutableMapping):
            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)


# ç¦ç”¨æ¢¯åº¦è®¡ç®—
@torch.no_grad()
def convert_siglip_checkpoint(model_name, pytorch_dump_folder_path, verify_logits=True, push_to_hub=False):
    """
    Copy/paste/tweak model's weights to our SigLIP structure.
    """

    # å®šä¹‰é»˜è®¤çš„ SigLIP é…ç½®
    config = get_siglip_config(model_name)

    # è·å–æ£€æŸ¥ç‚¹
    checkpoint = model_name_to_checkpoint[model_name]

    # è·å–è¯æ±‡æ–‡ä»¶
    if "i18n" in model_name:
        vocab_file = "/Users/nielsrogge/Documents/SigLIP/multilingual_vocab/sentencepiece.model"
    else:
        vocab_file = "/Users/nielsrogge/Documents/SigLIP/english_vocab/sentencepiece.model"

    # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
    data = load(checkpoint)
    state_dict = flatten_nested_dict(data)

    # åˆ é™¤å’Œé‡å‘½åä¸€äº›é”®
    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:
        rename_key(state_dict, src, dest, config)

    # æ³¨æ„åŠ›æ± åŒ–å¤´çš„ qkv çŸ©é˜µéœ€è¦ç‰¹æ®Šå¤„ç†
    read_in_q_k_v_head(state_dict, config)

    # åŠ è½½ HuggingFace æ¨¡å‹
    model = SiglipModel(config).eval()
    model.load_state_dict(state_dict)

    # åˆ›å»ºå¤„ç†å™¨
    # é‡è¦: ä½¿ä»¤ç‰ŒåŒ–å™¨ä¸è¿”å› attention_maskï¼Œå› ä¸ºåŸå§‹çš„ä¸éœ€è¦
    image_size = config.vision_config.image_size
    size = {"height": image_size, "width": image_size}
    image_processor = SiglipImageProcessor(size=size)
    tokenizer = SiglipTokenizer(vocab_file=vocab_file, model_input_names=["input_ids"])
    processor = SiglipProcessor(image_processor=image_processor, tokenizer=tokenizer)

    # åœ¨è™šæ‹Ÿå›¾ç‰‡å’Œæ–‡æœ¬ä¸Šè¿›è¡ŒéªŒè¯
    url_1 = "https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg"
    image_1 = Image.open(requests.get(url_1, stream=True).raw).convert("RGB")
    url_2 = "https://cdn.openai.com/multimodal-neurons/assets/apple/apple-blank.jpg"
    image_2 = Image.open(requests.get(url_2, stream=True).raw).convert("RGB")
    texts = ["an apple", "a picture of an apple"]

    inputs = processor(images=[image_1, image_2], text=texts, return_tensors="pt", padding="max_length")

    # é’ˆå¯¹åŸå§‹å€¼éªŒè¯ input_ids
    if image_size == 224:
        filename = "siglip_pixel_values.pt"
    elif image_size == 256:
        filename = "siglip_pixel_values_256.pt"
    elif image_size == 384:
        filename = "siglip_pixel_values_384.pt"
    elif image_size == 512:
        filename = "siglip_pixel_values_512.pt"
    else:
        raise ValueError("Image size not supported")

    filepath = hf_hub_download(repo_id="nielsr/test-image", filename=filename, repo_type="dataset")
    original_pixel_values = torch.load(filepath)
    filepath = hf_hub_download(repo_id="nielsr/test-image", filename="siglip_input_ids.pt", repo_type="dataset")
    original_input_ids = torch.load(filepath)

    if "i18n" not in model_name:
        assert inputs.input_ids.tolist() == original_input_ids.tolist()

    print("Mean of original pixel values:", original_pixel_values.mean())
    # æ‰“å°æ–°åƒç´ å€¼çš„å¹³å‡å€¼
    print("Mean of new pixel values:", inputs.pixel_values.mean())

    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨åŸå§‹åƒç´ å€¼è¿›è¡Œæµ‹è¯•ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ç¡®åˆ‡çš„åƒç´ å€¼
    # ä½¿ç”¨ torch.no_grad() æ¥ç¦ç”¨æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œä¼ å…¥è¾“å…¥æ–‡æœ¬çš„ IDs å’ŒåŸå§‹åƒç´ å€¼
        outputs = model(input_ids=inputs.input_ids, pixel_values=original_pixel_values)

    # æ‰“å°è¾“å‡º logits çš„å‰ä¸‰è¡Œä¸‰åˆ—
    print(outputs.logits_per_image[:3, :3])

    # è®¡ç®—æ¯ä¸ªåƒç´ å€¼å¯¹åº”çš„æ¦‚ç‡
    probs = torch.sigmoid(outputs.logits_per_image)  # è¿™äº›æ˜¯æ¦‚ç‡å€¼
    # æ‰“å°ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯æ–‡æœ¬0çš„æ¦‚ç‡
    print(f"{probs[0][0]:.1%} that image 0 is '{texts[0]}'")
    # æ‰“å°ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯æ–‡æœ¬1çš„æ¦‚ç‡
    print(f"{probs[0][1]:.1%} that image 0 is '{texts[1]}'")

    # å¦‚æœéœ€è¦éªŒè¯ logits
    if verify_logits:
        # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®é¢„æœŸçš„ slice
        if model_name == "siglip-base-patch16-224":
            expected_slice = torch.tensor(
                [[-2.9621, -2.1672], [-0.2713, 0.2910]],
            )
        elif model_name == "siglip-base-patch16-256":
            expected_slice = torch.tensor(
                [[-3.1146, -1.9894], [-0.7312, 0.6387]],
            )
        elif model_name == "siglip-base-patch16-384":
            expected_slice = torch.tensor(
                [[-2.8098, -2.1891], [-0.4242, 0.4102]],
            )
        elif model_name == "siglip-base-patch16-512":
            expected_slice = torch.tensor(
                [[-2.7899, -2.2668], [-0.4295, -0.0735]],
            )
        elif model_name == "siglip-large-patch16-256":
            expected_slice = torch.tensor(
                [[-1.5827, -0.5801], [-0.9153, 0.1363]],
            )
        elif model_name == "siglip-large-patch16-384":
            expected_slice = torch.tensor(
                [[-2.1523, -0.2899], [-0.2959, 0.7884]],
            )
        elif model_name == "siglip-so400m-patch14-384":
            expected_slice = torch.tensor([[-1.2441, -0.6649], [-0.7060, 0.7374]])
        elif model_name == "siglip-base-patch16-256-i18n":
            expected_slice = torch.tensor(
                [[-0.9064, 0.1073], [-0.0299, 0.5304]],
            )

        # ä½¿ç”¨ assert æ£€æŸ¥ logits æ˜¯å¦ä¸é¢„æœŸçš„ slice æ¥è¿‘
        assert torch.allclose(outputs.logits_per_image[:3, :3], expected_slice, atol=1e-4)
        print("Looks ok!")

    # å¦‚æœæŒ‡å®šäº† pytorch_dump_folder_path
    if pytorch_dump_folder_path is not None:
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        # æ‰“å°æ­£åœ¨ä¿å­˜æ¨¡å‹çš„æ¶ˆæ¯
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        # ä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # æ‰“å°æ­£åœ¨ä¿å­˜ processor çš„æ¶ˆæ¯
        print(f"Saving processor to {pytorch_dump_folder_path}")
        # ä¿å­˜ processor åˆ°æŒ‡å®šè·¯å¾„
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        # å°†æ¨¡å‹æ¨é€åˆ° Hub
        model.push_to_hub(f"nielsr/{model_name}")
        # å°† processor æ¨é€åˆ° Hub
        processor.push_to_hub(f"nielsr/{model_name}")
# å¦‚æœè„šæœ¬è¢«ç›´æ¥æ‰§è¡Œï¼ˆè€Œä¸æ˜¯è¢«å¯¼å…¥åˆ°å…¶ä»–è„šæœ¬ä¸­ï¼‰ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€çš„å‚æ•°
    parser.add_argument(
        "--model_name",  # æ¨¡å‹åç§°å‚æ•°
        default="siglip-base-patch16-224",  # é»˜è®¤æ¨¡å‹åç§°
        type=str,  # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        choices=model_name_to_checkpoint.keys(),  # å¯é€‰çš„æ¨¡å‹åç§°åˆ—è¡¨
        help="Name of the model you'd like to convert.",  # å¸®åŠ©ä¿¡æ¯
    )
    parser.add_argument(
        "--pytorch_dump_folder_path",  # PyTorchæ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„å‚æ•°
        default=None,  # é»˜è®¤ä¸ºNone
        type=str,  # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        help="Path to the output PyTorch model directory."  # å¸®åŠ©ä¿¡æ¯
    )
    parser.add_argument(
        "--verify_logits",  # éªŒè¯logitså‚æ•°
        action="store_false",  # è®¾ç½®ä¸ºFalseæ—¶æ‰§è¡ŒåŠ¨ä½œ
        help="Whether to verify logits against the original implementation."  # å¸®åŠ©ä¿¡æ¯
    )
    parser.add_argument(
        "--push_to_hub",  # æ¨é€åˆ°hubå‚æ•°
        action="store_true",  # è®¾ç½®ä¸ºTrueæ—¶æ‰§è¡ŒåŠ¨ä½œ
        help="Whether or not to push the converted model to the ğŸ¤— hub."  # å¸®åŠ©ä¿¡æ¯
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°ä»¥è½¬æ¢æ¨¡å‹
    convert_siglip_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.verify_logits, args.push_to_hub)
```