# `.\models\data2vec\convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py`

```py
#!/usr/bin/env python3
import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£æåº“
import json  # å¯¼å…¥ JSON æ“ä½œåº“

import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ åº“
from huggingface_hub import hf_hub_download  # å¯¼å…¥ Hugging Face Hub ä¸‹è½½å‡½æ•°
from PIL import Image  # å¯¼å…¥ PIL å›¾åƒå¤„ç†åº“
from timm.models import create_model  # å¯¼å…¥ timm æ¨¡å‹åˆ›å»ºå‡½æ•°

from transformers import (  # å¯¼å…¥ transformers åº“ä¸­çš„ä»¥ä¸‹æ¨¡å—
    BeitImageProcessor,  # Beit å›¾åƒå¤„ç†å™¨
    Data2VecVisionConfig,  # Data2Vec è§†è§‰é…ç½®ç±»
    Data2VecVisionForImageClassification,  # Data2Vec å›¾åƒåˆ†ç±»æ¨¡å‹
    Data2VecVisionModel,  # Data2Vec è§†è§‰æ¨¡å‹
)


def create_rename_keys(config, has_lm_head=False, is_semantic=False, hf_prefix="data2vec."):
    prefix = "backbone." if is_semantic else ""  # æ ¹æ®æ˜¯å¦è¯­ä¹‰åŒ–è®¾ç½®å‰ç¼€

    rename_keys = []  # åˆå§‹åŒ–é‡å‘½åé”®åˆ—è¡¨
    for i in range(config.num_hidden_layers):
        # encoder layers: output projection, 2 feedforward neural networks and 2 layernorms
        # ç¼–ç å™¨å±‚ï¼šè¾“å‡ºæŠ•å½±ã€2ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œå’Œ2ä¸ªå±‚å½’ä¸€åŒ–
        rename_keys.append(
            (f"{prefix}blocks.{i}.norm1.weight", f"{hf_prefix}encoder.layer.{i}.layernorm_before.weight")
        )  # æ·»åŠ æƒé‡å½’ä¸€åŒ–å‰çš„é‡å‘½åé”®
        rename_keys.append((f"{prefix}blocks.{i}.norm1.bias", f"{hf_prefix}encoder.layer.{i}.layernorm_before.bias"))  # æ·»åŠ åç½®å½’ä¸€åŒ–å‰çš„é‡å‘½åé”®
        rename_keys.append(
            (f"{prefix}blocks.{i}.attn.proj.weight", f"{hf_prefix}encoder.layer.{i}.attention.output.dense.weight")
        )  # æ·»åŠ æ³¨æ„åŠ›æŠ•å½±å±‚æƒé‡çš„é‡å‘½åé”®
        rename_keys.append(
            (f"{prefix}blocks.{i}.attn.proj.bias", f"{hf_prefix}encoder.layer.{i}.attention.output.dense.bias")
        )  # æ·»åŠ æ³¨æ„åŠ›æŠ•å½±å±‚åç½®çš„é‡å‘½åé”®
        rename_keys.append(
            (f"{prefix}blocks.{i}.norm2.weight", f"{hf_prefix}encoder.layer.{i}.layernorm_after.weight")
        )  # æ·»åŠ æƒé‡å½’ä¸€åŒ–åçš„é‡å‘½åé”®
        rename_keys.append((f"{prefix}blocks.{i}.norm2.bias", f"{hf_prefix}encoder.layer.{i}.layernorm_after.bias"))  # æ·»åŠ åç½®å½’ä¸€åŒ–åçš„é‡å‘½åé”®
        rename_keys.append(
            (f"{prefix}blocks.{i}.mlp.fc1.weight", f"{hf_prefix}encoder.layer.{i}.intermediate.dense.weight")
        )  # æ·»åŠ ä¸­é—´å±‚ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚æƒé‡çš„é‡å‘½åé”®
        rename_keys.append(
            (f"{prefix}blocks.{i}.mlp.fc1.bias", f"{hf_prefix}encoder.layer.{i}.intermediate.dense.bias")
        )  # æ·»åŠ ä¸­é—´å±‚ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚åç½®çš„é‡å‘½åé”®
        rename_keys.append((f"{prefix}blocks.{i}.mlp.fc2.weight", f"{hf_prefix}encoder.layer.{i}.output.dense.weight"))
        rename_keys.append((f"{prefix}blocks.{i}.mlp.fc2.bias", f"{hf_prefix}encoder.layer.{i}.output.dense.bias"))  # æ·»åŠ ä¸­é—´å±‚ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚åç½®çš„é‡å‘½åé”®

    # projection layer + position embeddings
    # æŠ•å½±å±‚ + ä½ç½®åµŒå…¥
    rename_keys.extend(
        [
            (f"{prefix}cls_token", f"{hf_prefix}embeddings.cls_token"),  # æ·»åŠ ç±»åˆ«æ ‡è®°çš„é‡å‘½åé”®
            (f"{prefix}patch_embed.proj.weight", f"{hf_prefix}embeddings.patch_embeddings.projection.weight"),  # æ·»åŠ æŠ•å½±å±‚æƒé‡çš„é‡å‘½åé”®
            (f"{prefix}patch_embed.proj.bias", f"{hf_prefix}embeddings.patch_embeddings.projection.bias"),  # æ·»åŠ æŠ•å½±å±‚åç½®çš„é‡å‘½åé”®
        ]
    )
    # å¦‚æœå…·æœ‰è¯­è¨€æ¨¡å‹å¤´éƒ¨
    if has_lm_head:
        # å°†ä»¥ä¸‹é”®å€¼å¯¹æ·»åŠ åˆ°é‡å‘½ååˆ—è¡¨ï¼Œç”¨äºé‡å‘½åæ¨¡å‹çš„ä¸åŒéƒ¨åˆ†
        rename_keys.extend(
            [
                ("mask_token", f"{hf_prefix}embeddings.mask_token"),  # é‡å‘½åæ©ç æ ‡è®°
                (
                    "rel_pos_bias.relative_position_bias_table",
                    f"{hf_prefix}encoder.relative_position_bias.relative_position_bias_table",  # é‡å‘½åç›¸å¯¹ä½ç½®åç½®è¡¨
                ),
                (
                    "rel_pos_bias.relative_position_index",
                    f"{hf_prefix}encoder.relative_position_bias.relative_position_index",  # é‡å‘½åç›¸å¯¹ä½ç½®ç´¢å¼•
                ),
                ("norm.weight", "layernorm.weight"),  # é‡å‘½åå½’ä¸€åŒ–å±‚æƒé‡
                ("norm.bias", "layernorm.bias"),  # é‡å‘½åå½’ä¸€åŒ–å±‚åç½®
            ]
        )
    # å¦‚æœæ˜¯è¯­ä¹‰ä»»åŠ¡
    elif is_semantic:
        # å°†ä»¥ä¸‹é”®å€¼å¯¹æ·»åŠ åˆ°é‡å‘½ååˆ—è¡¨ï¼Œç”¨äºè¯­ä¹‰åˆ†å‰²åˆ†ç±»å¤´éƒ¨çš„é‡å‘½å
        rename_keys.extend(
            [
                ("decode_head.conv_seg.weight", "decode_head.classifier.weight"),  # é‡å‘½åè§£ç å¤´éƒ¨å·ç§¯å±‚æƒé‡
                ("decode_head.conv_seg.bias", "decode_head.classifier.bias"),  # é‡å‘½åè§£ç å¤´éƒ¨å·ç§¯å±‚åç½®
                ("auxiliary_head.conv_seg.weight", "auxiliary_head.classifier.weight"),  # é‡å‘½åè¾…åŠ©å¤´éƒ¨å·ç§¯å±‚æƒé‡
                ("auxiliary_head.conv_seg.bias", "auxiliary_head.classifier.bias"),  # é‡å‘½åè¾…åŠ©å¤´éƒ¨å·ç§¯å±‚åç½®
            ]
        )
    else:
        # å°†ä»¥ä¸‹é”®å€¼å¯¹æ·»åŠ åˆ°é‡å‘½ååˆ—è¡¨ï¼Œç”¨äºå¸¸è§„çš„åˆ†ç±»ä»»åŠ¡å¤´éƒ¨é‡å‘½å
        rename_keys.extend(
            [
                ("fc_norm.weight", f"{hf_prefix}pooler.layernorm.weight"),  # é‡å‘½åå…¨è¿æ¥å±‚å½’ä¸€åŒ–å±‚æƒé‡
                ("fc_norm.bias", f"{hf_prefix}pooler.layernorm.bias"),  # é‡å‘½åå…¨è¿æ¥å±‚å½’ä¸€åŒ–å±‚åç½®
                ("head.weight", "classifier.weight"),  # é‡å‘½ååˆ†ç±»å¤´éƒ¨æƒé‡
                ("head.bias", "classifier.bias"),  # é‡å‘½ååˆ†ç±»å¤´éƒ¨åç½®
            ]
        )
    
    return rename_keys  # è¿”å›åŒ…å«æ‰€æœ‰é‡å‘½åé”®å€¼å¯¹çš„åˆ—è¡¨
# è¯»å–è¾“å…¥çš„çŠ¶æ€å­—å…¸ï¼Œæ ¹æ®é…ç½®å’Œæ¡ä»¶é‡æ–°ç»„ç»‡å…¶å†…å®¹
def read_in_q_k_v(state_dict, config, has_lm_head=False, is_semantic=False, hf_prefix="data2vec_vision."):
    # éå†é…ç½®ä¸­æŒ‡å®šæ•°é‡çš„éšè—å±‚
    for i in range(config.num_hidden_layers):
        # æ ¹æ®è¯­ä¹‰å’Œå‰ç¼€ç¡®å®šå½“å‰å±‚çš„å‰ç¼€
        prefix = "backbone." if is_semantic else ""

        # è¯»å–å¹¶ç§»é™¤å½“å‰å±‚æ³¨æ„åŠ›æœºåˆ¶çš„æŸ¥è¯¢ã€é”®å’Œå€¼çš„æƒé‡
        in_proj_weight = state_dict.pop(f"{prefix}blocks.{i}.attn.qkv.weight")
        q_bias = state_dict.pop(f"{prefix}blocks.{i}.attn.q_bias")
        v_bias = state_dict.pop(f"{prefix}blocks.{i}.attn.v_bias")

        # å°†æŸ¥è¯¢æƒé‡æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.query.weight"] = in_proj_weight[
            : config.hidden_size, :
        ]
        # å°†æŸ¥è¯¢åç½®æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.query.bias"] = q_bias
        # å°†é”®æƒé‡æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.key.weight"] = in_proj_weight[
            config.hidden_size : config.hidden_size * 2, :
        ]
        # å°†å€¼æƒé‡æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.value.weight"] = in_proj_weight[
            -config.hidden_size :, :
        ]
        # å°†å€¼åç½®æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.value.bias"] = v_bias

        # è¯»å–å¹¶ç§»é™¤å½“å‰å±‚çš„ gamma_1 å’Œ gamma_2
        gamma_1 = state_dict.pop(f"{prefix}blocks.{i}.gamma_1")
        gamma_2 = state_dict.pop(f"{prefix}blocks.{i}.gamma_2")

        # å°† gamma_1 æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.lambda_1"] = gamma_1
        # å°† gamma_2 æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.lambda_2"] = gamma_2

        # å¦‚æœæ²¡æœ‰è¯­è¨€æ¨¡å‹å¤´éƒ¨ï¼Œå¤„ç†ç›¸å¯¹ä½ç½®åç½®è¡¨å’Œç´¢å¼•
        if not has_lm_head:
            # ç§»é™¤å½“å‰å±‚çš„ç›¸å¯¹ä½ç½®åç½®è¡¨å’Œç´¢å¼•
            table = state_dict.pop(f"{prefix}blocks.{i}.attn.relative_position_bias_table")
            index = state_dict.pop(f"{prefix}blocks.{i}.attn.relative_position_index")

            # å°†ç›¸å¯¹ä½ç½®åç½®è¡¨æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
            state_dict[
                f"{hf_prefix}encoder.layer.{i}.attention.attention.relative_position_bias.relative_position_bias_table"
            ] = table
            # å°†ç›¸å¯¹ä½ç½®ç´¢å¼•æ”¾å…¥é¢„å®šä¹‰çš„ä½ç½®
            state_dict[
                f"{hf_prefix}encoder.layer.{i}.attention.attention.relative_position_bias.relative_position_index"
            ] = index


# è·å–å‘½ä»¤è¡Œå‚æ•°
def get_args():
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        "Convert Data2VecVision to HF for image classification and pretraining", add_help=False
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šHF æ£€æŸ¥ç‚¹åç§°
    parser.add_argument("--hf_checkpoint_name", type=str)
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè¾“å…¥å›¾åƒå¤§å°ï¼Œé»˜è®¤ä¸º 224
    parser.add_argument("--input_size", default=224, type=int, help="images input size")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šBEiT æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
    parser.add_argument("--beit_checkpoint", default="", help="beit checkpoint")

    # è§£æå¹¶è¿”å›å‘½ä»¤è¡Œå‚æ•°
    return parser.parse_args()


# åŠ è½½ BEiT æ¨¡å‹
def load_beit_model(args, is_finetuned, is_large):
    # åŠ è½½æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œç”¨äºæ¨¡å‹æƒé‡åˆå§‹åŒ–
    def load_state_dict(model, state_dict, prefix="", ignore_missing="relative_position_index"):
        # ç”¨äºå­˜å‚¨æ‰¾ä¸åˆ°çš„é”®çš„åˆ—è¡¨
        missing_keys = []
        # ç”¨äºå­˜å‚¨æ„å¤–çš„é”®çš„åˆ—è¡¨
        unexpected_keys = []
        # ç”¨äºå­˜å‚¨é”™è¯¯æ¶ˆæ¯çš„åˆ—è¡¨
        error_msgs = []

        # å¤åˆ¶ state_dict ä»¥ä¾¿ _load_from_state_dict å¯ä»¥ä¿®æ”¹å®ƒ
        metadata = getattr(state_dict, "_metadata", None)
        state_dict = state_dict.copy()
        if metadata is not None:
            state_dict._metadata = metadata

        # é€’å½’åŠ è½½æ¨¡å‹çš„æ¯ä¸ªæ¨¡å—çš„çŠ¶æ€å­—å…¸
        def load(module, prefix=""):
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            module._load_from_state_dict(
                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs
            )
            for name, child in module._modules.items():
                if child is not None:
                    load(child, prefix + name + ".")

        load(model, prefix=prefix)

        # æ ¹æ®æŒ‡å®šçš„ ignore_missing è§„åˆ™ç­›é€‰å‡ºéœ€è¦è­¦å‘Šçš„ç¼ºå¤±é”®å’Œéœ€è¦å¿½ç•¥çš„é”®
        warn_missing_keys = []
        ignore_missing_keys = []
        for key in missing_keys:
            keep_flag = True
            for ignore_key in ignore_missing.split("|"):
                if ignore_key in key:
                    keep_flag = False
                    break
            if keep_flag:
                warn_missing_keys.append(key)
            else:
                ignore_missing_keys.append(key)

        # æ›´æ–° missing_keys ä¸º warn_missing_keys
        missing_keys = warn_missing_keys

        # è¾“å‡ºæ¨¡å‹æƒé‡æœªåˆå§‹åŒ–çš„è­¦å‘Šä¿¡æ¯
        if len(missing_keys) > 0:
            print(
                "Weights of {} not initialized from pretrained model: {}".format(
                    model.__class__.__name__, missing_keys
                )
            )
        # è¾“å‡ºæœªä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„ä¿¡æ¯
        if len(unexpected_keys) > 0:
            print("Weights from pretrained model not used in {}: {}".format(model.__class__.__name__, unexpected_keys))
        # è¾“å‡ºè¢«å¿½ç•¥çš„æ¨¡å‹æƒé‡æœªåˆå§‹åŒ–çš„ä¿¡æ¯
        if len(ignore_missing_keys) > 0:
            print(
                "Ignored weights of {} not initialized from pretrained model: {}".format(
                    model.__class__.__name__, ignore_missing_keys
                )
            )
        # è¾“å‡ºåŠ è½½æ¨¡å‹è¿‡ç¨‹ä¸­çš„é”™è¯¯æ¶ˆæ¯
        if len(error_msgs) > 0:
            print("\n".join(error_msgs))

    # å®šä¹‰æ¨¡å‹çš„å…³é”®å­—å‚æ•°å­—å…¸
    model_kwargs = {
        "pretrained": False,
        "use_shared_rel_pos_bias": True,
        "use_abs_pos_emb": False,
        "init_values": 0.1,
    }

    # å¦‚æœæ˜¯å¾®è°ƒè¿‡çš„æ¨¡å‹ï¼Œæ›´æ–°æ¨¡å‹å…³é”®å­—å‚æ•°å­—å…¸
    if is_finetuned:
        model_kwargs.update(
            {
                "num_classes": 1000,
                "use_mean_pooling": True,
                "init_scale": 0.001,
                "use_rel_pos_bias": True,
            }
        )

    # åˆ›å»ºæŒ‡å®šé…ç½®çš„æ¨¡å‹å®ä¾‹
    model = create_model(
        "beit_large_patch16_224" if is_large else "beit_base_patch16_224",
        **model_kwargs,
    )
    # è·å–æ¨¡å‹çš„è¡¥ä¸åµŒå…¥å±‚çš„è¡¥ä¸å¤§å°
    patch_size = model.patch_embed.patch_size
    # è®¡ç®—çª—å£å¤§å°
    args.window_size = (args.input_size // patch_size[0], args.input_size // patch_size[1])
    # åŠ è½½ PyTorch æ¨¡å‹æ£€æŸ¥ç‚¹
    checkpoint = torch.load(args.beit_checkpoint, map_location="cpu")

    # æ‰“å°åŠ è½½æ£€æŸ¥ç‚¹çš„ä¿¡æ¯
    print(f"Load ckpt from {args.beit_checkpoint}")
    # åˆå§‹åŒ–æ£€æŸ¥ç‚¹æ¨¡å‹
    checkpoint_model = None
    # éå†æŒ‡å®šçš„æ¨¡å‹å…³é”®å­—åˆ—è¡¨ï¼Œæ£€æŸ¥æ£€æŸ¥ç‚¹ä¸­æ˜¯å¦å­˜åœ¨è¯¥å…³é”®å­—
    for model_key in ("model", "module"):
        # å¦‚æœæ‰¾åˆ°äº†æŒ‡å®šçš„æ¨¡å‹å…³é”®å­—
        if model_key in checkpoint:
            # ä»æ£€æŸ¥ç‚¹ä¸­è·å–ç›¸åº”æ¨¡å‹çš„çŠ¶æ€å­—å…¸
            checkpoint_model = checkpoint[model_key]
            # æ‰“å°åŠ è½½çŠ¶æ€å­—å…¸çš„æ¶ˆæ¯ï¼ŒæŒ‡å®šåŠ è½½çš„æ¨¡å‹å…³é”®å­—
            print(f"Load state_dict by model_key = {model_key}")
            # ä¸­æ–­å¾ªç¯ï¼Œå·²æ‰¾åˆ°å¹¶åŠ è½½äº†çŠ¶æ€å­—å…¸
            break

    # è·å–æ‰€æœ‰çŠ¶æ€å­—å…¸é”®çš„åˆ—è¡¨
    all_keys = list(checkpoint_model.keys())
    # éå†æ‰€æœ‰çŠ¶æ€å­—å…¸çš„é”®
    for key in all_keys:
        # å¦‚æœé”®åŒ…å«"relative_position_index"å­—ç¬¦ä¸²
        if "relative_position_index" in key:
            # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤è¯¥é”®åŠå…¶å¯¹åº”çš„å€¼
            checkpoint_model.pop(key)

        # å¦‚æœé”®åŒ…å«"relative_position_bias_table"å­—ç¬¦ä¸²
        if "relative_position_bias_table" in key:
            # è·å–ç›¸å¯¹ä½ç½®åç½®è¡¨çš„å€¼
            rel_pos_bias = checkpoint_model[key]
            # è·å–æºå’Œç›®æ ‡æ¨¡å‹ä¸­çš„ä½ç½®æ•°é‡åŠæ³¨æ„åŠ›å¤´æ•°
            src_num_pos, num_attn_heads = rel_pos_bias.size()
            dst_num_pos, _ = model.state_dict()[key].size()
            dst_patch_shape = model.patch_embed.patch_shape
            # æ£€æŸ¥ç›®æ ‡æ¨¡å‹çš„è¡¥ä¸å½¢çŠ¶æ˜¯å¦ä¸ºæ–¹å½¢ï¼Œè‹¥ä¸æ˜¯åˆ™æŠ›å‡ºæœªå®ç°çš„é”™è¯¯
            if dst_patch_shape[0] != dst_patch_shape[1]:
                raise NotImplementedError()

    # ä½¿ç”¨åŠ è½½çš„çŠ¶æ€å­—å…¸æ›´æ–°æ¨¡å‹çš„å‚æ•°
    load_state_dict(model, checkpoint_model, prefix="")

    # è¿”å›æ›´æ–°åçš„æ¨¡å‹
    return model
def main():
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()

    # æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†å¾®è°ƒ
    is_finetuned = "ft1k" in args.hf_checkpoint_name
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸ºå¤§æ¨¡å‹
    is_large = "large" in args.hf_checkpoint_name

    if is_finetuned:
        # å¦‚æœè¿›è¡Œäº†å¾®è°ƒï¼Œå¯¼å…¥å¾®è°ƒæ¨¡å‹çš„ä»£ç 
        # ä½ éœ€è¦å°† https://github.com/facebookresearch/data2vec_vision/blob/main/beit/modeling_finetune.py
        # å¤åˆ¶åˆ°å½“å‰æ–‡ä»¶å¤¹ä¸­ã€‚
        import modeling_finetune  # noqa: F401
    else:
        # å¦‚æœæ²¡æœ‰è¿›è¡Œå¾®è°ƒï¼Œå¯¼å…¥å‘¨æœŸæ€§æ¨¡å‹çš„ä»£ç 
        # ä½ éœ€è¦å°† https://github.com/facebookresearch/data2vec_vision/blob/main/beit/modeling_cyclical.py
        # å¤åˆ¶åˆ°å½“å‰æ–‡ä»¶å¤¹ä¸­ã€‚
        # æ³¨æ„ï¼šç›®å‰æˆ‘ä»¬åªè½¬æ¢äº†ä¸‹æ¸¸æ¨¡å‹è€Œä¸æ˜¯å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™æ„å‘³ç€åœ¨é›†æˆæµ‹è¯•ä¸­ï¼Œä½ éœ€è¦åœ¨ä»¥ä¸‹è¡Œä¹‹åæ·»åŠ  `return x`ï¼š
        # https://github.com/facebookresearch/data2vec_vision/blob/af9a36349aaed59ae66e69b5dabeef2d62fdc5da/beit/modeling_cyclical.py#L197
        import modeling_cyclical  # noqa: F401

    # 1. åˆ›å»ºæ¨¡å‹é…ç½®
    config = Data2VecVisionConfig()
    if is_finetuned:
        # å¦‚æœè¿›è¡Œäº†å¾®è°ƒï¼Œè®¾ç½®ç‰¹å®šçš„é…ç½®é€‰é¡¹
        config.use_relative_position_bias = True
        config.use_shared_relative_position_bias = False
        config.use_mean_pooling = True
        config.num_labels = 1000

        # ä¸‹è½½å¹¶åŠ è½½ ImageNet ç±»æ ‡ç­¾æ˜ å°„
        repo_id = "huggingface/label-files"
        filename = "imagenet-1k-id2label.json"
        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
        id2label = {int(k): v for k, v in id2label.items()}
        config.id2label = id2label
        config.label2id = {v: k for k, v in id2label.items()}
    else:
        # å¦‚æœæ²¡æœ‰è¿›è¡Œå¾®è°ƒï¼Œè®¾ç½®é»˜è®¤çš„é…ç½®é€‰é¡¹
        config.use_relative_position_bias = False
        config.use_shared_relative_position_bias = True
        config.use_mean_pooling = False

    if is_large:
        # å¦‚æœæ¨¡å‹æ˜¯å¤§æ¨¡å‹ï¼Œè®¾ç½®å¤§æ¨¡å‹ç‰¹æœ‰çš„é…ç½®é€‰é¡¹
        config.hidden_size = 1024
        config.intermediate_size = 4096
        config.num_hidden_layers = 24
        config.num_attention_heads = 16

    # 2. åŠ è½½ Beit æ¨¡å‹
    orig_model = load_beit_model(args, is_finetuned, is_large)
    orig_model.eval()

    # 3. å‰å‘ä¼ æ’­ Beit æ¨¡å‹
    image_processor = BeitImageProcessor(size=config.image_size, do_center_crop=False)
    image = Image.open("../../../../tests/fixtures/tests_samples/COCO/000000039769.png")
    encoding = image_processor(images=image, return_tensors="pt")
    pixel_values = encoding["pixel_values"]

    orig_args = (pixel_values,) if is_finetuned else (pixel_values, None)
    with torch.no_grad():
        orig_model_output = orig_model(*orig_args)

    # 4. åŠ è½½ HF Data2VecVision æ¨¡å‹
    if is_finetuned:
        # å¦‚æœè¿›è¡Œäº†å¾®è°ƒï¼Œä½¿ç”¨ Image Classification çš„é…ç½®åˆ›å»º HF Data2VecVision æ¨¡å‹
        hf_model = Data2VecVisionForImageClassification(config)
        hf_model.eval()
        has_lm_head = False
        hf_prefix = "data2vec_vision."
    else:
        # å¦‚æœæ²¡æœ‰è¿›è¡Œå¾®è°ƒï¼Œåˆ›å»ºæ ‡å‡† HF Data2VecVision æ¨¡å‹
        hf_model = Data2VecVisionModel(config)
        hf_model.eval()
        has_lm_head = True
        hf_prefix = ""
    # ä½¿ç”¨é…ç½®å’Œå‰ç¼€ç”Ÿæˆé‡å‘½åé”®åˆ—è¡¨
    rename_keys = create_rename_keys(config, hf_prefix=hf_prefix, has_lm_head=has_lm_head)
    # è·å–åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    state_dict = orig_model.state_dict()
    # æ ¹æ®é‡å‘½åé”®ï¼Œæ›´æ–°çŠ¶æ€å­—å…¸ä¸­çš„é”®å
    for src, dest in rename_keys:
        val = state_dict.pop(src)  # ç§»é™¤åŸå§‹é”®ï¼Œå¹¶è·å–å¯¹åº”çš„æ•°å€¼
        state_dict[dest] = val  # å°†æ•°å€¼ä¸æ–°çš„é”®åå…³è”èµ·æ¥

    # å°†æ›´æ–°åçš„çŠ¶æ€å­—å…¸è¯»å…¥æŸ¥è¯¢-é”®-å€¼åŠŸèƒ½
    read_in_q_k_v(state_dict, config, hf_prefix=hf_prefix, has_lm_head=has_lm_head)
    # åŠ è½½çŠ¶æ€å­—å…¸åˆ° HF æ¨¡å‹ä¸­ï¼Œå…è®¸ç¼ºå¤±çš„é”®
    missing_keys, unexpected_keys = hf_model.load_state_dict(state_dict, strict=False)
    print("HF missing", missing_keys)  # æ‰“å°ç¼ºå¤±çš„é”®åˆ—è¡¨
    print("HF unexpected_keys", unexpected_keys)  # æ‰“å°æ„å¤–çš„é”®åˆ—è¡¨

    # 5. Forward HF Data2VecVision model
    # ä½¿ç”¨ torch.no_grad() ä¸Šä¸‹æ–‡ï¼Œå‰å‘ä¼ æ’­ HF æ¨¡å‹ï¼Œè®¡ç®—åƒç´ å€¼çš„è¾“å‡º
    with torch.no_grad():
        hf_model_output = hf_model(pixel_values)

    # å¦‚æœæ˜¯å¾®è°ƒçŠ¶æ€ï¼Œé€‰æ‹© logitsï¼›å¦åˆ™é€‰æ‹©æœ€åçš„éšè—çŠ¶æ€
    hf_output = hf_model_output.logits if is_finetuned else hf_model_output.last_hidden_state

    # 6. Compare
    # è®¡ç®— HF è¾“å‡ºä¸åŸå§‹æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ç»å¯¹å·®å€¼
    max_absolute_diff = torch.max(torch.abs(hf_output - orig_model_output)).item()

    print(f"max_absolute_diff = {max_absolute_diff}")  # æ‰“å°æœ€å¤§ç»å¯¹å·®å€¼
    # æ£€æŸ¥ HF è¾“å‡ºä¸åŸå§‹æ¨¡å‹è¾“å‡ºæ˜¯å¦æ¥è¿‘ï¼ŒæŒ‡å®šç»å¯¹å®¹å·®
    success = torch.allclose(hf_output, orig_model_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")  # æ‰“å°æ¯”è¾ƒç»“æœ
    if not success:
        raise Exception("Something went wRoNg")  # å¦‚æœè¾“å‡ºä¸æ¥è¿‘ï¼ŒæŠ›å‡ºå¼‚å¸¸

    # 7. Save
    print(f"Saving to {args.hf_checkpoint_name}")  # æ‰“å°ä¿å­˜è·¯å¾„
    hf_model.save_pretrained(args.hf_checkpoint_name)  # å°† HF æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    image_processor.save_pretrained(args.hf_checkpoint_name)  # å°†å›¾åƒå¤„ç†å™¨ä¿å­˜åˆ°åŒä¸€è·¯å¾„
# å¦‚æœè¯¥è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™æ‰§è¡Œ main() å‡½æ•°
if __name__ == "__main__":
    main()
    # è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ ¼å¼ï¼š
    # python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #         --beit_checkpoint ./pretrained_base.pt \
    #         --hf_checkpoint_name "./data2vec-vision-base"
    # python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #         --beit_checkpoint ./finetuned_base.pt \
    #         --hf_checkpoint_name "./data2vec-vision-base-ft1k"
    # python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #         --beit_checkpoint ./pretrained_large.pt \
    #         --hf_checkpoint_name "./data2vec-vision-large"
    # python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #         --beit_checkpoint ./finetuned_large.pt \
    #         --hf_checkpoint_name "./data2vec-vision-large-ft1k"
```