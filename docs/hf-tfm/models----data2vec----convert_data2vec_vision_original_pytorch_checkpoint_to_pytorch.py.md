# `.\models\data2vec\convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py`

```
#!/usr/bin/env python3
# åœ¨Unixç³»ç»Ÿä¸­ï¼Œå‘Šè¯‰æ“ä½œç³»ç»Ÿä½¿ç”¨Python3è§£é‡Šå™¨æ‰§è¡Œè„šæœ¬

import argparse
# å¯¼å…¥argparseæ¨¡å—ï¼Œç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°å’Œç”Ÿæˆå¸®åŠ©æ–‡æ¡£

import json
# å¯¼å…¥jsonæ¨¡å—ï¼Œç”¨äºå¤„ç†JSONæ ¼å¼çš„æ•°æ®

import torch
# å¯¼å…¥torchæ¨¡å—ï¼ŒPyTorchæ·±åº¦å­¦ä¹ åº“

from huggingface_hub import hf_hub_download
# ä»huggingface_hubæ¨¡å—å¯¼å…¥hf_hub_downloadå‡½æ•°

from PIL import Image
# ä»PILæ¨¡å—å¯¼å…¥Imageç±»ï¼Œç”¨äºå¤„ç†å›¾ç‰‡

from timm.models import create_model
# ä»timm.modelsæ¨¡å—å¯¼å…¥create_modelå‡½æ•°ï¼Œç”¨äºåˆ›å»ºæ¨¡å‹

from transformers import (
    BeitImageProcessor,
    Data2VecVisionConfig,
    Data2VecVisionForImageClassification,
    Data2VecVisionModel,
)
# ä»transformersæ¨¡å—å¯¼å…¥BeitImageProcessorã€Data2VecVisionConfigã€Data2VecVisionForImageClassificationã€Data2VecVisionModelç±»

def create_rename_keys(config, has_lm_head=False, is_semantic=False, hf_prefix="data2vec."):
    # åˆ›å»ºå‡½æ•°create_rename_keysï¼Œæ ¹æ®æä¾›çš„å‚æ•°åˆ›å»ºé‡å‘½åé”®åˆ—è¡¨
    prefix = "backbone." if is_semantic else ""
    # å¦‚æœæ˜¯è¯­ä¹‰æ¨¡å‹ï¼Œå‰ç¼€ä¸º"backbone."ï¼Œå¦åˆ™ä¸ºç©ºå­—ç¬¦ä¸²

    rename_keys = []
    # åˆ›å»ºç©ºåˆ—è¡¨rename_keysï¼Œç”¨äºå­˜å‚¨é‡å‘½åçš„é”®å€¼å¯¹
    for i in range(config.num_hidden_layers):
        # éå†config.num_hidden_layersæ¬¡
        # encoder layers: output projection, 2 feedforward neural networks and 2 layernorms
        rename_keys.append(
            (f"{prefix}blocks.{i}.norm1.weight", f"{hf_prefix}encoder.layer.{i}.layernorm_before.weight")
        )
        # å°†(æƒé‡åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append((f"{prefix}blocks.{i}.norm1.bias", f"{hf_prefix}encoder.layer.{i}.layernorm_before.bias"))
        # å°†(åç½®åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append(
            (f"{prefix}blocks.{i}.attn.proj.weight", f"{hf_prefix}encoder.layer.{i}.attention.output.dense.weight")
        )
        # å°†(æƒé‡åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append(
            (f"{prefix}blocks.{i}.attn.proj.bias", f"{hf_prefix}encoder.layer.{i}.attention.output.dense.bias")
        )
        # å°†(åç½®åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append(
            (f"{prefix}blocks.{i}.norm2.weight", f"{hf_prefix}encoder.layer.{i}.layernorm_after.weight")
        )
        # å°†(æƒé‡åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append((f"{prefix}blocks.{i}.norm2.bias", f"{hf_prefix}encoder.layer.{i}.layernorm_after.bias"))
        # å°†(åç½®åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append(
            (f"{prefix}blocks.{i}.mlp.fc1.weight", f"{hf_prefix}encoder.layer.{i}.intermediate.dense.weight")
        )
        # å°†(æƒé‡åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append(
            (f"{prefix}blocks.{i}.mlp.fc1.bias", f"{hf_prefix}encoder.layer.{i}.intermediate.dense.bias")
        )
        # å°†(åç½®åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append((f"{prefix}blocks.{i}.mlp.fc2.weight", f"{hf_prefix}encoder.layer.{i}.output.dense.weight"))
        # å°†(æƒé‡åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­
        rename_keys.append((f"{prefix}blocks.{i}.mlp.fc2.bias", f"{hf_prefix}encoder.layer.{i}.output.dense.bias"))
        # å°†(åç½®åç§°, æ–°åç§°)çš„é”®å€¼å¯¹æ·»åŠ åˆ°rename_keysåˆ—è¡¨ä¸­

    # projection layer + position embeddings
    rename_keys.extend(
        [
            (f"{prefix}cls_token", f"{hf_prefix}embeddings.cls_token"),
            (f"{prefix}patch_embed.proj.weight", f"{hf_prefix}embeddings.patch_embeddings.projection.weight"),
            (f"{prefix}patch_embed.proj.bias", f"{hf_prefix}embeddings.patch_embeddings.projection.bias"),
        ]
    )
    # æ‰©å±•rename_keysåˆ—è¡¨ï¼Œæ·»åŠ æŠ•å½±å±‚å’Œä½ç½®åµŒå…¥çš„é‡å‘½åé”®å€¼å¯¹
    # å¦‚æœæœ‰ LM å¤´éƒ¨
    if has_lm_head:
        # å°†æŒ‡å®šçš„é”®æ·»åŠ åˆ°é‡å‘½ååˆ—è¡¨ä¸­
        rename_keys.extend(
            [
                ("mask_token", f"{hf_prefix}embeddings.mask_token"),  # é‡å‘½å mask token
                (
                    "rel_pos_bias.relative_position_bias_table",
                    f"{hf_prefix}encoder.relative_position_bias.relative_position_bias_table",
                ),  # é‡å‘½åç›¸å¯¹ä½ç½®åç½®è¡¨
                (
                    "rel_pos_bias.relative_position_index",
                    f"{hf_prefix}encoder.relative_position_bias.relative_position_index",
                ),  # é‡å‘½åç›¸å¯¹ä½ç½®ç´¢å¼•
                ("norm.weight", "layernorm.weight"),  # é‡å‘½åæƒé‡
                ("norm.bias", "layernorm.bias"),  # é‡å‘½ååç½®
            ]
        )
    # å¦‚æœæ˜¯è¯­ä¹‰
    elif is_semantic:
        # å°†æŒ‡å®šçš„é”®æ·»åŠ åˆ°é‡å‘½ååˆ—è¡¨ä¸­
        rename_keys.extend(
            [
                ("decode_head.conv_seg.weight", "decode_head.classifier.weight"),  # é‡å‘½åå·ç§¯å±‚æƒé‡
                ("decode_head.conv_seg.bias", "decode_head.classifier.bias"),  # é‡å‘½åå·ç§¯å±‚åç½®
                ("auxiliary_head.conv_seg.weight", "auxiliary_head.classifier.weight"),  # é‡å‘½åè¾…åŠ©å·ç§¯å±‚æƒé‡
                ("auxiliary_head.conv_seg.bias", "auxiliary_head.classifier.bias"),  # é‡å‘½åè¾…åŠ©å·ç§¯å±‚åç½®
            ]
        )
    # å¦‚æœä¸æ˜¯è¯­ä¹‰ä¹Ÿä¸æ˜¯ LM å¤´éƒ¨
    else:
        # å°†æŒ‡å®šçš„é”®æ·»åŠ åˆ°é‡å‘½ååˆ—è¡¨ä¸­
        rename_keys.extend(
            [
                ("fc_norm.weight", f"{hf_prefix}pooler.layernorm.weight"),  # é‡å‘½åæƒé‡
                ("fc_norm.bias", f"{hf_prefix}pooler.layernorm.bias"),  # é‡å‘½ååç½®
                ("head.weight", "classifier.weight"),  # é‡å‘½åå¤´éƒ¨æƒé‡
                ("head.bias", "classifier.bias"),  # é‡å‘½åå¤´éƒ¨åç½®
            ]
        )

    # è¿”å›é‡å‘½ååˆ—è¡¨
    return rename_keys
# ä» state_dict ä¸­è¯»å– qkv æƒé‡å’Œåç½®ï¼Œç”¨äºæ„å»ºæ³¨æ„åŠ›æœºåˆ¶çš„æŸ¥è¯¢ã€é”®å’Œå€¼
def read_in_q_k_v(state_dict, config, has_lm_head=False, is_semantic=False, hf_prefix="data2vec_vision."):
    # éå†æ‰€æœ‰éšè—å±‚
    for i in range(config.num_hidden_layers):
        # å¦‚æœæ˜¯è¯­ä¹‰æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ "backbone." å‰ç¼€
        prefix = "backbone." if is_semantic else ""
        
        # è¯»å–æ³¨æ„åŠ›æœºåˆ¶çš„æŸ¥è¯¢æƒé‡ã€æŸ¥è¯¢åç½®å’Œå€¼åç½®
        in_proj_weight = state_dict.pop(f"{prefix}blocks.{i}.attn.qkv.weight")
        q_bias = state_dict.pop(f"{prefix}blocks.{i}.attn.q_bias")
        v_bias = state_dict.pop(f"{prefix}blocks.{i}.attn.v_bias")
        
        # æ›´æ–° state_dict ä¸­çš„æ³¨æ„åŠ›æŸ¥è¯¢ã€é”®å’Œå€¼æƒé‡å’Œåç½®
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.query.weight"] = in_proj_weight[: config.hidden_size, :]
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.query.bias"] = q_bias
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.key.weight"] = in_proj_weight[config.hidden_size : config.hidden_size * 2, :]
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.value.weight"] = in_proj_weight[-config.hidden_size :, :]
        state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.value.bias"] = v_bias
        
        # è¯»å– gamma_1 å’Œ gamma_2ï¼Œæ›´æ–° state_dict ä¸­çš„ lambda_1 å’Œ lambda_2
        gamma_1 = state_dict.pop(f"{prefix}blocks.{i}.gamma_1")
        gamma_2 = state_dict.pop(f"{prefix}blocks.{i}.gamma_2")
        state_dict[f"{hf_prefix}encoder.layer.{i}.lambda_1"] = gamma_1
        state_dict[f"{hf_prefix}encoder.layer.{i}.lambda_2"] = gamma_2
        
        # å¦‚æœæ²¡æœ‰ LM å¤´éƒ¨ï¼Œåˆ™è¯»å–ç›¸å¯¹ä½ç½®åç½®è¡¨æ ¼å’Œç´¢å¼•ï¼Œæ›´æ–° state_dict ä¸­çš„ç›¸å¯¹ä½ç½®åç½®ç›¸å…³å†…å®¹
        if not has_lm_head:
            table = state_dict.pop(f"{prefix}blocks.{i}.attn.relative_position_bias_table")
            index = state_dict.pop(f"{prefix}blocks.{i}.attn.relative_position_index")
            state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.relative_position_bias.relative_position_bias_table"] = table
            state_dict[f"{hf_prefix}encoder.layer.{i}.attention.attention.relative_position_bias.relative_position_index"] = index


# è·å–å‘½ä»¤è¡Œå‚æ•°
def get_args():
    # åˆ›å»ºè§£æå™¨
    parser = argparse.ArgumentParser("Convert Data2VecVision to HF for image classification and pretraining", add_help=False)
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument("--hf_checkpoint_name", type=str)
    parser.add_argument("--input_size", default=224, type=int, help="images input size")
    parser.add_argument("--beit_checkpoint", default="", help="beit checkpoint")
    
    # è§£æå¹¶è¿”å›å‚æ•°
    return parser.parse_args()


# åŠ è½½ BEiT æ¨¡å‹
def load_beit_model(args, is_finetuned, is_large):
    # åŠ è½½ç»™å®šæ¨¡å‹çš„çŠ¶æ€å­—å…¸
    def load_state_dict(model, state_dict, prefix="", ignore_missing="relative_position_index"):
        # ç”¨æ¥å­˜å‚¨æœªæ‰¾åˆ°çš„é”®
        missing_keys = []
        # ç”¨æ¥å­˜å‚¨æ„å¤–çš„é”®
        unexpected_keys = []
        # ç”¨æ¥å­˜å‚¨é”™è¯¯æ¶ˆæ¯
        error_msgs = []
        # å¤åˆ¶state_dictä»¥ä¾¿_load_from_state_dictå¯ä»¥ä¿®æ”¹å®ƒ
        metadata = getattr(state_dict, "_metadata", None)
        state_dict = state_dict.copy()
        if metadata is not None:
            state_dict._metadata = metadata

        def load(module, prefix=""):
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            # ä»state_dictåŠ è½½æ¨¡å—
            module._load_from_state_dict(
                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs
            )
            for name, child in module._modules.items():
                if child is not None:
                    load(child, prefix + name + ".")

        # åŠ è½½æ¨¡å‹å’Œå‰ç¼€
        load(model, prefix=prefix)

        # è­¦å‘Šæœªæ‰¾åˆ°çš„é”®
        warn_missing_keys = []
        # å¿½ç•¥çš„é”®
        ignore_missing_keys = []
        for key in missing_keys:
            keep_flag = True
            for ignore_key in ignore_missing.split("|"):
                if ignore_key in key:
                    keep_flag = False
                    break
            if keep_flag:
                warn_missing_keys.append(key)
            else:
                ignore_missing_keys.append(key)

        missing_keys = warn_missing_keys

        if len(missing_keys) > 0:
            # æ‰“å°æœªä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åˆå§‹åŒ–çš„æ¨¡å‹æƒé‡
            print(
                "Weights of {} not initialized from pretrained model: {}".format(
                    model.__class__.__name__, missing_keys
                )
            )
        if len(unexpected_keys) > 0:
            # æ‰“å°æœªåœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨çš„æƒé‡
            print("Weights from pretrained model not used in {}: {}".format(model.__class__.__name__, unexpected_keys))
        if len(ignore_missing_keys) > 0:
            # æ‰“å°ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æœªåˆå§‹åŒ–çš„è¢«å¿½ç•¥æƒé‡
            print(
                "Ignored weights of {} not initialized from pretrained model: {}".format(
                    model.__class__.__name__, ignore_missing_keys
                )
            )
        if len(error_msgs) > 0:
            print("\n".join(error_msgs))

    # æ¨¡å‹çš„ä¸€äº›å‚æ•°è®¾ç½®
    model_kwargs = {
        "pretrained": False,
        "use_shared_rel_pos_bias": True,
        "use_abs_pos_emb": False,
        "init_values": 0.1,
    }

    # å¦‚æœæ˜¯å¾®è°ƒ
    if is_finetuned:
        model_kwargs.update(
            {
                "num_classes": 1000,
                "use_mean_pooling": True,
                "init_scale": 0.001,
                "use_rel_pos_bias": True,
            }
        )

    # åˆ›å»ºæ¨¡å‹å¹¶æ ¹æ®å‚æ•°æ›´æ–°æ¨¡å‹ç±»å‹
    model = create_model(
        "beit_large_patch16_224" if is_large else "beit_base_patch16_224",
        **model_kwargs,
    )
    patch_size = model.patch_embed.patch_size
    # è®¾ç½®çª—å£å¤§å°
    args.window_size = (args.input_size // patch_size[0], args.input_size // patch_size[1])
    checkpoint = torch.load(args.beit_checkpoint, map_location="cpu")

    # åŠ è½½æ£€æŸ¥ç‚¹
    print(f"Load ckpt from {args.beit_checkpoint}")
    checkpoint_model = None
    # éå†æ¨¡å‹å…³é”®å­—åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¨¡å‹ç›¸å…³çš„å…³é”®å­—ï¼ˆ"model"æˆ–"module"ï¼‰
    for model_key in ("model", "module"):
        # å¦‚æœå­˜åœ¨æŒ‡å®šçš„æ¨¡å‹å…³é”®å­—
        if model_key in checkpoint:
            # è·å–è¯¥å…³é”®å­—å¯¹åº”çš„æ¨¡å‹å‚æ•°
            checkpoint_model = checkpoint[model_key]
            # æ‰“å°åŠ è½½çŠ¶æ€å­—å…¸çš„æ¶ˆæ¯ï¼ŒæŒ‡æ˜åŠ è½½çš„æ¨¡å‹å…³é”®å­—
            print(f"Load state_dict by model_key = {model_key}")
            # ä¸­æ–­å¾ªç¯ï¼Œä»…åŠ è½½ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ¨¡å‹å…³é”®å­—å¯¹åº”çš„çŠ¶æ€å­—å…¸
            break

    # è·å–çŠ¶æ€å­—å…¸ä¸­çš„æ‰€æœ‰é”®
    all_keys = list(checkpoint_model.keys())
    # éå†æ‰€æœ‰é”®
    for key in all_keys:
        # å¦‚æœé”®ä¸­åŒ…å«"relative_position_index"
        if "relative_position_index" in key:
            # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤è¯¥é”®
            checkpoint_model.pop(key)

        # å¦‚æœé”®ä¸­åŒ…å«"relative_position_bias_table"
        if "relative_position_bias_table" in key:
            # è·å–ç›¸å¯¹ä½ç½®åç½®è¡¨
            rel_pos_bias = checkpoint_model[key]
            # è·å–æºä½ç½®æ•°å’Œæ³¨æ„åŠ›å¤´æ•°
            src_num_pos, num_attn_heads = rel_pos_bias.size()
            # è·å–ç›®æ ‡ä½ç½®æ•°å’Œç›®æ ‡æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­çš„é”®çš„å¤§å°
            dst_num_pos, _ = model.state_dict()[key].size()
            # è·å–ç›®æ ‡è¡¥ä¸å½¢çŠ¶
            dst_patch_shape = model.patch_embed.patch_shape
            # å¦‚æœç›®æ ‡è¡¥ä¸çš„å®½å’Œé«˜ä¸ç›¸ç­‰
            if dst_patch_shape[0] != dst_patch_shape[1]:
                # æŠ›å‡ºæœªå®ç°é”™è¯¯
                raise NotImplementedError()

    # åŠ è½½æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    load_state_dict(model, checkpoint_model, prefix="")

    # è¿”å›åŠ è½½åçš„æ¨¡å‹
    return model
# å®šä¹‰ä¸»å‡½æ•°
def main():
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()

    # æ£€æŸ¥æ˜¯å¦å·²å¾®è°ƒ
    is_finetuned = "ft1k" in args.hf_checkpoint_name
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸ºå¤§æ¨¡å‹
    is_large = "large" in args.hf_checkpoint_name

    # è‹¥å·²å¾®è°ƒ
    if is_finetuned:
        # éœ€è¦å°† Beit çš„ data2vec_vision è½¬æ¢ä¸º HF æ¨¡å‹ï¼Œéœ€è¦å°† modeling_finetune.py å¤åˆ¶åˆ°å½“å‰æ–‡ä»¶å¤¹
        import modeling_finetune  # noqa: F401
    # è‹¥æœªå¾®è°ƒ
    else:
        # éœ€è¦å°† Beit çš„ data2vec_vision è½¬æ¢ä¸º HF æ¨¡å‹ï¼Œéœ€è¦å°† modeling_cyclical.py å¤åˆ¶åˆ°å½“å‰æ–‡ä»¶å¤¹
        # æ³¨æ„ï¼šç›®å‰æˆ‘ä»¬åªè½¬æ¢äº†ä¸‹æ¸¸æ¨¡å‹è€Œä¸æ˜¯å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™æ„å‘³ç€åœ¨é›†æˆæµ‹è¯•ä¸­ï¼Œéœ€è¦åœ¨ä»¥ä¸‹è¡Œåæ·»åŠ  `return x`ï¼š
        # https://github.com/facebookresearch/data2vec_vision/blob/af9a36349aaed59ae66e69b5dabeef2d62fdc5da/beit/modeling_cyclical.py#L197
        import modeling_cyclical  # noqa: F401

    # 1. åˆ›å»ºæ¨¡å‹é…ç½®
    config = Data2VecVisionConfig()
    if is_finetuned:
        # é…ç½®å¾®è°ƒçš„å‚æ•°
        config.use_relative_position_bias = True
        config.use_shared_relative_position_bias = False
        config.use_mean_pooling = True
        config.num_labels = 1000

        # åŠ è½½ imagenet-1k-id2label.json æ–‡ä»¶
        repo_id = "huggingface/label-files"
        filename = "imagenet-1k-id2label.json"
        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
        id2label = {int(k): v for k, v in id2label.items()}
        config.id2label = id2label
        config.label2id = {v: k for k, v in id2label.items()}
    else:
        # é…ç½®é¢„è®­ç»ƒçš„å‚æ•°
        config.use_relative_position_bias = False
        config.use_shared_relative_position_bias = True
        config.use_mean_pooling = False

    # å¦‚æœæ¨¡å‹ä¸ºå¤§æ¨¡å‹
    if is_large:
        config.hidden_size = 1024
        config.intermediate_size = 4096
        config.num_hidden_layers = 24
        config.num_attention_heads = 16

    # 2. åŠ è½½ Beit æ¨¡å‹
    orig_model = load_beit_model(args, is_finetuned, is_large)
    orig_model.eval()

    # 3. å‰å‘ä¼ æ’­ Beit æ¨¡å‹
    image_processor = BeitImageProcessor(size=config.image_size, do_center_crop=False)
    image = Image.open("../../../../tests/fixtures/tests_samples/COCO/000000039769.png")
    encoding = image_processor(images=image, return_tensors="pt")
    pixel_values = encoding["pixel_values"]

    orig_args = (pixel_values,) if is_finetuned else (pixel_values, None)
    with torch.no_grad():
        orig_model_output = orig_model(*orig_args)

    # 4. åŠ è½½ HF Data2VecVision æ¨¡å‹
    if is_finetuned:
        hf_model = Data2VecVisionForImageClassification(config)
        hf_model.eval()
        has_lm_head = False
        hf_prefix = "data2vec_vision."
    else:
        hf_model = Data2VecVisionModel(config)
        hf_model.eval()
        has_lm_head = True
        hf_prefix = ""
    # åˆ›å»ºéœ€è¦é‡å‘½åçš„é”®å€¼å¯¹åˆ—è¡¨ï¼Œæ ¹æ®é…ç½®ã€HF å‰ç¼€å’Œ LM å¤´ä¿¡æ¯
    rename_keys = create_rename_keys(config, hf_prefix=hf_prefix, has_lm_head=has_lm_head)
    # è·å–åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    state_dict = orig_model.state_dict()
    # éå†é‡å‘½åé”®å€¼å¯¹åˆ—è¡¨ï¼Œå°†åŸå§‹æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­ç›¸åº”çš„é”®å€¼å¯¹è¿›è¡Œæ›¿æ¢
    for src, dest in rename_keys:
        val = state_dict.pop(src)
        state_dict[dest] = val

    # é€šè¿‡çŠ¶æ€å­—å…¸æ›´æ–° HF QKV æ¨¡å‹
    read_in_q_k_v(state_dict, config, hf_prefix=hf_prefix, has_lm_head=has_lm_head)
    # åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸åˆ° HF æ¨¡å‹ï¼Œstrict=False è¡¨ç¤ºå…è®¸ç¼ºå¤±æˆ–è€…å¤šä½™çš„é”®
    missing_keys, unexpected_keys = hf_model.load_state_dict(state_dict, strict=False)
    print("HF missing", missing_keys)
    print("HF unexpected_keys", unexpected_keys)

    # 5. å¯¹ HF Data2VecVision æ¨¡å‹è¿›è¡Œå‰å‘æ¨æ–­
    with torch.no_grad():
        hf_model_output = hf_model(pixel_values)

    # æ ¹æ®æ˜¯å¦ Fine-tuned æ¥é€‰æ‹© HF æ¨¡å‹è¾“å‡ºæ˜¯ logits è¿˜æ˜¯æœ€åéšè—å±‚çŠ¶æ€
    hf_output = hf_model_output.logits if is_finetuned else hf_model_output.last_hidden_state

    # 6. æ¯”è¾ƒ HF æ¨¡å‹è¾“å‡ºä¸åŸå§‹æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ç»å¯¹å·®å¼‚
    max_absolute_diff = torch.max(torch.abs(hf_output - orig_model_output)).item()

    print(f"max_absolute_diff = {max_absolute_diff}")
    # æ£€æŸ¥ HF æ¨¡å‹è¾“å‡ºæ˜¯å¦ä¸åŸå§‹æ¨¡å‹è¾“å‡ºè¿‘ä¼¼ç›¸ç­‰ï¼Œatol=1e-3 è¡¨ç¤ºè¯¯å·®å®¹é™
    success = torch.allclose(hf_output, orig_model_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")
    if not success:
        raise Exception("Something went wRoNg")

    # 7. ä¿å­˜æ¨¡å‹
    print(f"Saving to {args.hf_checkpoint_name}")
    hf_model.save_pretrained(args.hf_checkpoint_name)
    image_processor.save_pretrained(args.hf_checkpoint_name)
if __name__ == "__main__":
    # å¦‚æœå½“å‰æ¨¡å—è¢«ç›´æ¥æ‰§è¡Œï¼Œè°ƒç”¨ä¸»å‡½æ•°
    main()
    # è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥è½¬æ¢æ£€æŸ¥ç‚¹æ–‡ä»¶
    #  python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #          --beit_checkpoint ./pretrained_base.pt \
    #          --hf_checkpoint_name "./data2vec-vision-base"
    #  python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #          --beit_checkpoint ./finetuned_base.pt \
    #          --hf_checkpoint_name "./data2vec-vision-base-ft1k"
    #  python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #          --beit_checkpoint ./pretrained_large.pt \
    #          --hf_checkpoint_name "./data2vec-vision-large"
    #  python ./convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py \
    #          --beit_checkpoint ./finetuned_large.pt \
    #          --hf_checkpoint_name "./data2vec-vision-large-ft1k"
```