# `.\models\dinov2\convert_dinov2_to_hf.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç æ ¼å¼ä¸º utf-8
# ç‰ˆæƒå£°æ˜
# è®¸å¯è¯ä¿¡æ¯
"""ä»åŸå§‹ä»“åº“è½¬æ¢ DINOv2 æ£€æŸ¥ç‚¹ã€‚

URL: https://github.com/facebookresearch/dinov2/tree/main
"""

# å¯¼å…¥éœ€è¦çš„åº“
import argparse
import json
from pathlib import Path
import requests
import torch
import torch.nn as nn
from huggingface_hub import hf_hub_download
from PIL import Image
from torchvision import transforms
from transformers import BitImageProcessor, Dinov2Config, Dinov2ForImageClassification, Dinov2Model
from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling
from transformers.utils import logging

# è®¾ç½®æ—¥å¿—çš„è¯¦ç»†ç¨‹åº¦ä¸º info
logging.set_verbosity_info()
# è·å– logger å®ä¾‹
logger = logging.get_logger(__name__)

# æ ¹æ®æ¨¡å‹åç§°å’Œæ˜¯å¦æ˜¯å›¾ç‰‡åˆ†ç±»å™¨ï¼Œè·å– DINOv2 çš„é…ç½®ä¿¡æ¯
def get_dinov2_config(model_name, image_classifier=False):
    config = Dinov2Config(image_size=518, patch_size=14)

    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®ä¸åŒçš„éšè—å±‚å¤§å°å’Œæ³¨æ„åŠ›å¤´
    if "vits" in model_name:
        config.hidden_size = 384
        config.num_attention_heads = 6
    elif "vitb" in model_name:
        pass
    elif "vitl" in model_name:
        config.hidden_size = 1024
        config.num_hidden_layers = 24
        config.num_attention_heads = 16
    elif "vitg" in model_name:
        config.use_swiglu_ffn = True
        config.hidden_size = 1536
        config.num_hidden_layers = 40
        config.num_attention_heads = 24
    else:
        raise ValueError("Model not supported")

    # å¦‚æœæ˜¯å›¾ç‰‡åˆ†ç±»å™¨ï¼Œè®¾ç½® repo_id å’Œ filenameï¼Œå¹¶åŠ è½½é…ç½®ä¿¡æ¯
    if image_classifier:
        repo_id = "huggingface/label-files"
        filename = "imagenet-1k-id2label.json"
        config.num_labels = 1000
        config.id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
        config.id2label = {int(k): v for k, v in config.id2label.items()}

    return config

# åˆ›å»ºé‡å‘½åé”®åˆ—è¡¨
def create_rename_keys(config):
    rename_keys = []
    # fmt: off

    # patch embedding layer
    rename_keys.append(("cls_token", "embeddings.cls_token"))
    rename_keys.append(("mask_token", "embeddings.mask_token"))
    rename_keys.append(("pos_embed", "embeddings.position_embeddings"))
    rename_keys.append(("patch_embed.proj.weight", "embeddings.patch_embeddings.projection.weight"))
    rename_keys.append(("patch_embed.proj.bias", "embeddings.patch_embeddings.projection.bias"))
    # éå†éšè—å±‚çš„æ•°é‡ï¼Œè¿›è¡Œé‡å‘½åæ“ä½œ
    for i in range(config.num_hidden_layers):
        # layernorms
        # é‡å‘½å layernorms çš„æƒé‡å’Œåç½®å‚æ•°å¯¹åº”çš„é”®å€¼
        rename_keys.append((f"blocks.{i}.norm1.weight", f"encoder.layer.{i}.norm1.weight"))
        rename_keys.append((f"blocks.{i}.norm1.bias", f"encoder.layer.{i}.norm1.bias"))
        rename_keys.append((f"blocks.{i}.norm2.weight", f"encoder.layer.{i}.norm2.weight"))
        rename_keys.append((f"blocks.{i}.norm2.bias", f"encoder.layer.{i}.norm2.bias"))
        # MLP
        # æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨å“ªç§ MLP ç»“æ„ï¼Œå¹¶æ ¹æ®ä¸åŒé€‰æ‹©å¯¹åº”é‡å‘½åæƒé‡å’Œåç½®å‚æ•°çš„é”®å€¼
        if config.use_swiglu_ffn:
            rename_keys.append((f"blocks.{i}.mlp.w12.weight", f"encoder.layer.{i}.mlp.w12.weight"))
            rename_keys.append((f"blocks.{i}.mlp.w12.bias", f"encoder.layer.{i}.mlp.w12.bias"))
            rename_keys.append((f"blocks.{i}.mlp.w3.weight", f"encoder.layer.{i}.mlp.w3.weight"))
            rename_keys.append((f"blocks.{i}.mlp.w3.bias", f"encoder.layer.{i}.mlp.w3.bias"))
        else:
            rename_keys.append((f"blocks.{i}.mlp.fc1.weight", f"encoder.layer.{i}.mlp.fc1.weight"))
            rename_keys.append((f"blocks.{i}.mlp.fc1.bias", f"encoder.layer.{i}.mlp.fc1.bias"))
            rename_keys.append((f"blocks.{i}.mlp.fc2.weight", f"encoder.layer.{i}.mlp.fc2.weight"))
            rename_keys.append((f"blocks.{i}.mlp.fc2.bias", f"encoder.layer.{i}.mlp.fc2.bias"))
        # layerscale
        # é‡å‘½å layerscale çš„å‚æ•°å¯¹åº”çš„é”®å€¼
        rename_keys.append((f"blocks.{i}.ls1.gamma", f"encoder.layer.{i}.layer_scale1.lambda1"))
        rename_keys.append((f"blocks.{i}.ls2.gamma", f"encoder.layer.{i}.layer_scale2.lambda1"))
        # attention projection layer
        # é‡å‘½å attention projection layer çš„å‚æ•°å¯¹åº”çš„é”®å€¼
        rename_keys.append((f"blocks.{i}.attn.proj.weight", f"encoder.layer.{i}.attention.output.dense.weight"))
        rename_keys.append((f"blocks.{i}.attn.proj.bias", f"encoder.layer.{i}.attention.output.dense.bias"))

    # final layernorm
    # é‡å‘½åæœ€åä¸€å±‚ layernorm å‚æ•°å¯¹åº”çš„é”®å€¼
    rename_keys.append(("norm.weight", "layernorm.weight"))
    rename_keys.append(("norm.bias", "layernorm.bias"))

    # fmt: on
    # è¿”å›é‡å‘½ååçš„é”®å€¼åˆ—è¡¨
    return rename_keys
# é‡å‘½åå­—å…¸ä¸­çš„é”®
def rename_key(dct, old, new):
    # å¼¹å‡ºæ—§é”®å¯¹åº”çš„å€¼
    val = dct.pop(old)
    # å°†å€¼ä¸æ–°é”®å…³è”
    dct[new] = val


# å°†æ¯ä¸ªç¼–ç å™¨å±‚çš„çŸ©é˜µæ‹†åˆ†ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼
def read_in_q_k_v(state_dict, config):
    # éå†æ¯ä¸ªç¼–ç å™¨å±‚
    for i in range(config.num_hidden_layers):
        # è¯»å–è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®ï¼ˆåœ¨timmä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„çŸ©é˜µåŠ ä¸Šåç½®ï¼‰
        in_proj_weight = state_dict.pop(f"blocks.{i}.attn.qkv.weight")
        in_proj_bias = state_dict.pop(f"blocks.{i}.attn.qkv.bias")
        # æ¥ä¸‹æ¥ï¼ŒæŒ‰é¡ºåºæ·»åŠ æŸ¥è¯¢ã€é”®å’Œå€¼åˆ°çŠ¶æ€å­—å…¸
        state_dict[f"encoder.layer.{i}.attention.attention.query.weight"] = in_proj_weight[: config.hidden_size, :]
        state_dict[f"encoder.layer.{i}.attention.attention.query.bias"] = in_proj_bias[: config.hidden_size]
        state_dict[f"encoder.layer.{i}.attention.attention.key.weight"] = in_proj_weight[
            config.hidden_size : config.hidden_size * 2, :
        ]
        state_dict[f"encoder.layer.{i}.attention.attention.key.bias"] = in_proj_bias[
            config.hidden_size : config.hidden_size * 2
        ]
        state_dict[f"encoder.layer.{i}.attention.attention.value.weight"] = in_proj_weight[-config.hidden_size :, :]
        state_dict[f"encoder.layer.{i}.attention.attention.value.bias"] = in_proj_bias[-config.hidden_size :]


# æˆ‘ä»¬å°†åœ¨ä¸€å¼ å¯çˆ±çš„çŒ«å›¾ç‰‡ä¸ŠéªŒè¯æˆ‘ä»¬çš„ç»“æœ
def prepare_img():
    # å›¾ç‰‡é“¾æ¥
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # æ‰“å¼€å›¾ç‰‡
    image = Image.open(requests.get(url, stream=True).raw)
    # è¿”å›å›¾ç‰‡å¯¹è±¡
    return image


# ä½¿ç”¨torch.no_grad()ä¿®é¥°çš„å‡½æ•°ï¼Œè¡¨ç¤ºåœ¨è¯¥å‡½æ•°ä¸­ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
@torch.no_grad()
def convert_dinov2_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=False):
    """
    å¤åˆ¶/ç²˜è´´/è°ƒæ•´æ¨¡å‹çš„æƒé‡åˆ°æˆ‘ä»¬çš„DINOv2ç»“æ„ã€‚
    """

    # å®šä¹‰é»˜è®¤çš„Dinov2é…ç½®
    image_classifier = "1layer" in model_name
    config = get_dinov2_config(model_name, image_classifier=image_classifier)

    # ä»torch hubåŠ è½½åŸå§‹æ¨¡å‹
    original_model = torch.hub.load("facebookresearch/dinov2", model_name.replace("_1layer", ""))
    original_model.eval()

    # åŠ è½½åŸå§‹æ¨¡å‹çš„state_dictï¼Œç§»é™¤å’Œé‡å‘½åä¸€äº›é”®
    state_dict = original_model.state_dict()
    # åˆ›å»ºé‡å‘½åé”®
    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:
        # é‡å‘½åé”®
        rename_key(state_dict, src, dest)
    # è¯»å–æŸ¥è¯¢ã€é”®å’Œå€¼
    read_in_q_k_v(state_dict, config)

    # å¤åˆ¶state_dictå¹¶å¤„ç†ä¸€äº›é”®
    for key, val in state_dict.copy().items():
        val = state_dict.pop(key)
        if "w12" in key:
            key = key.replace("w12", "weights_in")
        if "w3" in key:
            key = key.replace("w3", "weights_out")
        # æ›´æ–°é”®
        state_dict[key] = val

    # åŠ è½½HuggingFaceæ¨¡å‹
    # å¦‚æœæœ‰å›¾åƒåˆ†ç±»å™¨
    if image_classifier:
        # åŠ è½½Dinov2ForImageClassificationæ¨¡å‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model = Dinov2ForImageClassification(config).eval()
        # è½½å…¥æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        model.dinov2.load_state_dict(state_dict)
        
        # è®¾ç½®æ¨¡å‹åç§°åˆ°åˆ†ç±»å™¨å­—å…¸URLçš„æ˜ å°„å…³ç³»
        model_name_to_classifier_dict_url = {
            "dinov2_vits14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth",
            "dinov2_vitb14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth",
            "dinov2_vitl14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth",
            "dinov2_vitg14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth",
        }
        
        # è·å–æ¨¡å‹åç§°å¯¹åº”çš„åˆ†ç±»å™¨å­—å…¸URL
        url = model_name_to_classifier_dict_url[model_name]
        # ä»URLåŠ è½½åˆ†ç±»å™¨çŠ¶æ€å­—å…¸
        classifier_state_dict = torch.hub.load_state_dict_from_url(url, map_location="cpu")
        # è®¾ç½®æ¨¡å‹çš„åˆ†ç±»å™¨æƒé‡å’Œåç½®
        model.classifier.weight = nn.Parameter(classifier_state_dict["weight"])
        model.classifier.bias = nn.Parameter(classifier_state_dict["bias"])
    else:
        # åŠ è½½Dinov2Modelæ¨¡å‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model = Dinov2Model(config).eval()
        # è½½å…¥æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        model.load_state_dict(state_dict)

    # åŠ è½½å›¾åƒ
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

    # å›¾åƒé¢„å¤„ç†
    transformations = transforms.Compose(
        [
            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),  # è°ƒæ•´å¤§å°ä¸º 256x256
            transforms.CenterCrop(224),  # ä¸­å¿ƒè£å‰ªä¸º 224x224
            transforms.ToTensor(),  # è½¬æ¢ä¸ºå¼ é‡
            transforms.Normalize(  # æ ‡å‡†åŒ–
                mean=IMAGENET_DEFAULT_MEAN,  # å›¾åƒç½‘é»˜è®¤å‡å€¼
                std=IMAGENET_DEFAULT_STD,  # å›¾åƒç½‘é»˜è®¤æ ‡å‡†å·®
            ),
        ]
    )

    # è¿›è¡Œé¢„å¤„ç†åçš„åƒç´ å€¼
    original_pixel_values = transformations(image).unsqueeze(0)  # æ’å…¥æ‰¹å¤„ç†ç»´åº¦

    # ä½¿ç”¨BitImageProcessoré¢„å¤„ç†å›¾åƒåƒç´ å€¼
    processor = BitImageProcessor(
        size={"shortest_edge": 256},  # æœ€çŸ­è¾¹ä¸º 256
        resample=PILImageResampling.BICUBIC,  # é‡é‡‡æ ·ç®—æ³•ä¸º BICUBIC
        image_mean=IMAGENET_DEFAULT_MEAN,  # å›¾åƒç½‘é»˜è®¤å‡å€¼
        image_std=IMAGENET_DEFAULT_STD,  # å›¾åƒç½‘é»˜è®¤æ ‡å‡†å·®
    )
    pixel_values = processor(image, return_tensors="pt").pixel_values

    # æ–­è¨€åŸå§‹åƒç´ å€¼ä¸å¤„ç†åçš„åƒç´ å€¼ç›¸ç­‰
    assert torch.allclose(original_pixel_values, pixel_values)

    # ç¦æ­¢æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        # è·å–æ¨¡å‹è¾“å‡ºï¼Œå¹¶è¿”å›éšè—çŠ¶æ€
        outputs = model(pixel_values, output_hidden_states=True)
        # è·å–åŸå§‹æ¨¡å‹çš„è¾“å‡º
        original_outputs = original_model(pixel_values)

    # æ–­è¨€æ•°å€¼
    if image_classifier:
        # å¦‚æœæœ‰å›¾åƒåˆ†ç±»å™¨ï¼Œåˆ™æ‰“å°é¢„æµ‹çš„ç±»åˆ«
        print("Predicted class:")
        class_idx = outputs.logits.argmax(-1).item()
        print(model.config.id2label[class_idx])
    else:
        # å¦åˆ™æ–­è¨€è¾“å‡ºçš„æœ€åéšè—çŠ¶æ€å’ŒåŸå§‹è¾“å‡ºå½¢çŠ¶ç›¸åŒï¼Œå¹¶ä¸”å€¼ç›¸ä¼¼
        assert outputs.last_hidden_state[:, 0].shape == original_outputs.shape
        assert torch.allclose(outputs.last_hidden_state[:, 0], original_outputs, atol=1e-3)
    # æ‰“å°ç»“æœ
    print("Looks ok!")
    # å¦‚æœæä¾›äº† PyTorch æ¨¡å‹å¯¼å‡ºç›®å½•è·¯å¾„ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if pytorch_dump_folder_path is not None:
        # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        # æ‰“å°æ¨¡å‹ä¿å­˜çš„ä¿¡æ¯
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜è‡³æŒ‡å®šçš„ PyTorch ç›®å½•
        model.save_pretrained(pytorch_dump_folder_path)
        # æ‰“å°å›¾åƒå¤„ç†å™¨ä¿å­˜çš„ä¿¡æ¯
        print(f"Saving image processor to {pytorch_dump_folder_path}")
        # å°†å›¾åƒå¤„ç†å™¨ä¿å­˜è‡³åŒä¸€ä¸ªæŒ‡å®šçš„ PyTorch ç›®å½•
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœè®¾ç½®ä¸ºæ¨é€åˆ° hubï¼ˆHugging Face Model Hubï¼‰
    if push_to_hub:
        # ä¸ºä¸åŒçš„æ¨¡å‹åç§°æ˜ å°„åˆ° Hugging Face Model Hub ä¸Šçš„å…·ä½“æ¨¡å‹åç§°
        model_name_to_hf_name = {
            "dinov2_vits14": "dinov2-small",
            "dinov2_vitb14": "dinov2-base",
            "dinov2_vitl14": "dinov2-large",
            "dinov2_vitg14": "dinov2-giant",
            "dinov2_vits14_1layer": "dinov2-small-imagenet1k-1-layer",
            "dinov2_vitb14_1layer": "dinov2-base-imagenet1k-1-layer",
            "dinov2_vitl14_1layer": "dinov2-large-imagenet1k-1-layer",
            "dinov2_vitg14_1layer": "dinov2-giant-imagenet1k-1-layer",
        }

        # æ ¹æ®æä¾›çš„æ¨¡å‹åç§°ï¼Œè·å–æ˜ å°„åçš„ Hub æ¨¡å‹åç§°
        name = model_name_to_hf_name[model_name]
        # å°†æ¨¡å‹æ¨é€åˆ° Hugging Face Model Hubï¼ŒåŒ…æ‹¬ç»„ç»‡åå’Œæ¨¡å‹å
        model.push_to_hub(f"facebook/{name}")
        # å°†å›¾åƒå¤„ç†å™¨æ¨é€åˆ° Hugging Face Model Hubï¼ŒåŒ…æ‹¬ç»„ç»‡åå’Œæ¨¡å‹å
        processor.push_to_hub(f"facebook/{name}")
# å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…è¦å‚æ•°
    parser.add_argument(
        "--model_name",
        default="dinov2_vitb14",
        type=str,
        choices=[
            "dinov2_vits14",
            "dinov2_vitb14",
            "dinov2_vitl14",
            "dinov2_vitg14",
            "dinov2_vits14_1layer",
            "dinov2_vitb14_1layer",
            "dinov2_vitl14_1layer",
            "dinov2_vitg14_1layer",
        ],
        help="Name of the model you'd like to convert."
    )
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æä¼ å…¥çš„å‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°ï¼Œä¼ å…¥è§£æåçš„å‚æ•°
    convert_dinov2_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
```