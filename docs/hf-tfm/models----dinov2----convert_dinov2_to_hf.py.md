# `.\models\dinov2\convert_dinov2_to_hf.py`

```
# è®¾ç½®ç¼–ç æ ¼å¼ä¸º UTF-8
# ç‰ˆæƒå£°æ˜åŠè®¸å¯ä¿¡æ¯ï¼ŒæŒ‡å®šæ­¤ä»£ç çš„ä½¿ç”¨æ¡ä»¶
# æ­¤è„šæœ¬ç”¨äºä»åŸå§‹å­˜å‚¨åº“è½¬æ¢ DINOv2 æ£€æŸ¥ç‚¹

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import json  # ç”¨äº JSON æ•°æ®å¤„ç†
from pathlib import Path  # ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„

import requests  # ç”¨äºå‘èµ· HTTP è¯·æ±‚
import torch  # PyTorch æ·±åº¦å­¦ä¹ åº“
import torch.nn as nn  # PyTorch ç¥ç»ç½‘ç»œæ¨¡å—
from huggingface_hub import hf_hub_download  # ç”¨äºä» Hugging Face Hub ä¸‹è½½æ–‡ä»¶
from PIL import Image  # Python å›¾åƒåº“ï¼Œç”¨äºå›¾åƒå¤„ç†
from torchvision import transforms  # PyTorch çš„è§†è§‰å¤„ç†å·¥å…·é›†

# å¯¼å…¥ Transformers åº“ä¸­ç›¸å…³çš„ç±»å’Œå‡½æ•°
from transformers import BitImageProcessor, Dinov2Config, Dinov2ForImageClassification, Dinov2Model
from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling
from transformers.utils import logging  # Transformers åº“çš„æ—¥å¿—å·¥å…·

# è®¾ç½®æ—¥å¿—è¾“å‡ºçº§åˆ«ä¸º info
logging.set_verbosity_info()
# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)


def get_dinov2_config(model_name, image_classifier=False):
    # åˆ›å»ºä¸€ä¸ª Dinov2Config å¯¹è±¡ï¼ŒæŒ‡å®šå›¾åƒå¤§å°å’Œè¡¥ä¸å¤§å°
    config = Dinov2Config(image_size=518, patch_size=14)

    # æ ¹æ®æ¨¡å‹åè°ƒæ•´é…ç½®å‚æ•°
    if "vits" in model_name:
        config.hidden_size = 384
        config.num_attention_heads = 6
    elif "vitb" in model_name:
        pass  # å¦‚æœæ¨¡å‹ååŒ…å« 'vitb'ï¼Œåˆ™ä¿æŒé»˜è®¤è®¾ç½®
    elif "vitl" in model_name:
        config.hidden_size = 1024
        config.num_hidden_layers = 24
        config.num_attention_heads = 16
    elif "vitg" in model_name:
        config.use_swiglu_ffn = True
        config.hidden_size = 1536
        config.num_hidden_layers = 40
        config.num_attention_heads = 24
    else:
        raise ValueError("Model not supported")  # æŠ›å‡ºå¼‚å¸¸ï¼ŒæŒ‡ç¤ºä¸æ”¯æŒçš„æ¨¡å‹

    # å¦‚æœéœ€è¦ä¸ºå›¾åƒåˆ†ç±»å™¨è®¾ç½®é…ç½®å‚æ•°
    if image_classifier:
        repo_id = "huggingface/label-files"
        filename = "imagenet-1k-id2label.json"
        # ä» Hugging Face Hub ä¸‹è½½å¹¶åŠ è½½æ ‡ç­¾æ–‡ä»¶ï¼Œå°†æ ‡ç­¾æ˜ å°„æ·»åŠ åˆ°é…ç½®ä¸­
        config.num_labels = 1000
        config.id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
        config.id2label = {int(k): v for k, v in config.id2label.items()}

    return config


def create_rename_keys(config):
    rename_keys = []

    # ä¸‹é¢çš„æ³¨é‡Šæ˜¯ä¸ºäº†æŒ‡å®šåˆ—è¡¨çš„æ ¼å¼
    # fmt: off

    # å°†åŸå§‹é”®åå’Œç›®æ ‡é”®åæ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­ï¼Œç”¨äºæ¨¡å‹æƒé‡åŠ è½½æ—¶çš„æ˜ å°„
    rename_keys.append(("cls_token", "embeddings.cls_token"))
    rename_keys.append(("mask_token", "embeddings.mask_token"))
    rename_keys.append(("pos_embed", "embeddings.position_embeddings"))
    rename_keys.append(("patch_embed.proj.weight", "embeddings.patch_embeddings.projection.weight"))
    rename_keys.append(("patch_embed.proj.bias", "embeddings.patch_embeddings.projection.bias"))
    # å¯¹æ¯ä¸ªéšè—å±‚è¿›è¡Œå¾ªç¯è¿­ä»£ï¼Œç”Ÿæˆéœ€è¦é‡å‘½åçš„é”®å€¼å¯¹åˆ—è¡¨
    for i in range(config.num_hidden_layers):
        # layernorms
        # æ·»åŠ æƒé‡å’Œåç½®çš„é‡å‘½åé”®å€¼å¯¹ï¼Œæ˜ å°„åˆ°ç¼–ç å™¨å±‚çš„ç¬¬iå±‚çš„ç¬¬1ä¸ªå½’ä¸€åŒ–å±‚
        rename_keys.append((f"blocks.{i}.norm1.weight", f"encoder.layer.{i}.norm1.weight"))
        rename_keys.append((f"blocks.{i}.norm1.bias", f"encoder.layer.{i}.norm1.bias"))
        rename_keys.append((f"blocks.{i}.norm2.weight", f"encoder.layer.{i}.norm2.weight"))
        rename_keys.append((f"blocks.{i}.norm2.bias", f"encoder.layer.{i}.norm2.bias"))
        
        # MLP
        # æ ¹æ®é…ç½®å†³å®šä½¿ç”¨ä¸åŒçš„MLPç»“æ„è¿›è¡Œé‡å‘½å
        if config.use_swiglu_ffn:
            # ä½¿ç”¨ SwiGLU ç»“æ„çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ·»åŠ ç›¸åº”çš„é‡å‘½åé”®å€¼å¯¹
            rename_keys.append((f"blocks.{i}.mlp.w12.weight", f"encoder.layer.{i}.mlp.w12.weight"))
            rename_keys.append((f"blocks.{i}.mlp.w12.bias", f"encoder.layer.{i}.mlp.w12.bias"))
            rename_keys.append((f"blocks.{i}.mlp.w3.weight", f"encoder.layer.{i}.mlp.w3.weight"))
            rename_keys.append((f"blocks.{i}.mlp.w3.bias", f"encoder.layer.{i}.mlp.w3.bias"))
        else:
            # ä½¿ç”¨æ™®é€šçš„å…¨è¿æ¥å±‚ç»“æ„ï¼Œæ·»åŠ ç›¸åº”çš„é‡å‘½åé”®å€¼å¯¹
            rename_keys.append((f"blocks.{i}.mlp.fc1.weight", f"encoder.layer.{i}.mlp.fc1.weight"))
            rename_keys.append((f"blocks.{i}.mlp.fc1.bias", f"encoder.layer.{i}.mlp.fc1.bias"))
            rename_keys.append((f"blocks.{i}.mlp.fc2.weight", f"encoder.layer.{i}.mlp.fc2.weight"))
            rename_keys.append((f"blocks.{i}.mlp.fc2.bias", f"encoder.layer.{i}.mlp.fc2.bias"))
        
        # layerscale
        # æ·»åŠ å±‚å°ºåº¦çš„é‡å‘½åé”®å€¼å¯¹ï¼Œæ˜ å°„åˆ°ç¼–ç å™¨å±‚çš„ç¬¬iå±‚çš„å±‚å°ºåº¦å‚æ•°
        rename_keys.append((f"blocks.{i}.ls1.gamma", f"encoder.layer.{i}.layer_scale1.lambda1"))
        rename_keys.append((f"blocks.{i}.ls2.gamma", f"encoder.layer.{i}.layer_scale2.lambda1"))
        
        # attention projection layer
        # æ·»åŠ æ³¨æ„åŠ›æŠ•å½±å±‚çš„é‡å‘½åé”®å€¼å¯¹ï¼Œæ˜ å°„åˆ°ç¼–ç å™¨å±‚çš„ç¬¬iå±‚çš„æ³¨æ„åŠ›è¾“å‡ºå±‚
        rename_keys.append((f"blocks.{i}.attn.proj.weight", f"encoder.layer.{i}.attention.output.dense.weight"))
        rename_keys.append((f"blocks.{i}.attn.proj.bias", f"encoder.layer.{i}.attention.output.dense.bias"))

    # final layernorm
    # æ·»åŠ æœ€ç»ˆçš„å½’ä¸€åŒ–å±‚æƒé‡å’Œåç½®çš„é‡å‘½åé”®å€¼å¯¹ï¼Œæ˜ å°„åˆ°æœ€ç»ˆçš„å½’ä¸€åŒ–å±‚
    rename_keys.append(("norm.weight", "layernorm.weight"))
    rename_keys.append(("norm.bias", "layernorm.bias"))

    # fmt: on
    # è¿”å›æ‰€æœ‰çš„é‡å‘½åé”®å€¼å¯¹åˆ—è¡¨
    return rename_keys
# ä»å­—å…¸ dct ä¸­ç§»é™¤é”® oldï¼Œå¹¶å°†å…¶å¯¹åº”çš„å€¼èµ‹ç»™å˜é‡ valï¼Œç„¶åå°†é”® new æ·»åŠ åˆ°å­—å…¸ dct ä¸­ï¼Œå…¶å€¼ä¸º val
def rename_key(dct, old, new):
    val = dct.pop(old)
    dct[new] = val

# å¯¹æ¯ä¸ªç¼–ç å™¨å±‚çš„çŠ¶æ€å­—å…¸ state_dict æ‰§è¡Œæ“ä½œï¼Œå°†æ¯ä¸€å±‚çš„æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰åˆ†åˆ«è¯»å–å¹¶æ·»åŠ åˆ° state_dict ä¸­
def read_in_q_k_v(state_dict, config):
    for i in range(config.num_hidden_layers):
        # è¯»å–è¾“å…¥æŠ•å½±å±‚ï¼ˆåœ¨ timm ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„çŸ©é˜µåŠ åç½®é¡¹ï¼‰çš„æƒé‡å’Œåç½®
        in_proj_weight = state_dict.pop(f"blocks.{i}.attn.qkv.weight")
        in_proj_bias = state_dict.pop(f"blocks.{i}.attn.qkv.bias")
        
        # å°†æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeyï¼‰ã€å€¼ï¼ˆvalueï¼‰ä¾æ¬¡æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ state_dict ä¸­
        state_dict[f"encoder.layer.{i}.attention.attention.query.weight"] = in_proj_weight[: config.hidden_size, :]
        state_dict[f"encoder.layer.{i}.attention.attention.query.bias"] = in_proj_bias[: config.hidden_size]
        state_dict[f"encoder.layer.{i}.attention.attention.key.weight"] = in_proj_weight[
            config.hidden_size : config.hidden_size * 2, :
        ]
        state_dict[f"encoder.layer.{i}.attention.attention.key.bias"] = in_proj_bias[
            config.hidden_size : config.hidden_size * 2
        ]
        state_dict[f"encoder.layer.{i}.attention.attention.value.weight"] = in_proj_weight[-config.hidden_size :, :]
        state_dict[f"encoder.layer.{i}.attention.attention.value.bias"] = in_proj_bias[-config.hidden_size :]

# å‡†å¤‡å›¾åƒï¼Œä»æŒ‡å®š URL è·å–å›¾åƒå¹¶è¿”å›
def prepare_img():
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    image = Image.open(requests.get(url, stream=True).raw)
    return image

# æ— éœ€æ¢¯åº¦çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨è£…é¥°å™¨ï¼Œç”¨äº DINOv2 æ¨¡å‹çš„æƒé‡è½¬æ¢æ“ä½œ
@torch.no_grad()
def convert_dinov2_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=False):
    """
    Copy/paste/tweak model's weights to our DINOv2 structure.
    """

    # æ ¹æ®æ¨¡å‹åç§°å’Œæ˜¯å¦ä¸º 1layer æ¨¡å‹è·å– DINOv2 çš„é…ç½®ä¿¡æ¯
    image_classifier = "1layer" in model_name
    config = get_dinov2_config(model_name, image_classifier=image_classifier)

    # ä» Torch Hub åŠ è½½åŸå§‹æ¨¡å‹
    original_model = torch.hub.load("facebookresearch/dinov2", model_name.replace("_1layer", ""))
    original_model.eval()

    # åŠ è½½åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œç§»é™¤å’Œé‡å‘½åä¸€äº›é”®
    state_dict = original_model.state_dict()
    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:
        rename_key(state_dict, src, dest)
    read_in_q_k_v(state_dict, config)

    # å¤åˆ¶çŠ¶æ€å­—å…¸çš„é”®å€¼å¯¹ï¼Œå¹¶æ ¹æ®éœ€è¦ä¿®æ”¹é”®å
    for key, val in state_dict.copy().items():
        val = state_dict.pop(key)
        if "w12" in key:
            key = key.replace("w12", "weights_in")
        if "w3" in key:
            key = key.replace("w3", "weights_out")
        state_dict[key] = val

    # åŠ è½½ HuggingFace æ¨¡å‹
    # å¦‚æœå­˜åœ¨å›¾åƒåˆ†ç±»å™¨ï¼Œåˆ™ä½¿ç”¨Dinov2ForImageClassificationæ¨¡å‹ï¼ŒåŠ è½½çŠ¶æ€å­—å…¸ï¼Œå¹¶è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    if image_classifier:
        model = Dinov2ForImageClassification(config).eval()
        model.dinov2.load_state_dict(state_dict)
        
        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©å¯¹åº”çš„åˆ†ç±»å™¨çŠ¶æ€å­—å…¸çš„ URL
        model_name_to_classifier_dict_url = {
            "dinov2_vits14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth",
            "dinov2_vitb14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth",
            "dinov2_vitl14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth",
            "dinov2_vitg14_1layer": "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth",
        }
        url = model_name_to_classifier_dict_url[model_name]
        
        # ä½¿ç”¨ torch.hub ä» URL åŠ è½½åˆ†ç±»å™¨çŠ¶æ€å­—å…¸åˆ°æœ¬åœ°ï¼Œå¹¶åœ¨ CPU ä¸ŠåŠ è½½
        classifier_state_dict = torch.hub.load_state_dict_from_url(url, map_location="cpu")
        
        # å°†åŠ è½½çš„åˆ†ç±»å™¨æƒé‡å’Œåç½®è®¾ä¸ºæ¨¡å‹çš„å‚æ•°
        model.classifier.weight = nn.Parameter(classifier_state_dict["weight"])
        model.classifier.bias = nn.Parameter(classifier_state_dict["bias"])
    else:
        # å¦åˆ™ä½¿ç”¨Dinov2Modelï¼Œå¹¶åŠ è½½çŠ¶æ€å­—å…¸
        model = Dinov2Model(config).eval()
        model.load_state_dict(state_dict)

    # åŠ è½½å›¾åƒæ•°æ®
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

    # å›¾åƒé¢„å¤„ç†æ­¥éª¤
    transformations = transforms.Compose(
        [
            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),  # è°ƒæ•´å›¾åƒå¤§å°
            transforms.CenterCrop(224),  # ä¸­å¿ƒè£å‰ªå›¾åƒ
            transforms.ToTensor(),  # è½¬æ¢ä¸ºå¼ é‡
            transforms.Normalize(  # æ ‡å‡†åŒ–å›¾åƒæ•°æ®
                mean=IMAGENET_DEFAULT_MEAN,  # å›¾åƒæ•°æ®çš„å‡å€¼
                std=IMAGENET_DEFAULT_STD,  # å›¾åƒæ•°æ®çš„æ ‡å‡†å·®
            ),
        ]
    )

    # å¯¹åŸå§‹åƒç´ å€¼åº”ç”¨é¢„å¤„ç†ï¼Œå¹¶å¢åŠ æ‰¹å¤„ç†ç»´åº¦
    original_pixel_values = transformations(image).unsqueeze(0)

    # ä½¿ç”¨ BitImageProcessor å¤„ç†å›¾åƒï¼Œè¿”å›å¤„ç†åçš„åƒç´ å€¼
    processor = BitImageProcessor(
        size={"shortest_edge": 256},  # æœ€çŸ­è¾¹è®¾ç½®ä¸º256åƒç´ 
        resample=PILImageResampling.BICUBIC,  # ä½¿ç”¨åŒä¸‰æ¬¡æ’å€¼é‡é‡‡æ ·
        image_mean=IMAGENET_DEFAULT_MEAN,  # å›¾åƒæ•°æ®çš„å‡å€¼
        image_std=IMAGENET_DEFAULT_STD,  # å›¾åƒæ•°æ®çš„æ ‡å‡†å·®
    )
    pixel_values = processor(image, return_tensors="pt").pixel_values

    # æ–­è¨€å¤„ç†åçš„åƒç´ å€¼ä¸åŸå§‹åƒç´ å€¼åœ¨æ•°å€¼ä¸Šçš„è¿‘ä¼¼æ€§
    assert torch.allclose(original_pixel_values, pixel_values)

    # å…³é—­æ¢¯åº¦è®¡ç®—ï¼Œåœ¨æ¨ç†é˜¶æ®µä¸æ›´æ–°æ¨¡å‹å‚æ•°
    with torch.no_grad():
        # è·å–æ¨¡å‹è¾“å‡ºåŠéšè—çŠ¶æ€
        outputs = model(pixel_values, output_hidden_states=True)
        original_outputs = original_model(pixel_values)

    # æ–­è¨€æ£€æŸ¥
    if image_classifier:
        # å¦‚æœæ˜¯å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œè¾“å‡ºé¢„æµ‹ç±»åˆ«
        print("Predicted class:")
        class_idx = outputs.logits.argmax(-1).item()
        print(model.config.id2label[class_idx])
    else:
        # å¦åˆ™ï¼Œæ–­è¨€åŸå§‹è¾“å‡ºå’Œå½“å‰è¾“å‡ºçš„æœ€åéšè—çŠ¶æ€çš„ä¸€è‡´æ€§
        assert outputs.last_hidden_state[:, 0].shape == original_outputs.shape
        assert torch.allclose(outputs.last_hidden_state[:, 0], original_outputs, atol=1e-3)
    print("Looks ok!")
    # å¦‚æœæŒ‡å®šäº† pytorch_dump_folder_pathï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if pytorch_dump_folder_path is not None:
        # åˆ›å»ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        # æ‰“å°ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # æ‰“å°ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ­£åœ¨ä¿å­˜å›¾åƒå¤„ç†å™¨åˆ°æŒ‡å®šè·¯å¾„
        print(f"Saving image processor to {pytorch_dump_folder_path}")
        # å°†å›¾åƒå¤„ç†å™¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        # æ ¹æ®æ¨¡å‹åç§°æ˜ å°„åˆ°å¯¹åº”çš„ Hub åç§°
        model_name_to_hf_name = {
            "dinov2_vits14": "dinov2-small",
            "dinov2_vitb14": "dinov2-base",
            "dinov2_vitl14": "dinov2-large",
            "dinov2_vitg14": "dinov2-giant",
            "dinov2_vits14_1layer": "dinov2-small-imagenet1k-1-layer",
            "dinov2_vitb14_1layer": "dinov2-base-imagenet1k-1-layer",
            "dinov2_vitl14_1layer": "dinov2-large-imagenet1k-1-layer",
            "dinov2_vitg14_1layer": "dinov2-giant-imagenet1k-1-layer",
        }
        
        # æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„ Hub åç§°
        name = model_name_to_hf_name[model_name]
        # å°†æ¨¡å‹æ¨é€åˆ° Hubï¼Œä½¿ç”¨æ ¼å¼åŒ–çš„ Hub è·¯å¾„
        model.push_to_hub(f"facebook/{name}")
        # å°†å›¾åƒå¤„ç†å™¨æ¨é€åˆ° Hubï¼Œä½¿ç”¨æ ¼å¼åŒ–çš„ Hub è·¯å¾„
        processor.push_to_hub(f"facebook/{name}")
if __name__ == "__main__":
    # å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    parser = argparse.ArgumentParser()
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡

    # Required parameters
    parser.add_argument(
        "--model_name",
        default="dinov2_vitb14",
        type=str,
        choices=[
            "dinov2_vits14",
            "dinov2_vitb14",
            "dinov2_vitl14",
            "dinov2_vitg14",
            "dinov2_vits14_1layer",
            "dinov2_vitb14_1layer",
            "dinov2_vitl14_1layer",
            "dinov2_vitg14_1layer",
        ],
        help="Name of the model you'd like to convert.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼ŒæŒ‡å®šæ¨¡å‹çš„åç§°ï¼Œå¿…é¡»ä»é¢„å®šä¹‰çš„é€‰é¡¹ä¸­é€‰æ‹©

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼ŒæŒ‡å®šè¾“å‡ºçš„PyTorchæ¨¡å‹ç›®å½•çš„è·¯å¾„

    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°é€‰é¡¹ï¼ŒæŒ‡å®šæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ°Hugging Face hub

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•° convert_dinov2_checkpointï¼Œä¼ å…¥è§£æåçš„å‚æ•°
    convert_dinov2_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
```