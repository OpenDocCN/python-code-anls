# `.\models\audio_spectrogram_transformer\convert_audio_spectrogram_transformer_original_to_pytorch.py`

```py
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8

# ç‰ˆæƒå£°æ˜ï¼Œå£°æ˜ä»£ç å½’ HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰ï¼Œé‡‡ç”¨ Apache License 2.0 ç‰ˆæœ¬è¿›è¡Œè®¸å¯
# é™¤éç¬¦åˆè®¸å¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨è¯¥æ–‡ä»¶
# å¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–è®¸å¯åè®®å†…å®¹ï¼šhttp://www.apache.org/licenses/LICENSE-2.0

"""ä»åŸå§‹ä»“åº“è½¬æ¢éŸ³é¢‘é¢‘è°±å˜æ¢å™¨æ£€æŸ¥ç‚¹ã€‚URL: https://github.com/YuanGongND/ast"""

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import json  # å¯¼å…¥å¤„ç† JSON æ ¼å¼æ•°æ®çš„æ¨¡å—
from pathlib import Path  # å¯¼å…¥å¤„ç†è·¯å¾„æ“ä½œçš„æ¨¡å—

import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ åº“
import torchaudio  # å¯¼å…¥å¤„ç†éŸ³é¢‘æ•°æ®çš„ PyTorch æ‰©å±•æ¨¡å—
from datasets import load_dataset  # å¯¼å…¥åŠ è½½æ•°æ®é›†çš„å‡½æ•°
from huggingface_hub import hf_hub_download  # å¯¼å…¥ä¸‹è½½ Hugging Face Hub æ¨¡å‹çš„å‡½æ•°

from transformers import ASTConfig, ASTFeatureExtractor, ASTForAudioClassification  # å¯¼å…¥éŸ³é¢‘é¢‘è°±å˜æ¢å™¨ç›¸å…³çš„ç±»
from transformers.utils import logging  # å¯¼å…¥æ—¥å¿—è®°å½•å·¥å…·

# è®¾ç½®æ—¥å¿—è®°å½•çš„è¯¦ç»†ç¨‹åº¦ä¸ºä¿¡æ¯çº§åˆ«
logging.set_verbosity_info()

# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨å¯¹è±¡
logger = logging.get_logger(__name__)


def get_audio_spectrogram_transformer_config(model_name):
    # åˆ›å»ºä¸€ä¸ªéŸ³é¢‘é¢‘è°±å˜æ¢å™¨é…ç½®å¯¹è±¡
    config = ASTConfig()

    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®ä¸åŒçš„é…ç½®å‚æ•°
    if "10-10" in model_name:
        pass  # å¦‚æœæ¨¡å‹åç§°åŒ…å« "10-10"ï¼Œåˆ™ä¸ä¿®æ”¹é…ç½®
    elif "speech-commands" in model_name:
        config.max_length = 128  # å¦‚æœæ¨¡å‹åç§°åŒ…å« "speech-commands"ï¼Œè®¾ç½®æœ€å¤§é•¿åº¦ä¸º 128
    elif "12-12" in model_name:
        config.time_stride = 12  # å¦‚æœæ¨¡å‹åç§°åŒ…å« "12-12"ï¼Œè®¾ç½®æ—¶é—´æ­¥é•¿ä¸º 12
        config.frequency_stride = 12  # è®¾ç½®é¢‘ç‡æ­¥é•¿ä¸º 12
    elif "14-14" in model_name:
        config.time_stride = 14  # å¦‚æœæ¨¡å‹åç§°åŒ…å« "14-14"ï¼Œè®¾ç½®æ—¶é—´æ­¥é•¿ä¸º 14
        config.frequency_stride = 14  # è®¾ç½®é¢‘ç‡æ­¥é•¿ä¸º 14
    elif "16-16" in model_name:
        config.time_stride = 16  # å¦‚æœæ¨¡å‹åç§°åŒ…å« "16-16"ï¼Œè®¾ç½®æ—¶é—´æ­¥é•¿ä¸º 16
        config.frequency_stride = 16  # è®¾ç½®é¢‘ç‡æ­¥é•¿ä¸º 16
    else:
        raise ValueError("Model not supported")  # å¦‚æœæ¨¡å‹åç§°ä¸åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼ŒæŠ›å‡ºæ•°å€¼é”™è¯¯å¼‚å¸¸

    # è®¾ç½®ä»“åº“ ID ç”¨äºä¸‹è½½æ ‡ç­¾æ–‡ä»¶
    repo_id = "huggingface/label-files"

    # æ ¹æ®æ¨¡å‹åç§°è¿›ä¸€æ­¥è®¾ç½®é…ç½®å¯¹è±¡çš„å±æ€§
    if "speech-commands" in model_name:
        config.num_labels = 35  # å¦‚æœæ¨¡å‹åç§°åŒ…å« "speech-commands"ï¼Œè®¾ç½®æ ‡ç­¾æ•°é‡ä¸º 35
        filename = "speech-commands-v2-id2label.json"  # è®¾ç½®è¦ä¸‹è½½çš„æ ‡ç­¾æ–‡ä»¶å
    else:
        config.num_labels = 527  # å¦åˆ™ï¼Œè®¾ç½®æ ‡ç­¾æ•°é‡ä¸º 527
        filename = "audioset-id2label.json"  # è®¾ç½®è¦ä¸‹è½½çš„æ ‡ç­¾æ–‡ä»¶å

    # ä½¿ç”¨ Hugging Face Hub ä¸‹è½½æŒ‡å®šä»“åº“ ID å’Œæ–‡ä»¶åçš„ JSON æ–‡ä»¶ï¼Œå¹¶åŠ è½½ä¸º Python å­—å…¸
    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
    # å°†åŠ è½½çš„æ ‡ç­¾å­—å…¸ä¸­çš„é”®è½¬æ¢ä¸ºæ•´æ•°ç±»å‹ï¼Œå€¼ä¿æŒä¸å˜
    id2label = {int(k): v for k, v in id2label.items()}
    config.id2label = id2label  # å°†è½¬æ¢åçš„æ ‡ç­¾å­—å…¸èµ‹å€¼ç»™é…ç½®å¯¹è±¡çš„ id2label å±æ€§
    config.label2id = {v: k for k, v in id2label.items()}  # åˆ›å»ºæ ‡ç­¾åˆ° ID çš„åå‘æ˜ å°„å­—å…¸

    return config  # è¿”å›é…ç½®å¯¹è±¡


def rename_key(name):
    # æ ¹æ®ç‰¹å®šè§„åˆ™é‡å‘½åè¾“å…¥çš„é”®åå­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›é‡å‘½ååçš„ç»“æœ

    if "module.v" in name:
        name = name.replace("module.v", "audio_spectrogram_transformer")
    if "cls_token" in name:
        name = name.replace("cls_token", "embeddings.cls_token")
    if "dist_token" in name:
        name = name.replace("dist_token", "embeddings.distillation_token")
    if "pos_embed" in name:
        name = name.replace("pos_embed", "embeddings.position_embeddings")
    if "patch_embed.proj" in name:
        name = name.replace("patch_embed.proj", "embeddings.patch_embeddings.projection")
    # æ›¿æ¢ transformer blocks ç›¸å…³çš„é”®å
    if "blocks" in name:
        name = name.replace("blocks", "encoder.layer")
    if "attn.proj" in name:
        name = name.replace("attn.proj", "attention.output.dense")
    if "attn" in name:
        name = name.replace("attn", "attention.self")

    # è¿”å›ä¿®æ”¹åçš„é”®åå­—ç¬¦ä¸²
    return name
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "norm1"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º "layernorm_before"
    if "norm1" in name:
        name = name.replace("norm1", "layernorm_before")
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "norm2"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º "layernorm_after"
    if "norm2" in name:
        name = name.replace("norm2", "layernorm_after")
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "mlp.fc1"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º "intermediate.dense"
    if "mlp.fc1" in name:
        name = name.replace("mlp.fc1", "intermediate.dense")
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "mlp.fc2"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º "output.dense"
    if "mlp.fc2" in name:
        name = name.replace("mlp.fc2", "output.dense")
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "audio_spectrogram_transformer.norm"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º "audio_spectrogram_transformer.layernorm"
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†å…¼å®¹ä¸åŒå‘½åè§„èŒƒä¸‹çš„æ¨¡å‹å‚æ•°
    if "audio_spectrogram_transformer.norm" in name:
        name = name.replace("audio_spectrogram_transformer.norm", "audio_spectrogram_transformer.layernorm")
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "module.mlp_head.0"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º "classifier.layernorm"
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†é‡å‘½ååˆ†ç±»å™¨å¤´éƒ¨çš„å±‚å½’ä¸€åŒ–å±‚
    if "module.mlp_head.0" in name:
        name = name.replace("module.mlp_head.0", "classifier.layernorm")
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "module.mlp_head.1"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º "classifier.dense"
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†é‡å‘½ååˆ†ç±»å™¨å¤´éƒ¨çš„å…¨è¿æ¥å±‚
    if "module.mlp_head.1" in name:
        name = name.replace("module.mlp_head.1", "classifier.dense")

    # è¿”å›ç»è¿‡å¤„ç†çš„æœ€ç»ˆå˜é‡ name
    return name
# å°†åŸå§‹çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºæ–°é…ç½®çš„çŠ¶æ€å­—å…¸
def convert_state_dict(orig_state_dict, config):
    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„æ‹·è´ä¸­çš„æ¯ä¸ªé”®
    for key in orig_state_dict.copy().keys():
        # å¼¹å‡ºå½“å‰é”®å¯¹åº”çš„å€¼
        val = orig_state_dict.pop(key)

        # å¦‚æœé”®ååŒ…å« "qkv"
        if "qkv" in key:
            # æ ¹æ® "." åˆ†å‰²é”®å
            key_split = key.split(".")
            # è·å–å±‚å·ï¼Œè¿™é‡Œå‡è®¾å±‚å·åœ¨ç¬¬4ä¸ªä½ç½®
            layer_num = int(key_split[3])
            # è·å–éšè—å±‚å¤§å°
            dim = config.hidden_size
            # å¦‚æœé”®ååŒ…å« "weight"
            if "weight" in key:
                # æ›´æ–°çŠ¶æ€å­—å…¸ä¸­çš„ queryã€keyã€value çš„æƒé‡å‚æ•°
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.query.weight"
                ] = val[:dim, :]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.key.weight"
                ] = val[dim : dim * 2, :]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.value.weight"
                ] = val[-dim:, :]
            else:
                # æ›´æ–°çŠ¶æ€å­—å…¸ä¸­çš„ queryã€keyã€value çš„åç½®å‚æ•°
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.query.bias"
                ] = val[:dim]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.key.bias"
                ] = val[dim : dim * 2]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.value.bias"
                ] = val[-dim:]
        else:
            # å¦‚æœé”®åä¸åŒ…å« "qkv"ï¼Œåˆ™é‡å‘½åé”®å¹¶ä¿ç•™å…¶å¯¹åº”çš„å€¼
            orig_state_dict[rename_key(key)] = val

    # è¿”å›æ›´æ–°åçš„åŸå§‹çŠ¶æ€å­—å…¸
    return orig_state_dict


# ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤æŒ‡å®šçš„é”®
def remove_keys(state_dict):
    # éœ€è¦å¿½ç•¥çš„é”®åˆ—è¡¨
    ignore_keys = [
        "module.v.head.weight",
        "module.v.head.bias",
        "module.v.head_dist.weight",
        "module.v.head_dist.bias",
    ]
    # éå†å¿½ç•¥é”®åˆ—è¡¨ï¼Œä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤å¯¹åº”çš„é”®
    for k in ignore_keys:
        state_dict.pop(k, None)


# åœ¨æ²¡æœ‰æ¢¯åº¦æ›´æ–°çš„æƒ…å†µä¸‹æ‰§è¡Œå‡½æ•°
@torch.no_grad()
def convert_audio_spectrogram_transformer_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=False):
    """
    å°†æ¨¡å‹çš„æƒé‡å¤åˆ¶/ç²˜è´´/è°ƒæ•´åˆ°æˆ‘ä»¬çš„éŸ³é¢‘é¢‘è°±å˜æ¢å™¨ç»“æ„ä¸­ã€‚
    """
    # è·å–éŸ³é¢‘é¢‘è°±å˜æ¢å™¨çš„é…ç½®
    config = get_audio_spectrogram_transformer_config(model_name)
    # æ¨¡å‹åç§°åˆ°é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ–‡ä»¶ä¸‹è½½é“¾æ¥çš„æ˜ å°„å­—å…¸
    model_name_to_url = {
        "ast-finetuned-audioset-10-10-0.4593": (
            "https://www.dropbox.com/s/ca0b1v2nlxzyeb4/audioset_10_10_0.4593.pth?dl=1"
        ),
        "ast-finetuned-audioset-10-10-0.450": (
            "https://www.dropbox.com/s/1tv0hovue1bxupk/audioset_10_10_0.4495.pth?dl=1"
        ),
        "ast-finetuned-audioset-10-10-0.448": (
            "https://www.dropbox.com/s/6u5sikl4b9wo4u5/audioset_10_10_0.4483.pth?dl=1"
        ),
        "ast-finetuned-audioset-10-10-0.448-v2": (
            "https://www.dropbox.com/s/kt6i0v9fvfm1mbq/audioset_10_10_0.4475.pth?dl=1"
        ),
        "ast-finetuned-audioset-12-12-0.447": (
            "https://www.dropbox.com/s/snfhx3tizr4nuc8/audioset_12_12_0.4467.pth?dl=1"
        ),
        "ast-finetuned-audioset-14-14-0.443": (
            "https://www.dropbox.com/s/z18s6pemtnxm4k7/audioset_14_14_0.4431.pth?dl=1"
        ),
        "ast-finetuned-audioset-16-16-0.442": (
            "https://www.dropbox.com/s/mdsa4t1xmcimia6/audioset_16_16_0.4422.pth?dl=1"
        ),
        "ast-finetuned-speech-commands-v2": (
            "https://www.dropbox.com/s/q0tbqpwv44pquwy/speechcommands_10_10_0.9812.pth?dl=1"
        ),
    }

    # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
    checkpoint_url = model_name_to_url[model_name]
    state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location="cpu")
    # ç§»é™¤éƒ¨åˆ†é”®
    remove_keys(state_dict)
    # é‡å‘½åéƒ¨åˆ†é”®
    new_state_dict = convert_state_dict(state_dict, config)

    # åŠ è½½ ğŸ¤— æ¨¡å‹
    model = ASTForAudioClassification(config)
    model.eval()

    model.load_state_dict(new_state_dict)

    # åœ¨è™šæ‹Ÿè¾“å…¥ä¸ŠéªŒè¯è¾“å‡º
    # æ¥æºï¼šhttps://github.com/YuanGongND/ast/blob/79e873b8a54d0a3b330dd522584ff2b9926cd581/src/run.py#L62
    mean = -4.2677393 if "speech-commands" not in model_name else -6.845978
    std = 4.5689974 if "speech-commands" not in model_name else 5.5654526
    max_length = 1024 if "speech-commands" not in model_name else 128
    feature_extractor = ASTFeatureExtractor(mean=mean, std=std, max_length=max_length)

    if "speech-commands" in model_name:
        # åŠ è½½ "speech-commands" æ•°æ®é›†çš„éªŒè¯é›†
        dataset = load_dataset("speech_commands", "v0.02", split="validation")
        waveform = dataset[0]["audio"]["array"]
    else:
        # ä¸‹è½½æ ·æœ¬éŸ³é¢‘æ–‡ä»¶
        filepath = hf_hub_download(
            repo_id="nielsr/audio-spectogram-transformer-checkpoint",
            filename="sample_audio.flac",
            repo_type="dataset",
        )

        # åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶è½¬æ¢ä¸º NumPy æ•°ç»„
        waveform, _ = torchaudio.load(filepath)
        waveform = waveform.squeeze().numpy()

    # ä½¿ç”¨ç‰¹å¾æå–å™¨å¤„ç†æ³¢å½¢æ•°æ®
    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors="pt")

    # å‰å‘ä¼ æ’­
    outputs = model(**inputs)
    logits = outputs.logits

    if model_name == "ast-finetuned-audioset-10-10-0.4593":
        expected_slice = torch.tensor([-0.8760, -7.0042, -8.6602])
    elif model_name == "ast-finetuned-audioset-10-10-0.450":
        expected_slice = torch.tensor([-1.1986, -7.0903, -8.2718])
    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®é¢„æœŸçš„è¾“å‡ºå‘é‡ç‰‡æ®µ
    elif model_name == "ast-finetuned-audioset-10-10-0.448":
        expected_slice = torch.tensor([-2.6128, -8.0080, -9.4344])
    elif model_name == "ast-finetuned-audioset-10-10-0.448-v2":
        expected_slice = torch.tensor([-1.5080, -7.4534, -8.8917])
    elif model_name == "ast-finetuned-audioset-12-12-0.447":
        expected_slice = torch.tensor([-0.5050, -6.5833, -8.0843])
    elif model_name == "ast-finetuned-audioset-14-14-0.443":
        expected_slice = torch.tensor([-0.3826, -7.0336, -8.2413])
    elif model_name == "ast-finetuned-audioset-16-16-0.442":
        expected_slice = torch.tensor([-1.2113, -6.9101, -8.3470])
    elif model_name == "ast-finetuned-speech-commands-v2":
        expected_slice = torch.tensor([6.1589, -8.0566, -8.7984])
    else:
        # å¦‚æœæ¨¡å‹åç§°æœªçŸ¥ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
        raise ValueError("Unknown model name")
    
    # æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„å‰ä¸‰ä¸ªå…ƒç´ æ˜¯å¦ä¸é¢„æœŸçš„å‘é‡ç‰‡æ®µéå¸¸æ¥è¿‘ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
    if not torch.allclose(logits[0, :3], expected_slice, atol=1e-4):
        raise ValueError("Logits don't match")
    
    # æ‰“å°æç¤ºä¿¡æ¯ï¼Œè¡¨ç¤ºæ£€æŸ¥é€šè¿‡
    print("Looks ok!")

    # å¦‚æœæŒ‡å®šäº† PyTorch æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if pytorch_dump_folder_path is not None:
        # ç¡®ä¿æŒ‡å®šè·¯å¾„å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        # æ‰“å°æ¨¡å‹ä¿å­˜çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹åç§°å’Œä¿å­˜è·¯å¾„
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # æ‰“å°ç‰¹å¾æå–å™¨ä¿å­˜çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¿å­˜è·¯å¾„
        print(f"Saving feature extractor to {pytorch_dump_folder_path}")
        # å°†ç‰¹å¾æå–å™¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        feature_extractor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        # æ‰“å°æ¨é€æ¨¡å‹å’Œç‰¹å¾æå–å™¨åˆ° Hub çš„æç¤ºä¿¡æ¯
        print("Pushing model and feature extractor to the hub...")
        # å°†æ¨¡å‹æ¨é€åˆ°æŒ‡å®š Hub è·¯å¾„
        model.push_to_hub(f"MIT/{model_name}")
        # å°†ç‰¹å¾æå–å™¨æ¨é€åˆ°æŒ‡å®š Hub è·¯å¾„
        feature_extractor.push_to_hub(f"MIT/{model_name}")
if __name__ == "__main__":
    # å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡

    # å¿…é€‰å‚æ•°
    parser.add_argument(
        "--model_name",
        default="ast-finetuned-audioset-10-10-0.4593",
        type=str,
        help="Name of the Audio Spectrogram Transformer model you'd like to convert."
    )
    # æ·»åŠ å‚æ•°ï¼šæ¨¡å‹åç§°ï¼ŒæŒ‡å®šé»˜è®¤å€¼å’Œå¸®åŠ©ä¿¡æ¯

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    # æ·»åŠ å‚æ•°ï¼šPyTorch æ¨¡å‹è¾“å‡ºç›®å½•çš„è·¯å¾„ï¼Œæ”¯æŒé»˜è®¤å€¼å’Œå¸®åŠ©ä¿¡æ¯

    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    # æ·»åŠ å‚æ•°ï¼šæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ° ğŸ¤— hubï¼Œé‡‡ç”¨å¸ƒå°”å‹æ ‡å¿—

    args = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶å­˜å‚¨åˆ° args å¯¹è±¡ä¸­

    convert_audio_spectrogram_transformer_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
    # è°ƒç”¨å‡½æ•° convert_audio_spectrogram_transformer_checkpointï¼Œä¼ é€’è§£æå¾—åˆ°çš„å‚æ•°
```