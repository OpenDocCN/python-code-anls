# `.\transformers\models\audio_spectrogram_transformer\convert_audio_spectrogram_transformer_original_to_pytorch.py`

```
# è®¾ç½®è„šæœ¬ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜å’Œè®¸å¯ä¿¡æ¯
# Copyright 2022 The HuggingFace Inc. team.
# æ ¹æ® Apache License, Version 2.0 è®¸å¯è¯æˆæƒï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è¦æ±‚ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯å‰¯æœ¬ï¼šhttp://www.apache.org/licenses/LICENSE-2.0
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæœ¬è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€æä¾›çš„ï¼Œæ— ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶ã€‚
# è¯·å‚é˜…è®¸å¯è¯äº†è§£ç‰¹å®šè¯­è¨€ä¸‹çš„æƒé™å’Œé™åˆ¶ã€‚
"""ä»åŸå§‹å­˜å‚¨åº“è½¬æ¢éŸ³é¢‘é¢‘è°±å˜æ¢å™¨æ£€æŸ¥ç‚¹ã€‚URL: https://github.com/YuanGongND/ast"""

# å¯¼å…¥æ¨¡å—
import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import json  # ç”¨äºå¤„ç† JSON æ ¼å¼çš„æ•°æ®
from pathlib import Path  # ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„

import torch  # PyTorch æ·±åº¦å­¦ä¹ åº“
import torchaudio  # ç”¨äºéŸ³é¢‘å¤„ç†çš„ PyTorch æ‰©å±•
from datasets import load_dataset  # åŠ è½½æ•°æ®é›†çš„å‡½æ•°
from huggingface_hub import hf_hub_download  # ç”¨äºä» Hugging Face Hub ä¸‹è½½æ–‡ä»¶çš„å‡½æ•°

from transformers import ASTConfig, ASTFeatureExtractor, ASTForAudioClassification  # ç”¨äºéŸ³é¢‘åˆ†ç±»çš„ AST æ¨¡å‹
from transformers.utils import logging  # ç”¨äºæ—¥å¿—è®°å½•çš„å·¥å…·æ¨¡å—

# è®¾ç½®æ—¥å¿—è®°å½•çš„è¯¦ç»†ç¨‹åº¦ä¸ºä¿¡æ¯çº§åˆ«
logging.set_verbosity_info()
# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨å¯¹è±¡
logger = logging.get_logger(__name__)

# å‡½æ•°ï¼šæ ¹æ®æ¨¡å‹åç§°è·å–éŸ³é¢‘é¢‘è°±å˜æ¢å™¨çš„é…ç½®ä¿¡æ¯
def get_audio_spectrogram_transformer_config(model_name):
    # åˆ›å»ºä¸€ä¸ª ASTConfig å®ä¾‹å¯¹è±¡
    config = ASTConfig()

    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®é…ç½®å‚æ•°
    if "10-10" in model_name:
        pass
    elif "speech-commands" in model_name:
        config.max_length = 128  # è®¾ç½®æœ€å¤§é•¿åº¦ä¸º 128
    elif "12-12" in model_name:
        config.time_stride = 12  # è®¾ç½®æ—¶é—´æ­¥é•¿ä¸º 12
        config.frequency_stride = 12  # è®¾ç½®é¢‘ç‡æ­¥é•¿ä¸º 12
    elif "14-14" in model_name:
        config.time_stride = 14  # è®¾ç½®æ—¶é—´æ­¥é•¿ä¸º 14
        config.frequency_stride = 14  # è®¾ç½®é¢‘ç‡æ­¥é•¿ä¸º 14
    elif "16-16" in model_name:
        config.time_stride = 16  # è®¾ç½®æ—¶é—´æ­¥é•¿ä¸º 16
        config.frequency_stride = 16  # è®¾ç½®é¢‘ç‡æ­¥é•¿ä¸º 16
    else:
        raise ValueError("Model not supported")  # æŠ›å‡ºå€¼é”™è¯¯å¼‚å¸¸ï¼Œæ¨¡å‹ä¸å—æ”¯æŒ

    # ç”¨äºä» Hugging Face Hub ä¸‹è½½æ–‡ä»¶çš„å­˜å‚¨åº“ ID
    repo_id = "huggingface/label-files"
    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®é…ç½®å‚æ•°
    if "speech-commands" in model_name:
        config.num_labels = 35  # è®¾ç½®æ ‡ç­¾æ•°é‡ä¸º 35
        filename = "speech-commands-v2-id2label.json"  # è®¾ç½®æ–‡ä»¶å
    else:
        config.num_labels = 527  # è®¾ç½®æ ‡ç­¾æ•°é‡ä¸º 527
        filename = "audioset-id2label.json"  # è®¾ç½®æ–‡ä»¶å

    # ä» Hugging Face Hub ä¸‹è½½æ ‡ç­¾æ–‡ä»¶ï¼Œå¹¶åŠ è½½ä¸º JSON æ•°æ®
    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
    # å°†æ ‡ç­¾ ID è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¹¶æ„å»º ID åˆ°æ ‡ç­¾çš„æ˜ å°„å­—å…¸
    id2label = {int(k): v for k, v in id2label.items()}
    # è®¾ç½®é…ç½®å¯¹è±¡ä¸­çš„ ID åˆ°æ ‡ç­¾çš„æ˜ å°„å­—å…¸
    config.id2label = id2label
    # è®¾ç½®é…ç½®å¯¹è±¡ä¸­çš„æ ‡ç­¾åˆ° ID çš„æ˜ å°„å­—å…¸
    config.label2id = {v: k for k, v in id2label.items()}

    # è¿”å›é…ç½®å¯¹è±¡
    return config

# å‡½æ•°ï¼šé‡å‘½åé”®å
def rename_key(name):
    # æ›¿æ¢é”®åä¸­çš„å­—ç¬¦ä¸²
    if "module.v" in name:
        name = name.replace("module.v", "audio_spectrogram_transformer")
    if "cls_token" in name:
        name = name.replace("cls_token", "embeddings.cls_token")
    if "dist_token" in name:
        name = name.replace("dist_token", "embeddings.distillation_token")
    if "pos_embed" in name:
        name = name.replace("pos_embed", "embeddings.position_embeddings")
    if "patch_embed.proj" in name:
        name = name.replace("patch_embed.proj", "embeddings.patch_embeddings.projection")
    # æ›¿æ¢é”®åä¸­çš„å­—ç¬¦ä¸²ï¼Œç”¨äºè½¬æ¢å™¨å—
    if "blocks" in name:
        name = name.replace("blocks", "encoder.layer")
    if "attn.proj" in name:
        name = name.replace("attn.proj", "attention.output.dense")
    if "attn" in name:
        name = name.replace("attn", "attention.self")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"norm1"ï¼Œåˆ™æ›¿æ¢ä¸º"layernorm_before"
    if "norm1" in name:
        name = name.replace("norm1", "layernorm_before")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"norm2"ï¼Œåˆ™æ›¿æ¢ä¸º"layernorm_after"
    if "norm2" in name:
        name = name.replace("norm2", "layernorm_after")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"mlp.fc1"ï¼Œåˆ™æ›¿æ¢ä¸º"intermediate.dense"
    if "mlp.fc1" in name:
        name = name.replace("mlp.fc1", "intermediate.dense")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"mlp.fc2"ï¼Œåˆ™æ›¿æ¢ä¸º"output.dense"
    if "mlp.fc2" in name:
        name = name.replace("mlp.fc2", "output.dense")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"audio_spectrogram_transformer.norm"ï¼Œåˆ™æ›¿æ¢ä¸º"audio_spectrogram_transformer.layernorm"
    if "audio_spectrogram_transformer.norm" in name:
        name = name.replace("audio_spectrogram_transformer.norm", "audio_spectrogram_transformer.layernorm")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"module.mlp_head.0"ï¼Œåˆ™æ›¿æ¢ä¸º"classifier.layernorm"
    if "module.mlp_head.0" in name:
        name = name.replace("module.mlp_head.0", "classifier.layernorm")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"module.mlp_head.1"ï¼Œåˆ™æ›¿æ¢ä¸º"classifier.dense"
    if "module.mlp_head.1" in name:
        name = name.replace("module.mlp_head.1", "classifier.dense")

    # è¿”å›å¤„ç†åçš„æ–‡ä»¶å
    return name
# å°†åŸå§‹çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºé€‚ç”¨äºæ–°æ¨¡å‹ç»“æ„çš„çŠ¶æ€å­—å…¸
def convert_state_dict(orig_state_dict, config):
    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®çš„å‰¯æœ¬
    for key in orig_state_dict.copy().keys():
        # å¼¹å‡ºé”®å¯¹åº”çš„å€¼
        val = orig_state_dict.pop(key)

        # å¦‚æœé”®ä¸­åŒ…å«"qkv"
        if "qkv" in key:
            # æ‹†åˆ†é”®å
            key_split = key.split(".")
            # è·å–å±‚ç¼–å·
            layer_num = int(key_split[3])
            # è·å–éšè—å±‚å¤§å°
            dim = config.hidden_size
            # å¦‚æœé”®ä¸­åŒ…å«"weight"
            if "weight" in key:
                # æ›´æ–°æ–°é”®å€¼å¯¹åº”çš„å€¼
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.query.weight"
                ] = val[:dim, :]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.key.weight"
                ] = val[dim : dim * 2, :]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.value.weight"
                ] = val[-dim:, :]
            else:
                # æ›´æ–°æ–°é”®å€¼å¯¹åº”çš„å€¼
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.query.bias"
                ] = val[:dim]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.key.bias"
                ] = val[dim : dim * 2]
                orig_state_dict[
                    f"audio_spectrogram_transformer.encoder.layer.{layer_num}.attention.attention.value.bias"
                ] = val[-dim:]
        else:
            # æ›´æ–°æ–°é”®å€¼å¯¹åº”çš„å€¼
            orig_state_dict[rename_key(key)] = val

    # è¿”å›æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    return orig_state_dict


# ç§»é™¤æŒ‡å®šé”®çš„å€¼
def remove_keys(state_dict):
    # éœ€è¦å¿½ç•¥çš„é”®åˆ—è¡¨
    ignore_keys = [
        "module.v.head.weight",
        "module.v.head.bias",
        "module.v.head_dist.weight",
        "module.v.head_dist.bias",
    ]
    # éå†å¿½ç•¥çš„é”®åˆ—è¡¨
    for k in ignore_keys:
        # å¼¹å‡ºæŒ‡å®šé”®çš„å€¼
        state_dict.pop(k, None)


# æ— éœ€æ¢¯åº¦è®¡ç®—
@torch.no_grad()
def convert_audio_spectrogram_transformer_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=False):
    """
    Copy/paste/tweak model's weights to our Audio Spectrogram Transformer structure.
    """
    # è·å–éŸ³é¢‘é¢‘è°±å˜æ¢å™¨çš„é…ç½®
    config = get_audio_spectrogram_transformer_config(model_name)
    # å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œå°†æ¨¡å‹åç§°æ˜ å°„åˆ°å¯¹åº”çš„ä¸‹è½½é“¾æ¥
    model_name_to_url = {
        "ast-finetuned-audioset-10-10-0.4593": (
            "https://www.dropbox.com/s/ca0b1v2nlxzyeb4/audioset_10_10_0.4593.pth?dl=1"
        ),
        "ast-finetuned-audioset-10-10-0.450": (
            "https://www.dropbox.com/s/1tv0hovue1bxupk/audioset_10_10_0.4495.pth?dl=1"
        ),
        "ast-finetuned-audioset-10-10-0.448": (
            "https://www.dropbox.com/s/6u5sikl4b9wo4u5/audioset_10_10_0.4483.pth?dl=1"
        ),
        "ast-finetuned-audioset-10-10-0.448-v2": (
            "https://www.dropbox.com/s/kt6i0v9fvfm1mbq/audioset_10_10_0.4475.pth?dl=1"
        ),
        "ast-finetuned-audioset-12-12-0.447": (
            "https://www.dropbox.com/s/snfhx3tizr4nuc8/audioset_12_12_0.4467.pth?dl=1"
        ),
        "ast-finetuned-audioset-14-14-0.443": (
            "https://www.dropbox.com/s/z18s6pemtnxm4k7/audioset_14_14_0.4431.pth?dl=1"
        ),
        "ast-finetuned-audioset-16-16-0.442": (
            "https://www.dropbox.com/s/mdsa4t1xmcimia6/audioset_16_16_0.4422.pth?dl=1"
        ),
        "ast-finetuned-speech-commands-v2": (
            "https://www.dropbox.com/s/q0tbqpwv44pquwy/speechcommands_10_10_0.9812.pth?dl=1"
        ),
    }

    # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
    checkpoint_url = model_name_to_url[model_name]
    # ä»æŒ‡å®š URL åŠ è½½ PyTorch æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå¹¶æŒ‡å®šæ˜ å°„åˆ° CPU
    state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location="cpu")
    # ç§»é™¤ä¸€äº›é”®
    remove_keys(state_dict)
    # é‡å‘½åä¸€äº›é”®
    new_state_dict = convert_state_dict(state_dict, config)

    # åŠ è½½ ğŸ¤— æ¨¡å‹
    model = ASTForAudioClassification(config)
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # åŠ è½½æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    model.load_state_dict(new_state_dict)

    # åœ¨è™šæ‹Ÿè¾“å…¥ä¸ŠéªŒè¯è¾“å‡º
    # æ¥æº: https://github.com/YuanGongND/ast/blob/79e873b8a54d0a3b330dd522584ff2b9926cd581/src/run.py#L62
    # å¦‚æœæ¨¡å‹åç§°ä¸­ä¸åŒ…å«"speech-commands"ï¼Œåˆ™è®¾ç½®å¹³å‡å€¼ä¸º-4.2677393ï¼Œå¦åˆ™è®¾ç½®ä¸º-6.845978
    mean = -4.2677393 if "speech-commands" not in model_name else -6.845978
    # å¦‚æœæ¨¡å‹åç§°ä¸­ä¸åŒ…å«"speech-commands"ï¼Œåˆ™è®¾ç½®æ ‡å‡†å·®ä¸º4.5689974ï¼Œå¦åˆ™è®¾ç½®ä¸º5.5654526
    std = 4.5689974 if "speech-commands" not in model_name else 5.5654526
    # å¦‚æœæ¨¡å‹åç§°ä¸­ä¸åŒ…å«"speech-commands"ï¼Œåˆ™è®¾ç½®æœ€å¤§é•¿åº¦ä¸º1024ï¼Œå¦åˆ™è®¾ç½®ä¸º128
    max_length = 1024 if "speech-commands" not in model_name else 128
    # åˆ›å»ºä¸€ä¸ª AST ç‰¹å¾æå–å™¨å¯¹è±¡ï¼Œè®¾ç½®å‡å€¼ã€æ ‡å‡†å·®å’Œæœ€å¤§é•¿åº¦
    feature_extractor = ASTFeatureExtractor(mean=mean, std=std, max_length=max_length)

    # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å«"speech-commands"ï¼Œåˆ™åŠ è½½ speech_commands æ•°æ®é›†çš„éªŒè¯é›†ï¼Œè·å–ç¬¬ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶çš„æ³¢å½¢
    if "speech-commands" in model_name:
        dataset = load_dataset("speech_commands", "v0.02", split="validation")
        waveform = dataset[0]["audio"]["array"]
    else:
        # å¦åˆ™ï¼Œä¸‹è½½ nielsr/audio-spectogram-transformer-checkpoint ä»“åº“ä¸­çš„æ ·æœ¬éŸ³é¢‘æ–‡ä»¶
        filepath = hf_hub_download(
            repo_id="nielsr/audio-spectogram-transformer-checkpoint",
            filename="sample_audio.flac",
            repo_type="dataset",
        )
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›éŸ³é¢‘æ•°æ®å’Œé‡‡æ ·ç‡ï¼Œå¹¶å°†æ•°æ®è½¬æ¢ä¸º NumPy æ•°ç»„
        waveform, _ = torchaudio.load(filepath)
        waveform = waveform.squeeze().numpy()

    # ä½¿ç”¨ç‰¹å¾æå–å™¨å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œè¿”å› PyTorch å¼ é‡æ ¼å¼çš„è¾“å…¥
    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors="pt")

    # å‰å‘ä¼ æ’­
    outputs = model(**inputs)
    # è·å–æ¨¡å‹è¾“å‡ºçš„ logits
    logits = outputs.logits

    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-audioset-10-10-0.4593"ï¼Œåˆ™è®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸º[-0.8760, -7.0042, -8.6602]
    if model_name == "ast-finetuned-audioset-10-10-0.4593":
        expected_slice = torch.tensor([-0.8760, -7.0042, -8.6602])
    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-audioset-10-10-0.450"ï¼Œåˆ™è®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸º[-1.1986, -7.0903, -8.2718]
    elif model_name == "ast-finetuned-audioset-10-10-0.450":
        expected_slice = torch.tensor([-1.1986, -7.0903, -8.2718])
    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-audioset-10-10-0.448"ï¼Œè®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸ºç»™å®šå¼ é‡
    elif model_name == "ast-finetuned-audioset-10-10-0.448":
        expected_slice = torch.tensor([-2.6128, -8.0080, -9.4344])
    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-audioset-10-10-0.448-v2"ï¼Œè®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸ºç»™å®šå¼ é‡
    elif model_name == "ast-finetuned-audioset-10-10-0.448-v2":
        expected_slice = torch.tensor([-1.5080, -7.4534, -8.8917])
    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-audioset-12-12-0.447"ï¼Œè®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸ºç»™å®šå¼ é‡
    elif model_name == "ast-finetuned-audioset-12-12-0.447":
        expected_slice = torch.tensor([-0.5050, -6.5833, -8.0843])
    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-audioset-14-14-0.443"ï¼Œè®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸ºç»™å®šå¼ é‡
    elif model_name == "ast-finetuned-audioset-14-14-0.443":
        expected_slice = torch.tensor([-0.3826, -7.0336, -8.2413])
    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-audioset-16-16-0.442"ï¼Œè®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸ºç»™å®šå¼ é‡
    elif model_name == "ast-finetuned-audioset-16-16-0.442":
        expected_slice = torch.tensor([-1.2113, -6.9101, -8.3470])
    # å¦‚æœæ¨¡å‹åç§°ä¸º"ast-finetuned-speech-commands-v2"ï¼Œè®¾ç½®é¢„æœŸåˆ‡ç‰‡ä¸ºç»™å®šå¼ é‡
    elif model_name == "ast-finetuned-speech-commands-v2":
        expected_slice = torch.tensor([6.1589, -8.0566, -8.7984])
    else:
        # å¦‚æœæ¨¡å‹åç§°æœªçŸ¥ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
        raise ValueError("Unknown model name")
    # å¦‚æœæ¨¡å‹è¾“å‡ºçš„å‰ä¸‰ä¸ªå€¼ä¸ä¸é¢„æœŸåˆ‡ç‰‡æ¥è¿‘ï¼ˆç»å¯¹è¯¯å·®å°äºç­‰äº1e-4ï¼‰ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯å¼‚å¸¸
    if not torch.allclose(logits[0, :3], expected_slice, atol=1e-4):
        raise ValueError("Logits don't match")
    # è¾“å‡ºæç¤ºä¿¡æ¯
    print("Looks ok!")

    # å¦‚æœ PyTorch æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ä¸ä¸ºç©ºï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if pytorch_dump_folder_path is not None:
        # åˆ›å»º PyTorch æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸åšä»»ä½•æ“ä½œ
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        # è¾“å‡ºä¿å­˜æ¨¡å‹çš„æç¤ºä¿¡æ¯
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # è¾“å‡ºä¿å­˜ç‰¹å¾æå–å™¨çš„æç¤ºä¿¡æ¯
        print(f"Saving feature extractor to {pytorch_dump_folder_path}")
        # å°†ç‰¹å¾æå–å™¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        feature_extractor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hubï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if push_to_hub:
        # è¾“å‡ºæ¨é€æ¨¡å‹å’Œç‰¹å¾æå–å™¨åˆ° Hub çš„æç¤ºä¿¡æ¯
        print("Pushing model and feature extractor to the hub...")
        # å°†æ¨¡å‹æ¨é€åˆ° Hub
        model.push_to_hub(f"MIT/{model_name}")
        # å°†ç‰¹å¾æå–å™¨æ¨é€åˆ° Hub
        feature_extractor.push_to_hub(f"MIT/{model_name}")
# å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€å‚æ•°
    parser.add_argument(
        "--model_name",  # æ¨¡å‹åç§°å‚æ•°
        default="ast-finetuned-audioset-10-10-0.4593",  # é»˜è®¤æ¨¡å‹åç§°
        type=str,  # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        help="Name of the Audio Spectrogram Transformer model you'd like to convert."  # å‚æ•°è¯´æ˜
    )
    parser.add_argument(
        "--pytorch_dump_folder_path",  # PyTorch æ¨¡å‹è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„å‚æ•°
        default=None,  # é»˜è®¤ä¸ºç©º
        type=str,  # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        help="Path to the output PyTorch model directory."  # å‚æ•°è¯´æ˜
    )
    parser.add_argument(
        "--push_to_hub",  # æ¨é€è‡³ğŸ¤— hub å‚æ•°
        action="store_true",  # è®¾ç½®ä¸º True æ—¶è§¦å‘è¯¥å‚æ•°
        help="Whether or not to push the converted model to the ğŸ¤— hub."  # å‚æ•°è¯´æ˜
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°å°†éŸ³é¢‘é¢‘è°±å˜æ¢å™¨æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
    convert_audio_spectrogram_transformer_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
```