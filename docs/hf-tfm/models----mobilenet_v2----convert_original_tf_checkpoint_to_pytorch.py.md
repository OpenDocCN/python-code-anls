# `.\models\mobilenet_v2\convert_original_tf_checkpoint_to_pytorch.py`

```py
# ä½¿ç”¨ UTF-8 ç¼–ç å£°æ˜æ–‡ä»¶ç¼–ç æ–¹å¼
# ç‰ˆæƒå£°æ˜åŠè®¸å¯ä¿¡æ¯ï¼Œä½¿ç”¨ Apache License 2.0
# å¯¼å…¥æ‰€éœ€æ¨¡å—å’Œåº“
import argparse  # å¯¼å…¥è§£æå‘½ä»¤è¡Œå‚æ•°çš„æ¨¡å—
import json  # å¯¼å…¥å¤„ç† JSON æ ¼å¼æ•°æ®çš„æ¨¡å—
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ“ä½œçš„æ¨¡å—
from pathlib import Path  # å¯¼å…¥å¤„ç†æ–‡ä»¶å’Œè·¯å¾„çš„æ¨¡å—

import requests  # å¯¼å…¥å¤„ç† HTTP è¯·æ±‚çš„æ¨¡å—
import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ åº“
from huggingface_hub import hf_hub_download  # ä» Hugging Face Hub ä¸‹è½½èµ„æº
from PIL import Image  # å¯¼å…¥ Python Imaging Library å¤„ç†å›¾åƒçš„æ¨¡å—

from transformers import (  # å¯¼å…¥ Hugging Face çš„ transformers åº“ä¸­çš„ç±»å’Œå‡½æ•°
    MobileNetV2Config,  # MobileNetV2 æ¨¡å‹çš„é…ç½®ç±»
    MobileNetV2ForImageClassification,  # ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡çš„ MobileNetV2 æ¨¡å‹
    MobileNetV2ForSemanticSegmentation,  # ç”¨äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„ MobileNetV2 æ¨¡å‹
    MobileNetV2ImageProcessor,  # å¤„ç† MobileNetV2 å›¾åƒçš„ç±»
    load_tf_weights_in_mobilenet_v2,  # åŠ è½½ TensorFlow æ¨¡å‹æƒé‡åˆ° MobileNetV2 çš„å‡½æ•°
)
from transformers.utils import logging  # å¯¼å…¥ transformers åº“çš„æ—¥å¿—æ¨¡å—

# è®¾ç½®æ—¥å¿—è¾“å‡ºçº§åˆ«ä¸ºä¿¡æ¯çº§åˆ«
logging.set_verbosity_info()
# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨å¯¹è±¡
logger = logging.get_logger(__name__)


def get_mobilenet_v2_config(model_name):
    # åˆ›å»º MobileNetV2 çš„é…ç½®å¯¹è±¡ï¼Œè®¾ç½®å±‚æ ‡å‡†åŒ–çš„ epsilon å€¼
    config = MobileNetV2Config(layer_norm_eps=0.001)

    # å¦‚æœæ¨¡å‹åç§°åŒ…å« "quant"ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯å¼‚å¸¸ï¼Œä¸æ”¯æŒé‡åŒ–æ¨¡å‹
    if "quant" in model_name:
        raise ValueError("Quantized models are not supported.")

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¨¡å‹åç§°ï¼Œæå–æ·±åº¦ä¹˜æ•°å’Œå›¾åƒå¤§å°ä¿¡æ¯
    matches = re.match(r"^.*mobilenet_v2_([^_]*)_([^_]*)$", model_name)
    if matches:
        config.depth_multiplier = float(matches[1])  # è®¾ç½®æ·±åº¦ä¹˜æ•°
        config.image_size = int(matches[2])  # è®¾ç½®å›¾åƒå¤§å°

    # å¦‚æœæ¨¡å‹åç§°ä»¥ "deeplabv3_" å¼€å¤´ï¼Œåˆ™é…ç½®é€‚ç”¨äº DeepLabV3 çš„ç‰¹å®šå‚æ•°
    if model_name.startswith("deeplabv3_"):
        config.output_stride = 8  # è®¾ç½®è¾“å‡ºæ­¥å¹…ä¸º 8
        config.num_labels = 21  # è®¾ç½®ç±»åˆ«æ•°é‡ä¸º 21
        filename = "pascal-voc-id2label.json"  # è®¾ç½®ç±»åˆ«æ˜ å°„æ–‡ä»¶å
    else:
        # å¯¹äºå…¶ä»– MobileNetV2 å˜ä½“ï¼Œé»˜è®¤è®¾ç½®ä¸ºé¢„æµ‹ 1001 ä¸ªç±»åˆ«ï¼ˆèƒŒæ™¯ + 1000 ç±»åˆ«ï¼‰
        config.num_labels = 1001  # è®¾ç½®ç±»åˆ«æ•°é‡ä¸º 1001
        filename = "imagenet-1k-id2label.json"  # è®¾ç½®ç±»åˆ«æ˜ å°„æ–‡ä»¶å

    # ä» Hugging Face Hub ä¸‹è½½ç±»åˆ«æ˜ å°„æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œå¹¶åŠ è½½ä¸ºå­—å…¸æ ¼å¼
    repo_id = "huggingface/label-files"
    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))

    # æ ¹æ®ç±»åˆ«æ•°é‡è°ƒæ•´ç±»åˆ«æ˜ å°„å­—å…¸
    if config.num_labels == 1001:
        id2label = {int(k) + 1: v for k, v in id2label.items()}
        id2label[0] = "background"  # å°†ç´¢å¼• 0 æ˜ å°„ä¸ºèƒŒæ™¯ç±»åˆ«
    else:
        id2label = {int(k): v for k, v in id2label.items()}

    # å°†ç±»åˆ«æ˜ å°„å­—å…¸è®¾ç½®åˆ°é…ç½®å¯¹è±¡ä¸­
    config.id2label = id2label
    config.label2id = {v: k for k, v in id2label.items()}  # åˆ›å»ºåå‘æ˜ å°„

    return config


# å‡†å¤‡ç”¨äºæµ‹è¯•çš„å›¾åƒæ•°æ®ï¼Œä» COCO æ•°æ®é›†ä¸­ä¸‹è½½ä¸€å¼ å¯çˆ±çŒ«å’ªçš„å›¾åƒ
# è¿”å› PIL.Image å¯¹è±¡
def prepare_img():
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    im = Image.open(requests.get(url, stream=True).raw)
    return im


# ä½¿ç”¨è£…é¥°å™¨æ ‡è®°ï¼Œå£°æ˜å‡½æ•°ä¸éœ€è¦è¿›è¡Œæ¢¯åº¦è®¡ç®—
@torch.no_grad()
def convert_movilevit_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub=False):
    """
    å°†æ¨¡å‹çš„æƒé‡å¤åˆ¶/ç²˜è´´/è°ƒæ•´åˆ°æˆ‘ä»¬çš„ MobileNetV2 ç»“æ„ä¸­ã€‚
    """
    # è·å– MobileNetV2 çš„é…ç½®å¯¹è±¡
    config = get_mobilenet_v2_config(model_name)

    # åŠ è½½ ğŸ¤— æ¨¡å‹
    if model_name.startswith("deeplabv3_"):
        model = MobileNetV2ForSemanticSegmentation(config).eval()  # åˆ›å»ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„ MobileNetV2 æ¨¡å‹å¯¹è±¡
    else:
        # å¦‚æœä¸æ˜¯ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª MobileNetV2ForImageClassification å®ä¾‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model = MobileNetV2ForImageClassification(config).eval()

    # ä» TensorFlow æ£€æŸ¥ç‚¹åŠ è½½æƒé‡åˆ° MobileNetV2 æ¨¡å‹
    load_tf_weights_in_mobilenet_v2(model, config, checkpoint_path)

    # ä½¿ç”¨ MobileNetV2ImageProcessor å‡†å¤‡å›¾åƒï¼Œè®¾ç½®è£å‰ªå¤§å°å’Œæœ€çŸ­è¾¹å¤§å°
    image_processor = MobileNetV2ImageProcessor(
        crop_size={"width": config.image_size, "height": config.image_size},
        size={"shortest_edge": config.image_size + 32},
    )
    # å‡†å¤‡å›¾åƒå¹¶ç¼–ç 
    encoding = image_processor(images=prepare_img(), return_tensors="pt")
    # å°†ç¼–ç åçš„å›¾åƒè¾“å…¥æ¨¡å‹å¾—åˆ°è¾“å‡º
    outputs = model(**encoding)
    # è·å–æ¨¡å‹è¾“å‡ºçš„ logits
    logits = outputs.logits

    # å¦‚æœæ¨¡å‹åç§°ä»¥ "deeplabv3_" å¼€å¤´
    if model_name.startswith("deeplabv3_"):
        # ç¡®ä¿ logits çš„å½¢çŠ¶ä¸º (1, 21, 65, 65)
        assert logits.shape == (1, 21, 65, 65)

        # å¦‚æœæ¨¡å‹åç§°ä¸º "deeplabv3_mobilenet_v2_1.0_513"
        if model_name == "deeplabv3_mobilenet_v2_1.0_513":
            # é¢„æœŸçš„ logits å€¼
            expected_logits = torch.tensor(
                [
                    [[17.5790, 17.7581, 18.3355], [18.3257, 18.4230, 18.8973], [18.6169, 18.8650, 19.2187]],
                    [[-2.1595, -2.0977, -2.3741], [-2.4226, -2.3028, -2.6835], [-2.7819, -2.5991, -2.7706]],
                    [[4.2058, 4.8317, 4.7638], [4.4136, 5.0361, 4.9383], [4.5028, 4.9644, 4.8734]],
                ]
            )

        else:
            # å¦‚æœæ¨¡å‹åç§°æœªçŸ¥ï¼ŒæŠ›å‡º ValueError å¼‚å¸¸
            raise ValueError(f"Unknown model name: {model_name}")

        # ç¡®ä¿ logits çš„å‰ 3x3 å­å¼ é‡ä¸é¢„æœŸå€¼éå¸¸æ¥è¿‘
        assert torch.allclose(logits[0, :3, :3, :3], expected_logits, atol=1e-4)
    else:
        # å¦‚æœæ¨¡å‹åç§°ä¸æ˜¯ä»¥ "deeplabv3_" å¼€å¤´ï¼Œç¡®ä¿ logits çš„å½¢çŠ¶ä¸º (1, 1001)
        assert logits.shape == (1, 1001)

        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©é¢„æœŸçš„ logits å€¼
        if model_name == "mobilenet_v2_1.4_224":
            expected_logits = torch.tensor([0.0181, -1.0015, 0.4688])
        elif model_name == "mobilenet_v2_1.0_224":
            expected_logits = torch.tensor([0.2445, -1.1993, 0.1905])
        elif model_name == "mobilenet_v2_0.75_160":
            expected_logits = torch.tensor([0.2482, 0.4136, 0.6669])
        elif model_name == "mobilenet_v2_0.35_96":
            expected_logits = torch.tensor([0.1451, -0.4624, 0.7192])
        else:
            expected_logits = None

        # å¦‚æœé¢„æœŸçš„ logits å€¼ä¸ä¸º Noneï¼Œåˆ™ç¡®ä¿ logits çš„å‰ 3 ä¸ªå€¼ä¸é¢„æœŸå€¼éå¸¸æ¥è¿‘
        if expected_logits is not None:
            assert torch.allclose(logits[0, :3], expected_logits, atol=1e-4)

    # ç¡®ä¿ PyTorch dump æ–‡ä»¶å¤¹è·¯å¾„å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
    print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
    # å°†æ¨¡å‹ä¿å­˜åˆ° PyTorch dump æ–‡ä»¶å¤¹è·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)
    print(f"Saving image processor to {pytorch_dump_folder_path}")
    # å°†å›¾åƒå¤„ç†å™¨ä¿å­˜åˆ° PyTorch dump æ–‡ä»¶å¤¹è·¯å¾„
    image_processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        print("Pushing to the hub...")
        # æ„å»º repo_idï¼Œå¹¶æ¨é€ image_processor å’Œ model åˆ° Hub
        repo_id = "google/" + model_name
        image_processor.push_to_hub(repo_id)
        model.push_to_hub(repo_id)
if __name__ == "__main__":
    # å¦‚æœè¿™ä¸ªè„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()

    # æ·»åŠ å¿…éœ€çš„å‚æ•°
    parser.add_argument(
        "--model_name",
        default="mobilenet_v2_1.0_224",
        type=str,
        help="Name of the MobileNetV2 model you'd like to convert. Should be in the form 'mobilenet_v2_<depth>_<size>'.",
    )

    # æ·»åŠ å¿…éœ€çš„å‚æ•°
    parser.add_argument(
        "--checkpoint_path",
        required=True,
        type=str,
        help="Path to the original TensorFlow checkpoint (.ckpt file)."
    )

    # æ·»åŠ å¿…éœ€çš„å‚æ•°
    parser.add_argument(
        "--pytorch_dump_folder_path",
        required=True,
        type=str,
        help="Path to the output PyTorch model directory."
    )

    # æ·»åŠ å¯é€‰çš„å‚æ•°
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•°æ¥æ‰§è¡Œ TensorFlow åˆ° PyTorch æ¨¡å‹çš„è½¬æ¢
    convert_movilevit_checkpoint(
        args.model_name, args.checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub
    )
```