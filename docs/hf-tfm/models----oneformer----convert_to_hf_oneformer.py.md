# `.\transformers\models\oneformer\convert_to_hf_oneformer.py`

```
# å®šä¹‰æ–‡ä»¶ç¼–ç æ ¼å¼
# ç‰ˆæƒå£°æ˜
#
# å¯¼å…¥æ¨¡å—
import os
import sys
from argparse import ArgumentParser
from dataclasses import dataclass
from pathlib import Path
from pprint import pformat
from typing import Any, Dict, Iterator, List, Set, Tuple

import requests
import torch
import torchvision.transforms as T
from PIL import Image
from torch import Tensor, nn


# å°è¯•å¯¼å…¥ detectron2 æ¡†æ¶ç›¸å…³æ¨¡å—ï¼Œå¦‚æœªå®‰è£…åˆ™å¿½ç•¥
try:
    from detectron2.checkpoint import DetectionCheckpointer
    from detectron2.config import get_cfg
    from detectron2.data import MetadataCatalog
    from detectron2.projects.deeplab import add_deeplab_config
except ImportError:
    pass

# å¯¼å…¥ä¸€äº›æ¥è‡ª Hugging Face çš„ transformer æ¨¡å—
from transformers import CLIPTokenizer, DinatConfig, SwinConfig
from transformers.models.oneformer.image_processing_oneformer import OneFormerImageProcessor
from transformers.models.oneformer.modeling_oneformer import (
    OneFormerConfig,
    OneFormerForUniversalSegmentation,
    OneFormerForUniversalSegmentationOutput,
    OneFormerModel,
    OneFormerModelOutput,
)
from transformers.models.oneformer.processing_oneformer import OneFormerProcessor
from transformers.utils import logging


# è®¾å®šçŠ¶æ€å­—å…¸çš„ç±»å‹
StateDict = Dict[str, Tensor]

# è®¾å®šæ—¥å¿—çº§åˆ«
logging.set_verbosity_info()
# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger()

# è®¾å®šéšæœºæ•°ç§å­
torch.manual_seed(0)


# å®šä¹‰ä¸€ä¸ªç±»ç”¨äºè·Ÿè¸ªçŠ¶æ€å­—å…¸
class TrackedStateDict:
    def __init__(self, to_track: Dict):
        """This class "tracks" a python dictionary by keeping track of which item is accessed.

        Args:
            to_track (Dict): The dictionary we wish to track
        """
        self.to_track = to_track
        self._seen: Set[str] = set()

    def __getitem__(self, key: str) -> Any:
        return self.to_track[key]

    def __setitem__(self, key: str, item: Any):
        self._seen.add(key)
        self.to_track[key] = item

    def diff(self) -> List[str]:
        """This method returns a set difference between the keys in the tracked state dict and the one we have access so far.
        This is an effective method to check if we have update all the keys

        Returns:
            List[str]: List of keys not yet updated
        """
        return set(self.to_track.keys()) - self._seen

    def copy(self) -> Dict:
        # proxy the call to the internal dictionary
        return self.to_track.copy()


# å‡†å¤‡ç”¨äºéªŒè¯ç»“æœçš„å›¾åƒ
def prepare_img():
    # å®šä¹‰ä¸€ä¸ªURLå˜é‡ï¼Œå­˜å‚¨å›¾ç‰‡çš„ç½‘ç»œåœ°å€
    url = "https://praeclarumjj3.github.io/files/coco.jpeg"
    # é€šè¿‡è¯·æ±‚è·å–å›¾ç‰‡æ•°æ®ï¼Œä½¿ç”¨æµæ¨¡å¼
    img_data = requests.get(url, stream=True).raw
    # é€šè¿‡è·å–çš„å›¾ç‰‡æ•°æ®åˆ›å»ºä¸€ä¸ªå›¾åƒå¯¹è±¡
    im = Image.open(img_data)
    # è¿”å›å›¾åƒå¯¹è±¡
    return im
from dataclasses import dataclass  # å¯¼å…¥dataclassè£…é¥°å™¨ç”¨äºåˆ›å»ºæ•°æ®ç±»
from detectron2.config import get_cfg  # å¯¼å…¥get_cfgå‡½æ•°ç”¨äºè·å–Detectron2é…ç½®
from detectron2.modeling import MetadataCatalog  # å¯¼å…¥MetadataCatalogç”¨äºè·å–å…ƒæ•°æ®
from typing import List, Tuple  # å¯¼å…¥ç±»å‹æç¤ºç”¨äºå‡½æ•°å‚æ•°å’Œè¿”å›å€¼çš„ç±»å‹å£°æ˜
import torch  # å¯¼å…¥torchç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹æ“ä½œ
from transformers import CLIPTokenizer  # å¯¼å…¥CLIPTokenizerç”¨äºå¤„ç†æ–‡æœ¬è¾“å…¥
from .oneformer_processor import OneFormerProcessor  # å¯¼å…¥OneFormerProcessorç±»
from .oneformer_image_processor import OneFormerImageProcessor  # å¯¼å…¥OneFormerImageProcessorç±»
from .utils import StateDict  # å¯¼å…¥StateDictç±»å‹æç¤ºç”¨äºæ¨¡å‹æƒé‡å­—å…¸æ“ä½œ

@dataclass
class Args:
    """æ¨¡æ‹Ÿä¸€ä¸ªéœ€è¦çš„å‘½ä»¤è¡Œå‚æ•°çš„æ•°æ®ç±»ï¼Œç”¨äºä¼ é€’é…ç½®æ–‡ä»¶è·¯å¾„"""

    config_file: str  # é…ç½®æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²

def setup_cfg(args: Args):
    # ä»æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°åŠ è½½é…ç½®
    cfg = get_cfg()  # è·å–Detectron2é…ç½®å¯¹è±¡
    # æ·»åŠ DeepLabç›¸å…³é…ç½®
    add_deeplab_config(cfg)
    # æ·»åŠ é€šç”¨é…ç½®
    add_common_config(cfg)
    # æ·»åŠ OneFormerç›¸å…³é…ç½®
    add_oneformer_config(cfg)
    # æ·»åŠ Swinç›¸å…³é…ç½®
    add_swin_config(cfg)
    # æ·»åŠ Dinatç›¸å…³é…ç½®
    add_dinat_config(cfg)
    cfg.merge_from_file(args.config_file)  # ä»æ–‡ä»¶ä¸­åŠ è½½é…ç½®è¦†ç›–é»˜è®¤é…ç½®
    cfg.freeze()  # å†»ç»“é…ç½®ï¼Œé˜²æ­¢æ„å¤–ä¿®æ”¹
    return cfg  # è¿”å›é…ç½®å¯¹è±¡

class OriginalOneFormerConfigToOursConverter:
    pass  # åŸå§‹é…ç½®åˆ°æˆ‘ä»¬é…ç½®çš„è½¬æ¢å™¨ç±»ï¼Œæš‚æ— å®ç°

class OriginalOneFormerConfigToProcessorConverter:
    def __call__(self, original_config: object, model_repo: str) -> OneFormerProcessor:
        # æå–åŸå§‹æ¨¡å‹çš„ç›¸å…³ä¿¡æ¯å’Œæ•°æ®é›†å…ƒæ•°æ®
        model = original_config.MODEL  # æå–æ¨¡å‹ä¿¡æ¯
        model_input = original_config.INPUT  # æå–æ¨¡å‹è¾“å…¥ä¿¡æ¯
        dataset_catalog = MetadataCatalog.get(original_config.DATASETS.TEST_PANOPTIC[0])  # è·å–æ•°æ®é›†å…ƒæ•°æ®

        # æ ¹æ®æ¨¡å‹æ‰€åœ¨çš„æ¨¡å‹åº“è®¾ç½®ç±»åˆ«ä¿¡æ¯æ–‡ä»¶
        if "ade20k" in model_repo:
            class_info_file = "ade20k_panoptic.json"
        elif "coco" in model_repo:
            class_info_file = "coco_panoptic.json"
        elif "cityscapes" in model_repo:
            class_info_file = "cityscapes_panoptic.json"
        else:
            raise ValueError("Invalid Dataset!")  # æŠ›å‡ºæ•°å€¼é”™è¯¯å¼‚å¸¸

        # åˆ›å»ºOneFormerå›¾åƒå¤„ç†å™¨å’ŒCLIPTokenizerå¯¹è±¡
        image_processor = OneFormerImageProcessor(
            image_mean=(torch.tensor(model.PIXEL_MEAN) / 255).tolist(),  # å›¾åƒå‡å€¼
            image_std=(torch.tensor(model.PIXEL_STD) / 255).tolist(),  # å›¾åƒæ ‡å‡†å·®
            size=model_input.MIN_SIZE_TEST,  # æœ€å°å›¾åƒå°ºå¯¸
            max_size=model_input.MAX_SIZE_TEST,  # æœ€å¤§å›¾åƒå°ºå¯¸
            num_labels=model.SEM_SEG_HEAD.NUM_CLASSES,  # åˆ†å‰²ç±»åˆ«æ•°
            ignore_index=dataset_catalog.ignore_label,  # å¿½ç•¥çš„ç´¢å¼•
            class_info_file=class_info_file,  # ç±»åˆ«ä¿¡æ¯æ–‡ä»¶
        )

        tokenizer = CLIPTokenizer.from_pretrained(model_repo)  # ä½¿ç”¨æ¨¡å‹åº“åˆ›å»ºCLIPTokenizerå¯¹è±¡

        return OneFormerProcessor(
            image_processor=image_processor,  # å›¾åƒå¤„ç†å™¨
            tokenizer=tokenizer,  # åˆ†è¯å™¨
            task_seq_length=original_config.INPUT.TASK_SEQ_LEN,  # ä»»åŠ¡åºåˆ—é•¿åº¦
            max_seq_length=original_config.INPUT.MAX_SEQ_LEN,  # æœ€å¤§åºåˆ—é•¿åº¦
        )

class OriginalOneFormerCheckpointToOursConverter:
    def __init__(self, original_model: nn.Module, config: OneFormerConfig):
        self.original_model = original_model  # åŸå§‹æ¨¡å‹
        self.config = config  # é…ç½®å¯¹è±¡

    def pop_all(self, renamed_keys: List[Tuple[str, str]], dst_state_dict: StateDict, src_state_dict: StateDict):
        # ä»æºçŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºæ‰€æœ‰æŒ‡å®šçš„é”®å€¼å¯¹åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸
        for src_key, dst_key in renamed_keys:
            dst_state_dict[dst_key] = src_state_dict.pop(src_key)  # å¼¹å‡ºå¹¶æ·»åŠ é”®å€¼å¯¹

    # Swin Backbone
    # Dinat Backbone
    # Backbone + Pixel Decoder
    # Transformer Decoder
    # æ›¿æ¢è§£ç å™¨ä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—çš„å‚æ•°
    def replace_keys_qkv_transformer_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸çš„å‰ç¼€
        dst_prefix: str = "transformer_module.decoder.layers"
        # æºçŠ¶æ€å­—å…¸çš„å‰ç¼€
        src_prefix: str = "sem_seg_head.predictor"
        # å¯¹è§£ç å™¨ä¸­æ¯ä¸€å±‚è¿›è¡Œè¿­ä»£
        for i in range(self.config.decoder_layers - 1):
            # è¯»å–è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®
            in_proj_weight = src_state_dict.pop(
                f"{src_prefix}.transformer_self_attention_layers.{i}.self_attn.in_proj_weight"
            )
            in_proj_bias = src_state_dict.pop(
                f"{src_prefix}.transformer_self_attention_layers.{i}.self_attn.in_proj_bias"
            )
            # æ¥ä¸‹æ¥ï¼ŒæŒ‰é¡ºåºå°†æŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
            # æŸ¥è¯¢æŠ•å½±å±‚çš„æƒé‡
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.q_proj.weight"] = in_proj_weight[:256, :]
            # æŸ¥è¯¢æŠ•å½±å±‚çš„åç½®
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.q_proj.bias"] = in_proj_bias[:256]
            # é”®æŠ•å½±å±‚çš„æƒé‡
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.k_proj.weight"] = in_proj_weight[256:512, :]
            # é”®æŠ•å½±å±‚çš„åç½®
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.k_proj.bias"] = in_proj_bias[256:512]
            # å€¼æŠ•å½±å±‚çš„æƒé‡
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.v_proj.weight"] = in_proj_weight[-256:, :]
            # å€¼æŠ•å½±å±‚çš„åç½®
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.v_proj.bias"] = in_proj_bias[-256:]

    # æ›¿æ¢ä»»åŠ¡MLPçš„å‚æ•°
    def replace_task_mlp(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸çš„å‰ç¼€
        dst_prefix: str = "task_encoder"
        # æºçŠ¶æ€å­—å…¸çš„å‰ç¼€
        src_prefix: str = "task_mlp"

        # é‡å‘½åæƒé‡å’Œåç½®çš„è¾…åŠ©å‡½æ•°
        def rename_keys_for_weight_bias(src_prefix: str, dst_prefix: str):
            return [
                (f"{src_prefix}.weight", f"{dst_prefix}.weight"),
                (f"{src_prefix}.bias", f"{dst_prefix}.bias"),
            ]

        renamed_keys = []

        # å¯¹ä»»åŠ¡MLPçš„æ¯ä¸€å±‚è¿›è¡Œè¿­ä»£
        for i in range(2):
            # å°†æƒé‡å’Œåç½®çš„é”®é‡å‘½åï¼Œå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
            renamed_keys.extend(
                rename_keys_for_weight_bias(f"{src_prefix}.layers.{i}", f"{dst_prefix}.task_mlp.layers.{i}.0")
            )

        # ä»æºçŠ¶æ€å­—å…¸ä¸­ç§»é™¤æ‰€æœ‰é‡å‘½åçš„é”®ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)

    # æ›¿æ¢æ–‡æœ¬æŠ•å½±å™¨çš„å‚æ•°
    def replace_text_projector(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸çš„å‰ç¼€
        dst_prefix: str = "text_mapper.text_projector"
        # æºçŠ¶æ€å­—å…¸çš„å‰ç¼€
        src_prefix: str = "text_projector"

        # é‡å‘½åæƒé‡å’Œåç½®çš„è¾…åŠ©å‡½æ•°
        def rename_keys_for_weight_bias(src_prefix: str, dst_prefix: str):
            return [
                (f"{src_prefix}.weight", f"{dst_prefix}.weight"),
                (f"{src_prefix}.bias", f"{dst_prefix}.bias"),
            ]

        renamed_keys = []

        # å¯¹æ–‡æœ¬æŠ•å½±å™¨çš„æ¯ä¸€å±‚è¿›è¡Œè¿­ä»£
        for i in range(self.config.text_encoder_config["text_encoder_proj_layers"]):
            # å°†æƒé‡å’Œåç½®çš„é”®é‡å‘½åï¼Œå¹¶æ·»åŠ åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
            renamed_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.layers.{i}", f"{dst_prefix}.{i}.0"))

        # ä»æºçŠ¶æ€å­—å…¸ä¸­ç§»é™¤æ‰€æœ‰é‡å‘½åçš„é”®ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
    # æ›¿æ¢æ–‡æœ¬æ˜ å°„å™¨çš„æƒé‡å’Œåç½®å‚æ•°ï¼Œå°†æºçŠ¶æ€å­—å…¸ä¸­çš„å‚æ•°æ›¿æ¢åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
    def replace_text_mapper(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # è®¾ç½®ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­çš„æƒé‡å’Œåç½®å‚æ•°çš„å‰ç¼€
        dst_prefix: str = "text_mapper.text_encoder"
        # è®¾ç½®æºçŠ¶æ€å­—å…¸ä¸­çš„æƒé‡å’Œåç½®å‚æ•°çš„å‰ç¼€
        src_prefix: str = "text_encoder"

        # è°ƒç”¨å†…éƒ¨å‡½æ•°æ›¿æ¢æ–‡æœ¬æŠ•å½±å™¨çš„æƒé‡å’Œåç½®å‚æ•°
        self.replace_text_projector(dst_state_dict, src_state_dict)

        # å®šä¹‰ç”¨äºæƒé‡å’Œåç½®å‚æ•°é‡å‘½åçš„å‡½æ•°ï¼Œé’ˆå¯¹æ³¨æ„åŠ›æœºåˆ¶
        def rename_keys_for_weight_bias(src_prefix: str, dst_prefix: str):
            return [
                (f"{src_prefix}.weight", f"{dst_prefix}.weight"),
                (f"{src_prefix}.bias", f"{dst_prefix}.bias"),
            ]

        # å®šä¹‰ç”¨äºæ³¨æ„åŠ›æœºåˆ¶çš„æƒé‡å’Œåç½®å‚æ•°é‡å‘½åçš„å‡½æ•°
        def rename_keys_for_attn(src_prefix: str, dst_prefix: str):
            # å®šä¹‰æ³¨æ„åŠ›æœºåˆ¶çš„å‚æ•°é”®åˆ—è¡¨
            attn_keys = [
                (f"{src_prefix}.in_proj_bias", f"{dst_prefix}.in_proj_bias"),
                (f"{src_prefix}.in_proj_weight", f"{dst_prefix}.in_proj_weight"),
            ]
            # å°†æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºæŠ•å½±å‚æ•°ä¹ŸåŠ å…¥åˆ°å‚æ•°é”®åˆ—è¡¨ä¸­
            attn_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.out_proj", f"{dst_prefix}.out_proj"))

            return attn_keys

        # å®šä¹‰ç”¨äºå±‚çš„æƒé‡å’Œåç½®å‚æ•°é‡å‘½åçš„å‡½æ•°
        def rename_keys_for_layer(src_prefix: str, dst_prefix: str):
            # å®šä¹‰æ®‹å·®å—çš„å‚æ•°é”®åˆ—è¡¨
            resblock_keys = []

            # å°†æ®‹å·®å—ä¸­çš„ MLP å±‚çš„å‚æ•°åŠ å…¥åˆ°å‚æ•°é”®åˆ—è¡¨ä¸­
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.mlp.c_fc", f"{dst_prefix}.mlp.fc1"))
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.mlp.c_proj", f"{dst_prefix}.mlp.fc2"))
            # å°†æ®‹å·®å—ä¸­çš„ Layer Normalization å±‚çš„å‚æ•°åŠ å…¥åˆ°å‚æ•°é”®åˆ—è¡¨ä¸­
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.ln_1", f"{dst_prefix}.layer_norm1"))
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.ln_2", f"{dst_prefix}.layer_norm2"))
            # å°†æ®‹å·®å—ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶çš„å‚æ•°åŠ å…¥åˆ°å‚æ•°é”®åˆ—è¡¨ä¸­
            resblock_keys.extend(rename_keys_for_attn(f"{src_prefix}.attn", f"{dst_prefix}.self_attn"))

            return resblock_keys

        # å®šä¹‰ç”¨äºæ‰€æœ‰å‚æ•°çš„é‡å‘½åé”®åˆ—è¡¨
        renamed_keys = [
            ("prompt_ctx.weight", "text_mapper.prompt_ctx.weight"),
        ]

        # å°†å…¶ä»–éœ€è¦é‡å‘½åçš„å‚æ•°é”®åŠ å…¥åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
        renamed_keys.extend(
            [
                (f"{src_prefix}.positional_embedding", f"{dst_prefix}.positional_embedding"),
                (f"{src_prefix}.token_embedding.weight", f"{dst_prefix}.token_embedding.weight"),
            ]
        )

        # å°†æœ€ç»ˆçš„ Layer Normalization å±‚çš„å‚æ•°é”®åŠ å…¥åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
        renamed_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.ln_final", f"{dst_prefix}.ln_final"))

        # éå†æ‰€æœ‰å±‚ï¼Œå¹¶å°†å„å±‚çš„å‚æ•°é”®åŠ å…¥åˆ°é‡å‘½åé”®åˆ—è¡¨ä¸­
        for i in range(self.config.text_encoder_config["text_encoder_num_layers"]):
            renamed_keys.extend(
                rename_keys_for_layer(
                    f"{src_prefix}.transformer.resblocks.{i}", f"{dst_prefix}.transformer.layers.{i}"
                )
            )

        # å°†æ‰€æœ‰éœ€è¦æ›¿æ¢çš„å‚æ•°ä»æºçŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºï¼Œå¹¶æ›´æ–°åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
    # å°†ä¼ å…¥çš„æ¨¡å‹å‚æ•°ä» OneFormerModel è½¬æ¢ä¸º OneFormerModelï¼Œå¹¶è¿”å›è½¬æ¢åçš„æ¨¡å‹å‚æ•°
    def convert(self, oneformer: OneFormerModel, is_swin: bool) -> OneFormerModel:
        # åˆ›å»ºç›®æ ‡æ¨¡å‹å‚æ•°çš„è·Ÿè¸ªçŠ¶æ€å­—å…¸ï¼Œåˆå§‹åŒ–ä¸ºä¼ å…¥æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸
        dst_state_dict = TrackedStateDict(oneformer.state_dict())
        # åˆ›å»ºåŸå§‹æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸ï¼Œä½¿ç”¨ self.original_model çš„çŠ¶æ€å­—å…¸åˆå§‹åŒ–
        src_state_dict = self.original_model.state_dict()

        # ç”¨åŸå§‹æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸æ›¿æ¢ç›®æ ‡æ¨¡å‹å‚æ•°çš„åƒç´ æ¨¡å—
        self.replace_pixel_module(dst_state_dict, src_state_dict, is_swin)
        # ç”¨åŸå§‹æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸æ›¿æ¢ç›®æ ‡æ¨¡å‹å‚æ•°çš„å˜æ¢å™¨æ¨¡å—
        self.replace_transformer_module(dst_state_dict, src_state_dict)
        # ç”¨åŸå§‹æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸æ›¿æ¢ç›®æ ‡æ¨¡å‹å‚æ•°çš„ä»»åŠ¡ MLP
        self.replace_task_mlp(dst_state_dict, src_state_dict)
        # å¦‚æœé…ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œç”¨åŸå§‹æ¨¡å‹å‚æ•°çš„çŠ¶æ€å­—å…¸æ›¿æ¢ç›®æ ‡æ¨¡å‹å‚æ•°çš„æ–‡æœ¬æ˜ å°„å™¨
        if self.config.is_training:
            self.replace_text_mapper(dst_state_dict, src_state_dict)

        # è®°å½•æœªæˆåŠŸæ›¿æ¢çš„é”®å’Œå€¼çš„å·®å¼‚
        logger.info(f"Missed keys are {pformat(dst_state_dict.diff())}")
        # è®°å½•æœªæˆåŠŸå¤åˆ¶çš„é”®
        logger.info(f"Not copied keys are {pformat(src_state_dict.keys())}")
        # è¾“å‡ºâ€œå®Œæˆâ€çš„ä¿¡æ¯
        logger.info("ğŸ™Œ Done")

        # åŠ è½½ç›®æ ‡æ¨¡å‹å‚æ•°åˆ°ä¼ å…¥çš„æ¨¡å‹å‚æ•°ä¸­
        oneformer.load_state_dict(dst_state_dict)

        # è¿”å›è½¬æ¢åçš„æ¨¡å‹å‚æ•°
        return oneformer

    # é™æ€æ–¹æ³•ï¼šä½¿ç”¨ç»™å®šçš„æ£€æŸ¥ç‚¹ç›®å½•å’Œé…ç½®æ–‡ä»¶ç›®å½•ï¼Œç”Ÿæˆæ£€æŸ¥ç‚¹æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶çš„è¿­ä»£å™¨
    @staticmethod
    def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[Tuple[object, Path, Path]]:
        # è·å–æ£€æŸ¥ç‚¹ç›®å½•ä¸­æ‰€æœ‰.pthæ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
        checkpoints: List[Path] = checkpoints_dir.glob("**/*.pth")

        # éå†æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        for checkpoint in checkpoints:
            # è¾“å‡ºâ€œè½¬æ¢â€ä¿¡æ¯å’Œå½“å‰æ£€æŸ¥ç‚¹æ–‡ä»¶çš„æ–‡ä»¶å
            logger.info(f"ğŸ’ª Converting {checkpoint.stem}")
            # æŸ¥æ‰¾ä¸å½“å‰æ£€æŸ¥ç‚¹æ–‡ä»¶ç›¸å…³è”çš„é…ç½®æ–‡ä»¶
            config: Path = config_dir / f"{checkpoint.stem}.yaml"

            # è¿”å›é…ç½®æ–‡ä»¶ã€æ£€æŸ¥ç‚¹æ–‡ä»¶çš„å…ƒç»„
            yield config, checkpoint
# å¯¹è¯­ä¹‰åˆ†å‰²çš„è¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œè°ƒæ•´è¾“å‡ºå°ºå¯¸ä¸ºæŒ‡å®šçš„å¤§å°
def post_process_sem_seg_output(outputs: OneFormerForUniversalSegmentationOutput, target_size: Tuple[int, int]):
    # è·å–ç±»åˆ«æŸ¥è¯¢çš„é€»è¾‘å€¼ï¼Œå½¢çŠ¶ä¸º[BATCH, QUERIES, CLASSES + 1]
    class_queries_logits = outputs.class_queries_logits
    # è·å–æ©ç æŸ¥è¯¢çš„é€»è¾‘å€¼ï¼Œå½¢çŠ¶ä¸º[BATCH, QUERIES, HEIGHT, WIDTH]
    masks_queries_logits = outputs.masks_queries_logits
    if target_size is not None:
        # è‹¥æŒ‡å®šäº†ç›®æ ‡å°ºå¯¸ï¼Œåˆ™å¯¹æ©ç æŸ¥è¯¢çš„é€»è¾‘å€¼è¿›è¡Œæ’å€¼è°ƒæ•´å°ºå¯¸
        masks_queries_logits = torch.nn.functional.interpolate(
            masks_queries_logits,
            size=target_size,
            mode="bilinear",
            align_corners=False,
        )
    # åˆ é™¤æœ€åä¸€ä¸ªç©ºç±»[..., :-1]
    masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]
    # æ©ç æ¦‚ç‡çš„å½¢çŠ¶ä¸º[BATCH, QUERIES, HEIGHT, WIDTH]
    masks_probs = masks_queries_logits.sigmoid()
    # ç°åœ¨æˆ‘ä»¬å¸Œæœ›å¯¹æŸ¥è¯¢æ±‚å’Œï¼Œ
    # $ out_{c,h,w} =  \sum_q p_{q,c} * m_{q,h,w} $
    # å…¶ä¸­ $ softmax(p) \in R^{q, c} $ æ˜¯æ©ç ç±»åˆ«
    # è€Œ $ sigmoid(m) \in R^{q, h, w}$ æ˜¯æ©ç æ¦‚ç‡
    # b(atch)q(uery)c(lasses), b(atch)q(uery)h(eight)w(idth)
    segmentation = torch.einsum("bqc, bqhw -> bchw", masks_classes, masks_probs)

    return segmentation

# æµ‹è¯•å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹
def test(
    original_model,
    our_model: OneFormerForUniversalSegmentation,
    processor: OneFormerProcessor,
    model_repo: str,
):
    # æ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤„ç†æ–‡æœ¬åˆ—è¡¨å¹¶è¿”å›tokenè¾“å…¥
    def _preprocess_text(text_list=None, max_length=77):
        if text_list is None:
            raise ValueError("tokens cannot be None.")
        
        # å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡Œå¤„ç†ï¼Œè·å–token
        tokens = tokenizer(text_list, padding="max_length", max_length=max_length, truncation=True)

        attention_masks, input_ids = tokens["attention_mask"], tokens["input_ids"]

        token_inputs = []
        for attn_mask, input_id in zip(attention_masks, input_ids):
            # ç”Ÿæˆtokenè¾“å…¥
            token = torch.tensor(attn_mask) * torch.tensor(input_id)
            token_inputs.append(token.unsqueeze(0))

        token_inputs = torch.cat(token_inputs, dim=0)
        return token_inputs
    # ä½¿ç”¨ PyTorch çš„ no_grad() æ¨¡å¼è¿›è¡Œæ¨ç†
    with torch.no_grad():
        # åŠ è½½ CLIP åˆ†è¯å™¨
        tokenizer = CLIPTokenizer.from_pretrained(model_repo)
        # è®¾ç½®åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        original_model = original_model.eval()
        our_model = our_model.eval()
    
        # å‡†å¤‡å›¾åƒ
        im = prepare_img()
    
        # å®šä¹‰å›¾åƒé¢„å¤„ç†è½¬æ¢å™¨
        tr = T.Compose(
            [
                # è°ƒæ•´å›¾åƒå¤§å°ä¸º 640x640
                T.Resize((640, 640)),
                # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
                T.ToTensor(),
                # å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–å¤„ç†
                T.Normalize(
                    mean=torch.tensor([123.675, 116.280, 103.530]) / 255.0,
                    std=torch.tensor([58.395, 57.120, 57.375]) / 255.0,
                ),
            ]
        )
    
        # å¯¹å›¾åƒåº”ç”¨é¢„å¤„ç†è½¬æ¢å™¨
        x = tr(im).unsqueeze(0)
    
        # å®šä¹‰ä»»åŠ¡è¾“å…¥
        task_input = ["the task is semantic"]
        # å¯¹ä»»åŠ¡è¾“å…¥è¿›è¡Œé¢„å¤„ç†
        task_token = _preprocess_text(task_input, max_length=processor.task_seq_length)
    
        # è·å–åŸå§‹æ¨¡å‹çš„ backbone ç‰¹å¾
        original_model_backbone_features = original_model.backbone(x.clone())
    
        # è·å–æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹çš„è¾“å‡º
        our_model_output: OneFormerModelOutput = our_model.model(x.clone(), task_token, output_hidden_states=True)
    
        # æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹çš„ backbone ç‰¹å¾æ˜¯å¦ç›¸åŒ
        for original_model_feature, our_model_feature in zip(
            original_model_backbone_features.values(), our_model_output.encoder_hidden_states
        ):
            assert torch.allclose(
                original_model_feature, our_model_feature, atol=3e-3
            ), "The backbone features are not the same."
    
        # ä»åŸå§‹æ¨¡å‹ä¸­è·å– pixel decoder ç‰¹å¾
        mask_features, _, multi_scale_features, _, _ = original_model.sem_seg_head.pixel_decoder.forward_features(
            original_model_backbone_features
        )
        original_pixel_decoder_features = []
        original_pixel_decoder_features.append(mask_features)
        for i in range(len(multi_scale_features)):
            original_pixel_decoder_features.append(multi_scale_features[i])
    
        # æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹çš„ pixel decoder ç‰¹å¾æ˜¯å¦ç›¸åŒ
        for original_model_feature, our_model_feature in zip(
            original_pixel_decoder_features, our_model_output.pixel_decoder_hidden_states
        ):
            assert torch.allclose(
                original_model_feature, our_model_feature, atol=3e-4
            ), "The pixel decoder feature are not the same"
    
        # å®šä¹‰å®Œæ•´çš„å›¾åƒé¢„å¤„ç†è½¬æ¢å™¨
        tr_complete = T.Compose(
            [
                # è°ƒæ•´å›¾åƒå¤§å°ä¸º 640x640
                T.Resize((640, 640)),
                # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
                T.ToTensor(),
            ]
        )
    
        # å¯¹å›¾åƒåº”ç”¨å®Œæ•´çš„é¢„å¤„ç†è½¬æ¢å™¨
        y = (tr_complete(im) * 255.0).to(torch.int).float()
    
        # ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²
        original_model_out = original_model([{"image": y.clone(), "task": "The task is semantic"}])
        original_segmentation = original_model_out[0]["sem_seg"]
    
        # ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²
        our_model_out: OneFormerForUniversalSegmentationOutput = our_model(
            x.clone(), task_token, output_hidden_states=True
        )
        our_segmentation = post_process_sem_seg_output(our_model_out, target_size=(640, 640))[0]
    
        # æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²ç»“æœæ˜¯å¦ç›¸åŒ
        assert torch.allclose(
            original_segmentation, our_segmentation, atol=1e-3
        ), "The segmentation image is not the same."
    
        # æ‰“å°æµ‹è¯•é€šè¿‡çš„æ¶ˆæ¯
        logger.info("âœ… Test passed!")
# æ ¹æ®æ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„è·å–æ¨¡å‹çš„åç§°
def get_name(checkpoint_file: Path):
    # ä»æ£€æŸ¥ç‚¹æ–‡ä»¶åä¸­è·å–æ¨¡å‹åŸå§‹åç§°
    model_name_raw: str = checkpoint_file.stem

    # åˆ¤æ–­æ¨¡å‹ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œç±»å‹æ˜¯ Swin æˆ–è€… Dino
    backbone = "swin" if "swin" in model_name_raw else "dinat"
    
    dataset = ""
    # æ ¹æ®æ¨¡å‹ååŒ…å«çš„å…³é”®è¯ç¡®å®šæ•°æ®é›†ç±»å‹
    if "coco" in model_name_raw:
        dataset = "coco"
    elif "ade20k" in model_name_raw:
        dataset = "ade20k"
    elif "cityscapes" in model_name_raw:
        dataset = "cityscapes"
    else:
        raise ValueError(
            f"{model_name_raw} must be wrong since we didn't find 'coco' or 'ade20k' or 'cityscapes' in it "
        )

    # å®šä¹‰å¯èƒ½çš„éª¨å¹²ç½‘ç»œç±»å‹
    backbone_types = ["tiny", "large"]

    # æ ¹æ®æ¨¡å‹åä¸­çš„å…³é”®è¯ç¡®å®šå…·ä½“çš„éª¨å¹²ç½‘ç»œç±»å‹
    backbone_type = list(filter(lambda x: x in model_name_raw, backbone_types))[0]

    # æ ¹æ®è·å¾—çš„ä¿¡æ¯ç»„åˆæ¨¡å‹åç§°
    model_name = f"oneformer_{dataset}_{backbone}_{backbone_type}"

    return model_name

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = ArgumentParser(
        description=(
            "Command line to convert the original oneformer models (with swin backbone) to transformers"
            " implementation."
        )
    )

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument(
        "--checkpoints_dir",
        type=Path,
        help=(
            "A directory containing the model's checkpoints. The directory has to have the following structure:"
            " structure: <DIR_NAME>/<DATASET_NAME>/<CONFIG_NAME>.pth; where <CONFIG_NAME> name must follow the"
            " following nomenclature nomenclature: oneformer_<DATASET_NAME>_<BACKBONE>_<BACKBONE_TYPE>"
        ),
    )
    parser.add_argument(
        "--configs_dir",
        type=Path,
        help=(
            "A directory containing the model's configs, see detectron2 doc. The directory has to have the following"
            " structure: <DIR_NAME>/<DATASET_NAME>/<CONFIG_NAME>.yaml; where <CONFIG_NAME> name must follow the"
            " following nomenclature nomenclature: oneformer_<DATASET_NAME>_<BACKBONE>_<BACKBONE_TYPE>"
        ),
    )
    parser.add_argument(
        "--pytorch_dump_folder_path",
        required=True,
        type=Path,
        help="Path to the folder to output PyTorch models.",
    )
    parser.add_argument(
        "--oneformer_dir",
        required=True,
        type=Path,
        help=(
            "A path to OneFormer's original implementation directory. You can download from here: "
            "https://github.com/SHI-Labs/OneFormer"
        ),
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è·å–å„ä¸ªå‚æ•°çš„å€¼
    checkpoints_dir: Path = args.checkpoints_dir
    config_dir: Path = args.configs_dir
    save_directory: Path = args.pytorch_dump_folder_path
    oneformer_dir: Path = args.oneformer_dir

    # å¦‚æœè¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
    if not save_directory.exists():
        save_directory.mkdir(parents=True)
    # éå† OriginalOneFormerCheckpointToOursConverter ç±»çš„ using_dirs æ–¹æ³•è¿”å›çš„æ¯å¯¹é…ç½®æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹æ–‡ä»¶
    for config_file, checkpoint_file in OriginalOneFormerCheckpointToOursConverter.using_dirs(
        checkpoints_dir, config_dir
    ):
        # åˆ›å»º OriginalOneFormerConfigToProcessorConverter å®ä¾‹ï¼Œæ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºå¤„ç†å™¨
        processor = OriginalOneFormerConfigToProcessorConverter()(
            setup_cfg(Args(config_file=config_file)), os.path.join("shi-labs", config_file.stem)
        )

        # æ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºåŸå§‹é…ç½®
        original_config = setup_cfg(Args(config_file=config_file))
        
        # ä»åŸå§‹é…ç½®ä¸­åˆ›å»º OneFormer æ¨¡å‹çš„å‚æ•°
        oneformer_kwargs = OriginalOneFormer.from_config(original_config)

        # åˆ›å»º OriginalOneFormer æ¨¡å‹çš„å®ä¾‹ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        original_model = OriginalOneFormer(**oneformer_kwargs).eval()

        # åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ° OriginalOneFormer æ¨¡å‹ä¸­
        DetectionCheckpointer(original_model).load(str(checkpoint_file))

        # æ£€æŸ¥ config_file.stem æ˜¯å¦åŒ…å« "swin"ï¼Œå¹¶èµ‹å€¼ç»™ is_swin
        is_swin = "swin" in config_file.stem

        # ä½¿ç”¨ OriginalOneFormerConfigToOursConverter å°†åŸå§‹é…ç½®è½¬æ¢ä¸ºæˆ‘ä»¬çš„é…ç½®
        config: OneFormerConfig = OriginalOneFormerConfigToOursConverter()(original_config, is_swin)

        # åˆ›å»º OneFormerModel æ¨¡å‹çš„å®ä¾‹ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        oneformer = OneFormerModel(config=config).eval()

        # åˆ›å»º OriginalOneFormerCheckpointToOursConverter çš„å®ä¾‹ï¼ŒåŒæ—¶ä¼ å…¥åŸå§‹æ¨¡å‹å’Œé…ç½®
        converter = OriginalOneFormerCheckpointToOursConverter(original_model, config)

        # å°†åŸå§‹æ¨¡å‹è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ¨¡å‹
        oneformer = converter.convert(oneformer, is_swin)

        # åˆ›å»º OneFormerForUniversalSegmentation æ¨¡å‹çš„å®ä¾‹ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        oneformer_for_universal_segmentation = OneFormerForUniversalSegmentation(config=config).eval()

        # è®¾ç½® OneFormerForUniversalSegmentation çš„æ¨¡å‹ä¸ºè½¬æ¢åçš„ OneFormer æ¨¡å‹
        oneformer_for_universal_segmentation.model = oneformer

        # æµ‹è¯• OriginalOneFormer å’Œè½¬æ¢åçš„æ¨¡å‹
        test(
            original_model,
            oneformer_for_universal_segmentation,
            processor,
            os.path.join("shi-labs", config_file.stem),
        )

        # è·å–æ¨¡å‹åç§°
        model_name = get_name(checkpoint_file)
        logger.info(f"ğŸª„ Saving {model_name}")

        # ä¿å­˜å¤„ç†å™¨é¢„è®­ç»ƒæ¨¡å‹åˆ°æŒ‡å®šç›®å½•ä¸‹
        processor.save_pretrained(save_directory / model_name)
        # ä¿å­˜ OneFormerForUniversalSegmentation æ¨¡å‹åˆ°æŒ‡å®šç›®å½•ä¸‹
        oneformer_for_universal_segmentation.save_pretrained(save_directory / model_name)

        # æ¨é€å¤„ç†å™¨åˆ°æŒ‡å®šä»“åº“
        processor.push_to_hub(
            repo_id=os.path.join("shi-labs", config_file.stem),
            commit_message="Add configs",
            use_temp_dir=True,
        )
        # æ¨é€ OneFormerForUniversalSegmentation æ¨¡å‹åˆ°æŒ‡å®šä»“åº“
        oneformer_for_universal_segmentation.push_to_hub(
            repo_id=os.path.join("shi-labs", config_file.stem),
            commit_message="Add model",
            use_temp_dir=True,
        )
```