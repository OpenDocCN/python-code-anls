# `.\models\oneformer\convert_to_hf_oneformer.py`

```py
# coding=utf-8
# å£°æ˜æ–‡ä»¶ç¼–ç æ ¼å¼ä¸º UTF-8

# ç‰ˆæƒå£°æ˜å’Œè®¸å¯è¯ä¿¡æ¯
# Copyright 2022 SHI Labs and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Convert OneFormer checkpoints from the original repository. URL: https://github.com/SHI-Labs/OneFormer"""
# æ–‡ä»¶æè¿°ï¼šä»åŸå§‹å­˜å‚¨åº“è½¬æ¢ OneFormer æ£€æŸ¥ç‚¹çš„åŠŸèƒ½

import os
import sys
from argparse import ArgumentParser
from dataclasses import dataclass
from pathlib import Path
from pprint import pformat
from typing import Any, Dict, Iterator, List, Set, Tuple

import requests
import torch
import torchvision.transforms as T
from PIL import Image
from torch import Tensor, nn

# å°è¯•å¯¼å…¥ä¾èµ–åº“ï¼ˆdetectron2ï¼‰ï¼Œå¦‚æœå¯¼å…¥å¤±è´¥åˆ™å¿½ç•¥
try:
    from detectron2.checkpoint import DetectionCheckpointer
    from detectron2.config import get_cfg
    from detectron2.data import MetadataCatalog
    from detectron2.projects.deeplab import add_deeplab_config
except ImportError:
    pass

# å¯¼å…¥ OneFormer ç›¸å…³æ¨¡å—å’Œç±»
from transformers import CLIPTokenizer, DinatConfig, SwinConfig
from transformers.models.oneformer.image_processing_oneformer import OneFormerImageProcessor
from transformers.models.oneformer.modeling_oneformer import (
    OneFormerConfig,
    OneFormerForUniversalSegmentation,
    OneFormerForUniversalSegmentationOutput,
    OneFormerModel,
    OneFormerModelOutput,
)
from transformers.models.oneformer.processing_oneformer import OneFormerProcessor
from transformers.utils import logging

# å®šä¹‰ StateDict ç±»å‹åˆ«å
StateDict = Dict[str, Tensor]

# è®¾ç½®æ—¥å¿—çš„è¯¦ç»†ç¨‹åº¦ä¸ºä¿¡æ¯çº§åˆ«
logging.set_verbosity_info()
logger = logging.get_logger()

# è®¾å®šéšæœºæ•°ç§å­
torch.manual_seed(0)


class TrackedStateDict:
    def __init__(self, to_track: Dict):
        """This class "tracks" a python dictionary by keeping track of which item is accessed.

        Args:
            to_track (Dict): The dictionary we wish to track
        """
        self.to_track = to_track
        self._seen: Set[str] = set()

    def __getitem__(self, key: str) -> Any:
        return self.to_track[key]

    def __setitem__(self, key: str, item: Any):
        self._seen.add(key)
        self.to_track[key] = item

    def diff(self) -> List[str]:
        """This method returns a set difference between the keys in the tracked state dict and the one we have access so far.
        This is an effective method to check if we have update all the keys

        Returns:
            List[str]: List of keys not yet updated
        """
        return set(self.to_track.keys()) - self._seen

    def copy(self) -> Dict:
        # proxy the call to the internal dictionary
        return self.to_track.copy()


# å‡†å¤‡ç”¨äºéªŒè¯ç»“æœçš„å›¾åƒ
def prepare_img():
    # å®šä¹‰ä¸€ä¸ª URL å˜é‡ï¼ŒæŒ‡å‘å›¾åƒæ–‡ä»¶çš„ç½‘ç»œåœ°å€
    url = "https://praeclarumjj3.github.io/files/coco.jpeg"
    # ä½¿ç”¨ requests åº“å‘èµ· GET è¯·æ±‚è·å–å›¾åƒæ•°æ®ï¼Œè®¾ç½® stream=True ä»¥è·å–åŸå§‹å­—èŠ‚æµ
    img_data = requests.get(url, stream=True).raw
    # ä½¿ç”¨ Image.open() æ–¹æ³•æ‰“å¼€å›¾åƒæ•°æ®æµï¼Œè¿”å›ä¸€ä¸ªå›¾åƒå¯¹è±¡
    im = Image.open(img_data)
    # è¿”å›æ‰“å¼€çš„å›¾åƒå¯¹è±¡
    return im
# å®šä¹‰ä¸€ä¸ªæ•°æ®ç±»ï¼Œç”¨äºå­˜å‚¨æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ç­‰å‘½ä»¤è¡Œå‚æ•°
@dataclass
class Args:
    """Fake command line arguments needed by oneformer/detectron2 implementation"""

    config_file: str


# é…ç½®æ¨¡å‹çš„å‡½æ•°ï¼Œä»æŒ‡å®šçš„é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°åŠ è½½é…ç½®
def setup_cfg(args: Args):
    # è·å–ä¸€ä¸ªç©ºçš„é…ç½®å¯¹è±¡
    cfg = get_cfg()
    # æ·»åŠ  Deeplab é…ç½®åˆ°é…ç½®å¯¹è±¡
    add_deeplab_config(cfg)
    # æ·»åŠ é€šç”¨é…ç½®åˆ°é…ç½®å¯¹è±¡
    add_common_config(cfg)
    # æ·»åŠ  OneFormer ç‰¹å®šé…ç½®åˆ°é…ç½®å¯¹è±¡
    add_oneformer_config(cfg)
    # æ·»åŠ  Swin æ¨¡å‹é…ç½®åˆ°é…ç½®å¯¹è±¡
    add_swin_config(cfg)
    # æ·»åŠ  Dinat æ¨¡å‹é…ç½®åˆ°é…ç½®å¯¹è±¡
    add_dinat_config(cfg)
    # ä»æŒ‡å®šçš„é…ç½®æ–‡ä»¶ä¸­åˆå¹¶é…ç½®åˆ°é…ç½®å¯¹è±¡
    cfg.merge_from_file(args.config_file)
    # å†»ç»“é…ç½®ï¼Œé˜²æ­¢è¿›ä¸€æ­¥ä¿®æ”¹
    cfg.freeze()
    # è¿”å›é…ç½®å¯¹è±¡
    return cfg


# å°†åŸå§‹ OneFormer é…ç½®è½¬æ¢ä¸ºæˆ‘ä»¬è‡ªå·±çš„å¤„ç†å™¨é…ç½®çš„ç±»
class OriginalOneFormerConfigToOursConverter:

# å°†åŸå§‹ OneFormer é…ç½®è½¬æ¢ä¸ºå¤„ç†å™¨é…ç½®çš„ç±»
class OriginalOneFormerConfigToProcessorConverter:

    # å°†åŸå§‹é…ç½®å¯¹è±¡è½¬æ¢ä¸º OneFormerProcessor å®ä¾‹çš„è°ƒç”¨æ–¹æ³•
    def __call__(self, original_config: object, model_repo: str) -> OneFormerProcessor:
        # è·å–åŸå§‹æ¨¡å‹å’Œè¾“å…¥é…ç½®
        model = original_config.MODEL
        model_input = original_config.INPUT
        # è·å–å…ƒæ•°æ®ç›®å½•ä¸­æŒ‡å®šæµ‹è¯•æ•°æ®é›†çš„ä¿¡æ¯
        dataset_catalog = MetadataCatalog.get(original_config.DATASETS.TEST_PANOPTIC[0])

        # æ ¹æ®æ¨¡å‹ä»“åº“åç§°é€‰æ‹©ç±»åˆ«ä¿¡æ¯æ–‡ä»¶
        if "ade20k" in model_repo:
            class_info_file = "ade20k_panoptic.json"
        elif "coco" in model_repo:
            class_info_file = "coco_panoptic.json"
        elif "cityscapes" in model_repo:
            class_info_file = "cityscapes_panoptic.json"
        else:
            raise ValueError("Invalid Dataset!")

        # åˆ›å»º OneFormerImageProcessor å®ä¾‹ï¼Œè®¾ç½®å›¾åƒå¤„ç†å‚æ•°å’Œç±»åˆ«ä¿¡æ¯æ–‡ä»¶
        image_processor = OneFormerImageProcessor(
            image_mean=(torch.tensor(model.PIXEL_MEAN) / 255).tolist(),
            image_std=(torch.tensor(model.PIXEL_STD) / 255).tolist(),
            size=model_input.MIN_SIZE_TEST,
            max_size=model_input.MAX_SIZE_TEST,
            num_labels=model.SEM_SEG_HEAD.NUM_CLASSES,
            ignore_index=dataset_catalog.ignore_label,
            class_info_file=class_info_file,
        )

        # ä»æ¨¡å‹ä»“åº“åŠ è½½ CLIPTokenizer å®ä¾‹
        tokenizer = CLIPTokenizer.from_pretrained(model_repo)

        # è¿”å›ä¸€ä¸ª OneFormerProcessor å®ä¾‹ï¼ŒåŒ…å«å›¾åƒå¤„ç†å™¨ã€åˆ†è¯å™¨åŠç›¸å…³é…ç½®
        return OneFormerProcessor(
            image_processor=image_processor,
            tokenizer=tokenizer,
            task_seq_length=original_config.INPUT.TASK_SEQ_LEN,
            max_seq_length=original_config.INPUT.MAX_SEQ_LEN,
        )


# å°†åŸå§‹ OneFormer æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºæˆ‘ä»¬è‡ªå·±çš„æ£€æŸ¥ç‚¹è½¬æ¢å™¨çš„ç±»
class OriginalOneFormerCheckpointToOursConverter:

    # åˆå§‹åŒ–å‡½æ•°ï¼Œæ¥å—åŸå§‹æ¨¡å‹å’Œ OneFormer é…ç½®å¯¹è±¡ä½œä¸ºå‚æ•°
    def __init__(self, original_model: nn.Module, config: OneFormerConfig):
        self.original_model = original_model
        self.config = config

    # å¼¹å‡ºæ‰€æœ‰é‡å‘½åçš„é”®åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
    def pop_all(self, renamed_keys: List[Tuple[str, str]], dst_state_dict: StateDict, src_state_dict: StateDict):
        for src_key, dst_key in renamed_keys:
            dst_state_dict[dst_key] = src_state_dict.pop(src_key)

    # Swin Backbone
    # Dinat Backbone
    # Backbone + Pixel Decoder
    # Transformer Decoder
    def replace_keys_qkv_transformer_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸çš„é”®å‰ç¼€
        dst_prefix: str = "transformer_module.decoder.layers"
        # æºçŠ¶æ€å­—å…¸çš„é”®å‰ç¼€
        src_prefix: str = "sem_seg_head.predictor"
        
        # éå†æ¯ä¸ªè§£ç å™¨å±‚
        for i in range(self.config.decoder_layers - 1):
            # ä»æºçŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºè‡ªæ³¨æ„åŠ›å±‚çš„è¾“å…¥æŠ•å½±å±‚æƒé‡å’Œåç½®
            in_proj_weight = src_state_dict.pop(
                f"{src_prefix}.transformer_self_attention_layers.{i}.self_attn.in_proj_weight"
            )
            in_proj_bias = src_state_dict.pop(
                f"{src_prefix}.transformer_self_attention_layers.{i}.self_attn.in_proj_bias"
            )
            
            # å°†æŸ¥è¯¢ã€é”®å’Œå€¼ï¼ˆæŒ‰é¡ºåºï¼‰æ·»åŠ åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.q_proj.weight"] = in_proj_weight[:256, :]
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.q_proj.bias"] = in_proj_bias[:256]
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.k_proj.weight"] = in_proj_weight[256:512, :]
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.k_proj.bias"] = in_proj_bias[256:512]
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.v_proj.weight"] = in_proj_weight[-256:, :]
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.self_attn.v_proj.bias"] = in_proj_bias[-256:]

    def replace_task_mlp(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸çš„é”®å‰ç¼€
        dst_prefix: str = "task_encoder"
        # æºçŠ¶æ€å­—å…¸çš„é”®å‰ç¼€
        src_prefix: str = "task_mlp"

        # å®šä¹‰ç”¨äºé‡å‘½åæƒé‡å’Œåç½®çš„å‡½æ•°
        def rename_keys_for_weight_bias(src_prefix: str, dst_prefix: str):
            return [
                (f"{src_prefix}.weight", f"{dst_prefix}.weight"),
                (f"{src_prefix}.bias", f"{dst_prefix}.bias"),
            ]

        renamed_keys = []

        # éå†ä¸¤ä¸ªMLPå±‚
        for i in range(2):
            # æ‰©å±•é‡å‘½åé”®åˆ—è¡¨ï¼Œå°†æºçŠ¶æ€å­—å…¸ä¸­çš„å¯¹åº”é”®æ˜ å°„åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸
            renamed_keys.extend(
                rename_keys_for_weight_bias(f"{src_prefix}.layers.{i}", f"{dst_prefix}.task_mlp.layers.{i}.0")
            )

        # è°ƒç”¨æ–¹æ³•ï¼Œä»ä¸¤ä¸ªçŠ¶æ€å­—å…¸ä¸­ç§»é™¤æ‰€æœ‰é‡å‘½åçš„é”®
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)

    def replace_text_projector(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸çš„é”®å‰ç¼€
        dst_prefix: str = "text_mapper.text_projector"
        # æºçŠ¶æ€å­—å…¸çš„é”®å‰ç¼€
        src_prefix: str = "text_projector"

        # å®šä¹‰ç”¨äºé‡å‘½åæƒé‡å’Œåç½®çš„å‡½æ•°
        def rename_keys_for_weight_bias(src_prefix: str, dst_prefix: str):
            return [
                (f"{src_prefix}.weight", f"{dst_prefix}.weight"),
                (f"{src_prefix}.bias", f"{dst_prefix}.bias"),
            ]

        renamed_keys = []

        # æ ¹æ®æ–‡æœ¬ç¼–ç å™¨é…ç½®ä¸­çš„æŠ•å½±å±‚æ•°é‡ï¼Œé‡å‘½åæƒé‡å’Œåç½®çš„é”®
        for i in range(self.config.text_encoder_config["text_encoder_proj_layers"]):
            renamed_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.layers.{i}", f"{dst_prefix}.{i}.0"))

        # è°ƒç”¨æ–¹æ³•ï¼Œä»ä¸¤ä¸ªçŠ¶æ€å­—å…¸ä¸­ç§»é™¤æ‰€æœ‰é‡å‘½åçš„é”®
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°†æºçŠ¶æ€å­—å…¸ä¸­çš„æ–‡æœ¬ç¼–ç å™¨éƒ¨åˆ†æ˜ å°„åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
    def replace_text_mapper(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­æ–‡æœ¬ç¼–ç å™¨çš„å‰ç¼€
        dst_prefix: str = "text_mapper.text_encoder"
        # æºçŠ¶æ€å­—å…¸ä¸­æ–‡æœ¬ç¼–ç å™¨çš„å‰ç¼€
        src_prefix: str = "text_encoder"

        # è°ƒç”¨å†…éƒ¨æ–¹æ³•ï¼Œå°†æºçŠ¶æ€å­—å…¸ä¸­çš„æŠ•å½±å™¨æ˜ å°„åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
        self.replace_text_projector(dst_state_dict, src_state_dict)

        # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œç”¨äºé‡å‘½åæƒé‡å’Œåç½®çš„é”®
        def rename_keys_for_weight_bias(src_prefix: str, dst_prefix: str):
            return [
                (f"{src_prefix}.weight", f"{dst_prefix}.weight"),
                (f"{src_prefix}.bias", f"{dst_prefix}.bias"),
            ]

        # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œç”¨äºé‡å‘½åæ³¨æ„åŠ›æœºåˆ¶ç›¸å…³çš„é”®
        def rename_keys_for_attn(src_prefix: str, dst_prefix: str):
            # åˆå§‹åŒ–æ³¨æ„åŠ›æœºåˆ¶ç›¸å…³çš„é”®
            attn_keys = [
                (f"{src_prefix}.in_proj_bias", f"{dst_prefix}.in_proj_bias"),
                (f"{src_prefix}.in_proj_weight", f"{dst_prefix}.in_proj_weight"),
            ]
            # æ‰©å±•æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æƒé‡å’Œåç½®é”®
            attn_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.out_proj", f"{dst_prefix}.out_proj"))

            return attn_keys

        # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œç”¨äºé‡å‘½åå±‚çº§çš„é”®
        def rename_keys_for_layer(src_prefix: str, dst_prefix: str):
            # åˆå§‹åŒ–å±‚çº§çš„é”®åˆ—è¡¨
            resblock_keys = []

            # æ‰©å±•å±‚çº§é”®åˆ—è¡¨ï¼ŒåŒ…æ‹¬å¤šå±‚æ„ŸçŸ¥æœºçš„æƒé‡å’Œåç½®
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.mlp.c_fc", f"{dst_prefix}.mlp.fc1"))
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.mlp.c_proj", f"{dst_prefix}.mlp.fc2"))
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.ln_1", f"{dst_prefix}.layer_norm1"))
            resblock_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.ln_2", f"{dst_prefix}.layer_norm2"))
            resblock_keys.extend(rename_keys_for_attn(f"{src_prefix}.attn", f"{dst_prefix}.self_attn"))

            return resblock_keys

        # åˆå§‹åŒ–å·²é‡å‘½åçš„é”®åˆ—è¡¨ï¼Œç›´æ¥åŒ…å«ç‰¹å®šçš„é‡å‘½åé”®
        renamed_keys = [
            ("prompt_ctx.weight", "text_mapper.prompt_ctx.weight"),
        ]

        # æ‰©å±•å·²é‡å‘½åçš„é”®åˆ—è¡¨ï¼ŒåŒ…æ‹¬ä½ç½®åµŒå…¥å’Œä»¤ç‰ŒåµŒå…¥çš„æƒé‡
        renamed_keys.extend(
            [
                (f"{src_prefix}.positional_embedding", f"{dst_prefix}.positional_embedding"),
                (f"{src_prefix}.token_embedding.weight", f"{dst_prefix}.token_embedding.weight"),
            ]
        )

        # æ‰©å±•å·²é‡å‘½åçš„é”®åˆ—è¡¨ï¼ŒåŒ…æ‹¬æœ€ç»ˆå±‚çº§çš„å±‚å½’ä¸€åŒ–å’Œå‰ç¼€çš„æƒé‡å’Œåç½®
        renamed_keys.extend(rename_keys_for_weight_bias(f"{src_prefix}.ln_final", f"{dst_prefix}.ln_final"))

        # å¾ªç¯éå†æ–‡æœ¬ç¼–ç å™¨é…ç½®ä¸­çš„æ‰€æœ‰å±‚ï¼Œé‡å‘½åæ¯ä¸ªå±‚çº§çš„é”®
        for i in range(self.config.text_encoder_config["text_encoder_num_layers"]):
            renamed_keys.extend(
                rename_keys_for_layer(
                    f"{src_prefix}.transformer.resblocks.{i}", f"{dst_prefix}.transformer.layers.{i}"
                )
            )

        # è°ƒç”¨å¯¹è±¡æ–¹æ³•ï¼Œä»ç›®æ ‡çŠ¶æ€å­—å…¸å’ŒæºçŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºæ‰€æœ‰å·²é‡å‘½åçš„é”®
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
    # å°†ç»™å®šçš„æ¨¡å‹è½¬æ¢ä¸ºç‰¹å®šæ ¼å¼çš„æ¨¡å‹å¯¹è±¡
    def convert(self, oneformer: OneFormerModel, is_swin: bool) -> OneFormerModel:
        # åˆ›å»ºç›®æ ‡æ¨¡å‹çŠ¶æ€å­—å…¸çš„è·Ÿè¸ªå¯¹è±¡ï¼Œå¤åˆ¶è¾“å…¥æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        dst_state_dict = TrackedStateDict(oneformer.state_dict())
        # è·å–åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        src_state_dict = self.original_model.state_dict()

        # æ›¿æ¢ç›®æ ‡æ¨¡å‹çš„åƒç´ æ¨¡å—ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹çš„å¯¹åº”éƒ¨åˆ†
        self.replace_pixel_module(dst_state_dict, src_state_dict, is_swin)
        # æ›¿æ¢ç›®æ ‡æ¨¡å‹çš„å˜æ¢æ¨¡å—ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹çš„å¯¹åº”éƒ¨åˆ†
        self.replace_transformer_module(dst_state_dict, src_state_dict)
        # æ›¿æ¢ç›®æ ‡æ¨¡å‹çš„ä»»åŠ¡ MLPï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹çš„å¯¹åº”éƒ¨åˆ†
        self.replace_task_mlp(dst_state_dict, src_state_dict)
        
        # å¦‚æœé…ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œåˆ™æ›¿æ¢ç›®æ ‡æ¨¡å‹çš„æ–‡æœ¬æ˜ å°„å™¨ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹çš„å¯¹åº”éƒ¨åˆ†
        if self.config.is_training:
            self.replace_text_mapper(dst_state_dict, src_state_dict)

        # è®°å½•ç›®æ ‡æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­æœªå¤åˆ¶çš„é”®
        logger.info(f"Missed keys are {pformat(dst_state_dict.diff())}")
        # è®°å½•åŸå§‹æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­æœªè¢«å¤åˆ¶çš„é”®
        logger.info(f"Not copied keys are {pformat(src_state_dict.keys())}")
        # è¾“å‡ºå®Œæˆä¿¡æ¯
        logger.info("ğŸ™Œ Done")

        # å°†æ›´æ–°åçš„çŠ¶æ€å­—å…¸åŠ è½½åˆ°è¾“å…¥çš„æ¨¡å‹å¯¹è±¡ä¸­
        oneformer.load_state_dict(dst_state_dict)

        # è¿”å›æ›´æ–°åçš„æ¨¡å‹å¯¹è±¡
        return oneformer

    @staticmethod
    # ä½¿ç”¨æŒ‡å®šçš„ç›®å½•æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶ï¼Œè¿”å›è¿­ä»£å™¨
    def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[Tuple[object, Path, Path]]:
        # è·å–æ‰€æœ‰ä»¥ .pth ç»“å°¾çš„æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ—è¡¨
        checkpoints: List[Path] = checkpoints_dir.glob("**/*.pth")

        # éå†æ¯ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶
        for checkpoint in checkpoints:
            # è®°å½•æ­£åœ¨è½¬æ¢çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¿¡æ¯
            logger.info(f"ğŸ’ª Converting {checkpoint.stem}")
            # æŸ¥æ‰¾å…³è”çš„é…ç½®æ–‡ä»¶ï¼Œæ ¹æ®æ£€æŸ¥ç‚¹æ–‡ä»¶åç”Ÿæˆé…ç½®æ–‡ä»¶è·¯å¾„
            config: Path = config_dir / f"{checkpoint.stem}.yaml"

            # è¿”å›é…ç½®æ–‡ä»¶è·¯å¾„ã€æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„çš„è¿­ä»£å™¨
            yield config, checkpoint
# å¯¹è¯­ä¹‰åˆ†å‰²æ¨¡å‹è¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œå°†è¾“å‡ºè°ƒæ•´åˆ°æŒ‡å®šçš„ç›®æ ‡å¤§å°
def post_process_sem_seg_output(outputs: OneFormerForUniversalSegmentationOutput, target_size: Tuple[int, int]):
    # è·å–ç±»åˆ«æŸ¥è¯¢çš„é€»è¾‘å›å½’è¾“å‡ºï¼Œå½¢çŠ¶ä¸º [BATCH, QUERIES, CLASSES + 1]
    class_queries_logits = outputs.class_queries_logits
    # è·å–æ©ç æŸ¥è¯¢çš„é€»è¾‘å›å½’è¾“å‡ºï¼Œå½¢çŠ¶ä¸º [BATCH, QUERIES, HEIGHT, WIDTH]
    masks_queries_logits = outputs.masks_queries_logits
    if target_size is not None:
        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å¤§å°ï¼Œåˆ™é€šè¿‡åŒçº¿æ€§æ’å€¼è°ƒæ•´æ©ç æŸ¥è¯¢çš„é€»è¾‘å›å½’è¾“å‡ºå°ºå¯¸
        masks_queries_logits = torch.nn.functional.interpolate(
            masks_queries_logits,
            size=target_size,
            mode="bilinear",
            align_corners=False,
        )
    # å»é™¤æ‰ç©ºç±»åˆ« `[..., :-1]`ï¼Œå¾—åˆ°æ©ç ç±»åˆ«æ¦‚ç‡ï¼Œå½¢çŠ¶ä¸º [BATCH, QUERIES, CLASSES]
    masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]
    # å°†æ©ç æŸ¥è¯¢çš„é€»è¾‘å›å½’è¾“å‡ºé€šè¿‡ sigmoid å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå½¢çŠ¶ä¸º [BATCH, QUERIES, HEIGHT, WIDTH]
    masks_probs = masks_queries_logits.sigmoid()
    # ä½¿ç”¨ Einstein Summation è®¡ç®—è¯­ä¹‰åˆ†å‰²ç»“æœï¼Œå½¢çŠ¶ä¸º [BATCH, CLASSES, HEIGHT, WIDTH]
    # å…¶ä¸­ masks_classes æ˜¯æ©ç ç±»åˆ«æ¦‚ç‡ï¼Œmasks_probs æ˜¯æ©ç æ¦‚ç‡
    segmentation = torch.einsum("bqc, bqhw -> bchw", masks_classes, masks_probs)

    return segmentation


def test(
    original_model,
    our_model: OneFormerForUniversalSegmentation,
    processor: OneFormerProcessor,
    model_repo: str,
):
    # å†…éƒ¨å‡½æ•°ï¼Œç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥çš„å¼ é‡
    def _preprocess_text(text_list=None, max_length=77):
        if text_list is None:
            raise ValueError("tokens cannot be None.")

        # ä½¿ç”¨ tokenizer å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡Œç¼–ç å¤„ç†ï¼Œè¿›è¡Œå¡«å……å’Œæˆªæ–­ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥è¦æ±‚
        tokens = tokenizer(text_list, padding="max_length", max_length=max_length, truncation=True)

        attention_masks, input_ids = tokens["attention_mask"], tokens["input_ids"]

        token_inputs = []
        # éå†ç”Ÿæˆæ¯ä¸ªæ–‡æœ¬çš„å¼ é‡è¾“å…¥
        for attn_mask, input_id in zip(attention_masks, input_ids):
            token = torch.tensor(attn_mask) * torch.tensor(input_id)
            token_inputs.append(token.unsqueeze(0))

        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶æŒ‰ç¬¬ä¸€ç»´æ‹¼æ¥ï¼Œå½¢æˆæœ€ç»ˆçš„è¾“å…¥å¼ é‡
        token_inputs = torch.cat(token_inputs, dim=0)
        return token_inputs
    # ä½¿ç”¨ torch.no_grad() ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥åŠ å¿«æ¨ç†é€Ÿåº¦
    with torch.no_grad():
        # ä½¿ç”¨ CLIPTokenizer ä»é¢„è®­ç»ƒæ¨¡å‹åº“åŠ è½½ tokenizer
        tokenizer = CLIPTokenizer.from_pretrained(model_repo)
        # å°†åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        original_model = original_model.eval()
        our_model = our_model.eval()

        # å‡†å¤‡å›¾åƒæ•°æ®
        im = prepare_img()

        # å®šä¹‰å›¾åƒé¢„å¤„ç†çš„è½¬æ¢æ“ä½œåºåˆ—
        tr = T.Compose(
            [
                # è°ƒæ•´å›¾åƒå¤§å°ä¸º (640, 640)
                T.Resize((640, 640)),
                # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
                T.ToTensor(),
                # æ ‡å‡†åŒ–å›¾åƒå¼ é‡
                T.Normalize(
                    mean=torch.tensor([123.675, 116.280, 103.530]) / 255.0,
                    std=torch.tensor([58.395, 57.120, 57.375]) / 255.0,
                ),
            ],
        )

        # å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†å¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œä»¥ç¬¦åˆæ¨¡å‹çš„è¾“å…¥è¦æ±‚
        x = tr(im).unsqueeze(0)

        # å®šä¹‰ä»»åŠ¡çš„è¾“å…¥æ–‡æœ¬
        task_input = ["the task is semantic"]
        # å¯¹ä»»åŠ¡æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œç¡®ä¿é•¿åº¦ä¸è¶…è¿‡å¤„ç†å™¨çš„æœ€å¤§åºåˆ—é•¿åº¦
        task_token = _preprocess_text(task_input, max_length=processor.task_seq_length)

        # æå–åŸå§‹æ¨¡å‹çš„éª¨å¹²ç½‘ç»œç‰¹å¾
        original_model_backbone_features = original_model.backbone(x.clone())

        # ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¹¶è¦æ±‚è¾“å‡ºéšè—çŠ¶æ€
        our_model_output: OneFormerModelOutput = our_model.model(x.clone(), task_token, output_hidden_states=True)

        # æ£€æŸ¥åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹çš„éª¨å¹²ç‰¹å¾æ˜¯å¦ç›¸ä¼¼
        for original_model_feature, our_model_feature in zip(
            original_model_backbone_features.values(), our_model_output.encoder_hidden_states
        ):
            assert torch.allclose(
                original_model_feature, our_model_feature, atol=3e-3
            ), "The backbone features are not the same."

        # æå–åŸå§‹æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²å¤´éƒ¨è§£ç å™¨ç‰¹å¾
        mask_features, _, multi_scale_features, _, _ = original_model.sem_seg_head.pixel_decoder.forward_features(
            original_model_backbone_features
        )

        # æ”¶é›†æ‰€æœ‰çš„åŸå§‹åƒç´ è§£ç å™¨ç‰¹å¾
        original_pixel_decoder_features = []
        original_pixel_decoder_features.append(mask_features)
        for i in range(len(multi_scale_features)):
            original_pixel_decoder_features.append(multi_scale_features[i])

        # æ£€æŸ¥åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹çš„åƒç´ è§£ç å™¨ç‰¹å¾æ˜¯å¦ç›¸ä¼¼
        for original_model_feature, our_model_feature in zip(
            original_pixel_decoder_features, our_model_output.pixel_decoder_hidden_states
        ):
            assert torch.allclose(
                original_model_feature, our_model_feature, atol=3e-4
            ), "The pixel decoder feature are not the same"

        # å®šä¹‰å®Œæ•´çš„å›¾åƒè½¬æ¢æ“ä½œåºåˆ—
        tr_complete = T.Compose(
            [
                T.Resize((640, 640)),
                T.ToTensor(),
            ],
        )

        # å¯¹å›¾åƒè¿›è¡Œå®Œæ•´çš„é¢„å¤„ç†å¹¶è½¬æ¢ä¸ºæ•´å‹å¼ é‡
        y = (tr_complete(im) * 255.0).to(torch.int).float()

        # æµ‹è¯•å®Œæ•´æ¨¡å‹çš„è¾“å‡º
        original_model_out = original_model([{"image": y.clone(), "task": "The task is semantic"}])

        # æå–åŸå§‹æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²ç»“æœ
        original_segmentation = original_model_out[0]["sem_seg"]

        # ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¹¶å¯¹è¯­ä¹‰åˆ†å‰²ç»“æœè¿›è¡Œåå¤„ç†
        our_model_out: OneFormerForUniversalSegmentationOutput = our_model(
            x.clone(), task_token, output_hidden_states=True
        )

        our_segmentation = post_process_sem_seg_output(our_model_out, target_size=(640, 640))[0]

        # æ£€æŸ¥åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²ç»“æœæ˜¯å¦ç›¸ä¼¼
        assert torch.allclose(
            original_segmentation, our_segmentation, atol=1e-3
        ), "The segmentation image is not the same."

        # è®°å½•æµ‹è¯•é€šè¿‡çš„æ¶ˆæ¯
        logger.info("âœ… Test passed!")
def get_name(checkpoint_file: Path):
    # ä»æ–‡ä»¶è·¯å¾„ä¸­è·å–æ¨¡å‹åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
    model_name_raw: str = checkpoint_file.stem

    # æ ¹æ®æ¨¡å‹åç§°åˆ¤æ–­ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œï¼ˆbackboneï¼‰
    backbone = "swin" if "swin" in model_name_raw else "dinat"

    # åˆå§‹åŒ–æ•°æ®é›†åç§°ä¸ºç©ºå­—ç¬¦ä¸²
    dataset = ""
    
    # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šæ•°æ®é›†ç±»å‹
    if "coco" in model_name_raw:
        dataset = "coco"
    elif "ade20k" in model_name_raw:
        dataset = "ade20k"
    elif "cityscapes" in model_name_raw:
        dataset = "cityscapes"
    else:
        # å¦‚æœæ¨¡å‹åç§°ä¸åŒ…å«é¢„æœŸçš„æ•°æ®é›†ç±»å‹ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯å¼‚å¸¸
        raise ValueError(
            f"{model_name_raw} must be wrong since we didn't find 'coco' or 'ade20k' or 'cityscapes' in it "
        )

    # æ”¯æŒçš„éª¨å¹²ç½‘ç»œç±»å‹åˆ—è¡¨
    backbone_types = ["tiny", "large"]

    # ä½¿ç”¨è¿‡æ»¤å™¨æ‰¾åˆ°æ¨¡å‹åç§°ä¸­åŒ…å«çš„éª¨å¹²ç½‘ç»œç±»å‹
    backbone_type = list(filter(lambda x: x in model_name_raw, backbone_types))[0]

    # æ„å»ºæœ€ç»ˆçš„æ¨¡å‹åç§°
    model_name = f"oneformer_{dataset}_{backbone}_{backbone_type}"

    return model_name


if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨ï¼Œæè¿°ç”¨é€”æ˜¯è½¬æ¢åŸå§‹ OneFormer æ¨¡å‹ï¼ˆä½¿ç”¨ swin éª¨å¹²ç½‘ç»œï¼‰ä¸º Transformers å®ç°çš„å‘½ä»¤è¡Œå·¥å…·
    parser = ArgumentParser(
        description=(
            "Command line to convert the original oneformer models (with swin backbone) to transformers"
            " implementation."
        )
    )

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•çš„è·¯å¾„
    parser.add_argument(
        "--checkpoints_dir",
        type=Path,
        help=(
            "A directory containing the model's checkpoints. The directory has to have the following structure:"
            " <DIR_NAME>/<DATASET_NAME>/<CONFIG_NAME>.pth; where <CONFIG_NAME> name must follow the"
            " following nomenclature: oneformer_<DATASET_NAME>_<BACKBONE>_<BACKBONE_TYPE>"
        ),
    )
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ¨¡å‹é…ç½®æ–‡ä»¶ç›®å½•çš„è·¯å¾„
    parser.add_argument(
        "--configs_dir",
        type=Path,
        help=(
            "A directory containing the model's configs, see detectron2 doc. The directory has to have the following"
            " structure: <DIR_NAME>/<DATASET_NAME>/<CONFIG_NAME>.yaml; where <CONFIG_NAME> name must follow the"
            " following nomenclature: oneformer_<DATASET_NAME>_<BACKBONE>_<BACKBONE_TYPE>"
        ),
    )
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¿…éœ€å‚æ•°ï¼‰
    parser.add_argument(
        "--pytorch_dump_folder_path",
        required=True,
        type=Path,
        help="Path to the folder to output PyTorch models.",
    )
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šåŸå§‹ OneFormer å®ç°ç›®å½•çš„è·¯å¾„ï¼ˆå¿…éœ€å‚æ•°ï¼‰
    parser.add_argument(
        "--oneformer_dir",
        required=True,
        type=Path,
        help=(
            "A path to OneFormer's original implementation directory. You can download from here: "
            "https://github.com/SHI-Labs/OneFormer"
        ),
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # åˆå§‹åŒ–å„å‚æ•°ä¸ºå¯¹åº”çš„è·¯å¾„å¯¹è±¡
    checkpoints_dir: Path = args.checkpoints_dir
    config_dir: Path = args.configs_dir
    save_directory: Path = args.pytorch_dump_folder_path
    oneformer_dir: Path = args.oneformer_dir

    # å¦‚æœè¾“å‡ºè·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
    if not save_directory.exists():
        save_directory.mkdir(parents=True)
    # éå† OriginalOneFormerCheckpointToOursConverter ç±»çš„ using_dirs æ–¹æ³•è¿”å›çš„è¿­ä»£å™¨ï¼Œ
    # è¯¥æ–¹æ³•æ ¹æ®ç»™å®šçš„ checkpoints_dir å’Œ config_dir è¿”å›é…ç½®æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹æ–‡ä»¶çš„å…ƒç»„
    for config_file, checkpoint_file in OriginalOneFormerCheckpointToOursConverter.using_dirs(
        checkpoints_dir, config_dir
    ):
        # åˆ›å»º OriginalOneFormerConfigToProcessorConverter çš„å®ä¾‹ï¼Œå°†é…ç½®æ–‡ä»¶è½¬æ¢ä¸ºå¤„ç†å™¨å¯¹è±¡
        processor = OriginalOneFormerConfigToProcessorConverter()(
            setup_cfg(Args(config_file=config_file)), os.path.join("shi-labs", config_file.stem)
        )

        # æ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºåŸå§‹é…ç½®å¯¹è±¡
        original_config = setup_cfg(Args(config_file=config_file))

        # æ ¹æ®åŸå§‹é…ç½®å¯¹è±¡è·å– OneFormer æ¨¡å‹çš„å…³é”®å­—å‚æ•°
        oneformer_kwargs = OriginalOneFormer.from_config(original_config)

        # åˆ›å»ºåŸå§‹çš„ OneFormer æ¨¡å‹ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        original_model = OriginalOneFormer(**oneformer_kwargs).eval()

        # åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ°åŸå§‹æ¨¡å‹ä¸­
        DetectionCheckpointer(original_model).load(str(checkpoint_file))

        # æ£€æŸ¥ config_file.stem æ˜¯å¦åŒ…å« "swin"ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸º Swin æ¨¡å‹
        is_swin = "swin" in config_file.stem

        # ä½¿ç”¨ OriginalOneFormerConfigToOursConverter å°†åŸå§‹é…ç½®è½¬æ¢ä¸ºæˆ‘ä»¬çš„é…ç½®å¯¹è±¡
        config: OneFormerConfig = OriginalOneFormerConfigToOursConverter()(original_config, is_swin)

        # åˆ›å»º OneFormerModel å¯¹è±¡ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        oneformer = OneFormerModel(config=config).eval()

        # ä½¿ç”¨ OriginalOneFormerCheckpointToOursConverter å°†åŸå§‹æ¨¡å‹å’Œé…ç½®è½¬æ¢ä¸ºæˆ‘ä»¬çš„ OneFormer æ¨¡å‹
        converter = OriginalOneFormerCheckpointToOursConverter(original_model, config)
        oneformer = converter.convert(oneformer, is_swin)

        # åˆ›å»ºç”¨äºé€šç”¨åˆ†å‰²çš„ OneFormerForUniversalSegmentation å¯¹è±¡ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        oneformer_for_universal_segmentation = OneFormerForUniversalSegmentation(config=config).eval()

        # å°†è½¬æ¢åçš„ OneFormer æ¨¡å‹è®¾ç½®ä¸ºé€šç”¨åˆ†å‰²æ¨¡å‹çš„å±æ€§
        oneformer_for_universal_segmentation.model = oneformer

        # æ‰§è¡Œæµ‹è¯•å‡½æ•°ï¼Œæµ‹è¯•åŸå§‹æ¨¡å‹å’Œè½¬æ¢åçš„é€šç”¨åˆ†å‰²æ¨¡å‹åœ¨å¤„ç†å™¨å’Œè·¯å¾„ä¸‹çš„è¡¨ç°
        test(
            original_model,
            oneformer_for_universal_segmentation,
            processor,
            os.path.join("shi-labs", config_file.stem),
        )

        # è·å–æ¨¡å‹åç§°ï¼Œç”¨äºä¿å­˜å’Œæ—¥å¿—è®°å½•
        model_name = get_name(checkpoint_file)

        # è®°å½•ä¿¡æ¯ï¼Œè¡¨æ˜æ­£åœ¨ä¿å­˜æ¨¡å‹
        logger.info(f"ğŸª„ Saving {model_name}")

        # å°†å¤„ç†å™¨å’Œé€šç”¨åˆ†å‰²æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸‹
        processor.save_pretrained(save_directory / model_name)
        oneformer_for_universal_segmentation.save_pretrained(save_directory / model_name)

        # å°†å¤„ç†å™¨å’Œé€šç”¨åˆ†å‰²æ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ Hub ä»“åº“
        processor.push_to_hub(
            repo_id=os.path.join("shi-labs", config_file.stem),
            commit_message="Add configs",
            use_temp_dir=True,
        )
        oneformer_for_universal_segmentation.push_to_hub(
            repo_id=os.path.join("shi-labs", config_file.stem),
            commit_message="Add model",
            use_temp_dir=True,
        )
```