# `.\models\esm\convert_esm.py`

```
# coding=utf-8
# ç‰ˆæƒå£°æ˜
# ä½¿ç”¨ Apache License, Version 2.0 è¿›è¡Œè®¸å¯
# å¦‚æœè¦ä½¿ç”¨è¿™ä¸ªæ–‡ä»¶ï¼Œå¿…é¡»ç¬¦åˆè®¸å¯åè®®
# å¯ä»¥é€šè¿‡ http://www.apache.org/licenses/LICENSE-2.0 è·å–è®¸å¯åè®®çš„å‰¯æœ¬
#
# é™¤éé€‚ç”¨çš„æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæœ¬è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€æä¾›çš„ï¼Œ
# ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶
# å‚è§è®¸å¯åè®®è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯
"""è½¬æ¢ ESM æ£€æŸ¥ç‚¹ã€‚"""


import argparse
import pathlib
from pathlib import Path
from tempfile import TemporaryDirectory

import esm as esm_module
import torch
from esm.esmfold.v1.misc import batch_encode_sequences as esmfold_encode_sequences
from esm.esmfold.v1.pretrained import esmfold_v1

from transformers.models.esm.configuration_esm import EsmConfig, EsmFoldConfig
from transformers.models.esm.modeling_esm import (
    EsmForMaskedLM,
    EsmForSequenceClassification,
    EsmIntermediate,
    EsmLayer,
    EsmOutput,
    EsmSelfAttention,
    EsmSelfOutput,
)
from transformers.models.esm.modeling_esmfold import EsmForProteinFolding
from transformers.models.esm.tokenization_esm import EsmTokenizer
from transformers.utils import logging


logging.set_verbosity_info()
logger = logging.get_logger(__name__)

SAMPLE_DATA = [
    (
        "protein1",
        "MNGTEGPNFYVPFSNATGVVRSPFEYPQYYLAEPWQFSMLAAYMFLLIVLGFPINFLTLYVTVQHKKLRTPLNYILLNLAVADLFMVLGGFTSTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVCKPMSNFRFGENHAIMGVAFTWVMALACAAPPLAGWSRYIPEGLQCSCGIDYYTLKPEVNNESFVIYMFVVHFTIPMIIIFFCYGQLVFTVKEAAAQQQESATTQKAEKEVTRMVIIMVIAFLICWVPYASVAFYIFTHQGSNFGPIFMTIPAFFAKSAAIYNPVIYIMMNKQFRNCMLTTICCGKNPLGDDEASATVSKTETSQVAPA",
    ),
    ("protein2", "MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLA"),
    ("protein3", "MKTVRQERLKSI<mask>RILERSKEPVSGAQLAEELS<mask>SRQVIVQDIAYLRSLGYN<mask>VATPRGYVLAGG"),
    ("protein4", "MKTVRQERLKSI<mask>RILERSKEPVSGAQLAEELS<mask>SRQVIVQDIAYLRSLGYN<mask>VATPRGYVLA"),
]

MODEL_MAPPING = {
    "esm1b_t33_650M_UR50S": esm_module.pretrained.esm1b_t33_650M_UR50S,
    "esm1v_t33_650M_UR90S_1": esm_module.pretrained.esm1v_t33_650M_UR90S_1,
    "esm1v_t33_650M_UR90S_2": esm_module.pretrained.esm1v_t33_650M_UR90S_2,
    "esm1v_t33_650M_UR90S_3": esm_module.pretrained.esm1v_t33_650M_UR90S_3,
    "esm1v_t33_650M_UR90S_4": esm_module.pretrained.esm1v_t33_650M_UR90S_4,
    "esm1v_t33_650M_UR90S_5": esm_module.pretrained.esm1v_t33_650M_UR90S_5,
    "esm2_t48_15B_UR50D": esm_module.pretrained.esm2_t48_15B_UR50D,
    "esm2_t36_3B_UR50D": esm_module.pretrained.esm2_t36_3B_UR50D,
    "esm2_t33_650M_UR50D": esm_module.pretrained.esm2_t33_650M_UR50D,
    "esm2_t30_150M_UR50D": esm_module.pretrained.esm2_t30_150M_UR50D,
    "esm2_t12_35M_UR50D": esm_module.pretrained.esm2_t12_35M_UR50D,
    # å°†é¢„è®­ç»ƒæ¨¡å‹â€œesm2_t6_8M_UR50Dâ€æ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œé”®ä¸º"esm2_t6_8M_UR50D"ï¼Œå€¼ä¸ºè¯¥é¢„è®­ç»ƒæ¨¡å‹
    "esm2_t6_8M_UR50D": esm_module.pretrained.esm2_t6_8M_UR50D,
    # å°†é¢„è®­ç»ƒæ¨¡å‹â€œesmfold_v1â€æ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œé”®ä¸º"esmfold_v1"ï¼Œå€¼ä¸ºè¯¥é¢„è®­ç»ƒæ¨¡å‹
    "esmfold_v1": esmfold_v1,
```  
}

# å®šä¹‰æ°¨åŸºé…¸æ®‹åŸºç±»å‹çš„åˆ—è¡¨
restypes = list("ARNDCQEGHILKMFPSTWYV")

# æ‰©å±•åŒ…å«'X'çš„æ°¨åŸºé…¸æ®‹åŸºç±»å‹åˆ—è¡¨
restypes_with_x = restypes + ["X"]

# æ·»åŠ ç‰¹æ®Šæ ‡è®°åçš„æ°¨åŸºé…¸æ®‹åŸºç±»å‹åˆ—è¡¨
restypes_with_extras = restypes_with_x + ["
    else:
        # è¡¨ç¤ºä¸º ESM-2 æ¨¡å‹
        embed_dim = esm.embed_dim
        num_layers = esm.num_layers
        num_attention_heads = esm.attention_heads
        intermediate_size = 4 * embed_dim  # è¿™åœ¨ ESM-2 ä¸­æ˜¯ç¡¬ç¼–ç çš„
        token_dropout = esm.token_dropout
        emb_layer_norm_before = False  # åœ¨ ESM-2 ä¸­ä¸å­˜åœ¨æ­¤ä»£ç è·¯å¾„
        position_embedding_type = "rotary"
        is_folding_model = False
        esmfold_config = None

    if is_folding_model:
        alphabet = esm.esm.alphabet
    vocab_list = tuple(alphabet.all_toks)
    mask_token_id = alphabet.mask_idx
    pad_token_id = alphabet.padding_idx

    if is_folding_model:
        original_esm_model = esm.esm
    else:
        original_esm_model = esm

    config = EsmConfig(
        vocab_size=original_esm_model.embed_tokens.num_embeddings,
        mask_token_id=mask_token_id,
        hidden_size=embed_dim,
        num_hidden_layers=num_layers,
        num_attention_heads=num_attention_heads,
        intermediate_size=intermediate_size,
        max_position_embeddings=1026,
        layer_norm_eps=1e-5,  # åœ¨ fairseq ä¸­ä½¿ç”¨ PyTorch é»˜è®¤å€¼
        attention_probs_dropout_prob=0.0,
        hidden_dropout_prob=0.0,
        pad_token_id=pad_token_id,
        emb_layer_norm_before=emb_layer_norm_before,
        token_dropout=token_dropout,
        position_embedding_type=position_embedding_type,
        is_folding_model=is_folding_model,
        esmfold_config=esmfold_config,
        vocab_list=vocab_list,
    )
    if classification_head:
        config.num_labels = esm.classification_heads["mnli"].out_proj.weight.shape[0]
    print("Our ESM config:", config)

    if model.startswith("esmfold"):
        model_class = EsmForProteinFolding
    elif classification_head:
        model_class = EsmForSequenceClassification
    else:
        model_class = EsmForMaskedLM
    model = model_class(config)
    model.eval()

    # ç°åœ¨è®©æˆ‘ä»¬å¤åˆ¶æ‰€æœ‰æƒé‡ã€‚
    # Embeddings
    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight
    if position_embedding_type == "absolute":
        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight

    if config.emb_layer_norm_before:
        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight
        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias

    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight
    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias
    # å¦‚æœæ˜¯æŠ˜å æ¨¡å‹
    if is_folding_model:
        # å°†ESMæ¨¡å‹çš„æ•°æ®èµ‹å€¼ç»™æ¨¡å‹çš„å±æ€§
        model.esm_s_combine.data = esm.esm_s_combine.data
        model.af2_to_esm.data = esm.af2_to_esm.data
        # è¿ç§»å¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.embedding, model.embedding)
        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)
        transfer_and_check_weights(esm.trunk, model.trunk)
        transfer_and_check_weights(esm.distogram_head, model.distogram_head)
        transfer_and_check_weights(esm.ptm_head, model.ptm_head)
        transfer_and_check_weights(esm.lm_head, model.lm_head)
        transfer_and_check_weights(esm.lddt_head, model.lddt_head)

    # å¦‚æœæ˜¯åˆ†ç±»å¤´
    elif classification_head:
        # å°†ESMæ¨¡å‹çš„åˆ†ç±»å¤´æ•°æ®èµ‹å€¼ç»™æ¨¡å‹çš„å±æ€§
        model.classifier.dense.weight = esm.esm.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = esm.classification_heads["mnli"].dense.bias
        model.classifier.out_proj.weight = esm.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = esm.classification_heads["mnli"].out_proj.bias
    else:
        # LMå¤´
        model.lm_head.dense.weight = esm.lm_head.dense.weight
        model.lm_head.dense.bias = esm.lm_head.dense.bias
        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias
        model.lm_head.decoder.weight = esm.lm_head.weight
        model.lm_head.bias = esm.lm_head.bias

    # è”ç»œé¢„æµ‹å¤´
    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)

    # å‡†å¤‡æ•°æ®ï¼ˆä»ESMStructuralSplitDatasetè¶…çº§å®¶æ—ä¸­è·å–å‰2ä¸ªåºåˆ—/ 4ï¼‰
    if is_folding_model:
        # æŠ˜å æ¨¡å‹ä¸æ˜¯åœ¨å±è”½è¾“å…¥ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¹Ÿä¸å–œæ¬¢æ©ç æ ‡è®°ã€‚
        sample_data = SAMPLE_DATA[:2]
    else:
        sample_data = SAMPLE_DATA

    if is_folding_model:
        # è·å–ESMæŠ˜å çš„æ ‡è®°å™¨å’Œæ ‡è®°ï¼Œå¹¶è¿›è¡Œæ¯”è¾ƒ
        hf_tokenizer = get_esmfold_tokenizer()
        hf_tokens = hf_tokenizer(
            [row[1] for row in sample_data], return_tensors="pt", padding=True, add_special_tokens=False
        )
        esmfold_aas, esmfold_mask, _, _, _ = esmfold_encode_sequences([row[1] for row in sample_data])
        success = torch.all(hf_tokens["input_ids"] == esmfold_aas) and torch.all(
            hf_tokens["attention_mask"] == esmfold_mask
        )
    else:
        # æ£€æŸ¥æ˜¯å¦è·å¾—ç›¸åŒçš„ç»“æœ
        batch_converter = alphabet.get_batch_converter()
        batch_labels, batch_strs, batch_tokens = batch_converter(sample_data)
        # å‡†å¤‡æ ‡è®°å™¨å¹¶ç¡®ä¿å…¶åŒ¹é…
        with TemporaryDirectory() as tempdir:
            vocab = "\n".join(alphabet.all_toks)
            vocab_file = Path(tempdir) / "vocab.txt"
            vocab_file.write_text(vocab)
            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))

        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors="pt", padding=True)
        success = torch.all(hf_tokens["input_ids"] == batch_tokens)

    # è¾“å‡ºä¸¤ä¸ªæ¨¡å‹æ ‡è®°å™¨è¾“å‡ºç›¸åŒçš„æ ‡è®°
    print("Do both models tokenizers output the same tokens?", "ğŸ”¥" if success else "ğŸ’©")
    if not success:
        # å¦‚æœæ¡ä»¶ä¸æ»¡è¶³ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ï¼ŒæŒ‡ç¤ºä»¤ç‰ŒåŒ–ä¸åŒ¹é…
        raise Exception("Tokenization does not match!")

    with torch.no_grad():
        if is_folding_model:
            # æµ‹è¯•æ¨¡å‹çš„å„ä¸ªéƒ¨åˆ†
            # ESMFold æ€»æ˜¯ä¼šå°† ESM stem è½¬æ¢ä¸º float16ï¼Œè¿™éœ€è¦ float16 è¿ç®—ï¼Œè€Œåœ¨ CPU ä¸Šä¸æ”¯æŒã€‚
            # å› æ­¤ï¼Œä¸ºäº†æµ‹è¯•å®ƒï¼Œæˆ‘ä»¬éœ€è¦åœ¨ GPU ä¸Šè¿è¡Œå®ƒã€‚ç„¶è€Œï¼Œå¤§å‹æ¨¡å‹ï¼ˆ"big boy"ï¼‰ä¼šå¯¼è‡´åŸå§‹æ¨¡å‹å’Œè½¬æ¢åçš„æ¨¡å‹åœ¨åŒä¸€æ—¶é—´æ”¾åœ¨ GPU ä¸Šï¼Œå› æ­¤è¦å°½é‡é¿å…è¿™ç§æƒ…å†µã€‚
            their_output = esm.cuda().infer([row[1] for row in sample_data])
            our_output = model.cuda()(
                input_ids=hf_tokens["input_ids"].cuda(), attention_mask=hf_tokens["attention_mask"].cuda()
            )
        else:
            our_output = model(**hf_tokens, output_hidden_states=True)
            our_output = our_output["logits"]
            if classification_head:
                their_output = esm.model.classification_heads["mnli"](esm.extract_features(batch_tokens))
            else:
                their_output = esm(hf_tokens["input_ids"], repr_layers=list(range(999)))
                their_output = their_output["logits"]

        if is_folding_model:
            max_absolute_diff = torch.max(torch.abs(our_output["positions"] - their_output["positions"])).item()
            success = torch.allclose(our_output["positions"], their_output["positions"], atol=1e-5)
        else:
            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
            success = torch.allclose(our_output, their_output, atol=1e-5)

        print(f"max_absolute_diff = {max_absolute_diff}")  # æœ€å¤§ç»å¯¹å·® ~1e-5
        print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")

        if not success:
            raise Exception("Something went wRoNg")

        if not is_folding_model:
            # è®©æˆ‘ä»¬ä¹Ÿæ£€æŸ¥è”ç³»é¢„æµ‹
            our_output = model.predict_contacts(hf_tokens["input_ids"], hf_tokens["attention_mask"])
            their_output = esm.predict_contacts(hf_tokens["input_ids"])
            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
            success = torch.allclose(our_output, their_output, atol=1e-5)

            print("Contact prediction testing:")
            print(f"max_absolute_diff = {max_absolute_diff}")  # æœ€å¤§ç»å¯¹å·® ~1e-5
            print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")

            if not success:
                raise Exception("Something went wRoNg")

        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
        print(f"Saving model to {pytorch_dump_folder_path}")
        model.save_pretrained(pytorch_dump_folder_path)

        del esm  # åœ¨ç»§ç»­ä¹‹å‰é‡Šæ”¾ä¸€äº›å†…å­˜

    print(f"Saving tokenizer to {pytorch_dump_folder_path}")
    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)
    # å¦‚æœ push_to_repo ä¸ºçœŸï¼Œåˆ™å°†æ¨¡å‹æ¨é€åˆ°ä»“åº“
    if push_to_repo:
        # è°ƒç”¨ model å¯¹è±¡çš„ push_to_hub æ–¹æ³•ï¼Œå°†æ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ä»“åº“ï¼Œä½¿ç”¨æŒ‡å®šçš„è®¤è¯ä»¤ç‰Œ
        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)
        # è°ƒç”¨ hf_tokenizer å¯¹è±¡çš„ push_to_hub æ–¹æ³•ï¼Œå°†åˆ†è¯å™¨æ¨é€åˆ°æŒ‡å®šçš„ä»“åº“ï¼Œä½¿ç”¨æŒ‡å®šçš„è®¤è¯ä»¤ç‰Œ
        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)
# å¦‚æœè¯¥è„šæœ¬è¢«ç›´æ¥è¿è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…è¦å‚æ•°
    parser.add_argument(
        "--pytorch_dump_folder_path", type=str, required=True, help="Path to the output PyTorch model."
    )
    # æ˜¯å¦è½¬æ¢æœ€ç»ˆçš„åˆ†ç±»å¤´éƒ¨
    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # è¦è½¬æ¢çš„æ¨¡å‹çš„åç§°
    parser.add_argument("--model", default=None, type=str, required=True, help="Name of model to convert.")
    # è¦ä¸Šä¼ åˆ°çš„ä»“åº“ï¼ˆåŒ…æ‹¬ç”¨æˆ·åï¼‰
    parser.add_argument("--push_to_repo", type=str, help="Repo to upload to (including username!).")
    # HuggingFace è®¤è¯ä»¤ç‰Œ
    parser.add_argument("--auth_token", type=str, help="HuggingFace auth token.")
    # è§£æå‚æ•°
    args = parser.parse_args()
    # è½¬æ¢ ESM æ¨¡å‹æ£€æŸ¥ç‚¹ä¸º PyTorch æ¨¡å‹
    convert_esm_checkpoint_to_pytorch(
        args.model, args.pytorch_dump_folder_path, args.classification_head, args.push_to_repo, args.auth_token
    )
```