# `.\models\esm\convert_esm.py`

```
# coding=utf-8
# ç‰ˆæƒ 2022 å¹´ HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼ˆ"è®¸å¯è¯"ï¼‰æˆæƒï¼›
# é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æœ¬è½¯ä»¶æ ¹æ®"åŸæ ·"çš„åŸºç¡€åˆ†å‘ï¼Œ
# ä¸æä¾›ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºä¿è¯æˆ–æ¡ä»¶ã€‚
# æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚

"""Convert ESM checkpoint."""

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import pathlib   # å¯¼å…¥è·¯å¾„æ“ä½œæ¨¡å—
from pathlib import Path  # å¯¼å…¥è·¯å¾„æ“ä½œæ¨¡å—ä¸­çš„Pathç±»
from tempfile import TemporaryDirectory  # å¯¼å…¥ä¸´æ—¶ç›®å½•æ¨¡å—

# å¯¼å…¥ESMç›¸å…³çš„æ¨¡å—å’Œç±»
import esm as esm_module  # å¯¼å…¥ESMæ¨¡å—
import torch  # å¯¼å…¥PyTorchåº“
from esm.esmfold.v1.misc import batch_encode_sequences as esmfold_encode_sequences  # å¯¼å…¥åºåˆ—ç¼–ç å‡½æ•°
from esm.esmfold.v1.pretrained import esmfold_v1  # å¯¼å…¥ESM-Fold v1é¢„è®­ç»ƒæ¨¡å‹

# å¯¼å…¥Transformersç›¸å…³çš„ç±»å’Œå‡½æ•°
from transformers.models.esm.configuration_esm import EsmConfig, EsmFoldConfig  # å¯¼å…¥ESMå’ŒESM-Foldçš„é…ç½®ç±»
from transformers.models.esm.modeling_esm import (  # å¯¼å…¥ESMæ¨¡å‹ç›¸å…³ç±»
    EsmForMaskedLM,
    EsmForSequenceClassification,
    EsmIntermediate,
    EsmLayer,
    EsmOutput,
    EsmSelfAttention,
    EsmSelfOutput,
)
from transformers.models.esm.modeling_esmfold import EsmForProteinFolding  # å¯¼å…¥è›‹ç™½è´¨æŠ˜å ç›¸å…³çš„ESMæ¨¡å‹ç±»
from transformers.models.esm.tokenization_esm import EsmTokenizer  # å¯¼å…¥ESMçš„åˆ†è¯å™¨ç±»
from transformers.utils import logging  # å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—

# è®¾ç½®æ—¥å¿—çš„è¯¦ç»†çº§åˆ«ä¸ºä¿¡æ¯çº§åˆ«
logging.set_verbosity_info()
# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# å®šä¹‰æ ·æœ¬æ•°æ®ï¼ŒåŒ…å«è›‹ç™½è´¨åºåˆ—å’Œæ ‡è¯†
SAMPLE_DATA = [
    (
        "protein1",
        "MNGTEGPNFYVPFSNATGVVRSPFEYPQYYLAEPWQFSMLAAYMFLLIVLGFPINFLTLYVTVQHKKLRTPLNYILLNLAVADLFMVLGGFTSTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVCKPMSNFRFGENHAIMGVAFTWVMALACAAPPLAGWSRYIPEGLQCSCGIDYYTLKPEVNNESFVIYMFVVHFTIPMIIIFFCYGQLVFTVKEAAAQQQESATTQKAEKEVTRMVIIMVIAFLICWVPYASVAFYIFTHQGSNFGPIFMTIPAFFAKSAAIYNPVIYIMMNKQFRNCMLTTICCGKNPLGDDEASATVSKTETSQVAPA",
    ),
    ("protein2", "MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLA"),
    ("protein3", "MKTVRQERLKSI<mask>RILERSKEPVSGAQLAEELS<mask>SRQVIVQDIAYLRSLGYN<mask>VATPRGYVLAGG"),
    ("protein4", "MKTVRQERLKSI<mask>RILERSKEPVSGAQLAEELS<mask>SRQVIVQDIAYLRSLGYN<mask>VATPRGYVLA"),
]

# å®šä¹‰ESMæ¨¡å‹çš„åç§°ä¸æ¨¡å‹å¯¹è±¡çš„æ˜ å°„å…³ç³»
MODEL_MAPPING = {
    "esm1b_t33_650M_UR50S": esm_module.pretrained.esm1b_t33_650M_UR50S,
    "esm1v_t33_650M_UR90S_1": esm_module.pretrained.esm1v_t33_650M_UR90S_1,
    "esm1v_t33_650M_UR90S_2": esm_module.pretrained.esm1v_t33_650M_UR90S_2,
    "esm1v_t33_650M_UR90S_3": esm_module.pretrained.esm1v_t33_650M_UR90S_3,
    "esm1v_t33_650M_UR90S_4": esm_module.pretrained.esm1v_t33_650M_UR90S_4,
    "esm1v_t33_650M_UR90S_5": esm_module.pretrained.esm1v_t33_650M_UR90S_5,
    "esm2_t48_15B_UR50D": esm_module.pretrained.esm2_t48_15B_UR50D,
    "esm2_t36_3B_UR50D": esm_module.pretrained.esm2_t36_3B_UR50D,
    "esm2_t33_650M_UR50D": esm_module.pretrained.esm2_t33_650M_UR50D,
    "esm2_t30_150M_UR50D": esm_module.pretrained.esm2_t30_150M_UR50D,
    "esm2_t12_35M_UR50D": esm_module.pretrained.esm2_t12_35M_UR50D,
}
    # å°†æ¨¡å‹åç§°æ˜ å°„åˆ°é¢„è®­ç»ƒæ¨¡å‹å¯¹è±¡çš„å¼•ç”¨ï¼š"esm2_t6_8M_UR50D"æ˜ å°„åˆ°esm_module.pretrained.esm2_t6_8M_UR50D
    "esm2_t6_8M_UR50D": esm_module.pretrained.esm2_t6_8M_UR50D,
    # å°†æ¨¡å‹åç§°æ˜ å°„åˆ°é¢„è®­ç»ƒæ¨¡å‹å¯¹è±¡çš„å¼•ç”¨ï¼š"esmfold_v1"æ˜ å°„åˆ°esmfold_v1
    "esmfold_v1": esmfold_v1,
}

# å®šä¹‰æ°¨åŸºé…¸ç±»å‹åˆ—è¡¨
restypes = list("ARNDCQEGHILKMFPSTWYV")

# åœ¨æ°¨åŸºé…¸ç±»å‹åˆ—è¡¨ä¸­åŠ å…¥é¢å¤–çš„å­—ç¬¦ 'X'
restypes_with_x = restypes + ["X"]

# åœ¨å¸¦æœ‰ 'X' çš„æ°¨åŸºé…¸ç±»å‹åˆ—è¡¨ä¸­å†åŠ å…¥ç‰¹æ®Šçš„ token
restypes_with_extras = restypes_with_x + ["<pad>", "<mask>", "<cls>", "<sep>", "<eos>"]

# è¿”å›ä¸€ä¸ª ESM æ¨¡å‹çš„ tokenizer å¯¹è±¡
def get_esmfold_tokenizer():
    # ä½¿ç”¨ä¸´æ—¶ç›®å½•åˆ›å»ºè¯æ±‡è¡¨æ–‡ä»¶å¹¶å†™å…¥å­—ç¬¦åˆ—è¡¨
    with TemporaryDirectory() as tempdir:
        vocab = "\n".join(restypes_with_extras)
        vocab_file = Path(tempdir) / "vocab.txt"
        vocab_file.write_text(vocab)
        # ä½¿ç”¨è¯æ±‡è¡¨æ–‡ä»¶åˆ›å»º ESM tokenizer å¯¹è±¡
        hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))
    # è®¾ç½® padding token çš„ ID
    hf_tokenizer.pad_token_id = 0  # ä¸ 'A' é‡å ï¼Œä½†è¿™ä¼¼ä¹æ˜¯ä»–ä»¬æƒ³è¦çš„
    return hf_tokenizer

# å°†åŸå§‹æ¨¡å‹çš„æƒé‡è½¬ç§»å¹¶æ£€æŸ¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­
def transfer_and_check_weights(original_module, our_module):
    status = our_module.load_state_dict(original_module.state_dict())
    # å¦‚æœæœ‰ç¼ºå¤±çš„é”®ï¼Œåˆ™å¼•å‘ ValueError å¼‚å¸¸
    if status.missing_keys:
        raise ValueError(f"Missing keys: {status.missing_keys}")
    # å¦‚æœæœ‰æ„å¤–çš„é”®ï¼Œåˆ™å¼•å‘ ValueError å¼‚å¸¸
    if status.unexpected_keys:
        raise ValueError(f"Unexpected keys: {status.unexpected_keys}")

# å°† ESM æ¨¡å‹æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch çš„æ ¼å¼
def convert_esm_checkpoint_to_pytorch(
    model: str, pytorch_dump_folder_path: str, classification_head: bool, push_to_repo: str, auth_token: str
):
    """
    å¤åˆ¶/ç²˜è´´/è°ƒæ•´ esm çš„æƒé‡åˆ°æˆ‘ä»¬çš„ BERT ç»“æ„ä¸­ã€‚
    """
    # å¦‚æœæ¨¡å‹ä»¥ "esmfold" å¼€å¤´ï¼Œåˆ™åˆ›å»ºç›¸åº”çš„ ESM æ¨¡å‹å®ä¾‹
    if model.startswith("esmfold"):
        esm = MODEL_MAPPING[model]()
    else:
        esm, alphabet = MODEL_MAPPING[model]()
    
    # å°†æ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨ dropout
    esm.eval()

    # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®å„ç§å‚æ•°å’Œé…ç½®
    if model.startswith("esmfold"):
        embed_dim = esm.esm.embed_dim
        num_layers = esm.esm.num_layers
        num_attention_heads = esm.esm.attention_heads
        intermediate_size = 4 * embed_dim
        token_dropout = esm.esm.token_dropout
        emb_layer_norm_before = False  # è¿™æ¡ä»£ç è·¯å¾„åœ¨ ESM-2 ä¸­ä¸å­˜åœ¨
        position_embedding_type = "rotary"
        is_folding_model = True
        esmfold_config = EsmFoldConfig()
        # æ›´æ–° ESMFoldConfig å¯¹è±¡çš„é…ç½®é¡¹
        for key, val in esm.cfg.items():
            if hasattr(esmfold_config, key) and key != "trunk":
                setattr(esmfold_config, key, val)
        for key, val in esm.cfg.trunk.items():
            if hasattr(esmfold_config.trunk, key) and key != "structure_module":
                setattr(esmfold_config.trunk, key, val)
        for key, val in esm.cfg.trunk.structure_module.items():
            if hasattr(esmfold_config.trunk.structure_module, key):
                setattr(esmfold_config.trunk.structure_module, key, val)
    elif hasattr(esm, "args"):
        # è¡¨æ˜æ˜¯ ESM-1b æˆ– ESM-1v æ¨¡å‹
        embed_dim = esm.args.embed_dim
        num_layers = esm.args.layers
        num_attention_heads = esm.args.attention_heads
        intermediate_size = esm.args.ffn_embed_dim
        token_dropout = esm.args.token_dropout
        emb_layer_norm_before = True if esm.emb_layer_norm_before else False
        position_embedding_type = "absolute"
        is_folding_model = False
        esmfold_config = None
    else:
        # è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ª ESM-2 æ¨¡å‹
        embed_dim = esm.embed_dim
        num_layers = esm.num_layers
        num_attention_heads = esm.attention_heads
        intermediate_size = 4 * embed_dim  # è¿™ä¸ªå€¼åœ¨ ESM-2 ä¸­æ˜¯ç¡¬ç¼–ç çš„
        token_dropout = esm.token_dropout
        emb_layer_norm_before = False  # è¿™ä¸ªä»£ç è·¯å¾„åœ¨ ESM-2 ä¸­ä¸å­˜åœ¨
        position_embedding_type = "rotary"
        is_folding_model = False
        esmfold_config = None

    if is_folding_model:
        alphabet = esm.esm.alphabet
    vocab_list = tuple(alphabet.all_toks)
    mask_token_id = alphabet.mask_idx
    pad_token_id = alphabet.padding_idx

    if is_folding_model:
        original_esm_model = esm.esm
    else:
        original_esm_model = esm

    config = EsmConfig(
        vocab_size=original_esm_model.embed_tokens.num_embeddings,
        mask_token_id=mask_token_id,
        hidden_size=embed_dim,
        num_hidden_layers=num_layers,
        num_attention_heads=num_attention_heads,
        intermediate_size=intermediate_size,
        max_position_embeddings=1026,
        layer_norm_eps=1e-5,  # åœ¨ fairseq ä¸­ä½¿ç”¨çš„ PyTorch é»˜è®¤å€¼
        attention_probs_dropout_prob=0.0,
        hidden_dropout_prob=0.0,
        pad_token_id=pad_token_id,
        emb_layer_norm_before=emb_layer_norm_before,
        token_dropout=token_dropout,
        position_embedding_type=position_embedding_type,
        is_folding_model=is_folding_model,
        esmfold_config=esmfold_config,
        vocab_list=vocab_list,
    )
    if classification_head:
        config.num_labels = esm.classification_heads["mnli"].out_proj.weight.shape[0]
    print("Our ESM config:", config)

    if model.startswith("esmfold"):
        model_class = EsmForProteinFolding
    elif classification_head:
        model_class = EsmForSequenceClassification
    else:
        model_class = EsmForMaskedLM
    model = model_class(config)
    model.eval()

    # ç°åœ¨æˆ‘ä»¬æ¥å¤åˆ¶æ‰€æœ‰çš„æƒé‡ã€‚
    # Embeddings
    model.esm.embeddings.word_embeddings.weight = original_esm_model.embed_tokens.weight
    if position_embedding_type == "absolute":
        model.esm.embeddings.position_embeddings.weight = original_esm_model.embed_positions.weight

    if config.emb_layer_norm_before:
        model.esm.embeddings.layer_norm.weight = original_esm_model.emb_layer_norm_before.weight
        model.esm.embeddings.layer_norm.bias = original_esm_model.emb_layer_norm_before.bias

    model.esm.encoder.emb_layer_norm_after.weight = original_esm_model.emb_layer_norm_after.weight
    model.esm.encoder.emb_layer_norm_after.bias = original_esm_model.emb_layer_norm_after.bias
    # å¦‚æœæ˜¯æŠ˜å æ¨¡å‹ï¼ˆfolding modelï¼‰ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if is_folding_model:
        # å°† ESM æ¨¡å‹çš„ esm_s_combine æ•°æ®ä¼ è¾“åˆ° model çš„ esm_s_combine ä¸­
        model.esm_s_combine.data = esm.esm_s_combine.data
        # å°† ESM æ¨¡å‹çš„ af2_to_esm æ•°æ®ä¼ è¾“åˆ° model çš„ af2_to_esm ä¸­
        model.af2_to_esm.data = esm.af2_to_esm.data
        # å°† ESM æ¨¡å‹çš„ embedding æ•°æ®ä¼ è¾“åˆ° model çš„ embedding ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.embedding, model.embedding)
        # å°† ESM æ¨¡å‹çš„ esm_s_mlp æ•°æ®ä¼ è¾“åˆ° model çš„ esm_s_mlp ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.esm_s_mlp, model.esm_s_mlp)
        # å°† ESM æ¨¡å‹çš„ trunk æ•°æ®ä¼ è¾“åˆ° model çš„ trunk ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.trunk, model.trunk)
        # å°† ESM æ¨¡å‹çš„ distogram_head æ•°æ®ä¼ è¾“åˆ° model çš„ distogram_head ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.distogram_head, model.distogram_head)
        # å°† ESM æ¨¡å‹çš„ ptm_head æ•°æ®ä¼ è¾“åˆ° model çš„ ptm_head ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.ptm_head, model.ptm_head)
        # å°† ESM æ¨¡å‹çš„ lm_head æ•°æ®ä¼ è¾“åˆ° model çš„ lm_head ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.lm_head, model.lm_head)
        # å°† ESM æ¨¡å‹çš„ lddt_head æ•°æ®ä¼ è¾“åˆ° model çš„ lddt_head ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
        transfer_and_check_weights(esm.lddt_head, model.lddt_head)

    # å¦åˆ™ï¼Œå¦‚æœæ˜¯åˆ†ç±»å¤´ï¼ˆclassification headï¼‰ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œ
    elif classification_head:
        # å°† ESM æ¨¡å‹çš„ "mnli" åˆ†ç±»å¤´çš„æƒé‡ä¼ è¾“åˆ° model çš„ classifier.dense.weight ä¸­
        model.classifier.dense.weight = esm.esm.classification_heads["mnli"].dense.weight
        # å°† ESM æ¨¡å‹çš„ "mnli" åˆ†ç±»å¤´çš„åç½®ä¼ è¾“åˆ° model çš„ classifier.dense.bias ä¸­
        model.classifier.dense.bias = esm.classification_heads["mnli"].dense.bias
        # å°† ESM æ¨¡å‹çš„ "mnli" åˆ†ç±»å¤´çš„è¾“å‡ºæŠ•å½±æƒé‡ä¼ è¾“åˆ° model çš„ classifier.out_proj.weight ä¸­
        model.classifier.out_proj.weight = esm.classification_heads["mnli"].out_proj.weight
        # å°† ESM æ¨¡å‹çš„ "mnli" åˆ†ç±»å¤´çš„è¾“å‡ºæŠ•å½±åç½®ä¼ è¾“åˆ° model çš„ classifier.out_proj.bias ä¸­
        model.classifier.out_proj.bias = esm.classification_heads["mnli"].out_proj.bias

    # å¦åˆ™ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼ˆé€šå¸¸æ˜¯è¯­è¨€æ¨¡å‹å¤´ï¼‰
    else:
        # å°† ESM æ¨¡å‹çš„ lm_head çš„ dense.weight æ•°æ®ä¼ è¾“åˆ° model çš„ lm_head.dense.weight ä¸­
        model.lm_head.dense.weight = esm.lm_head.dense.weight
        # å°† ESM æ¨¡å‹çš„ lm_head çš„ dense.bias æ•°æ®ä¼ è¾“åˆ° model çš„ lm_head.dense.bias ä¸­
        model.lm_head.dense.bias = esm.lm_head.dense.bias
        # å°† ESM æ¨¡å‹çš„ lm_head çš„ layer_norm.weight æ•°æ®ä¼ è¾“åˆ° model çš„ lm_head.layer_norm.weight ä¸­
        model.lm_head.layer_norm.weight = esm.lm_head.layer_norm.weight
        # å°† ESM æ¨¡å‹çš„ lm_head çš„ layer_norm.bias æ•°æ®ä¼ è¾“åˆ° model çš„ lm_head.layer_norm.bias ä¸­
        model.lm_head.layer_norm.bias = esm.lm_head.layer_norm.bias
        # å°† ESM æ¨¡å‹çš„ lm_head çš„ weight æ•°æ®ä¼ è¾“åˆ° model çš„ lm_head.decoder.weight ä¸­
        model.lm_head.decoder.weight = esm.lm_head.weight
        # å°† ESM æ¨¡å‹çš„ lm_head çš„ bias æ•°æ®ä¼ è¾“åˆ° model çš„ lm_head.bias ä¸­
        model.lm_head.bias = esm.lm_head.bias

    # å°† ESM æ¨¡å‹çš„ contact_head æ•°æ®ä¼ è¾“åˆ° model çš„ esm.contact_head ä¸­ï¼Œå¹¶æ£€æŸ¥æƒé‡
    transfer_and_check_weights(esm.contact_head, model.esm.contact_head)

    # å‡†å¤‡æ•°æ®ï¼ˆæ¥è‡ª ESMStructuralSplitDataset è¶…å®¶æ—çš„å‰ä¸¤ä¸ªåºåˆ— / 4ï¼‰
    if is_folding_model:
        # å¯¹äºæŠ˜å æ¨¡å‹ï¼Œé‡‡æ ·å‰ä¸¤ä¸ªæ•°æ®æ ·æœ¬ï¼Œå› ä¸ºæŠ˜å æ¨¡å‹ä¸ä¼šä½¿ç”¨æ©ç è¾“å…¥ä¸”ä¸å–œæ¬¢æ©ç ä»¤ç‰Œ
        sample_data = SAMPLE_DATA[:2]
    else:
        # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œé‡‡æ ·å…¨éƒ¨æ•°æ®æ ·æœ¬
        sample_data = SAMPLE_DATA

    if is_folding_model:
        # è·å– ESMFold çš„ tokenizer
        hf_tokenizer = get_esmfold_tokenizer()
        # ä½¿ç”¨ ESMFold tokenizer å¤„ç†æ ·æœ¬æ•°æ®ï¼Œè¿”å› PyTorch å¼ é‡æ ¼å¼ï¼Œè¿›è¡Œå¡«å……ï¼Œå¹¶ä¸æ·»åŠ ç‰¹æ®Šä»¤ç‰Œ
        hf_tokens = hf_tokenizer(
            [row[1] for row in sample_data], return_tensors="pt", padding=True, add_special_tokens=False
        )
        # ä½¿ç”¨ ESMFold ç¼–ç å‡½æ•°å¤„ç†æ ·æœ¬æ•°æ®ï¼Œè·å–æ°¨åŸºé…¸åºåˆ—ã€æ©ç ç­‰ä¿¡æ¯
        esmfold_aas, esmfold_mask, _, _, _ = esmfold_encode_sequences([row[1] for row in sample_data])
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŒ¹é… ESMFold tokenizer è¾“å‡ºçš„ input_ids å’Œ attention_mask ä¸ hf_tokens ä¸­çš„å¯¹åº”å€¼
        success = torch.all(hf_tokens["input_ids"] == esmfold_aas) and torch.all(
            hf_tokens["attention_mask"] == esmfold_mask
        )
    else:
        # å¦åˆ™ï¼Œæ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹çš„ tokenizer æ˜¯å¦è¾“å‡ºç›¸åŒçš„ tokens
        batch_converter = alphabet.get_batch_converter()
        # ä½¿ç”¨ batch_converter å¤„ç†æ ·æœ¬æ•°æ®ï¼Œè¿”å›æ‰¹æ¬¡æ ‡ç­¾ã€å­—ç¬¦ä¸²å’Œ tokens
        batch_labels, batch_strs, batch_tokens = batch_converter(sample_data)
        # å‡†å¤‡ tokenizerï¼Œå¹¶ç¡®ä¿å…¶ä¸ batch_tokens åŒ¹é…
        with TemporaryDirectory() as tempdir:
            # åˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œå†™å…¥ alphabet çš„å…¨éƒ¨ tokens ä½œä¸º vocab
            vocab = "\n".join(alphabet.all_toks)
            vocab_file = Path(tempdir) / "vocab.txt"
            vocab_file.write_text(vocab)
            # ä½¿ç”¨ EsmTokenizer åˆå§‹åŒ– hf_tokenizer
            hf_tokenizer = EsmTokenizer(vocab_file=str(vocab_file))

        # ä½¿ç”¨ hf_tokenizer å¤„ç†æ ·æœ¬æ•°æ®ï¼Œè¿”å› PyTorch å¼ é‡æ ¼å¼ï¼Œè¿›è¡Œå¡«å……
        hf_tokens = hf_tokenizer([row[1] for row in sample_data], return_tensors="pt", padding=True)
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŒ¹é… hf_tokens çš„ input_ids ä¸ batch_tokens ä¸­çš„å¯¹åº”å€¼
        success = torch.all(hf_tokens["input_ids"] == batch_tokens)

    # æ‰“å°æ˜¯å¦ä¸¤ä¸ªæ¨¡å‹çš„ tokenizer è¾“å‡ºç›¸åŒçš„ tokensï¼Œå¦‚æœç›¸åŒåˆ™è¾“å‡º "ğŸ”¥"ï¼Œå¦åˆ™è¾“å‡º "ğŸ’©"
    print("Do both models tokenizers output the same tokens?", "ğŸ”¥" if success else "ğŸ’©")
    # å¦‚æœæˆåŠŸæ ‡å¿—ä¸ºå‡ï¼Œåˆ™å¼•å‘å¼‚å¸¸å¹¶æ˜¾ç¤ºæ¶ˆæ¯
    if not success:
        raise Exception("Tokenization does not match!")

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œå› ä¸ºè¿™æ˜¯æ¨æ–­é˜¶æ®µ
    with torch.no_grad():
        # å¦‚æœæ˜¯æŠ˜å æ¨¡å‹
        if is_folding_model:
            # åˆ†é˜¶æ®µæµ‹è¯•æ¨¡å‹
            # ESMFold æ€»æ˜¯å°† ESM stem è½¬æ¢ä¸º float16ï¼Œéœ€è¦åœ¨ GPU ä¸Šæ‰§è¡Œ float16 æ“ä½œ
            # è¿™åœ¨ CPU ä¸Šä¸æ”¯æŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ GPU ä¸Šè¿è¡Œå®ƒã€‚ç„¶è€Œï¼Œ
            # ESMFold æ˜¯ç¤¾åŒºä¸­æ‰€è°“çš„â€œå¤§å‹æ¨¡å‹â€ï¼Œå› æ­¤æˆ‘ä»¬å¼ºçƒˆé¿å…åŒæ—¶å°†åŸå§‹æ¨¡å‹å’Œè½¬æ¢åçš„æ¨¡å‹æ”¾åœ¨ GPU ä¸Šã€‚
            their_output = esm.cuda().infer([row[1] for row in sample_data])
            # ä½¿ç”¨æ¨¡å‹åœ¨ GPU ä¸Šè¿è¡Œæ¨ç†
            our_output = model.cuda()(
                input_ids=hf_tokens["input_ids"].cuda(), attention_mask=hf_tokens["attention_mask"].cuda()
            )
        else:
            # åœ¨æ¨¡å‹ä¸Šè¿è¡Œè¾“å…¥ä»¥ç”Ÿæˆè¾“å‡ºéšè—çŠ¶æ€
            our_output = model(**hf_tokens, output_hidden_states=True)
            # ä»è¾“å‡ºä¸­æå–é€»è¾‘å›å½’å±‚ç»“æœ
            our_output = our_output["logits"]
            if classification_head:
                # å¦‚æœæ˜¯åˆ†ç±»å¤´ï¼Œä½¿ç”¨ ESM æ¨¡å‹çš„å¤šåŠŸèƒ½è‡ªç„¶è¯­è¨€æ¨ç†åˆ†ç±»
                their_output = esm.model.classification_heads["mnli"](esm.extract_features(batch_tokens))
            else:
                # ä½¿ç”¨ ESM æ¨¡å‹å¯¹è¾“å…¥è¿›è¡Œæ¨ç†å¹¶è¿”å›é€»è¾‘å›å½’ç»“æœ
                their_output = esm(hf_tokens["input_ids"], repr_layers=list(range(999)))
                their_output = their_output["logits"]

        # å¦‚æœæ˜¯æŠ˜å æ¨¡å‹ï¼Œåˆ™è®¡ç®—ä½ç½®å·®çš„æœ€å¤§ç»å¯¹å€¼ï¼Œå¹¶æ£€æŸ¥è¾“å‡ºæ˜¯å¦å…¨éƒ¨æ¥è¿‘
        if is_folding_model:
            max_absolute_diff = torch.max(torch.abs(our_output["positions"] - their_output["positions"])).item()
            success = torch.allclose(our_output["positions"], their_output["positions"], atol=1e-5)
        else:
            # å¦åˆ™è®¡ç®—è¾“å‡ºå·®çš„æœ€å¤§ç»å¯¹å€¼ï¼Œå¹¶æ£€æŸ¥è¾“å‡ºæ˜¯å¦å…¨éƒ¨æ¥è¿‘
            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
            success = torch.allclose(our_output, their_output, atol=1e-5)

        # æ‰“å°æœ€å¤§ç»å¯¹å·®å¼‚çš„å€¼
        print(f"max_absolute_diff = {max_absolute_diff}")  # å¤§çº¦ä¸º 1e-5
        # æ‰“å°æ¨¡å‹æ˜¯å¦è¾“å‡ºç›¸åŒçš„å¼ é‡
        print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")

        # å¦‚æœæ²¡æœ‰æˆåŠŸåŒ¹é…è¾“å‡ºï¼Œåˆ™å¼•å‘å¼‚å¸¸
        if not success:
            raise Exception("Something went wRoNg")

        # å¦‚æœä¸æ˜¯æŠ˜å æ¨¡å‹ï¼Œè¿›è¡Œæ¥è§¦é¢„æµ‹æµ‹è¯•
        if not is_folding_model:
            # ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ¥è§¦ç‚¹
            our_output = model.predict_contacts(hf_tokens["input_ids"], hf_tokens["attention_mask"])
            # ä½¿ç”¨ ESM æ¨¡å‹é¢„æµ‹æ¥è§¦ç‚¹
            their_output = esm.predict_contacts(hf_tokens["input_ids"])
            # è®¡ç®—æ¥è§¦é¢„æµ‹çš„æœ€å¤§ç»å¯¹å€¼å·®å¼‚ï¼Œå¹¶æ£€æŸ¥è¾“å‡ºæ˜¯å¦å…¨éƒ¨æ¥è¿‘
            max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
            success = torch.allclose(our_output, their_output, atol=1e-5)

            # æ‰“å°æ¥è§¦é¢„æµ‹æµ‹è¯•ç»“æœ
            print("Contact prediction testing:")
            print(f"max_absolute_diff = {max_absolute_diff}")  # å¤§çº¦ä¸º 1e-5
            print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")

            # å¦‚æœæ²¡æœ‰æˆåŠŸåŒ¹é…è¾“å‡ºï¼Œåˆ™å¼•å‘å¼‚å¸¸
            if not success:
                raise Exception("Something went wRoNg")

        # åˆ›å»ºç›®å½•ä»¥ä¿å­˜ PyTorch æ¨¡å‹
        pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
        # æ‰“å°æ­£åœ¨ä¿å­˜æ¨¡å‹çš„æ¶ˆæ¯
        print(f"Saving model to {pytorch_dump_folder_path}")
        # ä¿å­˜æ¨¡å‹çš„é¢„è®­ç»ƒå‚æ•°åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)

        # åœ¨ç»§ç»­ä¹‹å‰é‡Šæ”¾éƒ¨åˆ†å†…å­˜
        del esm

    # æ‰“å°æ­£åœ¨ä¿å­˜åˆ†è¯å™¨çš„æ¶ˆæ¯
    print(f"Saving tokenizer to {pytorch_dump_folder_path}")
    # ä¿å­˜åˆ†è¯å™¨çš„é¢„è®­ç»ƒå‚æ•°åˆ°æŒ‡å®šè·¯å¾„
    hf_tokenizer.save_pretrained(pytorch_dump_folder_path)
    # å¦‚æœ push_to_repo ä¸ºçœŸï¼Œåˆ™æ‰§è¡Œä¸‹é¢çš„æ“ä½œ
    if push_to_repo:
        # è°ƒç”¨ model å¯¹è±¡çš„ push_to_hub æ–¹æ³•ï¼Œå°†æ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ä»“åº“
        model.push_to_hub(repo_id=push_to_repo, token_token=auth_token)
        # è°ƒç”¨ hf_tokenizer å¯¹è±¡çš„ push_to_hub æ–¹æ³•ï¼Œå°† tokenizer æ¨é€åˆ°æŒ‡å®šçš„ä»“åº“
        hf_tokenizer.push_to_hub(repo_id=push_to_repo, token_token=auth_token)
if __name__ == "__main__":
    # å¦‚æœè„šæœ¬ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡

    # å¿…å¡«å‚æ•°
    parser.add_argument(
        "--pytorch_dump_folder_path", type=str, required=True, help="Path to the output PyTorch model."
    )
    # å‚æ•°ï¼špytorch_dump_folder_pathï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œå¿…å¡«ï¼Œç”¨äºæŒ‡å®šè¾“å‡º PyTorch æ¨¡å‹çš„è·¯å¾„

    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # å‚æ•°ï¼šclassification_headï¼Œå¦‚æœå­˜åœ¨åˆ™è®¾ç½®ä¸º Trueï¼Œç”¨äºæŒ‡å®šæ˜¯å¦è½¬æ¢æœ€ç»ˆåˆ†ç±»å¤´éƒ¨

    parser.add_argument("--model", default=None, type=str, required=True, help="Name of model to convert.")
    # å‚æ•°ï¼šmodelï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸º Noneï¼Œå¿…å¡«ï¼Œç”¨äºæŒ‡å®šè¦è½¬æ¢çš„æ¨¡å‹çš„åç§°

    parser.add_argument("--push_to_repo", type=str, help="Repo to upload to (including username!).")
    # å‚æ•°ï¼špush_to_repoï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å®šè¦ä¸Šä¼ çš„ä»“åº“ï¼ˆåŒ…æ‹¬ç”¨æˆ·åï¼‰

    parser.add_argument("--auth_token", type=str, help="HuggingFace auth token.")
    # å‚æ•°ï¼šauth_tokenï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å®š HuggingFace çš„è®¤è¯ä»¤ç‰Œ

    args = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶å­˜å‚¨åˆ° args å¯¹è±¡ä¸­

    convert_esm_checkpoint_to_pytorch(
        args.model, args.pytorch_dump_folder_path, args.classification_head, args.push_to_repo, args.auth_token
    )
    # è°ƒç”¨å‡½æ•° convert_esm_checkpoint_to_pytorchï¼Œä¼ å…¥è§£æåçš„å‚æ•°æ¥æ‰§è¡Œæ¨¡å‹è½¬æ¢æ“ä½œ
```