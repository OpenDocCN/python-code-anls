# `.\models\vits\convert_original_checkpoint.py`

```
# è®¾ç½®ç¼–ç æ ¼å¼ä¸º UTF-8

# ç‰ˆæƒå£°æ˜å’Œè®¸å¯è¯ä¿¡æ¯ï¼ŒæŒ‡å®šäº† Apache License, Version 2.0 çš„ä½¿ç”¨æ¡ä»¶å’Œé™åˆ¶
# æ‚¨å¯ä»¥é€šè¿‡è®¿é—®æŒ‡å®šçš„ URL æŸ¥çœ‹è®¸å¯è¯çš„è¯¦ç»†å†…å®¹ï¼šhttp://www.apache.org/licenses/LICENSE-2.0

"""Convert VITS checkpoint."""

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import argparse  # è§£æå‘½ä»¤è¡Œå‚æ•°çš„åº“
import json  # å¤„ç† JSON æ ¼å¼æ•°æ®çš„åº“
import tempfile  # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å’Œç›®å½•çš„åº“

import torch  # PyTorch æ·±åº¦å­¦ä¹ åº“
from huggingface_hub import hf_hub_download  # Hugging Face Hub ä¸‹è½½æ¨¡å—

from transformers import VitsConfig, VitsModel, VitsTokenizer, logging  # Hugging Face Transformers åº“ä¸­çš„ç›¸å…³æ¨¡å—

# è®¾ç½®æ—¥å¿—çš„è¯¦ç»†ç¨‹åº¦ä¸º info çº§åˆ«
logging.set_verbosity_info()

# è·å–æˆ–åˆ›å»ºåä¸º "transformers.models.vits" çš„æ—¥å¿—è®°å½•å™¨å¯¹è±¡
logger = logging.get_logger("transformers.models.vits")

# å°† VITS æ¨¡å‹ä¸­æ–‡æœ¬ç¼–ç å™¨ç›¸å…³å‚æ•°çš„æ˜ å°„å®šä¹‰ä¸ºå­—å…¸
MAPPING_TEXT_ENCODER = {
    "enc_p.emb": "text_encoder.embed_tokens",
    "enc_p.encoder.attn_layers.*.conv_k": "text_encoder.encoder.layers.*.attention.k_proj",
    "enc_p.encoder.attn_layers.*.conv_v": "text_encoder.encoder.layers.*.attention.v_proj",
    "enc_p.encoder.attn_layers.*.conv_q": "text_encoder.encoder.layers.*.attention.q_proj",
    "enc_p.encoder.attn_layers.*.conv_o": "text_encoder.encoder.layers.*.attention.out_proj",
    "enc_p.encoder.attn_layers.*.emb_rel_k": "text_encoder.encoder.layers.*.attention.emb_rel_k",
    "enc_p.encoder.attn_layers.*.emb_rel_v": "text_encoder.encoder.layers.*.attention.emb_rel_v",
    "enc_p.encoder.norm_layers_1.*.gamma": "text_encoder.encoder.layers.*.layer_norm.weight",
    "enc_p.encoder.norm_layers_1.*.beta": "text_encoder.encoder.layers.*.layer_norm.bias",
    "enc_p.encoder.ffn_layers.*.conv_1": "text_encoder.encoder.layers.*.feed_forward.conv_1",
    "enc_p.encoder.ffn_layers.*.conv_2": "text_encoder.encoder.layers.*.feed_forward.conv_2",
    "enc_p.encoder.norm_layers_2.*.gamma": "text_encoder.encoder.layers.*.final_layer_norm.weight",
    "enc_p.encoder.norm_layers_2.*.beta": "text_encoder.encoder.layers.*.final_layer_norm.bias",
    "enc_p.proj": "text_encoder.project",
}

# å°† VITS æ¨¡å‹ä¸­éšæœºæŒç»­æ—¶é—´é¢„æµ‹å™¨ç›¸å…³å‚æ•°çš„æ˜ å°„å®šä¹‰ä¸ºå­—å…¸
MAPPING_STOCHASTIC_DURATION_PREDICTOR = {
    "dp.pre": "duration_predictor.conv_pre",
    "dp.proj": "duration_predictor.conv_proj",
    "dp.convs.convs_sep.*": "duration_predictor.conv_dds.convs_dilated.*",
    "dp.convs.convs_1x1.*": "duration_predictor.conv_dds.convs_pointwise.*",
    "dp.convs.norms_1.*.gamma": "duration_predictor.conv_dds.norms_1.*.weight",
    "dp.convs.norms_1.*.beta": "duration_predictor.conv_dds.norms_1.*.bias",
    "dp.convs.norms_2.*.gamma": "duration_predictor.conv_dds.norms_2.*.weight",
    "dp.convs.norms_2.*.beta": "duration_predictor.conv_dds.norms_2.*.bias",
    "dp.flows.0.logs": "duration_predictor.flows.0.log_scale",
    "dp.flows.0.m": "duration_predictor.flows.0.translate",
    "dp.flows.*.pre": "duration_predictor.flows.*.conv_pre",
}
    # å°†æ¨¡å‹å‚æ•°ä¸­çš„è·¯å¾„æ˜ å°„è½¬æ¢ä¸ºæ–°çš„è·¯å¾„ï¼Œç”¨äºæ¨¡å‹æƒé‡åŠ è½½å’Œè¿ç§»
    "dp.flows.*.proj": "duration_predictor.flows.*.conv_proj",
    # è½¬æ¢å·ç§¯å±‚çš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.convs_1x1.0": "duration_predictor.flows.*.conv_dds.convs_pointwise.0",
    # è½¬æ¢å·ç§¯å±‚çš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.convs_1x1.1": "duration_predictor.flows.*.conv_dds.convs_pointwise.1",
    # è½¬æ¢å·ç§¯å±‚çš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.convs_1x1.2": "duration_predictor.flows.*.conv_dds.convs_pointwise.2",
    # è½¬æ¢åˆ†ç¦»å·ç§¯å±‚çš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.convs_sep.0": "duration_predictor.flows.*.conv_dds.convs_dilated.0",
    # è½¬æ¢åˆ†ç¦»å·ç§¯å±‚çš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.convs_sep.1": "duration_predictor.flows.*.conv_dds.convs_dilated.1",
    # è½¬æ¢åˆ†ç¦»å·ç§¯å±‚çš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.convs_sep.2": "duration_predictor.flows.*.conv_dds.convs_dilated.2",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_1.0.gamma": "duration_predictor.flows.*.conv_dds.norms_1.0.weight",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_1.0.beta": "duration_predictor.flows.*.conv_dds.norms_1.0.bias",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_1.1.gamma": "duration_predictor.flows.*.conv_dds.norms_1.1.weight",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_1.1.beta": "duration_predictor.flows.*.conv_dds.norms_1.1.bias",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_1.2.gamma": "duration_predictor.flows.*.conv_dds.norms_1.2.weight",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_1.2.beta": "duration_predictor.flows.*.conv_dds.norms_1.2.bias",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_2.0.gamma": "duration_predictor.flows.*.conv_dds.norms_2.0.weight",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_2.0.beta": "duration_predictor.flows.*.conv_dds.norms_2.0.bias",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_2.1.gamma": "duration_predictor.flows.*.conv_dds.norms_2.1.weight",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_2.1.beta": "duration_predictor.flows.*.conv_dds.norms_2.1.bias",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_2.2.gamma": "duration_predictor.flows.*.conv_dds.norms_2.2.weight",
    # è½¬æ¢å½’ä¸€åŒ–å±‚çš„ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.flows.*.convs.norms_2.2.beta": "duration_predictor.flows.*.conv_dds.norms_2.2.bias",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_pre": "duration_predictor.post_conv_pre",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_proj": "duration_predictor.post_conv_proj",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„åˆ†ç¦»å·ç§¯å±‚è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_convs.convs_sep.*": "duration_predictor.post_conv_dds.convs_dilated.*",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„ 1x1 å·ç§¯å±‚è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_convs.convs_1x1.*": "duration_predictor.post_conv_dds.convs_pointwise.*",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å½’ä¸€åŒ–å±‚ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_convs.norms_1.*.gamma": "duration_predictor.post_conv_dds.norms_1.*.weight",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å½’ä¸€åŒ–å±‚ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_convs.norms_1.*.beta": "duration_predictor.post_conv_dds.norms_1.*.bias",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å½’ä¸€åŒ–å±‚ gamma å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_convs.norms_2.*.gamma": "duration_predictor.post_conv_dds.norms_2.*.weight",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å½’ä¸€åŒ–å±‚ beta å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_convs.norms_2.*.beta": "duration_predictor.post_conv_dds.norms_2.*.bias",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„ logs å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_flows.0.logs": "duration_predictor.post_flows.0.log_scale",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„ m å‚æ•°è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_flows.0.m": "duration_predictor.post_flows.0.translate",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å‰å¤„ç†è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_flows.*.pre": "duration_predictor.post_flows.*.conv_pre",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„æŠ•å½±è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_flows.*.proj": "duration_predictor.post_flows.*.conv_proj",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å·ç§¯å±‚è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_flows.*.convs.convs_1x1.0": "duration_predictor.post_flows.*.conv_dds.convs_pointwise.0",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å·ç§¯å±‚è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp.post_flows.*.convs.convs_1x1.1": "duration_predictor.post_flows.*.conv_dds.convs_pointwise.1",
    # è½¬æ¢åå¤„ç†é˜¶æ®µçš„å·ç§¯å±‚è·¯å¾„ï¼Œå°†åŸè·¯å¾„æ˜ å°„åˆ°æ–°çš„è·¯å¾„
    "dp
    # å®šä¹‰ä¸€ç»„æ˜ å°„å…³ç³»ï¼Œå°†æºå­—ç¬¦ä¸²è·¯å¾„æ˜ å°„åˆ°ç›®æ ‡å­—ç¬¦ä¸²è·¯å¾„
    "dp.post_flows.*.convs.convs_sep.0": "duration_predictor.post_flows.*.conv_dds.convs_dilated.0",
    "dp.post_flows.*.convs.convs_sep.1": "duration_predictor.post_flows.*.conv_dds.convs_dilated.1",
    "dp.post_flows.*.convs.convs_sep.2": "duration_predictor.post_flows.*.conv_dds.convs_dilated.2",
    # æ˜ å°„ gamma å‚æ•°çš„è·¯å¾„
    "dp.post_flows.*.convs.norms_1.0.gamma": "duration_predictor.post_flows.*.conv_dds.norms_1.0.weight",
    "dp.post_flows.*.convs.norms_1.0.beta": "duration_predictor.post_flows.*.conv_dds.norms_1.0.bias",
    "dp.post_flows.*.convs.norms_1.1.gamma": "duration_predictor.post_flows.*.conv_dds.norms_1.1.weight",
    "dp.post_flows.*.convs.norms_1.1.beta": "duration_predictor.post_flows.*.conv_dds.norms_1.1.bias",
    "dp.post_flows.*.convs.norms_1.2.gamma": "duration_predictor.post_flows.*.conv_dds.norms_1.2.weight",
    "dp.post_flows.*.convs.norms_1.2.beta": "duration_predictor.post_flows.*.conv_dds.norms_1.2.bias",
    "dp.post_flows.*.convs.norms_2.0.gamma": "duration_predictor.post_flows.*.conv_dds.norms_2.0.weight",
    "dp.post_flows.*.convs.norms_2.0.beta": "duration_predictor.post_flows.*.conv_dds.norms_2.0.bias",
    "dp.post_flows.*.convs.norms_2.1.gamma": "duration_predictor.post_flows.*.conv_dds.norms_2.1.weight",
    "dp.post_flows.*.convs.norms_2.1.beta": "duration_predictor.post_flows.*.conv_dds.norms_2.1.bias",
    "dp.post_flows.*.convs.norms_2.2.gamma": "duration_predictor.post_flows.*.conv_dds.norms_2.2.weight",
    "dp.post_flows.*.convs.norms_2.2.beta": "duration_predictor.post_flows.*.conv_dds.norms_2.2.bias",
    # æ˜ å°„æ¡ä»¶å‚æ•°è·¯å¾„
    "dp.cond": "duration_predictor.cond",  # num_speakers > 1
```python`
}
# å®šä¹‰ä¸€ä¸ªæ˜ å°„å­—å…¸ï¼Œç”¨äºå°†æŸäº›æƒé‡é”®æ˜ å°„åˆ°ä¸åŒçš„é”®
MAPPING_FLOW = {
    "flow.flows.*.pre": "flow.flows.*.conv_pre",  # å°† 'flow.flows.*.pre' æ˜ å°„åˆ° 'flow.flows.*.conv_pre'
    "flow.flows.*.enc.in_layers.0": "flow.flows.*.wavenet.in_layers.0",  # å°† 'flow.flows.*.enc.in_layers.0' æ˜ å°„åˆ° 'flow.flows.*.wavenet.in_layers.0'
    "flow.flows.*.enc.in_layers.1": "flow.flows.*.wavenet.in_layers.1",  # å°† 'flow.flows.*.enc.in_layers.1' æ˜ å°„åˆ° 'flow.flows.*.wavenet.in_layers.1'
    "flow.flows.*.enc.in_layers.2": "flow.flows.*.wavenet.in_layers.2",  # å°† 'flow.flows.*.enc.in_layers.2' æ˜ å°„åˆ° 'flow.flows.*.wavenet.in_layers.2'
    "flow.flows.*.enc.in_layers.3": "flow.flows.*.wavenet.in_layers.3",  # å°† 'flow.flows.*.enc.in_layers.3' æ˜ å°„åˆ° 'flow.flows.*.wavenet.in_layers.3'
    "flow.flows.*.enc.res_skip_layers.0": "flow.flows.*.wavenet.res_skip_layers.0",  # å°† 'flow.flows.*.enc.res_skip_layers.0' æ˜ å°„åˆ° 'flow.flows.*.wavenet.res_skip_layers.0'
    "flow.flows.*.enc.res_skip_layers.1": "flow.flows.*.wavenet.res_skip_layers.1",  # å°† 'flow.flows.*.enc.res_skip_layers.1' æ˜ å°„åˆ° 'flow.flows.*.wavenet.res_skip_layers.1'
    "flow.flows.*.enc.res_skip_layers.2": "flow.flows.*.wavenet.res_skip_layers.2",  # å°† 'flow.flows.*.enc.res_skip_layers.2' æ˜ å°„åˆ° 'flow.flows.*.wavenet.res_skip_layers.2'
    "flow.flows.*.enc.res_skip_layers.3": "flow.flows.*.wavenet.res_skip_layers.3",  # å°† 'flow.flows.*.enc.res_skip_layers.3' æ˜ å°„åˆ° 'flow.flows.*.wavenet.res_skip_layers.3'
    "flow.flows.*.enc.cond_layer": "flow.flows.*.wavenet.cond_layer",  # å½“ num_speakers > 1 æ—¶ï¼Œå°† 'flow.flows.*.enc.cond_layer' æ˜ å°„åˆ° 'flow.flows.*.wavenet.cond_layer'
    "flow.flows.*.post": "flow.flows.*.conv_post",  # å°† 'flow.flows.*.post' æ˜ å°„åˆ° 'flow.flows.*.conv_post'
}
# å®šä¹‰ä¸€ä¸ªæ˜ å°„å­—å…¸ï¼Œç”¨äºå°†ç”Ÿæˆå™¨çš„æƒé‡é”®æ˜ å°„åˆ°ä¸åŒçš„é”®
MAPPING_GENERATOR = {
    "dec.conv_pre": "decoder.conv_pre",  # å°† 'dec.conv_pre' æ˜ å°„åˆ° 'decoder.conv_pre'
    "dec.ups.0": "decoder.upsampler.0",  # å°† 'dec.ups.0' æ˜ å°„åˆ° 'decoder.upsampler.0'
    "dec.ups.1": "decoder.upsampler.1",  # å°† 'dec.ups.1' æ˜ å°„åˆ° 'decoder.upsampler.1'
    "dec.ups.2": "decoder.upsampler.2",  # å°† 'dec.ups.2' æ˜ å°„åˆ° 'decoder.upsampler.2'
    "dec.ups.3": "decoder.upsampler.3",  # å°† 'dec.ups.3' æ˜ å°„åˆ° 'decoder.upsampler.3'
    "dec.resblocks.*.convs1.0": "decoder.resblocks.*.convs1.0",  # å°† 'dec.resblocks.*.convs1.0' æ˜ å°„åˆ° 'decoder.resblocks.*.convs1.0'
    "dec.resblocks.*.convs1.1": "decoder.resblocks.*.convs1.1",  # å°† 'dec.resblocks.*.convs1.1' æ˜ å°„åˆ° 'decoder.resblocks.*.convs1.1'
    "dec.resblocks.*.convs1.2": "decoder.resblocks.*.convs1.2",  # å°† 'dec.resblocks.*.convs1.2' æ˜ å°„åˆ° 'decoder.resblocks.*.convs1.2'
    "dec.resblocks.*.convs2.0": "decoder.resblocks.*.convs2.0",  # å°† 'dec.resblocks.*.convs2.0' æ˜ å°„åˆ° 'decoder.resblocks.*.convs2.0'
    "dec.resblocks.*.convs2.1": "decoder.resblocks.*.convs2.1",  # å°† 'dec.resblocks.*.convs2.1' æ˜ å°„åˆ° 'decoder.resblocks.*.convs2.1'
    "dec.resblocks.*.convs2.2": "decoder.resblocks.*.convs2.2",  # å°† 'dec.resblocks.*.convs2.2' æ˜ å°„åˆ° 'decoder.resblocks.*.convs2.2'
    "dec.conv_post": "decoder.conv_post",  # å°† 'dec.conv_post' æ˜ å°„åˆ° 'decoder.conv_post'
    "dec.cond": "decoder.cond",  # å½“ num_speakers > 1 æ—¶ï¼Œå°† 'dec.cond' æ˜ å°„åˆ° 'decoder.cond'
}
# å®šä¹‰ä¸€ä¸ªæ˜ å°„å­—å…¸ï¼Œç”¨äºå°†åéªŒç¼–ç å™¨çš„æƒé‡é”®æ˜ å°„åˆ°ä¸åŒçš„é”®
MAPPING_POSTERIOR_ENCODER = {
    "enc_q.pre": "posterior_encoder.conv_pre",  # å°† 'enc_q.pre' æ˜ å°„åˆ° 'posterior_encoder.conv_pre'
    "enc_q.enc.in_layers.*": "posterior_encoder.wavenet.in_layers.*",  # å°† 'enc_q.enc.in_layers.*' æ˜ å°„åˆ° 'posterior_encoder.wavenet.in_layers.*'
    "enc_q.enc.res_skip_layers.*": "posterior_encoder.wavenet.res_skip_layers.*",  # å°† 'enc_q.enc.res_skip_layers.*' æ˜ å°„åˆ° 'posterior_encoder.wavenet.res_skip_layers.*'
    "enc_q.enc.cond_layer": "posterior_encoder.wavenet.cond_layer",  # å½“ num_speakers > 1 æ—¶ï¼Œå°† 'enc_q.enc.cond_layer' æ˜ å°„åˆ° 'posterior_encoder.wavenet.cond_layer'
    "enc_q.proj": "posterior_encoder.conv_proj",  # å°† 'enc_q.proj' æ˜ å°„åˆ° 'posterior_encoder.conv_proj'
}
# åˆå¹¶æ‰€æœ‰æ˜ å°„å­—å…¸
MAPPING = {
    **MAPPING_TEXT_ENCODER,  # å°† MAPPING_TEXT_ENCODER ä¸­çš„é”®å€¼å¯¹åŠ å…¥åˆ° MAPPING å­—å…¸ä¸­
    **MAPPING_STOCHASTIC_DURATION_PREDICTOR,  # å°† MAPPING_STOCHASTIC_DURATION_PREDICTOR ä¸­çš„é”®å€¼å¯¹åŠ å…¥åˆ° MAPPING å­—å…¸ä¸­
    **MAPPING_FLOW,  # å°† MAPPING_FLOW ä¸­çš„é”®å€¼å¯¹åŠ å…¥åˆ° MAPPING å­—å…¸ä¸­
    **MAPPING_GENERATOR,  # å°† MAPPING_GENERATOR ä¸­çš„é”®å€¼å¯¹åŠ å…¥åˆ° MAPPING å­—å…¸ä¸­
    **MAPPING_POSTERIOR_ENCODER,  # å°† MAPPING_POSTERIOR_ENCODER ä¸­çš„é”®å€¼å¯¹åŠ å…¥åˆ° MAPPING å­—å…¸ä¸­
    "emb_g": "embed_speaker",  # å½“ num_speakers > 1 æ—¶ï¼Œå°† 'emb_g' æ˜ å°„åˆ° 'embed_speaker'
}
# åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨é¡¶çº§é”®
TOP_LEVEL_KEYS = []
# åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å¿½ç•¥çš„é”®
IGNORE_KEYS = []


def set_recursively(hf_pointer, key, value, full_name, weight_type):
    # éå†é”®ï¼Œä¾æ¬¡è·å– hf_pointer å¯¹è±¡çš„å±æ€§
    for attribute in key.split("."):
        hf_pointer = getattr(hf_pointer, attribute)

    # è·å–æŒ‡å®šæƒé‡ç±»å‹çš„å½¢çŠ¶
    if weight_type is not None:
        hf_shape = getattr(hf_pointer, weight_type).shape
    else:
        hf_shape = hf_pointer.shape

    # å¦‚æœé”®ä»¥ç‰¹å®šåç¼€ç»“å°¾ï¼Œå°† value çš„æœ€åä¸€ä¸ªç»´åº¦å»æ‰ï¼ˆåŸå§‹æƒé‡æ˜¯ Conv1dï¼‰
    if key.endswith(".k_proj") or key.endswith(".v_proj") or key.endswith(".q_proj") or key.endswith(".out_proj"):
        value = value.squeeze(-1)

    # æ£€æŸ¥ hf_shape å’Œ value.shape æ˜¯å¦åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…åˆ™æŠ›å‡ºå¼‚å¸¸
    if hf_shape != value.shape:
        raise ValueError(
            f"Shape of hf {key + '.' + weight_type if weight_type is not None else ''} is {hf_shape}, but should be"
            f" {value.shape} for {full_name}"
        )

    # å¦‚æœ weight_type æ˜¯ 'weight'ï¼Œå°† hf_pointer çš„æƒé‡æ•°æ®```python
}
# æœ«å°¾å¤šä½™çš„å¤§æ‹¬å·ï¼Œå¯èƒ½æ˜¯ä»£ç ç‰‡æ®µå¤åˆ¶è¿‡ç¨‹ä¸­çš„é”™è¯¯

MAPPING_FLOW = {
    "flow.flows.*.pre": "flow.flows.*.conv_pre",
    "flow.flows.*.enc.in_layers.0": "flow.flows.*.wavenet.in_layers.0",
    "flow.flows.*.enc.in_layers.1": "flow.flows.*.wavenet.in_layers.1",
    "flow.flows.*.enc.in_layers.2": "flow.flows.*.wavenet.in_layers.2",
    "flow.flows.*.enc.in_layers.3": "flow.flows.*.wavenet.in_layers.3",
    "flow.flows.*.enc.res_skip_layers.0": "flow.flows.*.wavenet.res_skip_layers.0",
    "flow.flows.*.enc.res_skip_layers.1": "flow.flows.*.wavenet.res_skip_layers.1",
    "flow.flows.*.enc.res_skip_layers.2": "flow.flows.*.wavenet.res_skip_layers.2",
    "flow.flows.*.enc.res_skip_layers.3": "flow.flows.*.wavenet.res_skip_layers.3",
    "flow.flows.*.enc.cond_layer": "flow.flows.*.wavenet.cond_layer",  # å½“ num_speakers > 1 æ—¶ä½¿ç”¨
    # MAPPING_FLOW ä¸­çš„æ˜ å°„å…³ç³»ï¼Œç”¨äºæŒ‡å®šæµæ¨¡å‹çš„å±‚å¯¹åº”å…³ç³»
}

MAPPING_GENERATOR = {
    "dec.conv_pre": "decoder.conv_pre",
    "dec.ups.0": "decoder.upsampler.0",
    "dec.ups.1": "decoder.upsampler.1",
    "dec.ups.2": "decoder.upsampler.2",
    "dec.ups.3": "decoder.upsampler.3",
    "dec.resblocks.*.convs1.0": "decoder.resblocks.*.convs1.0",
    "dec.resblocks.*.convs1.1": "decoder.resblocks.*.convs1.1",
    "dec.resblocks.*.convs1.2": "decoder.resblocks.*.convs1.2",
    "dec.resblocks.*.convs2.0": "decoder.resblocks.*.convs2.0",
    "dec.resblocks.*.convs2.1": "decoder.resblocks.*.convs2.1",
    "dec.resblocks.*.convs2.2": "decoder.resblocks.*.convs2.2",
    "dec.conv_post": "decoder.conv_post",
    "dec.cond": "decoder.cond",  # å½“ num_speakers > 1 æ—¶ä½¿ç”¨
    # MAPPING_GENERATOR ä¸­```python
}
# æœ«å°¾å¤šä½™çš„å¤§æ‹¬å·ï¼Œå¯èƒ½æ˜¯ä»£ç ç‰‡æ®µå¤åˆ¶è¿‡ç¨‹ä¸­çš„é”™è¯¯

MAPPING_FLOW = {
    "flow.flows.*.pre": "flow.flows.*.conv_pre",
    "flow.flows.*.enc.in_layers.0": "flow.flows.*.wavenet.in_layers.0",
    "flow.flows.*.enc.in_layers.1": "flow.flows.*.wavenet.in_layers.1",
    "flow.flows.*.enc.in_layers.2": "flow.flows.*.wavenet.in_layers.2",
    "flow.flows.*.enc.in_layers.3": "flow.flows.*.wavenet.in_layers.3",
    "flow.flows.*.enc.res_skip_layers.0": "flow.flows.*.wavenet.res_skip_layers.0",
    "flow.flows.*.enc.res_skip_layers.1": "flow.flows.*.wavenet.res_skip_layers.1",
    "flow.flows.*.enc.res_skip_layers.2": "flow.flows.*.wavenet.res_skip_layers.2",
    "flow.flows.*.enc.res_skip_layers.3": "flow.flows.*.wavenet.res_skip_layers.3",
    "flow.flows.*.enc.cond_layer": "flow.flows.*.wavenet.cond_layer",  # å½“ num_speakers > 1 æ—¶ä½¿ç”¨
    # MAPPING_FLOW ä¸­çš„æ˜ å°„å…³ç³»ï¼Œç”¨äºæŒ‡å®šæµæ¨¡å‹çš„å±‚å¯¹åº”å…³ç³»
}

MAPPING_GENERATOR = {
    "dec.conv_pre": "decoder.conv_pre",
    "dec.ups.0": "decoder.upsampler.0",
    "dec.ups.1": "decoder.upsampler.1",
    "dec.ups.2": "decoder.upsampler.2",
    "dec.ups.3": "decoder.upsampler.3",
    "dec.resblocks.*.convs1.0": "decoder.resblocks.*.convs1.0",
    "dec.resblocks.*.convs1.1": "decoder.resblocks.*.convs1.1",
    "dec.resblocks.*.convs1.2": "decoder.resblocks.*.convs1.2",
    "dec.resblocks.*.convs2.0": "decoder.resblocks.*.convs2.0",
    "dec.resblocks.*.convs2.1": "decoder.resblocks.*.convs2.1",
    "dec.resblocks.*.convs2.2": "decoder.resblocks.*.convs2.2",
    "dec.conv_post": "decoder.conv_post",
    "dec.cond": "decoder.cond",  # å½“ num_speakers > 1 æ—¶ä½¿ç”¨
    # MAPPING_GENERATOR ä¸­çš„æ˜ å°„å…³ç³»ï¼Œç”¨äºæŒ‡å®šç”Ÿæˆå™¨æ¨¡å‹çš„å±‚å¯¹åº”å…³ç³»
}

MAPPING_POSTERIOR_ENCODER = {
    "enc_q.pre": "posterior_encoder.conv_pre",
    "enc_q.enc.in_layers.*": "posterior_encoder.wavenet.in_layers.*",
    "enc_q.enc.res_skip_layers.*": "posterior_encoder.wavenet.res_skip_layers.*",
    "enc_q.enc.cond_layer": "posterior_encoder.wavenet.cond_layer",  # å½“ num_speakers > 1 æ—¶ä½¿ç”¨
    # MAPPING_POSTERIOR_ENCODER ä¸­çš„æ˜ å°„å…³ç³»ï¼Œç”¨äºæŒ‡å®šåéªŒç¼–ç å™¨æ¨¡å‹çš„å±‚å¯¹åº”å…³ç³»
}

MAPPING = {
    **MAPPING_TEXT_ENCODER,
    **MAPPING_STOCHASTIC_DURATION_PREDICTOR,
    **MAPPING_FLOW,
    **MAPPING_GENERATOR,
    **MAPPING_POSTERIOR_ENCODER,
    "emb_g": "embed_speaker",  # å½“ num_speakers > 1 æ—¶ä½¿ç”¨
    # MAPPING åŒ…å«äº†æ‰€æœ‰æ¨¡å‹çš„æ˜ å°„å…³ç³»ï¼Œæ•´åˆäº†å„ä¸ªå­æ˜ å°„å­—å…¸
}

TOP_LEVEL_KEYS = []
IGNORE_KEYS = []


def set_recursively(hf_pointer, key, value, full_name, weight_type):
    # é€’å½’è®¾ç½® hf_pointer ä¸­æŒ‡å®šçš„ key å±æ€§å€¼ä¸º value

    for attribute in key.split("."):
        # é€šè¿‡å¾ªç¯é€çº§è·å–å±æ€§ï¼Œç›´åˆ°è¾¾åˆ°æŒ‡å®šçš„ key æ‰€åœ¨çš„å±æ€§ä½ç½®
        hf_pointer = getattr(hf_pointer, attribute)

    if weight_type is not None:
        # å¦‚æœæŒ‡å®šäº† weight_typeï¼Œåˆ™è·å–å¯¹åº”çš„å½¢çŠ¶ä¿¡æ¯
        hf_shape = getattr(hf_pointer, weight_type).shape
    else:
        # å¦åˆ™è·å–æ•´ä½“çš„å½¢çŠ¶ä¿¡æ¯
        hf_shape = hf_pointer.shape

    # å¦‚æœ key ä»¥ç‰¹å®šå­—ç¬¦ä¸²ç»“å°¾ï¼Œåˆ™å‹ç¼©æ‰æœ€åçš„æ ¸å¿ƒç»´åº¦ï¼ˆåŸå§‹æƒé‡ä¸º Conv1dï¼‰
    if key.endswith(".k_proj") or key.endswith(".v_proj") or key.endswith(".q_proj") or key.endswith(".out_proj"):
        value = value.squeeze(-1)

    # æ£€æŸ¥å€¼çš„å½¢çŠ¶æ˜¯å¦ä¸ hf_pointer çš„å½¢çŠ¶ç›¸åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…åˆ™æŠ›å‡º ValueError
    if hf_shape != value.shape:
        raise ValueError(
            f"Shape of hf {key + '.' + weight_type if weight_type is not None else ''} is {hf_shape}, but should be"
            f" {value.shape} for {full_name}"
        )

    if weight_type == "weight":
        # å¦‚æœ weight_type æ˜¯ 'weight'ï¼Œåˆ™å°† hf_pointer çš„æƒé‡æ•°æ®è®¾ä¸º value
        hf_pointer.weight.data = value
    # å¦‚æœæƒé‡ç±»å‹æ˜¯ "weight_g"ï¼Œåˆ™å°†å€¼èµ‹ç»™ç›¸åº”çš„ hf_pointer å¯¹è±¡çš„ weight_g å±æ€§
    elif weight_type == "weight_g":
        hf_pointer.weight_g.data = value
    # å¦‚æœæƒé‡ç±»å‹æ˜¯ "weight_v"ï¼Œåˆ™å°†å€¼èµ‹ç»™ç›¸åº”çš„ hf_pointer å¯¹è±¡çš„ weight_v å±æ€§
    elif weight_type == "weight_v":
        hf_pointer.weight_v.data = value
    # å¦‚æœæƒé‡ç±»å‹æ˜¯ "bias"ï¼Œåˆ™å°†å€¼èµ‹ç»™ç›¸åº”çš„ hf_pointer å¯¹è±¡çš„ bias å±æ€§
    elif weight_type == "bias":
        hf_pointer.bias.data = value
    # å¦‚æœæƒé‡ç±»å‹æ˜¯ "running_mean"ï¼Œåˆ™å°†å€¼èµ‹ç»™ç›¸åº”çš„ hf_pointer å¯¹è±¡çš„ running_mean å±æ€§
    elif weight_type == "running_mean":
        hf_pointer.running_mean.data = value
    # å¦‚æœæƒé‡ç±»å‹æ˜¯ "running_var"ï¼Œåˆ™å°†å€¼èµ‹ç»™ç›¸åº”çš„ hf_pointer å¯¹è±¡çš„ running_var å±æ€§
    elif weight_type == "running_var":
        hf_pointer.running_var.data = value
    # å¦‚æœæƒé‡ç±»å‹æ˜¯ "num_batches_tracked"ï¼Œåˆ™å°†å€¼èµ‹ç»™ç›¸åº”çš„ hf_pointer å¯¹è±¡çš„ num_batches_tracked å±æ€§
    elif weight_type == "num_batches_tracked":
        hf_pointer.num_batches_tracked.data = value
    # å¦‚æœæƒé‡ç±»å‹ä¸å±äºä»¥ä¸Šä»»ä½•ä¸€ç§æƒ…å†µï¼Œåˆ™å°†å€¼ç›´æ¥èµ‹ç»™ hf_pointer å¯¹è±¡çš„ data å±æ€§
    else:
        hf_pointer.data = value

    # è®°å½•åˆå§‹åŒ–æ—¥å¿—ä¿¡æ¯ï¼Œæè¿°å“ªä¸ªé”®çš„å“ªç§æƒé‡ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰ä»å®Œæ•´åç§° full_name åŠ è½½å¾—æ¥
    logger.info(f"{key + ('.' + weight_type if weight_type is not None else '')} was initialized from {full_name}.")
# æ£€æŸ¥ç»™å®šçš„åç§°æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥ï¼Œæ ¹æ®å¿½ç•¥è§„åˆ™åˆ—è¡¨ ignore_keys
def should_ignore(name, ignore_keys):
    for key in ignore_keys:
        # å¦‚æœè§„åˆ™ä»¥ ".*" ç»“å°¾ï¼Œæ£€æŸ¥åç§°æ˜¯å¦ä»¥å»æ‰æœ€åä¸€ä¸ªå­—ç¬¦çš„è§„åˆ™å¼€å¤´ï¼Œå¦‚æœæ˜¯åˆ™å¿½ç•¥è¯¥åç§°
        if key.endswith(".*"):
            if name.startswith(key[:-1]):
                return True
        # å¦‚æœè§„åˆ™ä¸­åŒ…å« ".*."ï¼Œåˆ™æŒ‰å‰ç¼€å’Œåç¼€è¿›è¡Œåˆ†å‰²ï¼Œæ£€æŸ¥åç§°ä¸­æ˜¯å¦åŒæ—¶åŒ…å«å‰ç¼€å’Œåç¼€ï¼Œå¦‚æœæ˜¯åˆ™å¿½ç•¥è¯¥åç§°
        elif ".*." in key:
            prefix, suffix = key.split(".*.")
            if prefix in name and suffix in name:
                return True
        # å¦åˆ™ï¼Œç›´æ¥æ£€æŸ¥åç§°æ˜¯å¦åŒ…å«è§„åˆ™ä¸­æŒ‡å®šçš„å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯åˆ™å¿½ç•¥è¯¥åç§°
        elif key in name:
            return True
    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œåˆ™ä¸å¿½ç•¥è¯¥åç§°
    return False


# é€’å½’åœ°åŠ è½½ Fairseq æ¨¡å‹çš„æƒé‡åˆ° Hugging Face æ¨¡å‹ä¸­
def recursively_load_weights(fairseq_dict, hf_model):
    unused_weights = []

    # éå† Fairseq æ¨¡å‹å­—å…¸ä¸­çš„æ¯ä¸ªåç§°å’Œå¯¹åº”çš„å€¼
    for name, value in fairseq_dict.items():
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥è¯¥åç§°çš„åŠ è½½
        if should_ignore(name, IGNORE_KEYS):
            # å¦‚æœéœ€è¦å¿½ç•¥ï¼Œè®°å½•æ—¥å¿—å¹¶ç»§ç»­ä¸‹ä¸€ä¸ªåç§°çš„å¤„ç†
            logger.info(f"{name} was ignored")
            continue

        is_used = False
        # éå†æ˜ å°„è§„åˆ™ MAPPING ä¸­çš„æ¯å¯¹é”®å€¼å¯¹
        for key, mapped_key in MAPPING.items():
            # å¦‚æœæ˜ å°„è§„åˆ™ä»¥ ".*" ç»“å°¾ï¼Œå»æ‰æœ€åä¸€ä¸ªå­—ç¬¦
            if key.endswith(".*"):
                key = key[:-1]
            # å¦‚æœæ˜ å°„è§„åˆ™ä¸­åŒ…å« "*"ï¼ŒæŒ‰ç…§å‰ç¼€å’Œåç¼€è¿›è¡Œåˆ†å‰²
            elif "*" in key:
                prefix, suffix = key.split(".*.")
                if prefix in name and suffix in name:
                    key = suffix

            # æ£€æŸ¥å½“å‰åç§°æ˜¯å¦åŒ¹é…æ˜ å°„è§„åˆ™ä¸­çš„é”®
            if key in name:
                is_used = True
                # æ ¹æ®æ˜ å°„è§„åˆ™ä¿®æ”¹ mapped_key ä¸­çš„ "*"ï¼Œç”¨åç§°ä¸­çš„ç´¢å¼•æ›¿æ¢
                if mapped_key.endswith(".*"):
                    layer_index = name.split(key)[-1].split(".")[0]
                    mapped_key = mapped_key.replace("*", layer_index)
                elif "*" in mapped_key:
                    layer_index = name.split(key)[0].split(".")[-2]

                    # æ ¹æ®ç‰¹å®šè§„åˆ™é‡æ–°æ˜ å°„å±‚ç´¢å¼•
                    if "flow.flows" in mapped_key:
                        layer_index = str(int(layer_index) // 2)
                    if "duration_predictor.flows" in mapped_key or "duration_predictor.post_flows" in mapped_key:
                        layer_index = str(int(layer_index) // 2 + 1)

                    mapped_key = mapped_key.replace("*", layer_index)
                
                # æ ¹æ®åç§°ä¸­çš„æ ‡è¯†ç¡®å®šæƒé‡ç±»å‹
                if "weight_g" in name:
                    weight_type = "weight_g"
                elif "weight_v" in name:
                    weight_type = "weight_v"
                elif "bias" in name:
                    weight_type = "bias"
                elif "weight" in name:
                    weight_type = "weight"
                elif "running_mean" in name:
                    weight_type = "running_mean"
                elif "running_var" in name:
                    weight_type = "running_var"
                elif "num_batches_tracked" in name:
                    weight_type = "num_batches_tracked"
                else:
                    weight_type = None
                
                # ä½¿ç”¨é€’å½’è®¾ç½®å‡½æ•°å°†å€¼åŠ è½½åˆ° Hugging Face æ¨¡å‹ä¸­çš„æŒ‡å®šä½ç½®
                set_recursively(hf_model, mapped_key, value, name, weight_type)
            continue
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ˜ å°„è§„åˆ™ï¼Œåˆ™è®°å½•ä¸ºæœªä½¿ç”¨çš„æƒé‡
        if not is_used:
            unused_weights.append(name)

    # è®°å½•æœªä½¿ç”¨çš„æƒé‡ä¿¡æ¯åˆ°æ—¥å¿—ä¸­
    logger.warning(f"Unused weights: {unused_weights}")


# ä½¿ç”¨ Torch çš„ no_grad è£…é¥°å™¨ï¼Œå°† PyTorch æ¨¡å‹æƒé‡è½¬æ¢ä¸º Transformers è®¾è®¡çš„å‡½æ•°
@torch.no_grad()
def convert_checkpoint(
    pytorch_dump_folder_path,
    checkpoint_path=None,
    config_path=None,
    vocab_path=None,
    language=None,
    num_speakers=None,
    sampling_rate=None,
    repo_id=None,
):
    """
    å°†æ¨¡å‹æƒé‡ä» PyTorch å¤åˆ¶/ç²˜è´´/è°ƒæ•´åˆ° Transformers è®¾è®¡ä¸­ã€‚
    """
    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œåˆ™ä»é¢„è®­ç»ƒé…ç½®ä¸­åŠ è½½é…ç½®
    if config_path is not None:
        config = VitsConfig.from_pretrained(config_path)
    else:
        # å¦åˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ VitsConfig å¯¹è±¡
        config = VitsConfig()

    # å¦‚æœæä¾›äº†è¯´è¯äººæ•°é‡ï¼Œåˆ™æ›´æ–°é…ç½®ä¸­çš„è¯´è¯äººæ•°é‡å’Œè¯´è¯äººåµŒå…¥å¤§å°
    if num_speakers:
        config.num_speakers = num_speakers
        config.speaker_embedding_size = 256

    # å¦‚æœæä¾›äº†é‡‡æ ·ç‡ï¼Œåˆ™æ›´æ–°é…ç½®ä¸­çš„é‡‡æ ·ç‡
    if sampling_rate:
        config.sampling_rate = sampling_rate

    # å¦‚æœæœªæä¾›æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œåˆ™ä¸‹è½½å¹¶å‡†å¤‡ Facebook MMS-TTS æ¨¡å‹æ‰€éœ€çš„è¯æ±‡è¡¨ã€é…ç½®æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹è·¯å¾„
    if checkpoint_path is None:
        logger.info(f"***Converting model: facebook/mms-tts {language}***")

        # ä¸‹è½½è¯æ±‡è¡¨
        vocab_path = hf_hub_download(
            repo_id="facebook/mms-tts",
            filename="vocab.txt",
            subfolder=f"models/{language}",
        )
        # ä¸‹è½½é…ç½®æ–‡ä»¶
        config_file = hf_hub_download(
            repo_id="facebook/mms-tts",
            filename="config.json",
            subfolder=f"models/{language}",
        )
        # ä¸‹è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        checkpoint_path = hf_hub_download(
            repo_id="facebook/mms-tts",
            filename="G_100000.pth",
            subfolder=f"models/{language}",
        )

        # è¯»å–å¹¶åŠ è½½é…ç½®æ–‡ä»¶ä¸­çš„è¶…å‚æ•°
        with open(config_file, "r") as f:
            data = f.read()
            hps = json.loads(data)

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦é’ˆå¯¹ uroman æ•°æ®é›†è®­ç»ƒï¼Œå¦‚æœæ˜¯åˆ™å‘å‡ºè­¦å‘Š
        is_uroman = hps["data"]["training_files"].split(".")[-1] == "uroman"
        if is_uroman:
            logger.warning("For this checkpoint, you should use `uroman` to convert input text before tokenizing it!")
    else:
        # å¦‚æœæä¾›äº†æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œåˆ™è®°å½•ä¿¡æ¯å¹¶è®¾ç½® is_uroman ä¸º False
        logger.info(f"***Converting model: {checkpoint_path}***")
        is_uroman = False

    # å¦‚æœè¯æ±‡è¡¨è·¯å¾„ä¸ºç©ºï¼Œåˆ™è®¾ç½®é»˜è®¤çš„ç¬¦å·åˆ—è¡¨å’Œç¬¦å·åˆ°ç´¢å¼•æ˜ å°„å…³ç³»
    if vocab_path is None:
        _pad = "_"
        _punctuation = ';:,.!?Â¡Â¿â€”â€¦"Â«Â»â€œâ€ '
        _letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
        _letters_ipa = "É‘ÉÉ’Ã¦É“Ê™Î²É”É•Ã§É—É–Ã°Ê¤É™É˜ÉšÉ›ÉœÉÉÉŸÊ„É¡É É¢Ê›É¦É§Ä§É¥ÊœÉ¨ÉªÊÉ­É¬É«É®ÊŸÉ±É¯É°Å‹É³É²É´Ã¸ÉµÉ¸Î¸Å“É¶Ê˜É¹ÉºÉ¾É»Ê€ÊÉ½Ê‚ÊƒÊˆÊ§Ê‰ÊŠÊ‹â±±ÊŒÉ£É¤ÊÏ‡ÊÊÊ‘ÊÊ’Ê”Ê¡Ê•Ê¢Ç€ÇÇ‚ÇƒËˆËŒËË‘Ê¼Ê´Ê°Ê±Ê²Ê·Ë Ë¤Ëâ†“â†‘â†’â†—â†˜'Ì©'áµ»"
        symbols = _pad + _punctuation + _letters + _letters_ipa
        symbol_to_id = {s: i for i, s in enumerate(symbols)}
        phonemize = True
    else:
        # å¦åˆ™ï¼Œä»ç»™å®šçš„è¯æ±‡è¡¨è·¯å¾„è¯»å–ç¬¦å·åˆ—è¡¨ï¼Œå¹¶åˆ›å»ºç¬¦å·åˆ°ç´¢å¼•æ˜ å°„å…³ç³»
        symbols = [line.replace("\n", "") for line in open(vocab_path, encoding="utf-8").readlines()]
        symbol_to_id = {s: i for i, s in enumerate(symbols)}
        # MMS-TTS æ¨¡å‹ä¸ä½¿ç”¨ <pad> æ ‡è®°ï¼Œæ‰€ä»¥å°†å…¶è®¾ç½®ä¸ºç”¨äºé—´éš”å­—ç¬¦çš„æ ‡è®°
        _pad = symbols[0]
        phonemize = False

    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œå°†ç¬¦å·åˆ°ç´¢å¼•æ˜ å°„å…³ç³»ä¿å­˜ä¸º JSON æ ¼å¼
    with tempfile.NamedTemporaryFile() as tf:
        with open(tf.name, "w", encoding="utf-8") as f:
            f.write(json.dumps(symbol_to_id, indent=2, sort_keys=True, ensure_ascii=False) + "\n")

        # æ ¹æ®ä¸´æ—¶æ–‡ä»¶ä¸­çš„ç¬¦å·åˆ°ç´¢å¼•æ˜ å°„å…³ç³»åˆ›å»ºä¸€ä¸ª VitsTokenizer å¯¹è±¡
        tokenizer = VitsTokenizer(tf.name, language=language, phonemize=phonemize, is_uroman=is_uroman, pad_token=_pad)

    # è®¾ç½®é…ç½®å¯¹è±¡ä¸­çš„è¯æ±‡è¡¨å¤§å°
    config.vocab_size = len(symbols)
    
    # åŸºäºé…ç½®å¯¹è±¡åˆ›å»º VitsModel æ¨¡å‹
    model = VitsModel(config)

    # å¯¹æ¨¡å‹çš„è§£ç å™¨åº”ç”¨æƒé‡å½’ä¸€åŒ–
    model.decoder.apply_weight_norm()

    # åŠ è½½åŸå§‹æ£€æŸ¥ç‚¹çš„æƒé‡åˆ°æ¨¡å‹ä¸­
    orig_checkpoint = torch.load(checkpoint_path, map_location=torch.device("cpu"))
    recursively_load_weights(orig_checkpoint["model"], model)

    # ç§»é™¤æ¨¡å‹çš„è§£ç å™¨ä¸Šçš„æƒé‡å½’ä¸€åŒ–
    model.decoder.remove_weight_norm()

    # å°†æ¨¡å‹å’Œ tokenizer çš„é¢„è®­ç»ƒæƒé‡å’Œè¯æ±‡è¡¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)
    tokenizer.save_pretrained(pytorch_dump_folder_path)
    # å¦‚æœ repo_id å­˜åœ¨ï¼ˆå³éç©ºï¼‰ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if repo_id:
        # æ‰“å°ä¿¡æ¯ï¼šæ­£åœ¨æ¨é€åˆ°ä¸­å¿ƒåº“...
        print("Pushing to the hub...")
        # è°ƒç”¨ tokenizer å¯¹è±¡çš„ push_to_hub æ–¹æ³•ï¼Œå°†æ¨¡å‹çš„ tokenizer æ¨é€åˆ°æŒ‡å®šçš„ repo_id
        tokenizer.push_to_hub(repo_id)
        # è°ƒç”¨ model å¯¹è±¡çš„ push_to_hub æ–¹æ³•ï¼Œå°†æ¨¡å‹æœ¬èº«æ¨é€åˆ°æŒ‡å®šçš„ repo_id
        model.push_to_hub(repo_id)
# ä¸»ç¨‹åºå…¥å£ï¼Œç”¨äºæ‰§è¡Œè„šæœ¬çš„å…¥å£ç‚¹
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šåŸå§‹æ£€æŸ¥ç‚¹çš„æœ¬åœ°è·¯å¾„
    parser.add_argument("--checkpoint_path", default=None, type=str, help="Local path to original checkpoint")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®švocab.txtæ–‡ä»¶çš„è·¯å¾„
    parser.add_argument("--vocab_path", default=None, type=str, help="Path to vocab.txt")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šå¾…è½¬æ¢æ¨¡å‹çš„hf config.jsonæ–‡ä»¶çš„è·¯å¾„
    parser.add_argument("--config_path", default=None, type=str, help="Path to hf config.json of model to convert")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šåˆ†è¯å™¨è¯­è¨€çš„ä¸‰å­—æ¯ä»£ç 
    parser.add_argument("--language", default=None, type=str, help="Tokenizer language (three-letter code)")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šè¯´è¯è€…çš„æ•°é‡
    parser.add_argument("--num_speakers", default=None, type=int, help="Number of speakers")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šæ¨¡å‹è®­ç»ƒæ—¶çš„é‡‡æ ·ç‡
    parser.add_argument(
        "--sampling_rate", default=None, type=int, help="Sampling rate on which the model was trained."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œå¿…éœ€å‚æ•°ï¼Œç”¨äºæŒ‡å®šè¾“å‡ºçš„PyTorchæ¨¡å‹çš„è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", required=True, default=None, type=str, help="Path to the output PyTorch model."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šè½¬æ¢åæ¨¡å‹ä¸Šä¼ è‡³ğŸ¤— hubçš„ä½ç½®
    parser.add_argument(
        "--push_to_hub", default=None, type=str, help="Where to upload the converted model on the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # è°ƒç”¨å‡½æ•°ï¼Œå°†æŒ‡å®šå‚æ•°ä¼ é€’ç»™convert_checkpointå‡½æ•°è¿›è¡Œæ£€æŸ¥ç‚¹è½¬æ¢
    convert_checkpoint(
        args.pytorch_dump_folder_path,
        args.checkpoint_path,
        args.config_path,
        args.vocab_path,
        args.language,
        args.num_speakers,
        args.sampling_rate,
        args.push_to_hub,
    )
```