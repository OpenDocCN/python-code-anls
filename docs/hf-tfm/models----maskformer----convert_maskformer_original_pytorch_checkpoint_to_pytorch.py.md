# `.\transformers\models\maskformer\convert_maskformer_original_pytorch_checkpoint_to_pytorch.py`

```
# è®¾ç½® Python æ–‡ä»¶çš„ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜ï¼šè¯¥ä»£ç ç‰ˆæƒå½’ Meta Platforms, Inc. å’Œ The HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼Œé™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯å‰¯æœ¬ï¼š
#     http://www.apache.org/licenses/LICENSE-2.0
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™è½¯ä»¶æ ¹æ®â€œåŸæ ·â€åˆ†å‘ï¼Œ
# æ²¡æœ‰ä»»ä½•æ‹…ä¿æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºçš„è¿˜æ˜¯æš—ç¤ºçš„
# æœ‰å…³è®¸å¯è¯çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è®¸å¯è¯
# å¯¼å…¥ sys æ¨¡å—ï¼Œç”¨äºä¸ Python è§£é‡Šå™¨è¿›è¡Œäº¤äº’
import sys
# ä» argparse æ¨¡å—å¯¼å…¥ ArgumentParser ç±»ï¼Œç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
from argparse import ArgumentParser
# ä» dataclasses æ¨¡å—å¯¼å…¥ dataclass è£…é¥°å™¨ï¼Œç”¨äºåˆ›å»ºä¸å¯å˜æ•°æ®ç±»
from dataclasses import dataclass
# ä» pathlib æ¨¡å—å¯¼å…¥ Path ç±»ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„
from pathlib import Path
# ä» pprint æ¨¡å—å¯¼å…¥ pformat å‡½æ•°ï¼Œç”¨äºæ ¼å¼åŒ–è¾“å‡º Python å¯¹è±¡
from pprint import pformat
# ä» typing æ¨¡å—å¯¼å…¥ Anyã€Dictã€Iteratorã€Listã€Setã€Tuple ç­‰ç±»å‹
from typing import Any, Dict, Iterator, List, Set, Tuple
# å¯¼å…¥ requests æ¨¡å—ï¼Œç”¨äºå‘é€ HTTP è¯·æ±‚
import requests
# å¯¼å…¥ torch æ¨¡å—ï¼Œç”¨äºæ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹
import torch
# ä» torchvision.transforms æ¨¡å—å¯¼å…¥ T æ¨¡å—ï¼Œç”¨äºå›¾åƒè½¬æ¢
import torchvision.transforms as T
# ä» detectron2.checkpoint æ¨¡å—å¯¼å…¥ DetectionCheckpointer ç±»ï¼Œç”¨äºåŠ è½½æ£€æŸ¥ç‚¹
from detectron2.checkpoint import DetectionCheckpointer
# ä» detectron2.config æ¨¡å—å¯¼å…¥ get_cfg å‡½æ•°ï¼Œç”¨äºè·å–é…ç½®å¯¹è±¡
from detectron2.config import get_cfg
# ä» detectron2.data æ¨¡å—å¯¼å…¥ MetadataCatalog ç±»ï¼Œç”¨äºç®¡ç†å…ƒæ•°æ®
from detectron2.data import MetadataCatalog
# ä» detectron2.projects.deeplab æ¨¡å—å¯¼å…¥ add_deeplab_config å‡½æ•°ï¼Œç”¨äºæ·»åŠ  DeepLab é…ç½®
from detectron2.projects.deeplab import add_deeplab_config
# ä» PIL æ¨¡å—å¯¼å…¥ Image ç±»ï¼Œç”¨äºå›¾åƒå¤„ç†
from PIL import Image
# ä» torch æ¨¡å—å¯¼å…¥ Tensorã€nn ç­‰ç±»ï¼Œç”¨äºæ„å»ºç¥ç»ç½‘ç»œ
from torch import Tensor, nn
# ä» transformers.models.maskformer.feature_extraction_maskformer æ¨¡å—å¯¼å…¥ MaskFormerImageProcessor ç±»ï¼Œç”¨äºç‰¹å¾æå–
from transformers.models.maskformer.feature_extraction_maskformer import MaskFormerImageProcessor
# ä» transformers.models.maskformer.modeling_maskformer æ¨¡å—å¯¼å…¥ MaskFormerConfigã€MaskFormerForInstanceSegmentationã€MaskFormerForInstanceSegmentationOutputã€MaskFormerModelã€MaskFormerModelOutput ç±»ï¼Œç”¨äº MaskFormer æ¨¡å‹
from transformers.models.maskformer.modeling_maskformer import (
    MaskFormerConfig,
    MaskFormerForInstanceSegmentation,
    MaskFormerForInstanceSegmentationOutput,
    MaskFormerModel,
    MaskFormerModelOutput,
)
# ä» transformers.utils æ¨¡å—å¯¼å…¥ logging å‡½æ•°ï¼Œç”¨äºè®°å½•æ—¥å¿—
from transformers.utils import logging

# å®šä¹‰ StateDict ç±»å‹
StateDict = Dict[str, Tensor]

# è®¾ç½®æ—¥å¿—è®°å½•çº§åˆ«ä¸ºä¿¡æ¯
logging.set_verbosity_info()
# è·å–è®°å½•å™¨å¯¹è±¡
logger = logging.get_logger()
# è®¾ç½®éšæœºç§å­ä¸º 0
torch.manual_seed(0)


# å®šä¹‰ TrackedStateDict ç±»
class TrackedStateDict:
    def __init__(self, to_track: Dict):
        """This class "tracks" a python dictionary by keeping track of which item is accessed.

        Args:
            to_track (Dict): The dictionary we wish to track
        """
        # åˆå§‹åŒ–å¯¹è±¡å±æ€§
        self.to_track = to_track
        self._seen: Set[str] = set()

    # è·å–å­—å…¸ä¸­æŒ‡å®šé”®çš„å€¼
    def __getitem__(self, key: str) -> Any:
        return self.to_track[key]

    # è®¾ç½®å­—å…¸ä¸­æŒ‡å®šé”®çš„å€¼
    def __setitem__(self, key: str, item: Any):
        self._seen.add(key)
        self.to_track[key] = item

    # è¿”å›å­—å…¸ä¸­æœªè¢«è®¿é—®çš„é”®çš„åˆ—è¡¨
    def diff(self) -> List[str]:
        """This method returns a set difference between the keys in the tracked state dict and the one we have access so far.
        This is an effective method to check if we have update all the keys

        Returns:
            List[str]: List of keys not yet updated
        """
        return set(self.to_track.keys()) - self._seen

    # è¿”å›å­—å…¸çš„å‰¯æœ¬
    def copy(self) -> Dict:
        # é€šè¿‡è°ƒç”¨å†…éƒ¨å­—å…¸çš„ copy æ–¹æ³•æ¥è·å–å‰¯æœ¬
        return self.to_track.copy()


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå‡†å¤‡å›¾åƒæ•°æ®
def prepare_img():
    # å›¾ç‰‡ URL åœ°å€
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # å‘é€ HTTP è¯·æ±‚è·å–å›¾åƒæ•°æ®
    img_data = requests.get(url, stream=True).raw
    # æ‰“å¼€å›¾åƒ
    im = Image.open(img_data)
    # è¿”å›å›¾åƒå¯¹è±¡
    return im


# å®šä¹‰ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç±»
@dataclass
class Args:
    """Fake command line arguments needed by maskformer/detectron implementation"""

    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_file: str
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºè®¾ç½®é…ç½®å‚æ•°ï¼Œæ¥å—ä¸€ä¸ªå‚æ•°å¯¹è±¡ Args
def setup_cfg(args: Args):
    # è·å–ä¸€ä¸ªç©ºçš„é…ç½®å¯¹è±¡
    cfg = get_cfg()
    # æ·»åŠ  DeepLab çš„é…ç½®åˆ° cfg
    add_deeplab_config(cfg)
    # æ·»åŠ  MaskFormer çš„é…ç½®åˆ° cfg
    add_mask_former_config(cfg)
    # ä»æ–‡ä»¶ä¸­åŠ è½½é…ç½®ï¼Œå¹¶åˆå¹¶å‘½ä»¤è¡Œå‚æ•°
    cfg.merge_from_file(args.config_file)
    # å†»ç»“é…ç½®ï¼Œä½¿å…¶ä¸å¯æ›´æ”¹
    cfg.freeze()
    # è¿”å›é…ç½®å¯¹è±¡
    return cfg


# å®šä¹‰ä¸€ä¸ªç±»ï¼Œç”¨äºå°†åŸå§‹çš„ MaskFormer é…ç½®è½¬æ¢ä¸ºæˆ‘ä»¬æ‰€éœ€çš„é…ç½®
class OriginalMaskFormerConfigToOursConverter:
    # å®ç°ç±»çš„è°ƒç”¨æ–¹æ³•ï¼Œæ¥å—ä¸€ä¸ªåŸå§‹é…ç½®å¯¹è±¡ï¼Œè¿”å›æˆ‘ä»¬éœ€è¦çš„ MaskFormerConfig å¯¹è±¡
    def __call__(self, original_config: object) -> MaskFormerConfig:
        # ä»åŸå§‹é…ç½®ä¸­è·å–æ¨¡å‹å¯¹è±¡
        model = original_config.MODEL
        # ä»æ¨¡å‹å¯¹è±¡ä¸­è·å– MaskFormer å¯¹è±¡
        mask_former = model.MASK_FORMER
        # ä»æ¨¡å‹å¯¹è±¡ä¸­è·å– Swin Transformer å¯¹è±¡
        swin = model.SWIN

        # ä»å…ƒæ•°æ®ç›®å½•ä¸­è·å–æ•°æ®é›†ç±»åˆ«ä¿¡æ¯
        dataset_catalog = MetadataCatalog.get(original_config.DATASETS.TEST[0])
        # æ„å»ºç±»åˆ« id åˆ°æ ‡ç­¾åç§°çš„å­—å…¸
        id2label = dict(enumerate(dataset_catalog.stuff_classes))
        # æ„å»ºæ ‡ç­¾åç§°åˆ°ç±»åˆ« id çš„å­—å…¸
        label2id = {label: idx for idx, label in id2label.items()}

        # æ„å»º MaskFormerConfig å¯¹è±¡
        config: MaskFormerConfig = MaskFormerConfig(
            # FPN ç‰¹å¾çš„ç»´åº¦å¤§å°
            fpn_feature_size=model.SEM_SEG_HEAD.CONVS_DIM,
            # Mask ç‰¹å¾çš„ç»´åº¦å¤§å°
            mask_feature_size=model.SEM_SEG_HEAD.MASK_DIM,
            # ç±»åˆ«æ•°é‡
            num_labels=model.SEM_SEG_HEAD.NUM_CLASSES,
            # æ— ç›®æ ‡æƒé‡
            no_object_weight=mask_former.NO_OBJECT_WEIGHT,
            # æŸ¥è¯¢å¯¹è±¡æ•°é‡
            num_queries=mask_former.NUM_OBJECT_QUERIES,
            # éª¨å¹²ç½‘ç»œé…ç½®
            backbone_config={
                "pretrain_img_size": swin.PRETRAIN_IMG_SIZE,
                "image_size": swin.PRETRAIN_IMG_SIZE,
                "in_channels": 3,
                "patch_size": swin.PATCH_SIZE,
                "embed_dim": swin.EMBED_DIM,
                "depths": swin.DEPTHS,
                "num_heads": swin.NUM_HEADS,
                "window_size": swin.WINDOW_SIZE,
                "drop_path_rate": swin.DROP_PATH_RATE,
                "model_type": "swin",
            },
            # Dice æŸå¤±æƒé‡
            dice_weight=mask_former.DICE_WEIGHT,
            # äº¤å‰ç†µæŸå¤±æƒé‡
            ce_weight=1.0,
            # Mask æŸå¤±æƒé‡
            mask_weight=mask_former.MASK_WEIGHT,
            # è§£ç å™¨é…ç½®
            decoder_config={
                "model_type": "detr",
                "max_position_embeddings": 1024,
                "encoder_layers": 6,
                "encoder_ffn_dim": 2048,
                "encoder_attention_heads": 8,
                "decoder_layers": mask_former.DEC_LAYERS,
                "decoder_ffn_dim": mask_former.DIM_FEEDFORWARD,
                "decoder_attention_heads": mask_former.NHEADS,
                "encoder_layerdrop": 0.0,
                "decoder_layerdrop": 0.0,
                "d_model": mask_former.HIDDEN_DIM,
                "dropout": mask_former.DROPOUT,
                "attention_dropout": 0.0,
                "activation_dropout": 0.0,
                "init_std": 0.02,
                "init_xavier_std": 1.0,
                "scale_embedding": False,
                "auxiliary_loss": False,
                "dilation": False,
                # é»˜è®¤é¢„è®­ç»ƒé…ç½®å€¼
            },
            # ç±»åˆ« id åˆ°æ ‡ç­¾åç§°çš„æ˜ å°„å­—å…¸
            id2label=id2label,
            # æ ‡ç­¾åç§°åˆ°ç±»åˆ« id çš„æ˜ å°„å­—å…¸
            label2id=label2id,
        )

        # è¿”å› MaskFormerConfig å¯¹è±¡
        return config


class OriginalMaskFormerConfigToImageProcessorConverter:
    # å¾…å®ç°çš„ç±»ï¼Œç”¨äºå°†åŸå§‹çš„ MaskFormer é…ç½®è½¬æ¢ä¸ºå›¾åƒå¤„ç†å™¨çš„é…ç½®
```  
    # __call__ æ–¹æ³•æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„å®ä¾‹æ–¹æ³•ï¼Œå…è®¸ç±»å®ä¾‹åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨
    def __call__(self, original_config: object) -> MaskFormerImageProcessor:
        # ä» original_config å¯¹è±¡ä¸­è·å– MODEL é…ç½®
        model = original_config.MODEL
        # ä» original_config å¯¹è±¡ä¸­è·å– INPUT é…ç½®
        model_input = original_config.INPUT
        # ä» MetadataCatalog è·å–æµ‹è¯•æ•°æ®é›†çš„ç›¸å…³ä¿¡æ¯
        dataset_catalog = MetadataCatalog.get(original_config.DATASETS.TEST[0])
    
        # è¿”å›ä¸€ä¸ª MaskFormerImageProcessor çš„å®ä¾‹ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹å‚æ•°è¿›è¡Œé…ç½®:
        return MaskFormerImageProcessor(
            # å°†æ¨¡å‹çš„åƒç´ å‡å€¼ä» 0-255 çš„èŒƒå›´è½¬æ¢åˆ° 0-1 çš„èŒƒå›´
            image_mean=(torch.tensor(model.PIXEL_MEAN) / 255).tolist(),
            # å°†æ¨¡å‹çš„åƒç´ æ ‡å‡†å·®ä» 0-255 çš„èŒƒå›´è½¬æ¢åˆ° 0-1 çš„èŒƒå›´
            image_std=(torch.tensor(model.PIXEL_STD) / 255).tolist(),
            # è®¾ç½®æµ‹è¯•æ—¶çš„æœ€å°è¾“å…¥å°ºå¯¸
            size=model_input.MIN_SIZE_TEST,
            # è®¾ç½®æµ‹è¯•æ—¶çš„æœ€å¤§è¾“å…¥å°ºå¯¸
            max_size=model_input.MAX_SIZE_TEST,
            # è®¾ç½®è¯­ä¹‰åˆ†å‰²çš„ç±»åˆ«æ•°
            num_labels=model.SEM_SEG_HEAD.NUM_CLASSES,
            # è®¾ç½®éœ€è¦å¿½ç•¥çš„æ ‡ç­¾
            ignore_index=dataset_catalog.ignore_label,
            # è®¾ç½®è¾“å…¥å°ºå¯¸çš„å¯¹é½å€¼ä¸º 32ï¼Œè¿™æ˜¯ Swin Transformer æ¨¡å‹çš„è¦æ±‚
            size_divisibility=32,
        )
class OriginalMaskFormerCheckpointToOursConverter:
    # å®šä¹‰ OriginalMaskFormerCheckpointToOursConverter ç±»
    def __init__(self, original_model: nn.Module, config: MaskFormerConfig):
        # åˆå§‹åŒ–å‡½æ•°ï¼Œæ¥å—åŸå§‹æ¨¡å‹å’Œé…ç½®å‚æ•°
        self.original_model = original_model
        # å°†åŸå§‹æ¨¡å‹ä¿å­˜åˆ°å®ä¾‹å˜é‡ä¸­
        self.config = config
        # å°†é…ç½®å‚æ•°ä¿å­˜åˆ°å®ä¾‹å˜é‡ä¸­

    def pop_all(self, renamed_keys: List[Tuple[str, str]], dst_state_dict: StateDict, src_state_dict: StateDict):
        # å®šä¹‰ pop_all æ–¹æ³•ï¼Œæ¥å—é‡å‘½åé”®å€¼å¯¹åˆ—è¡¨ã€ç›®æ ‡çŠ¶æ€å­—å…¸å’ŒæºçŠ¶æ€å­—å…¸ä½œä¸ºå‚æ•°
        for src_key, dst_key in renamed_keys:
            # éå†é‡å‘½åé”®å€¼å¯¹åˆ—è¡¨
            dst_state_dict[dst_key] = src_state_dict.pop(src_key)
            # å°†æºçŠ¶æ€å­—å…¸çš„é”®å€¼å¯¹å¼¹å‡ºå¹¶æ·»åŠ åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­

    def replace_pixel_module(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # å®šä¹‰ replace_pixel_module æ–¹æ³•ï¼Œæ¥å—ç›®æ ‡çŠ¶æ€å­—å…¸å’ŒæºçŠ¶æ€å­—å…¸ä½œä¸ºå‚æ•°
        dst_prefix: str = "pixel_level_module.decoder"
        # è®¾ç½®ç›®æ ‡çŠ¶æ€å­—å…¸å‰ç¼€
        src_prefix: str = "sem_seg_head.pixel_decoder"
        # è®¾ç½®æºçŠ¶æ€å­—å…¸å‰ç¼€

        self.replace_backbone(dst_state_dict, src_state_dict, self.config)
        # è°ƒç”¨ replace_backbone æ–¹æ³•ï¼Œæ›¿æ¢backboneéƒ¨åˆ†çš„æ¨¡å‹å‚æ•°

        def rename_keys_for_conv(detectron_conv: str, mine_conv: str):
            # å®šä¹‰å†…éƒ¨å‡½æ•° rename_keys_for_convï¼Œæ¥å—Detectronå’Œè‡ªå®šä¹‰å·ç§¯å±‚å‰ç¼€ä½œä¸ºå‚æ•°
            return [
                (f"{detectron_conv}.weight", f"{mine_conv}.0.weight"),
                # è¿”å›ä¸€ç»„é‡å‘½åé”®å€¼å¯¹
                (f"{detectron_conv}.norm.weight", f"{mine_conv}.1.weight"),
                (f"{detectron_conv}.norm.bias", f"{mine_conv}.1.bias"),
            ]

        renamed_keys = [
            (f"{src_prefix}.mask_features.weight", f"{dst_prefix}.mask_projection.weight"),
            # å®šä¹‰ä¸€ç»„é‡å‘½åé”®å€¼å¯¹
            (f"{src_prefix}.mask_features.bias", f"{dst_prefix}.mask_projection.bias"),
            # å®šä¹‰ä¸€ç»„é‡å‘½åé”®å€¼å¯¹
        ]

        renamed_keys.extend(rename_keys_for_conv(f"{src_prefix}.layer_4", f"{dst_prefix}.fpn.stem"))
        # æ‰©å±•é‡å‘½åé”®å€¼å¯¹åˆ—è¡¨ï¼Œç”¨äºé‡å‘½åconvolutionå±‚

        for src_i, dst_i in zip(range(3, 0, -1), range(0, 3)):
            # éå†å¾ªç¯ï¼Œå®šä¹‰src_iå’Œdst_i
            renamed_keys.extend(
                rename_keys_for_conv(f"{src_prefix}.adapter_{src_i}", f"{dst_prefix}.fpn.layers.{dst_i}.proj")
            )
            # æ‰©å±•é‡å‘½åé”®å€¼å¯¹åˆ—è¡¨ï¼Œç”¨äºé‡å‘½åadapterå’Œprojectionå±‚
            renamed_keys.extend(
                rename_keys_for_conv(f"{src_prefix}.layer_{src_i}", f"{dst_prefix}.fpn.layers.{dst_i}.block")
            )
            # æ‰©å±•é‡å‘½åé”®å€¼å¯¹åˆ—è¡¨ï¼Œç”¨äºé‡å‘½ålayerå’Œblockå±‚

        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
        # è°ƒç”¨pop_allæ–¹æ³•ï¼Œåº”ç”¨æ‰€æœ‰é‡å‘½åé”®å€¼å¯¹åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸å’ŒæºçŠ¶æ€å­—å…¸
        # é‡å‘½åä¼ å…¥çš„çŠ¶æ€å­—å…¸ä¸­çš„é”®å€¼å¯¹ï¼Œå°†æºçŠ¶æ€å­—å…¸ä¸­çš„æŒ‡å®šå‰ç¼€æ”¹ä¸ºç›®æ ‡çŠ¶æ€å­—å…¸ä¸­çš„æŒ‡å®šå‰ç¼€
        def rename_keys_in_detr_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict):
            # è®¾ç½®ç›®æ ‡çŠ¶æ€å­—å…¸çš„å‰ç¼€
            dst_prefix: str = "transformer_module.decoder"
            # è®¾ç½®æºçŠ¶æ€å­—å…¸çš„å‰ç¼€
            src_prefix: str = "sem_seg_head.predictor.transformer.decoder"
            # not sure why we are not popping direcetly here!
            # è¿™é‡Œåˆ—å‡ºäº†æ‰€æœ‰éœ€è¦é‡å‘½åçš„é”®å€¼å¯¹ï¼ˆåŸå§‹åç§°åœ¨å·¦è¾¹ï¼Œæˆ‘ä»¬çš„åç§°åœ¨å³è¾¹ï¼‰
            rename_keys = []
            # æ ¹æ®è§£ç å™¨å±‚æ•°è¿›è¡Œå¾ªç¯ï¼Œè·å–å„å±‚çš„å‚æ•°
            for i in range(self.config.decoder_config.decoder_layers):
                # è·å–self-attentionå±‚ä¸­çš„å‚æ•°ï¼Œå¹¶ä¿®æ”¹åç§°
                rename_keys.append(
                    (
                        f"{src_prefix}.layers.{i}.self_attn.out_proj.weight",
                        f"{dst_prefix}.layers.{i}.self_attn.out_proj.weight",
                    )
                )
                rename_keys.append(
                    (
                        f"{src_prefix}.layers.{i}.self_attn.out_proj.bias",
                        f"{dst_prefix}.layers.{i}.self_attn.out_proj.bias",
                    )
                )
                # è·å–multi-head attentionå±‚ä¸­çš„å‚æ•°ï¼Œå¹¶ä¿®æ”¹åç§°
                rename_keys.append(
                    (
                        f"{src_prefix}.layers.{i}.multihead_attn.out_proj.weight",
                        f"{dst_prefix}.layers.{i}.encoder_attn.out_proj.weight",
                    )
                )
                rename_keys.append(
                    (
                        f"{src_prefix}.layers.{i}.multihead_attn.out_proj.bias",
                        f"{dst_prefix}.layers.{i}.encoder_attn.out_proj.bias",
                    )
                )
                # è·å–çº¿æ€§å±‚1çš„å‚æ•°ï¼Œå¹¶ä¿®æ”¹åç§°
                rename_keys.append((f"{src_prefix}.layers.{i}.linear1.weight", f"{dst_prefix}.layers.{i}.fc1.weight"))
                rename_keys.append((f"{src_prefix}.layers.{i}.linear1.bias", f"{dst_prefix}.layers.{i}.fc1.bias"))
                # è·å–çº¿æ€§å±‚2çš„å‚æ•°ï¼Œå¹¶ä¿®æ”¹åç§°
                rename_keys.append((f"{src_prefix}.layers.{i}.linear2.weight", f"{dst_prefix}.layers.{i}.fc2.weight"))
                rename_keys.append((f"{src_prefix}.layers.{i}.linear2.bias", f"{dst_prefix}.layers.{i}.fc2.bias"))
                # è·å–layernormå±‚çš„å‚æ•°ï¼Œå¹¶ä¿®æ”¹åç§°
                rename_keys.append(
                    (f"{src_prefix}.layers.{i}.norm1.weight", f"{dst_prefix}.layers.{i}.self_attn_layer_norm.weight")
                )
                rename_keys.append(
                    (f"{src_prefix}.layers.{i}.norm1.bias", f"{dst_prefix}.layers.{i}.self_attn_layer_norm.bias")
                )
                rename_keys.append(
                    (f"{src_prefix}.layers.{i}.norm2.weight", f"{dst_prefix}.layers.{i}.encoder_attn_layer_norm.weight")
                )
                rename_keys.append(
                    (f"{src_prefix}.layers.{i}.norm2.bias", f"{dst_prefix}.layers.{i}.encoder_attn_layer_norm.bias")
                )
                rename_keys.append(
                    (f"{src_prefix}.layers.{i}.norm3.weight", f"{dst_prefix}.layers.{i}.final_layer_norm.weight")
                )
                rename_keys.append(
                    (f"{src_prefix}.layers.{i}.norm3.bias", f"{dst_prefix}.layers.{i}.final_layer_norm.bias")
                )

            # è¿”å›é‡å‘½ååçš„é”®å€¼å¯¹åˆ—è¡¨
            return rename_keys
    # æ›¿æ¢ DETR æ¨¡å‹çš„è§£ç å™¨ä¸­çš„æƒé‡å’Œåç½®
    def replace_q_k_v_in_detr_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # å£°æ˜ç›®æ ‡å’Œæºçš„çŠ¶æ€å­—å…¸å‰ç¼€
        dst_prefix: str = "transformer_module.decoder"
        src_prefix: str = "sem_seg_head.predictor.transformer.decoder"
        # éå†è§£ç å™¨å±‚ï¼Œæ ¹æ®å±‚æ•°æ›¿æ¢æƒé‡å’Œåç½®
        for i in range(self.config.decoder_config.decoder_layers):
            # è¯»å–è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®
            in_proj_weight = src_state_dict.pop(f"{src_prefix}.layers.{i}.self_attn.in_proj_weight")
            in_proj_bias = src_state_dict.pop(f"{src_prefix}.layers.{i}.self_attn.in_proj_bias")
            # å°†æŸ¥è¯¢ã€é”®å’Œå€¼ï¼ˆæŒ‰é¡ºåºï¼‰æ·»åŠ åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
            dst_state_dict[f"{dst_prefix}.layers.{i}.self_attn.q_proj.weight"] = in_proj_weight[:256, :]
            dst_state_dict[f"{dst_prefix}.layers.{i}.self_attn.q_proj.bias"] = in_proj_bias[:256]
            dst_state_dict[f"{dst_prefix}.layers.{i}.self_attn.k_proj.weight"] = in_proj_weight[256:512, :]
            dst_state_dict[f"{dst_prefix}.layers.{i}.self_attn.k_proj.bias"] = in_proj_bias[256:512]
            dst_state_dict[f"{dst_prefix}.layers.{i}.self_attn.v_proj.weight"] = in_proj_weight[-256:, :]
            dst_state_dict[f"{dst_prefix}.layers.{i}.self_attn.v_proj.bias"] = in_proj_bias[-256:]
            # è¯»å–äº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®
            in_proj_weight_cross_attn = src_state_dict.pop(f"{src_prefix}.layers.{i}.multihead_attn.in_proj_weight")
            in_proj_bias_cross_attn = src_state_dict.pop(f"{src_prefix}.layers.{i}.multihead_attn.in_proj_bias")
            # å°†æŸ¥è¯¢ã€é”®å’Œå€¼ï¼ˆæŒ‰é¡ºåºï¼‰æ·»åŠ åˆ°ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
            dst_state_dict[f"{dst_prefix}.layers.{i}.encoder_attn.q_proj.weight"] = in_proj_weight_cross_attn[:256, :]
            dst_state_dict[f"{dst_prefix}.layers.{i}.encoder_attn.q_proj.bias"] = in_proj_bias_cross_attn[:256]
            dst_state_dict[f"{dst_prefix}.layers.{i}.encoder_attn.k_proj.weight"] = in_proj_weight_cross_attn[256:512, :]
            dst_state_dict[f"{dst_prefix}.layers.{i}.encoder_attn.k_proj.bias"] = in_proj_bias_cross_attn[256:512]
            dst_state_dict[f"{dst_prefix}.layers.{i}.encoder_attn.v_proj.weight"] = in_proj_weight_cross_attn[-256:, :]
            dst_state_dict[f"{dst_prefix}.layers.{i}.encoder_attn.v_proj.bias"] = in_proj_bias_cross_attn[-256:]
    # æ›¿æ¢ DETR è§£ç å™¨çš„æƒé‡
    def replace_detr_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡æƒé‡çš„å‰ç¼€
        dst_prefix: str = "transformer_module.decoder"
        # æºæƒé‡çš„å‰ç¼€
        src_prefix: str = "sem_seg_head.predictor.transformer.decoder"
        
        # é‡å‘½å DETR è§£ç å™¨ä¸­çš„æƒé‡
        renamed_keys = self.rename_keys_in_detr_decoder(dst_state_dict, src_state_dict)
        
        # æ·»åŠ æ›´å¤šçš„æ˜ å°„å…³ç³»
        renamed_keys.extend(
            [
                (f"{src_prefix}.norm.weight", f"{dst_prefix}.layernorm.weight"),
                (f"{src_prefix}.norm.bias", f"{dst_prefix}.layernorm.bias"),
            ]
        )

        # ç§»é™¤æ‰€æœ‰æŒ‡å®šçš„æ˜ å°„å…³ç³»
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)

        # æ›¿æ¢ DETR è§£ç å™¨ä¸­çš„ Qã€Kã€V
        self.replace_q_k_v_in_detr_decoder(dst_state_dict, src_state_dict)

    # æ›¿æ¢ Transformer æ¨¡å—çš„æƒé‡
    def replace_transformer_module(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡æƒé‡çš„å‰ç¼€
        dst_prefix: str = "transformer_module"
        # æºæƒé‡çš„å‰ç¼€
        src_prefix: str = "sem_seg_head.predictor"

        # è°ƒç”¨æ›¿æ¢ DETR è§£ç å™¨çš„å‡½æ•°
        self.replace_detr_decoder(dst_state_dict, src_state_dict)

        # å®šä¹‰éœ€è¦é‡å‘½åçš„é”®å€¼å¯¹
        renamed_keys = [
            (f"{src_prefix}.query_embed.weight", f"{dst_prefix}.queries_embedder.weight"),
            (f"{src_prefix}.input_proj.weight", f"{dst_prefix}.input_projection.weight"),
            (f"{src_prefix}.input_proj.bias", f"{dst_prefix}.input_projection.bias"),
        ]

        # ç§»é™¤æ‰€æœ‰æŒ‡å®šçš„æ˜ å°„å…³ç³»
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)

    # æ›¿æ¢å®ä¾‹åˆ†å‰²æ¨¡å—çš„æƒé‡
    def replace_instance_segmentation_module(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # åœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰å‰ç¼€ï¼Œæ‰€ä»¥åé¢çš„é”®ä¸­è¦å»æ‰ "."
        dst_prefix: str = ""
        src_prefix: str = "sem_seg_head.predictor"

        # å®šä¹‰éœ€è¦é‡å‘½åçš„é”®å€¼å¯¹
        renamed_keys = [
            (f"{src_prefix}.class_embed.weight", f"{dst_prefix}class_predictor.weight"),
            (f"{src_prefix}.class_embed.bias", f"{dst_prefix}class_predictor.bias"),
        ]

        mlp_len = 3
        for i in range(mlp_len):
            renamed_keys.extend(
                [
                    (f"{src_prefix}.mask_embed.layers.{i}.weight", f"{dst_prefix}mask_embedder.{i}.0.weight"),
                    (f"{src_prefix}.mask_embed.layers.{i}.bias", f"{dst_prefix}mask_embedder.{i}.0.bias"),
                ]
            )
        # è¾“å‡ºæ—¥å¿—ä¿¡ï¿½ï¿½
        logger.info(f"Replacing keys {pformat(renamed_keys)}")
        # ç§»é™¤æ‰€æœ‰æŒ‡å®šçš„æ˜ å°„å…³ç³»
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)

    # è½¬æ¢æ¨¡å‹
    def convert(self, mask_former: MaskFormerModel) -> MaskFormerModel:
        # åˆ›å»ºç›®æ ‡æ¨¡å‹æƒé‡çŠ¶æ€å­—å…¸
        dst_state_dict = TrackedStateDict(mask_former.state_dict())
        # è·å–åŸå§‹æ¨¡å‹çš„æƒé‡çŠ¶æ€å­—å…¸
        src_state_dict = self.original_model.state_dict()

        # æ›¿æ¢åƒç´ æ¨¡å—çš„æƒé‡
        self.replace_pixel_module(dst_state_dict, src_state_dict)
        # æ›¿æ¢ Transformer æ¨¡å—çš„æƒé‡
        self.replace_transformer_module(dst_state_dict, src_state_dict)

        # è¾“å‡ºæ—¥å¿—ä¿¡æ¯
        logger.info(f"Missed keys are {pformat(dst_state_dict.diff())}")
        logger.info(f"Not copied keys are {pformat(src_state_dict.keys())}")
        logger.info("ğŸ™Œ Done")

        # åŠ è½½ç›®æ ‡æ¨¡å‹çš„æƒé‡
        mask_former.load_state_dict(dst_state_dict)

        return mask_former
    # å°†å®ä¾‹åˆ†å‰²æ¨¡å‹è½¬æ¢ä¸ºå¦ä¸€ä¸ªå®ä¾‹åˆ†å‰²æ¨¡å‹
    def convert_instance_segmentation(
        self, mask_former: MaskFormerForInstanceSegmentation
    ) -> MaskFormerForInstanceSegmentation:
        # åˆ›å»ºç›®æ ‡çŠ¶æ€å­—å…¸ï¼Œå¤åˆ¶åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        dst_state_dict = TrackedStateDict(mask_former.state_dict())
        # è·å–åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        src_state_dict = self.original_model.state_dict()

        # æ›¿æ¢å®ä¾‹åˆ†å‰²æ¨¡å‹çš„æ¨¡å—
        self.replace_instance_segmentation_module(dst_state_dict, src_state_dict)

        # åŠ è½½ç›®æ ‡çŠ¶æ€å­—å…¸åˆ°å®ä¾‹åˆ†å‰²æ¨¡å‹
        mask_former.load_state_dict(dst_state_dict)

        # è¿”å›è½¬æ¢åçš„å®ä¾‹åˆ†å‰²æ¨¡å‹
        return mask_former

    # é™æ€æ–¹æ³•ï¼Œç”¨äºéå†æ£€æŸ¥ç‚¹ç›®å½•å’Œé…ç½®ç›®å½•ï¼Œè¿”å›è¿­ä»£å™¨
    @staticmethod
    def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[Tuple[object, Path, Path]]:
        # è·å–æ£€æŸ¥ç‚¹ç›®å½•ä¸‹æ‰€æœ‰.pklæ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
        checkpoints: List[Path] = checkpoints_dir.glob("**/*.pkl")

        # éå†æ£€æŸ¥ç‚¹åˆ—è¡¨
        for checkpoint in checkpoints:
            # æ‰“å°ä¿¡æ¯ï¼Œè¡¨ç¤ºæ­£åœ¨è½¬æ¢è¯¥æ£€æŸ¥ç‚¹
            logger.info(f"ğŸ’ª Converting {checkpoint.stem}")
            # æŸ¥æ‰¾å…³è”çš„é…ç½®æ–‡ä»¶
            config: Path = config_dir / checkpoint.parents[0].stem / "swin" / f"{checkpoint.stem}.yaml"

            # è¿”å›é…ç½®æ–‡ä»¶è·¯å¾„å’Œæ£€æŸ¥ç‚¹è·¯å¾„çš„å…ƒç»„
            yield config, checkpoint
# å®šä¹‰ä¸€ä¸ªæµ‹è¯•å‡½æ•°ï¼Œç”¨äºæµ‹è¯•åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹
def test(original_model, our_model: MaskFormerForInstanceSegmentation, image_processor: MaskFormerImageProcessor):
    # ç¦ç”¨æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        # å°†åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        original_model = original_model.eval()
        our_model = our_model.eval()

        # å‡†å¤‡å›¾åƒæ•°æ®
        im = prepare_img()

        # å›¾åƒé¢„å¤„ç†æ“ä½œ
        tr = T.Compose(
            [
                T.Resize((384, 384)),
                T.ToTensor(),
                T.Normalize(
                    mean=torch.tensor([123.675, 116.280, 103.530]) / 255.0,
                    std=torch.tensor([58.395, 57.120, 57.375]) / 255.0,
                ),
            ],
        )

        # å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†æ“ä½œ
        x = tr(im).unsqueeze(0)

        # è·å–åŸå§‹æ¨¡å‹çš„éª¨å¹²ç‰¹å¾
        original_model_backbone_features = original_model.backbone(x.clone())

        # è·å–æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºï¼ŒåŒ…æ‹¬éšè—çŠ¶æ€
        our_model_output: MaskFormerModelOutput = our_model.model(x.clone(), output_hidden_states=True)

        # æ£€æŸ¥åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬æ¨¡å‹çš„éª¨å¹²ç‰¹å¾æ˜¯å¦ç›¸ä¼¼
        for original_model_feature, our_model_feature in zip(
            original_model_backbone_features.values(), our_model_output.encoder_hidden_states
        ):
            assert torch.allclose(
                original_model_feature, our_model_feature, atol=1e-3
            ), "The backbone features are not the same."

        # è·å–åŸå§‹æ¨¡å‹çš„åƒç´ è¾“å‡º
        original_model_pixel_out = original_model.sem_seg_head.pixel_decoder.forward_features(
            original_model_backbone_features
        )

        # æ£€æŸ¥åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬æ¨¡å‹çš„åƒç´ è¾“å‡ºæ˜¯å¦ç›¸ä¼¼
        assert torch.allclose(
            original_model_pixel_out[0], our_model_output.pixel_decoder_last_hidden_state, atol=1e-4
        ), "The pixel decoder feature are not the same"

        # æµ‹è¯•å®Œæ•´æ¨¡å‹
        original_model_out = original_model([{"image": x.squeeze(0)}])

        # è·å–åŸå§‹æ¨¡å‹çš„åˆ†å‰²ç»“æœ
        original_segmentation = original_model_out[0]["sem_seg"]

        # è·å–æˆ‘ä»¬æ¨¡å‹çš„åˆ†å‰²ç»“æœ
        our_model_out: MaskFormerForInstanceSegmentationOutput = our_model(x)

        # å¯¹æˆ‘ä»¬æ¨¡å‹çš„åˆ†å‰²ç»“æœè¿›è¡Œåå¤„ç†
        our_segmentation = image_processor.post_process_segmentation(our_model_out, target_size=(384, 384))

        # æ£€æŸ¥åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬æ¨¡å‹çš„åˆ†å‰²ç»“æœæ˜¯å¦ç›¸ä¼¼
        assert torch.allclose(
            original_segmentation, our_segmentation, atol=1e-3
        ), "The segmentation image is not the same."

        # è¾“å‡ºæµ‹è¯•é€šè¿‡ä¿¡æ¯
        logger.info("âœ… Test passed!")


# è·å–æ¨¡å‹åç§°å‡½æ•°
def get_name(checkpoint_file: Path):
    # è·å–æ¨¡å‹æ–‡ä»¶å
    model_name_raw: str = checkpoint_file.stem
    # çˆ¶ç›®å½•åç§°
    parent_name: str = checkpoint_file.parents[0].stem
    backbone = "swin"
    dataset = ""
    # æ ¹æ®çˆ¶ç›®å½•åç§°ç¡®å®šæ•°æ®é›†ç±»å‹
    if "coco" in parent_name:
        dataset = "coco"
    elif "ade" in parent_name:
        dataset = "ade"
    else:
        raise ValueError(f"{parent_name} must be wrong since we didn't find 'coco' or 'ade' in it ")

    # å®šä¹‰éª¨å¹²ç±»å‹åˆ—è¡¨
    backbone_types = ["tiny", "small", "base", "large"]

    # è·å–æ¨¡å‹åç§°ä¸­çš„éª¨å¹²ç±»å‹
    backbone_type = list(filter(lambda x: x in model_name_raw, backbone_types))[0]

    # ç»„åˆæ¨¡å‹åç§°
    model_name = f"maskformer-{backbone}-{backbone_type}-{dataset}"

    return model_name


# ä¸»å‡½æ•°å…¥å£
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = ArgumentParser(
        description="Command line to convert the original maskformers (with swin backbone) to our implementations."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šæ¨¡å‹æ£€æŸ¥ç‚¹çš„ç›®å½•
    parser.add_argument(
        "--checkpoints_dir",
        type=Path,
        help=(
            "A directory containing the model's checkpoints. The directory has to have the following structure:"
            " <DIR_NAME>/<DATASET_NAME>/<CONFIG_NAME>.pkl"
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šæ¨¡å‹é…ç½®æ–‡ä»¶çš„ç›®å½•
    parser.add_argument(
        "--configs_dir",
        type=Path,
        help=(
            "A directory containing the model's configs, see detectron2 doc. The directory has to have the following"
            " structure: <DIR_NAME>/<DATASET_NAME>/<CONFIG_NAME>.yaml"
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path",
        required=True,
        type=Path,
        help="Path to the folder to output PyTorch models.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®š MaskFormer åŸå§‹å®ç°çš„ç›®å½•è·¯å¾„
    parser.add_argument(
        "--maskformer_dir",
        required=True,
        type=Path,
        help=(
            "A path to MaskFormer's original implementation directory. You can download from here:"
            " https://github.com/facebookresearch/MaskFormer"
        ),
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # å°†å‘½ä»¤è¡Œå‚æ•°è½¬æ¢ä¸º Path ç±»å‹
    checkpoints_dir: Path = args.checkpoints_dir
    config_dir: Path = args.configs_dir
    save_directory: Path = args.pytorch_dump_folder_path
    maskformer_dir: Path = args.maskformer_dir
    # å°† MaskFormer ç›®å½•çš„çˆ¶ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­
    sys.path.append(str(maskformer_dir.parent))
    # å¯¼å…¥æ‰€éœ€çš„æ¨¡å—
    from MaskFormer.mask_former import add_mask_former_config
    from MaskFormer.mask_former.mask_former_model import MaskFormer as OriginalMaskFormer

    # å¦‚æœè¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
    if not save_directory.exists():
        save_directory.mkdir(parents=True)

    # éå†åŸå§‹ MaskFormer æ£€æŸ¥ç‚¹å’Œé…ç½®æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºå½“å‰æ¨¡å‹æ‰€éœ€çš„æ ¼å¼
    for config_file, checkpoint_file in OriginalMaskFormerCheckpointToOursConverter.using_dirs(
        checkpoints_dir, config_dir
        ):
        # ä½¿ç”¨ OriginalMaskFormerConfigToImageProcessorConverter è½¬æ¢å™¨å°†é…ç½®æ–‡ä»¶è½¬æ¢ä¸ºå›¾åƒå¤„ç†å™¨
        image_processor = OriginalMaskFormerConfigToImageProcessorConverter()(setup_cfg(Args(config_file=config_file)))

        # æ ¹æ®é…ç½®æ–‡ä»¶è®¾ç½®åŸå§‹é…ç½®
        original_config = setup_cfg(Args(config_file=config_file))
        # ä»åŸå§‹é…ç½®ä¸­è·å– MaskFormer çš„å‚æ•°
        mask_former_kwargs = OriginalMaskFormer.from_config(original_config)

        # æ ¹æ® MaskFormer çš„å‚æ•°åˆ›å»ºåŸå§‹ MaskFormer æ¨¡å‹
        original_model = OriginalMaskFormer(**mask_former_kwargs).eval()

        # åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ°åŸå§‹æ¨¡å‹
        DetectionCheckpointer(original_model).load(str(checkpoint_file))

        # å°†åŸå§‹é…ç½®è½¬æ¢ä¸º MaskFormerConfig ç±»å‹çš„é…ç½®
        config: MaskFormerConfig = OriginalMaskFormerConfigToOursConverter()(original_config)

        # æ ¹æ®é…ç½®åˆ›å»º MaskFormerModel æ¨¡å‹
        mask_former = MaskFormerModel(config=config).eval()

        # åˆ›å»ºåŸå§‹ MaskFormer æ¨¡å‹åˆ°æˆ‘ä»¬çš„ MaskFormer æ¨¡å‹çš„è½¬æ¢å™¨
        converter = OriginalMaskFormerCheckpointToOursConverter(original_model, config)

        # å°†åŸå§‹ MaskFormer æ¨¡å‹è½¬æ¢ä¸ºæˆ‘ä»¬çš„ MaskFormer æ¨¡å‹
        maskformer = converter.convert(mask_former)

        # åˆ›å»ºç”¨äºå®ä¾‹åˆ†å‰²çš„ MaskFormerForInstanceSegmentation æ¨¡å‹
        mask_former_for_instance_segmentation = MaskFormerForInstanceSegmentation(config=config).eval()

        # å°† mask_former è®¾ç½®ä¸º mask_former_for_instance_segmentation çš„æ¨¡å‹
        mask_former_for_instance_segmentation.model = mask_former
        # å°† mask_former_for_instance_segmentation è½¬æ¢ä¸ºæˆ‘ä»¬çš„å®ä¾‹åˆ†å‰²æ¨¡å‹
        mask_former_for_instance_segmentation = converter.convert_instance_segmentation(
            mask_former_for_instance_segmentation
        )

        # æµ‹è¯•åŸå§‹æ¨¡å‹å’Œå®ä¾‹åˆ†å‰²æ¨¡å‹
        test(original_model, mask_former_for_instance_segmentation, image_processor)

        # è·å–æ¨¡å‹åç§°
        model_name = get_name(checkpoint_file)
        logger.info(f"ğŸª„ Saving {model_name}")

        # ä¿å­˜å›¾åƒå¤„ç†å™¨å’Œå®ä¾‹åˆ†å‰²æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
        image_processor.save_pretrained(save_directory / model_name)
        mask_former_for_instance_segmentation.save_pretrained(save_directory / model_name)

        # å°†å›¾åƒå¤„ç†å™¨æ¨é€åˆ° Hub
        image_processor.push_to_hub(
            repo_path_or_name=save_directory / model_name,
            commit_message="Add model",
            use_temp_dir=True,
        )
        # å°†å®ä¾‹åˆ†å‰²æ¨¡å‹æ¨é€åˆ° Hub
        mask_former_for_instance_segmentation.push_to_hub(
            repo_path_or_name=save_directory / model_name,
            commit_message="Add model",
            use_temp_dir=True,
        )
```