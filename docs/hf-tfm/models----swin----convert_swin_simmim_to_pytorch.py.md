# `.\models\swin\convert_swin_simmim_to_pytorch.py`

```py
# ç¼–ç å£°æ˜ï¼ŒæŒ‡å®šä½¿ç”¨ UTF-8 ç¼–ç æ ¼å¼
# Copyright 2022 The HuggingFace Inc. team.
# ç‰ˆæƒå£°æ˜ï¼Œç‰ˆæƒå½’ The HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰
#
# æ ¹æ® Apache è®¸å¯è¯ç‰ˆæœ¬ 2.0 è®¸å¯ä½¿ç”¨æ­¤æ–‡ä»¶ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„æ¡æ¬¾ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯çš„å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æŒ‰â€œåŸæ ·â€åˆ†å‘è½¯ä»¶
# æ— ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºæ‹…ä¿æˆ–æ¡ä»¶
# è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šçš„è¯­è¨€æƒé™å’Œé™åˆ¶

"""ä»åŸå§‹å­˜å‚¨åº“ä¸­è½¬æ¢ Swin SimMIM æ£€æŸ¥ç‚¹ã€‚

URL: https://github.com/microsoft/Swin-Transformer/blob/main/MODELHUB.md#simmim-pretrained-swin-v1-models"""

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import argparse  # å‚æ•°è§£ææ¨¡å—

import requests  # HTTP è¯·æ±‚åº“
import torch  # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from PIL import Image  # Python å›¾åƒå¤„ç†åº“

from transformers import SwinConfig, SwinForMaskedImageModeling, ViTImageProcessor  # å¯¼å…¥ Transformers åº“ä¸­çš„ç±»


def get_swin_config(model_name):
    # æ ¹æ®æ¨¡å‹åç§°è·å– Swin æ¨¡å‹é…ç½®
    config = SwinConfig(image_size=192)

    if "base" in model_name:
        # å¦‚æœæ¨¡å‹åç§°åŒ…å«â€œbaseâ€ï¼Œè®¾ç½®ç‰¹å®šå‚æ•°
        window_size = 6
        embed_dim = 128
        depths = (2, 2, 18, 2)
        num_heads = (4, 8, 16, 32)
    elif "large" in model_name:
        # å¦‚æœæ¨¡å‹åç§°åŒ…å«â€œlargeâ€ï¼Œè®¾ç½®ç‰¹å®šå‚æ•°
        window_size = 12
        embed_dim = 192
        depths = (2, 2, 18, 2)
        num_heads = (6, 12, 24, 48)
    else:
        # æŠ›å‡ºé”™è¯¯ï¼Œä»…æ”¯æŒâ€œbaseâ€å’Œâ€œlargeâ€å˜ä½“çš„æ¨¡å‹
        raise ValueError("Model not supported, only supports base and large variants")

    # è®¾ç½®é…ç½®å¯¹è±¡çš„å‚æ•°
    config.window_size = window_size
    config.embed_dim = embed_dim
    config.depths = depths
    config.num_heads = num_heads

    return config


def rename_key(name):
    # é‡å‘½åæ¨¡å‹çš„é”®åç§°ï¼Œä»¥ä¾¿é€‚åº” Swin æ¨¡å‹çš„ç»“æ„
    if "encoder.mask_token" in name:
        name = name.replace("encoder.mask_token", "embeddings.mask_token")
    if "encoder.patch_embed.proj" in name:
        name = name.replace("encoder.patch_embed.proj", "embeddings.patch_embeddings.projection")
    if "encoder.patch_embed.norm" in name:
        name = name.replace("encoder.patch_embed.norm", "embeddings.norm")
    if "attn.proj" in name:
        name = name.replace("attn.proj", "attention.output.dense")
    if "attn" in name:
        name = name.replace("attn", "attention.self")
    if "norm1" in name:
        name = name.replace("norm1", "layernorm_before")
    if "norm2" in name:
        name = name.replace("norm2", "layernorm_after")
    if "mlp.fc1" in name:
        name = name.replace("mlp.fc1", "intermediate.dense")
    if "mlp.fc2" in name:
        name = name.replace("mlp.fc2", "output.dense")

    if name == "encoder.norm.weight":
        name = "layernorm.weight"
    if name == "encoder.norm.bias":
        name = "layernorm.bias"

    # å¦‚æœä¸åŒ…å«â€œdecoderâ€ï¼Œåˆ™æ·»åŠ å‰ç¼€â€œswin.â€
    if "decoder" in name:
        pass
    else:
        name = "swin." + name

    return name


def convert_state_dict(orig_state_dict, model):
    # å®šä¹‰å‡½æ•°ï¼Œç”¨äºè½¬æ¢æ¨¡å‹çŠ¶æ€å­—å…¸
    # éå†åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„é”®åˆ—è¡¨å‰¯æœ¬
    for key in orig_state_dict.copy().keys():
        # å¼¹å‡ºå½“å‰é”®å¯¹åº”çš„å€¼
        val = orig_state_dict.pop(key)

        # å¦‚æœé”®ååŒ…å« "attn_mask"ï¼Œåˆ™è·³è¿‡ä¸å¤„ç†
        if "attn_mask" in key:
            pass
        # å¦‚æœé”®ååŒ…å« "qkv"
        elif "qkv" in key:
            # æ ¹æ® "." åˆ†å‰²é”®åï¼Œæå–å±‚å·å’Œå—å·
            key_split = key.split(".")
            layer_num = int(key_split[2])
            block_num = int(key_split[4])
            # è·å–å½“å‰æ³¨æ„åŠ›å±‚çš„ç»´åº¦ä¿¡æ¯
            dim = model.swin.encoder.layers[layer_num].blocks[block_num].attention.self.all_head_size

            # å¦‚æœé”®ååŒ…å« "weight"
            if "weight" in key:
                # æ›´æ–°é”®åå’Œå¯¹åº”çš„å€¼åˆ°åŸå§‹çŠ¶æ€å­—å…¸ä¸­ï¼Œåˆ†åˆ«æ›´æ–°æŸ¥è¯¢ã€é”®ã€å€¼çš„æƒé‡éƒ¨åˆ†
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.weight"
                ] = val[:dim, :]
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.weight"
                ] = val[dim : dim * 2, :]
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.weight"
                ] = val[-dim:, :]
            else:
                # æ›´æ–°é”®åå’Œå¯¹åº”çš„å€¼åˆ°åŸå§‹çŠ¶æ€å­—å…¸ä¸­ï¼Œåˆ†åˆ«æ›´æ–°æŸ¥è¯¢ã€é”®ã€å€¼çš„åç½®éƒ¨åˆ†
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.bias"
                ] = val[:dim]
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.bias"
                ] = val[dim : dim * 2]
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.bias"
                ] = val[-dim:]
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å°†é”®åè½¬æ¢åæ›´æ–°åˆ°åŸå§‹çŠ¶æ€å­—å…¸ä¸­
            orig_state_dict[rename_key(key)] = val

    # è¿”å›æ›´æ–°åçš„åŸå§‹çŠ¶æ€å­—å…¸
    return orig_state_dict
# å¯¼å…¥å¿…è¦çš„åº“
import argparse
import requests
from PIL import Image
import torch
from transformers import SwinForMaskedImageModeling, ViTImageProcessor

# å®šä¹‰å‡½æ•°ï¼Œç”¨äºå°†æŒ‡å®šçš„ Swin æ¨¡å‹æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
def convert_swin_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub):
    # åŠ è½½æŒ‡å®šè·¯å¾„çš„æ£€æŸ¥ç‚¹ï¼Œå¹¶ä»ä¸­æå–æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    state_dict = torch.load(checkpoint_path, map_location="cpu")["model"]

    # è·å–æŒ‡å®šæ¨¡å‹åç§°çš„é…ç½®
    config = get_swin_config(model_name)
    # æ ¹æ®é…ç½®åˆ›å»º Swin æ¨¡å‹å¯¹è±¡
    model = SwinForMaskedImageModeling(config)
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # è½¬æ¢æ¨¡å‹çš„çŠ¶æ€å­—å…¸æ ¼å¼
    new_state_dict = convert_state_dict(state_dict, model)
    # åŠ è½½è½¬æ¢åçš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­
    model.load_state_dict(new_state_dict)

    # éœ€è¦å¤„ç†çš„å›¾ç‰‡çš„ URL
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"

    # åˆ›å»ºå›¾åƒå¤„ç†å™¨å¯¹è±¡ï¼ŒæŒ‡å®šè¾“å‡ºå›¾åƒçš„å¤§å°
    image_processor = ViTImageProcessor(size={"height": 192, "width": 192})
    # ä½¿ç”¨ requests åº“è·å–å¹¶æ‰“å¼€æŒ‡å®š URL çš„å›¾åƒï¼Œå¹¶ç”¨ PIL åº“æ‰“å¼€
    image = Image.open(requests.get(url, stream=True).raw)
    # ä½¿ç”¨å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒï¼Œå°†å›¾åƒè½¬æ¢ä¸º PyTorch å¼ é‡æ ¼å¼
    inputs = image_processor(images=image, return_tensors="pt")

    # å…³é—­ PyTorch è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œå› ä¸ºåªéœ€è¦è¿›è¡Œæ¨æ–­
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œè·å–è¾“å‡º logits
        outputs = model(**inputs).logits

    # æ‰“å°æ¨¡å‹è¾“å‡ºçš„é”®
    print(outputs.keys())
    # è¾“å‡ºæ¶ˆæ¯ï¼Œç¡®è®¤ä¸€åˆ‡æ­£å¸¸
    print("Looks ok!")

    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•è·¯å¾„
    if pytorch_dump_folder_path is not None:
        # æ‰“å°æ¶ˆæ¯ï¼Œä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•
        model.save_pretrained(pytorch_dump_folder_path)

        # æ‰“å°æ¶ˆæ¯ï¼Œä¿å­˜å›¾åƒå¤„ç†å™¨åˆ°æŒ‡å®šç›®å½•
        print(f"Saving image processor to {pytorch_dump_folder_path}")
        # å°†å›¾åƒå¤„ç†å™¨ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•
        image_processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦å°†æ¨¡å‹æ¨é€åˆ° hub
    if push_to_hub:
        # æ‰“å°æ¶ˆæ¯ï¼Œå°†æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨æ¨é€åˆ° hub
        print(f"Pushing model and image processor for {model_name} to hub")
        # æ¨é€æ¨¡å‹åˆ° hub
        model.push_to_hub(f"microsoft/{model_name}")
        # æ¨é€å›¾åƒå¤„ç†å™¨åˆ° hub
        image_processor.push_to_hub(f"microsoft/{model_name}")


# å¦‚æœè¯¥è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    
    # æ·»åŠ å¿…éœ€å‚æ•°ï¼šæ¨¡å‹åç§°
    parser.add_argument(
        "--model_name",
        default="swin-base-simmim-window6-192",
        type=str,
        choices=["swin-base-simmim-window6-192", "swin-large-simmim-window12-192"],
        help="Name of the Swin SimMIM model you'd like to convert.",
    )
    
    # æ·»åŠ å¿…éœ€å‚æ•°ï¼šåŸå§‹æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    parser.add_argument(
        "--checkpoint_path",
        default="/Users/nielsrogge/Documents/SwinSimMIM/simmim_pretrain__swin_base__img192_window6__100ep.pth",
        type=str,
        help="Path to the original PyTorch checkpoint (.pth file).",
    )
    
    # æ·»åŠ å¯é€‰å‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹ç›®å½•è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    
    # æ·»åŠ å¯é€‰æ ‡å¿—ï¼šæ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° hub
    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨è½¬æ¢å‡½æ•°ï¼Œä¼ å…¥å‘½ä»¤è¡Œå‚æ•°
    convert_swin_checkpoint(args.model_name, args.checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub)
```