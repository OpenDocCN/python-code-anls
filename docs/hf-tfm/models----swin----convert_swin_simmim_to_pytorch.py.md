# `.\transformers\models\swin\convert_swin_simmim_to_pytorch.py`

```
# è®¾ç½®ç¼–ç æ ¼å¼ä¸º utf-8
# ç‰ˆæƒå£°æ˜ï¼Œç‰ˆæƒå½’The HuggingFace Inc.å›¢é˜Ÿæ‰€æœ‰
# æ ¹æ®Apacheè®¸å¯è¯2.0è·å¾—è®¸å¯ï¼Œé™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯çš„å‰¯æœ¬
# http://www.apache.org/licenses/LICENSE-2.0
# é™¤éæ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™ä¸å¾—åˆ†å‘è®¸å¯ä¸‹çš„è½¯ä»¶
# æ ¹æ®è®¸å¯è¯ä»¥â€œç°çŠ¶â€åŸºç¡€åˆ†å‘ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶
# è¯¦è§è®¸å¯è¯çš„ç‰¹å®šè¯­è¨€ç®¡ç†æƒé™å’Œé™åˆ¶
"""ä»åŸå§‹å­˜å‚¨åº“ä¸­è½¬æ¢Swin SimMIMæ£€æŸ¥ç‚¹ã€‚

URLï¼šhttps://github.com/microsoft/Swin-Transformer/blob/main/MODELHUB.md#simmim-pretrained-swin-v1-models"""
# å¯¼å…¥ä¾èµ–åº“
import argparse
import requests
import torch
from PIL import Image
from transformers import SwinConfig, SwinForMaskedImageModeling, ViTImageProcessor

# è·å–Swinæ¨¡å‹çš„é…ç½®
def get_swin_config(model_name):
    config = SwinConfig(image_size=192)

    if "base" in model_name:
        window_size = 6
        embed_dim = 128
        depths = (2, 2, 18, 2)
        num_heads = (4, 8, 16, 32)
    elif "large" in model_name:
        window_size = 12
        embed_dim = 192
        depths = (2, 2, 18, 2)
        num_heads = (6, 12, 24, 48)
    else:
        raise ValueError("Model not supported, only supports base and large variants")

    config.window_size = window_size
    config.embed_dim = embed_dim
    config.depths = depths
    config.num_heads = num_heads

    return config

# é‡å‘½åé”®å€¼
def rename_key(name):
    if "encoder.mask_token" in name:
        name = name.replace("encoder.mask_token", "embeddings.mask_token")
    if "encoder.patch_embed.proj" in name:
        name = name.replace("encoder.patch_embed.proj", "embeddings.patch_embeddings.projection")
    if "encoder.patch_embed.norm" in name:
        name = name.replace("encoder.patch_embed.norm", "embeddings.norm")
    if "attn.proj" in name:
        name = name.replace("attn.proj", "attention.output.dense")
    if "attn" in name:
        name = name.replace("attn", "attention.self")
    if "norm1" in name:
        name = name.replace("norm1", "layernorm_before")
    if "norm2" in name:
        name = name.replace("norm2", "layernorm_after")
    if "mlp.fc1" in name:
        name = name.replace("mlp.fc1", "intermediate.dense")
    if "mlp.fc2" in name:
        name = name.replace("mlp.fc2", "output.dense")

    if name == "encoder.norm.weight":
        name = "layernorm.weight"
    if name == "encoder.norm.bias":
        name = "layernorm.bias"

    if "decoder" in name:
        pass
    else:
        name = "swin." + name

    return name

# è½¬æ¢çŠ¶æ€å­—å…¸
def convert_state_dict(orig_state_dict, model):
    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®ï¼ˆkeyï¼‰
    for key in orig_state_dict.copy().keys():
        # å¼¹å‡ºå½“å‰é”®å¯¹åº”çš„å€¼ï¼ˆvalï¼‰ï¼ŒåŒæ—¶ä»åŸå§‹çŠ¶æ€å­—å…¸ä¸­åˆ é™¤è¯¥é”®å€¼å¯¹
        val = orig_state_dict.pop(key)
    
        # æ£€æŸ¥é”®åä¸­æ˜¯å¦åŒ…å«"attn_mask"
        if "attn_mask" in key:
            # å¦‚æœåŒ…å«"attn_mask"ï¼Œåˆ™è·³è¿‡ä¸åšå¤„ç†
            pass
        # æ£€æŸ¥é”®åä¸­æ˜¯å¦åŒ…å«"qkv"
        elif "qkv" in key:
            # æ‹†åˆ†é”®åä»¥è·å–å±‚å·å’Œå—å·
            key_split = key.split(".")
            # è§£æå±‚å·
            layer_num = int(key_split[2])
            # è§£æå—å·
            block_num = int(key_split[4])
            # è·å–å½“å‰å—çš„æ³¨æ„åŠ›å¤´å¤§å°
            dim = model.swin.encoder.layers[layer_num].blocks[block_num].attention.self.all_head_size
    
            # æ£€æŸ¥é”®åä¸­æ˜¯å¦åŒ…å«"weight"
            if "weight" in key:
                # å¦‚æœåŒ…å«"weight"ï¼Œåˆ™æ›´æ–°æŸ¥è¯¢ã€é”®ã€å€¼çš„æƒé‡
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.weight"
                ] = val[:dim, :]  # æ›´æ–°æŸ¥è¯¢æƒé‡
                orig_state_dict[f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.weight"] = val[
                    dim : dim * 2, :
                ]  # æ›´æ–°é”®æƒé‡
                orig_state_dict[
                    f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.weight"
                ] = val[-dim:, :]  # æ›´æ–°å€¼æƒé‡
            else:
                # å¦‚æœä¸åŒ…å«"weight"ï¼Œåˆ™æ›´æ–°æŸ¥è¯¢ã€é”®ã€å€¼çš„åç½®
                orig_state_dict[f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.bias"] = val[
                    :dim
                ]  # æ›´æ–°æŸ¥è¯¢åç½®
                orig_state_dict[f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.bias"] = val[
                    dim : dim * 2
                ]  # æ›´æ–°é”®åç½®
                orig_state_dict[f"swin.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.bias"] = val[
                    -dim:
                ]  # æ›´æ–°å€¼åç½®
        else:
            # å¦‚æœé”®åä¸åŒ…å«"attn_mask"æˆ–"qkv"ï¼Œåˆ™é€šè¿‡æŒ‡å®šçš„å‡½æ•°é‡å‘½åé”®åå°†å…¶é‡æ–°æ”¾å…¥åŸå§‹çŠ¶æ€å­—å…¸
            orig_state_dict[rename_key(key)] = val
    
    # è¿”å›æ›´æ–°åçš„åŸå§‹çŠ¶æ€å­—å…¸
    return orig_state_dict
def convert_swin_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub):
    # ä»æŒ‡å®šè·¯å¾„åŠ è½½ PyTorch æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½¿ç”¨ CPU è¿›è¡Œè®¡ç®—
    state_dict = torch.load(checkpoint_path, map_location="cpu")["model"]
    
    # æ ¹æ®æ¨¡å‹åç§°è·å– Swin æ¨¡å‹çš„é…ç½®å‚æ•°
    config = get_swin_config(model_name)
    # åˆ›å»º Swin æ¨¡å‹å¯¹è±¡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = SwinForMaskedImageModeling(config)
    model.eval()
    
    # å°†åŠ è½½çš„æ¨¡å‹çŠ¶æ€è½¬æ¢ä¸ºæ–°çš„çŠ¶æ€å­—å…¸ï¼Œä»¥ä¾¿åŠ è½½åˆ° Swin æ¨¡å‹ä¸­
    new_state_dict = convert_state_dict(state_dict, model)
    model.load_state_dict(new_state_dict)
    
    # éœ€è¦å¤„ç†çš„å›¾ç‰‡ URL
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    
    # åˆ›å»º ViTImageProcessor å¯¹è±¡ï¼ŒæŒ‡å®šå›¾ç‰‡å¤§å°ä¸º 192x192
    image_processor = ViTImageProcessor(size={"height": 192, "width": 192})
    # ä»æŒ‡å®š URL è·å–å›¾ç‰‡å¹¶ä½¿ç”¨ Image.open æ‰“å¼€ï¼Œè½¬æ¢ä¸º PyTorch å¼ é‡
    image = Image.open(requests.get(url, stream=True).raw)
    inputs = image_processor(images=image, return_tensors="pt")
    
    # ä½¿ç”¨æ¨¡å‹æ¨ç†å¤„ç†è¾“å…¥æ•°æ®
    with torch.no_grad():
        outputs = model(**inputs).logits
    
    # æ‰“å°æ¨¡å‹è¾“å‡ºçš„é”®
    print(outputs.keys())
    print("Looks ok!")
    
    if pytorch_dump_folder_path is not None:
        # ä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
        print(f"Saving model {model_name} to {pytorch_dump_folder_path}")
        model.save_pretrained(pytorch_dump_folder_path)
        
        # ä¿å­˜å›¾åƒå¤„ç†å™¨åˆ°æŒ‡å®šç›®å½•
        print(f"Saving image processor to {pytorch_dump_folder_path}")
        image_processor.save_pretrained(pytorch_dump_folder_path)
    
    if push_to_hub:
        # æ¨é€æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨è‡³ ğŸ¤— hub
        print(f"Pushing model and image processor for {model_name} to hub")
        model.push_to_hub(f"microsoft/{model_name}")
        image_processor.push_to_hub(f"microsoft/{model_name}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # å¿…éœ€å‚æ•°
    # æ¨¡å‹åç§°å‚æ•°ï¼Œé»˜è®¤ä¸º "swin-base-simmim-window6-192"
    parser.add_argument(
        "--model_name",
        default="swin-base-simmim-window6-192",
        type=str,
        choices=["swin-base-simmim-window6-192", "swin-large-simmim-window12-192"],
        help="Name of the Swin SimMIM model you'd like to convert.",
    )
    # æ£€æŸ¥ç‚¹è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸ºæŒ‡å®šçš„äººå‘˜æ–‡ä»¶è·¯å¾„
    parser.add_argument(
        "--checkpoint_path",
        default="/Users/nielsrogge/Documents/SwinSimMIM/simmim_pretrain__swin_base__img192_window6__100ep.pth",
        type=str,
        help="Path to the original PyTorch checkpoint (.pth file).",
    )
    # PyTorch æ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸º None
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to the output PyTorch model directory."
    )
    # æ˜¯å¦å°†æ¨¡å‹æ¨é€è‡³ ğŸ¤— hub å‚æ•°ï¼Œé»˜è®¤ä¸º False
    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨ convert_swin_checkpoint å‡½æ•°ï¼Œæ ¹æ®ä¼ å…¥çš„å‚æ•°è¿›è¡Œæ¨¡å‹è½¬æ¢æ“ä½œ
    convert_swin_checkpoint(args.model_name, args.checkpoint_path, args.pytorch_dump_folder_path, args.push_to_hub)
```