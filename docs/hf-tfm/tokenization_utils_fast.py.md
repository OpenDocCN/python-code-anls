# `.\tokenization_utils_fast.py`

```py
# è®¾ç½®è„šæœ¬çš„å­—ç¬¦ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜ï¼š2020å¹´ç”± HuggingFace Inc. å›¢é˜Ÿæä¾›
#
# æ ¹æ® Apache è®¸å¯è¯ç‰ˆæœ¬ 2.0ï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒä½¿ç”¨æ­¤æ–‡ä»¶ï¼›
# é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æœ¬è½¯ä»¶æŒ‰â€œåŸæ ·â€åˆ†å‘ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–
# æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚
# æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚
"""
 Tokenization classes for fast tokenizers (provided by HuggingFace's tokenizers library). For slow (python) tokenizers
 see tokenization_utils.py
"""

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import copy
import json
import os
from collections import defaultdict
from typing import Any, Dict, List, Optional, Tuple, Union

# å¯¼å…¥ fast tokenizers ç›¸å…³æ¨¡å—å’Œç±»
import tokenizers.pre_tokenizers as pre_tokenizers_fast
from tokenizers import Encoding as EncodingFast
from tokenizers import Tokenizer as TokenizerFast
from tokenizers.decoders import Decoder as DecoderFast
from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer

# å¯¼å…¥å…¶ä»–æ¨¡å—å’Œç±»
from .convert_slow_tokenizer import convert_slow_tokenizer
from .tokenization_utils import PreTrainedTokenizer
from .tokenization_utils_base import (
    INIT_TOKENIZER_DOCSTRING,
    AddedToken,
    BatchEncoding,
    PreTokenizedInput,
    PreTokenizedInputPair,
    PreTrainedTokenizerBase,
    SpecialTokensMixin,
    TextInput,
    TextInputPair,
    TruncationStrategy,
)
from .utils import PaddingStrategy, add_end_docstrings, logging

# è·å– logger å¯¹è±¡
logger = logging.get_logger(__name__)

# å®šä¹‰æ–‡ä»¶åå¸¸é‡
# fast tokenizers å¯ä»¥ä¿å­˜åœ¨å•ä¸ªæ–‡ä»¶ä¸­
TOKENIZER_FILE = "tokenizer.json"
SPECIAL_TOKENS_MAP_FILE = "special_tokens_map.json"
TOKENIZER_CONFIG_FILE = "tokenizer_config.json"

# slow tokenizers éœ€è¦é¢å¤–çš„æ·»åŠ  tokens æ–‡ä»¶
ADDED_TOKENS_FILE = "added_tokens.json"

# æ›´æ–° INIT_TOKENIZER_DOCSTRING æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå¢åŠ å…³äº tokenizer_object å’Œ tokenizer_file çš„è¯´æ˜
INIT_TOKENIZER_DOCSTRING += """
        tokenizer_object ([`tokenizers.Tokenizer`]):
            A [`tokenizers.Tokenizer`] object from ğŸ¤— tokenizers to instantiate from. See [Using tokenizers from ğŸ¤—
            tokenizers](../fast_tokenizers) for more information.
        tokenizer_file ([`str`]):
            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from ğŸ¤—
            tokenizers.
"""

# æ˜ å°„æ¨¡å‹ç±»å‹åˆ°å¯¹åº”çš„ Trainer ç±»
MODEL_TO_TRAINER_MAPPING = {
    "BPE": BpeTrainer,
    "Unigram": UnigramTrainer,
    "WordLevel": WordLevelTrainer,
    "WordPiece": WordPieceTrainer,
}

# å®šä¹‰ VOCAB_FILES_NAMES å­—å…¸ï¼ŒæŒ‡å®šäº† tokenizer_file çš„æ–‡ä»¶å
VOCAB_FILES_NAMES = {"tokenizer_file": TOKENIZER_FILE}


# ä½¿ç”¨è£…é¥°å™¨å°† INIT_TOKENIZER_DOCSTRING æ·»åŠ åˆ°ç±» PreTrainedTokenizerFast ä¸Šï¼Œå¹¶ç»§æ‰¿è‡ª PreTrainedTokenizerBase
@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
    """
    Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).

    Inherits from [`~tokenization_utils_base.PreTrainedTokenizerBase`].
    """
    """
    Handles all the shared methods for tokenization and special tokens, as well as methods for
    downloading/caching/loading pretrained tokenizers, as well as adding tokens to the vocabulary.

    This class also contains the added tokens in a unified way on top of all tokenizers so we don't have to handle the
    specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).
    """

    # å®šä¹‰ä¸€ä¸ªç±»å±æ€§ï¼Œç”¨äºå­˜å‚¨è¯æ±‡è¡¨æ–‡ä»¶çš„åç§°
    vocab_files_names = VOCAB_FILES_NAMES
    # æ…¢é€Ÿåˆ†è¯å™¨ç±»çš„ç±»å‹æç¤ºï¼Œé»˜è®¤ä¸º None
    slow_tokenizer_class: PreTrainedTokenizer = None

    @property
    def is_fast(self) -> bool:
        # è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨äº†å¿«é€Ÿåˆ†è¯å™¨
        return True

    @property
    def can_save_slow_tokenizer(self) -> bool:
        """
        `bool`: Whether or not the slow tokenizer can be saved. Usually for sentencepiece based slow tokenizer, this
        can only be `True` if the original `"sentencepiece.model"` was not deleted.
        """
        # è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦å¯ä»¥ä¿å­˜æ…¢é€Ÿåˆ†è¯å™¨
        return True

    @property
    def vocab_size(self) -> int:
        """
        `int`: Size of the base vocabulary (without the added tokens).
        """
        # è¿”å›åŸºæœ¬è¯æ±‡è¡¨çš„å¤§å°ï¼ˆä¸åŒ…æ‹¬æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼‰
        return self._tokenizer.get_vocab_size(with_added_tokens=False)

    def get_vocab(self) -> Dict[str, int]:
        # è¿”å›åŒ…æ‹¬æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°åœ¨å†…çš„è¯æ±‡è¡¨
        return self._tokenizer.get_vocab(with_added_tokens=True)

    @property
    def vocab(self) -> Dict[str, int]:
        # è¿”å›åŒ…æ‹¬æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°åœ¨å†…çš„è¯æ±‡è¡¨
        return self.get_vocab()

    @property
    def added_tokens_encoder(self) -> Dict[str, int]:
        """
        Returns the sorted mapping from string to index. The added tokens encoder is cached for performance
        optimisation in `self._added_tokens_encoder` for the slow tokenizers.
        """
        # è¿”å›ä»å­—ç¬¦ä¸²åˆ°ç´¢å¼•çš„æ’åºæ˜ å°„ï¼Œç”¨äºæ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ç¼–ç å™¨
        return {k.content: v for v, k in sorted(self.added_tokens_decoder.items(), key=lambda item: item[0])}

    @property
    def added_tokens_decoder(self) -> Dict[int, AddedToken]:
        """
        Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.

        Returns:
            `Dict[str, int]`: The added tokens.
        """
        # è¿”å›è¯æ±‡è¡¨ä¸­æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œä½œä¸ºç´¢å¼•åˆ° AddedToken å¯¹è±¡çš„å­—å…¸
        return self._tokenizer.get_added_tokens_decoder()

    def get_added_vocab(self) -> Dict[str, int]:
        """
        Returns the added tokens in the vocabulary as a dictionary of token to index.

        Returns:
            `Dict[str, int]`: The added tokens.
        """
        # è¿”å›è¯æ±‡è¡¨ä¸­æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œä½œä¸º token åˆ°ç´¢å¼•çš„å­—å…¸
        return {k.content: v for v, k in sorted(self.added_tokens_decoder.items(), key=lambda item: item[0])}

    def __len__(self) -> int:
        """
        Size of the full vocabulary with the added tokens.
        """
        # è¿”å›åŒ…æ‹¬æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°åœ¨å†…çš„è¯æ±‡è¡¨çš„å¤§å°
        return self._tokenizer.get_vocab_size(with_added_tokens=True)

    @property
    def backend_tokenizer(self) -> TokenizerFast:
        """
        `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.
        """
        # è¿”å›ä½œä¸ºåç«¯ä½¿ç”¨çš„ Rust åˆ†è¯å™¨å¯¹è±¡
        return self._tokenizer

    @property
    def decoder(self) -> DecoderFast:
        """
        `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.
        """
        # è¿”å›ç”¨äºæ­¤åˆ†è¯å™¨çš„ Rust è§£ç å™¨å¯¹è±¡
        return self._tokenizer.decoder
    def _convert_encoding(
        self,
        encoding: EncodingFast,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
    ) -> Tuple[Dict[str, Any], List[EncodingFast]]:
        """
        Convert the encoding representation (from low-level HuggingFace tokenizer output) to a python Dict and a list
        of encodings, take care of building a batch from overflowing tokens.

        Overflowing tokens are converted to additional examples (like batches) so the output values of the dict are
        lists (overflows) of lists (tokens).

        Output shape: (overflows, sequence length)
        """
        # Determine if `return_token_type_ids` should be inferred based on model input names
        if return_token_type_ids is None:
            return_token_type_ids = "token_type_ids" in self.model_input_names
        # Determine if `return_attention_mask` should be inferred based on model input names
        if return_attention_mask is None:
            return_attention_mask = "attention_mask" in self.model_input_names

        # Initialize `encodings` with current encoding or handle overflowing tokens
        if return_overflowing_tokens and encoding.overflowing is not None:
            encodings = [encoding] + encoding.overflowing
        else:
            encodings = [encoding]

        # Initialize a defaultdict to collect various encoding attributes as lists
        encoding_dict = defaultdict(list)
        # Iterate over each encoding in `encodings`
        for e in encodings:
            # Append token ids to the `input_ids` list in `encoding_dict`
            encoding_dict["input_ids"].append(e.ids)

            # Append token type ids if `return_token_type_ids` is enabled
            if return_token_type_ids:
                encoding_dict["token_type_ids"].append(e.type_ids)
            # Append attention mask if `return_attention_mask` is enabled
            if return_attention_mask:
                encoding_dict["attention_mask"].append(e.attention_mask)
            # Append special tokens mask if `return_special_tokens_mask` is enabled
            if return_special_tokens_mask:
                encoding_dict["special_tokens_mask"].append(e.special_tokens_mask)
            # Append offset mappings if `return_offsets_mapping` is enabled
            if return_offsets_mapping:
                encoding_dict["offset_mapping"].append(e.offsets)
            # Append length of token ids if `return_length` is enabled
            if return_length:
                encoding_dict["length"].append(len(e.ids))

        # Return the collected encoding attributes as `encoding_dict` and the list of `encodings`
        return encoding_dict, encodings

    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:
        """
        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
        vocabulary.

        Args:
            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).

        Returns:
            `int` or `List[int]`: The token id or list of token ids.
        """
        # If `tokens` is None, return None
        if tokens is None:
            return None

        # If `tokens` is a string, convert it to token id using `_convert_token_to_id_with_added_voc`
        if isinstance(tokens, str):
            return self._convert_token_to_id_with_added_voc(tokens)

        # If `tokens` is a list of strings, convert each token to token ids using `_convert_token_to_id_with_added_voc`
        return [self._convert_token_to_id_with_added_voc(token) for token in tokens]

    def _convert_token_to_id_with_added_voc(self, token: str) -> int:
        # Get the token id from `_tokenizer`, or return `unk_token_id` if token is unknown
        index = self._tokenizer.token_to_id(token)
        if index is None:
            return self.unk_token_id
        return index
    # æ ¹æ®ç»™å®šçš„ç´¢å¼•å°†å…¶è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡è®°ï¼ˆå­—ç¬¦ä¸²ï¼‰
    def _convert_id_to_token(self, index: int) -> Optional[str]:
        return self._tokenizer.id_to_token(int(index))

    # å‘åˆ†è¯å™¨æ·»åŠ æ–°çš„æ ‡è®°ï¼ˆå•è¯æˆ–ç‰¹æ®Šæ ‡è®°ï¼‰
    def _add_tokens(self, new_tokens: List[Union[str, AddedToken]], special_tokens=False) -> int:
        if special_tokens:
            # æ·»åŠ ç‰¹æ®Šæ ‡è®°åˆ°åˆ†è¯å™¨
            return self._tokenizer.add_special_tokens(new_tokens)
        else:
            # æ·»åŠ æ™®é€šæ ‡è®°åˆ°åˆ†è¯å™¨
            return self._tokenizer.add_tokens(new_tokens)

    # è¿”å›ç¼–ç åºåˆ—æ—¶æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ•°é‡
    def num_special_tokens_to_add(self, pair: bool = False) -> int:
        """
        è¿”å›åœ¨ç¼–ç åºåˆ—æ—¶æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ•°é‡ã€‚

        <Tip>

        è¿™ä¼šå¯¹è™šæ‹Ÿè¾“å…¥è¿›è¡Œç¼–ç å¹¶æ£€æŸ¥æ·»åŠ çš„æ ‡è®°æ•°é‡ï¼Œå› æ­¤æ•ˆç‡è¾ƒä½ã€‚ä¸è¦å°†æ­¤å‡½æ•°æ”¾åœ¨è®­ç»ƒå¾ªç¯ä¸­ã€‚

        </Tip>

        Args:
            pair (`bool`, *optional*, é»˜è®¤ä¸º `False`):
                æ˜¯å¦åœ¨åºåˆ—å¯¹ï¼ˆsequence pairï¼‰æƒ…å†µä¸‹è®¡ç®—æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ•°é‡ï¼Œæˆ–å•ç‹¬åºåˆ—çš„æƒ…å†µã€‚

        Returns:
            `int`: æ·»åŠ åˆ°åºåˆ—ä¸­çš„ç‰¹æ®Šæ ‡è®°æ•°é‡ã€‚
        """
        return self._tokenizer.num_special_tokens_to_add(pair)

    # å°†ç»™å®šçš„æ ‡è®°ç´¢å¼•æˆ–ç´¢å¼•åˆ—è¡¨è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡è®°æˆ–æ ‡è®°åˆ—è¡¨
    def convert_ids_to_tokens(
        self, ids: Union[int, List[int]], skip_special_tokens: bool = False
    ) -> Union[str, List[str]]:
        """
        ä½¿ç”¨è¯æ±‡è¡¨å’Œå·²æ·»åŠ çš„æ ‡è®°ï¼Œå°†å•ä¸ªç´¢å¼•æˆ–ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ ‡è®°æˆ–æ ‡è®°åºåˆ—ã€‚

        Args:
            ids (`int` æˆ– `List[int]`):
                è¦è½¬æ¢ä¸ºæ ‡è®°æˆ–æ ‡è®°åºåˆ—çš„æ ‡è®° IDï¼ˆæˆ–æ ‡è®° IDsï¼‰ã€‚
            skip_special_tokens (`bool`, *optional*, é»˜è®¤ä¸º `False`):
                æ˜¯å¦åœ¨è§£ç æ—¶è·³è¿‡ç‰¹æ®Šæ ‡è®°ã€‚

        Returns:
            `str` æˆ– `List[str]`: è§£ç åçš„æ ‡è®°ï¼ˆæˆ–æ ‡è®°åˆ—è¡¨ï¼‰ã€‚
        """
        if isinstance(ids, int):
            return self._tokenizer.id_to_token(ids)
        tokens = []
        for index in ids:
            index = int(index)
            if skip_special_tokens and index in self.all_special_ids:
                continue
            tokens.append(self._tokenizer.id_to_token(index))
        return tokens

    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œè¿”å›æ ‡è®°åˆ—è¡¨
    def tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]:
        return self.encode_plus(text=text, text_pair=pair, add_special_tokens=add_special_tokens, **kwargs).tokens()

    # è®¾ç½®æˆªæ–­å’Œå¡«å……ç­–ç•¥ï¼Œä»¥åŠç›¸å…³çš„å‚æ•°
    def set_truncation_and_padding(
        self,
        padding_strategy: PaddingStrategy,
        truncation_strategy: TruncationStrategy,
        max_length: int,
        stride: int,
        pad_to_multiple_of: Optional[int],
        """
        Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers
        library) and restore the tokenizer settings afterwards.

        The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a
        padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed
        section.

        Args:
            padding_strategy ([`~utils.PaddingStrategy`]):
                The kind of padding that will be applied to the input
            truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):
                The kind of truncation that will be applied to the input
            max_length (`int`):
                The maximum size of a sequence.
            stride (`int`):
                The stride to use when handling overflow.
            pad_to_multiple_of (`int`, *optional*):
                If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
                the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).
        """
        # Preserve the current truncation and padding settings of the tokenizer
        _truncation = self._tokenizer.truncation
        _padding = self._tokenizer.padding

        # Set truncation strategy on the backend tokenizer
        if truncation_strategy == TruncationStrategy.DO_NOT_TRUNCATE:
            # If DO_NOT_TRUNCATE is specified, ensure no truncation is applied
            if _truncation is not None:
                self._tokenizer.no_truncation()
        else:
            # Define the target truncation settings
            target = {
                "max_length": max_length,
                "stride": stride,
                "strategy": truncation_strategy.value,
                "direction": self.truncation_side,
            }

            # Compare current truncation settings with the target settings
            if _truncation is None:
                current = None
            else:
                current = {k: _truncation.get(k, None) for k in target}

            # Enable truncation if current settings differ from the target settings
            if current != target:
                self._tokenizer.enable_truncation(**target)

        # Set padding strategy on the backend tokenizer
        if padding_strategy == PaddingStrategy.DO_NOT_PAD:
            # If DO_NOT_PAD is specified, ensure no padding is applied
            if _padding is not None:
                self._tokenizer.no_padding()
        else:
            # Define the target padding settings
            length = max_length if padding_strategy == PaddingStrategy.MAX_LENGTH else None
            target = {
                "length": length,
                "direction": self.padding_side,
                "pad_id": self.pad_token_id,
                "pad_token": self.pad_token,
                "pad_type_id": self.pad_token_type_id,
                "pad_to_multiple_of": pad_to_multiple_of,
            }

            # Compare current padding settings with the target settings
            if _padding != target:
                self._tokenizer.enable_padding(**target)
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºæ‰¹é‡ç¼–ç æ–‡æœ¬æˆ–æ–‡æœ¬å¯¹
    def _batch_encode_plus(
        self,
        batch_text_or_text_pairs: Union[
            List[TextInput], List[TextInputPair], List[PreTokenizedInput], List[PreTokenizedInputPair]
        ],
        add_special_tokens: bool = True,  # æ˜¯å¦æ·»åŠ ç‰¹æ®Šçš„æ ‡è®°ç¬¦å·ï¼Œé»˜è®¤ä¸ºTrue
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,  # å¡«å……ç­–ç•¥ï¼Œé»˜è®¤ä¸å¡«å……
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,  # æˆªæ–­ç­–ç•¥ï¼Œé»˜è®¤ä¸æˆªæ–­
        max_length: Optional[int] = None,  # æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œé»˜è®¤ä¸ºæ— é™åˆ¶
        stride: int = 0,  # æ­¥é•¿ï¼Œé»˜è®¤ä¸º0
        is_split_into_words: bool = False,  # è¾“å…¥æ˜¯å¦å·²åˆ†æˆå•è¯ï¼Œé»˜è®¤ä¸ºFalse
        pad_to_multiple_of: Optional[int] = None,  # å¡«å……åˆ°æŒ‡å®šçš„å€æ•°ï¼Œé»˜è®¤ä¸ºä¸å¡«å……åˆ°å€æ•°
        return_tensors: Optional[str] = None,  # è¿”å›çš„å¼ é‡ç±»å‹ï¼Œé»˜è®¤ä¸ºNone
        return_token_type_ids: Optional[bool] = None,  # æ˜¯å¦è¿”å›tokenç±»å‹IDï¼Œé»˜è®¤ä¸ºNone
        return_attention_mask: Optional[bool] = None,  # æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œé»˜è®¤ä¸ºNone
        return_overflowing_tokens: bool = False,  # æ˜¯å¦è¿”å›æº¢å‡ºçš„tokenï¼Œé»˜è®¤ä¸ºFalse
        return_special_tokens_mask: bool = False,  # æ˜¯å¦è¿”å›ç‰¹æ®Štokençš„æ©ç ï¼Œé»˜è®¤ä¸ºFalse
        return_offsets_mapping: bool = False,  # æ˜¯å¦è¿”å›åç§»æ˜ å°„ï¼Œé»˜è®¤ä¸ºFalse
        return_length: bool = False,  # æ˜¯å¦è¿”å›é•¿åº¦ï¼Œé»˜è®¤ä¸ºFalse
        verbose: bool = True,  # æ˜¯å¦è¯¦ç»†è¾“å‡ºä¿¡æ¯ï¼Œé»˜è®¤ä¸ºTrue
    ):
    
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºç¼–ç å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬å¯¹
    def _encode_plus(
        self,
        text: Union[TextInput, PreTokenizedInput],  # è¾“å…¥çš„æ–‡æœ¬æˆ–é¢„åˆ†è¯çš„æ–‡æœ¬
        text_pair: Optional[Union[TextInput, PreTokenizedInput]] = None,  # å¯é€‰çš„æ–‡æœ¬å¯¹
        add_special_tokens: bool = True,  # æ˜¯å¦æ·»åŠ ç‰¹æ®Šçš„æ ‡è®°ç¬¦å·ï¼Œé»˜è®¤ä¸ºTrue
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,  # å¡«å……ç­–ç•¥ï¼Œé»˜è®¤ä¸å¡«å……
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,  # æˆªæ–­ç­–ç•¥ï¼Œé»˜è®¤ä¸æˆªæ–­
        max_length: Optional[int] = None,  # æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œé»˜è®¤ä¸ºæ— é™åˆ¶
        stride: int = 0,  # æ­¥é•¿ï¼Œé»˜è®¤ä¸º0
        is_split_into_words: bool = False,  # è¾“å…¥æ˜¯å¦å·²åˆ†æˆå•è¯ï¼Œé»˜è®¤ä¸ºFalse
        pad_to_multiple_of: Optional[int] = None,  # å¡«å……åˆ°æŒ‡å®šçš„å€æ•°ï¼Œé»˜è®¤ä¸ºä¸å¡«å……åˆ°å€æ•°
        return_tensors: Optional[bool] = None,  # è¿”å›çš„å¼ é‡ç±»å‹ï¼Œé»˜è®¤ä¸ºNone
        return_token_type_ids: Optional[bool] = None,  # æ˜¯å¦è¿”å›tokenç±»å‹IDï¼Œé»˜è®¤ä¸ºNone
        return_attention_mask: Optional[bool] = None,  # æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œé»˜è®¤ä¸ºNone
        return_overflowing_tokens: bool = False,  # æ˜¯å¦è¿”å›æº¢å‡ºçš„tokenï¼Œé»˜è®¤ä¸ºFalse
        return_special_tokens_mask: bool = False,  # æ˜¯å¦è¿”å›ç‰¹æ®Štokençš„æ©ç ï¼Œé»˜è®¤ä¸ºFalse
        return_offsets_mapping: bool = False,  # æ˜¯å¦è¿”å›åç§»æ˜ å°„ï¼Œé»˜è®¤ä¸ºFalse
        return_length: bool = False,  # æ˜¯å¦è¿”å›é•¿åº¦ï¼Œé»˜è®¤ä¸ºFalse
        verbose: bool = True,  # æ˜¯å¦è¯¦ç»†è¾“å‡ºä¿¡æ¯ï¼Œé»˜è®¤ä¸ºTrue
        **kwargs,  # å…¶ä»–å…³é”®å­—å‚æ•°ï¼Œç”¨äºæ‰©å±•åŠŸèƒ½
    ):
    ) -> BatchEncoding:
        # å°†è¾“å…¥æ–‡æœ¬å’Œå¯èƒ½å­˜åœ¨çš„æ–‡æœ¬å¯¹ä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡è¾“å…¥ï¼Œæ ¹æ®éœ€è¦åŒ…è£…æˆå…ƒç»„
        batched_input = [(text, text_pair)] if text_pair else [text]
        # è°ƒç”¨å†…éƒ¨æ–¹æ³•è¿›è¡Œæ‰¹é‡ç¼–ç å¤„ç†ï¼Œç”Ÿæˆæ‰¹æ¬¡è¾“å‡º
        batched_output = self._batch_encode_plus(
            batched_input,
            is_split_into_words=is_split_into_words,
            add_special_tokens=add_special_tokens,
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            pad_to_multiple_of=pad_to_multiple_of,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            return_overflowing_tokens=return_overflowing_tokens,
            return_special_tokens_mask=return_special_tokens_mask,
            return_offsets_mapping=return_offsets_mapping,
            return_length=return_length,
            verbose=verbose,
            **kwargs,
        )

        # å¦‚æœæ²¡æœ‰è¿”å›å¼ é‡å¹¶ä¸”æ²¡æœ‰è¿”å›æº¢å‡ºçš„tokenï¼Œåˆ™ç§»é™¤å‰å¯¼çš„æ‰¹æ¬¡è½´
        # å¦‚æœæº¢å‡ºçš„tokenä½œä¸ºä¸€æ‰¹è¾“å‡ºè¿”å›ï¼Œåˆ™åœ¨æ­¤æƒ…å†µä¸‹ä¿ç•™å®ƒä»¬
        if return_tensors is None and not return_overflowing_tokens:
            # é‡æ–°å¤„ç†æ‰¹æ¬¡è¾“å‡ºï¼Œç¡®ä¿æ¯ä¸ªå€¼æ­£ç¡®å¤„ç†ä¸ºå•ä¸ªå…ƒç´ æˆ–åˆ—è¡¨çš„å½¢å¼
            batched_output = BatchEncoding(
                {
                    key: value[0] if len(value) > 0 and isinstance(value[0], list) else value
                    for key, value in batched_output.items()
                },
                batched_output.encodings,
            )

        # æ£€æŸ¥å¹¶è­¦å‘Šåºåˆ—é•¿åº¦æ˜¯å¦è¶…è¿‡è®¾å®šçš„æœ€å¤§é•¿åº¦
        self._eventual_warn_about_too_long_sequence(batched_output["input_ids"], max_length, verbose)

        # è¿”å›å¤„ç†åçš„æ‰¹æ¬¡è¾“å‡º
        return batched_output

    def convert_tokens_to_string(self, tokens: List[str]) -> str:
        # ä½¿ç”¨åç«¯çš„tokenizer decoderå°†tokenåˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return self.backend_tokenizer.decoder.decode(tokens)

    def _decode(
        self,
        token_ids: Union[int, List[int]],
        skip_special_tokens: bool = False,
        clean_up_tokenization_spaces: bool = None,
        **kwargs,
    ) -> str:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨æºtokenizerè¿›è¡Œè§£ç 
        self._decode_use_source_tokenizer = kwargs.pop("use_source_tokenizer", False)

        # å°†token_idsè½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼ï¼ˆå¦‚æœè¾“å…¥ä¸ºå•ä¸ªæ•´æ•°ï¼‰
        if isinstance(token_ids, int):
            token_ids = [token_ids]
        # ä½¿ç”¨å†…éƒ¨çš„tokenizerè§£ç token_idsï¼Œæ ¹æ®éœ€è¦è·³è¿‡ç‰¹æ®Štoken
        text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†tokenåŒ–ç©ºé—´
        clean_up_tokenization_spaces = (
            clean_up_tokenization_spaces
            if clean_up_tokenization_spaces is not None
            else self.clean_up_tokenization_spaces
        )
        # å¦‚æœéœ€è¦æ¸…ç†tokenåŒ–ç©ºé—´ï¼Œåˆ™æ‰§è¡Œæ¸…ç†æ“ä½œå¹¶è¿”å›æ¸…ç†åçš„æ–‡æœ¬
        if clean_up_tokenization_spaces:
            clean_text = self.clean_up_tokenization(text)
            return clean_text
        else:
            # å¦åˆ™ç›´æ¥è¿”å›è§£ç åçš„æ–‡æœ¬
            return text

    def _save_pretrained(
        self,
        save_directory: Union[str, os.PathLike],
        file_names: Tuple[str],
        legacy_format: Optional[bool] = None,
        filename_prefix: Optional[str] = None,
    ) -> Tuple[str]:
        """
        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens as well as in a unique JSON
        file containing {config + vocab + added-tokens}.
        """
        # å°†ä¿å­˜ç›®å½•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        save_directory = str(save_directory)

        # å¦‚æœæ²¡æœ‰å®šä¹‰æ…¢é€Ÿåˆ†è¯å™¨çš„ç±»ä¸”éœ€è¦é—ç•™æ ¼å¼ï¼Œåˆ™å¼•å‘å€¼é”™è¯¯
        if self.slow_tokenizer_class is None and legacy_format is True:
            raise ValueError(
                "Your tokenizer does not have a legacy version defined and therefore cannot register this version. You"
                " might consider leaving the legacy_format at `None` or setting it to `False`."
            )

        # å†³å®šæ˜¯å¦ä¿å­˜æ…¢é€Ÿåˆ†è¯å™¨
        save_slow = (
            (legacy_format is None or legacy_format is True)
            and self.slow_tokenizer_class is not None
            and self.can_save_slow_tokenizer
        )
        # å†³å®šæ˜¯å¦ä¿å­˜å¿«é€Ÿåˆ†è¯å™¨
        save_fast = legacy_format is None or legacy_format is False

        # å¦‚æœéœ€è¦ä¿å­˜æ…¢é€Ÿåˆ†è¯å™¨
        if save_slow:
            # æ„é€ æ·»åŠ çš„æ ‡è®°æ–‡ä»¶è·¯å¾„
            added_tokens_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + ADDED_TOKENS_FILE
            )
            # ç¡®ä¿å¯¹æœªæ¥å…¼å®¹
            added_vocab = {tok: index for tok, index in self.added_tokens_encoder.items() if index >= self.vocab_size}
            # å¦‚æœæœ‰æ·»åŠ çš„è¯æ±‡ï¼Œå†™å…¥JSONæ–‡ä»¶
            if added_vocab:
                with open(added_tokens_file, "w", encoding="utf-8") as f:
                    out_str = json.dumps(added_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
                    f.write(out_str)

            # ä¿å­˜è¯æ±‡è¡¨æ–‡ä»¶å¹¶è·å–æ–‡ä»¶ååˆ—è¡¨
            vocab_files = self.save_vocabulary(save_directory, filename_prefix=filename_prefix)
            file_names = file_names + vocab_files + (added_tokens_file,)

        # å¦‚æœéœ€è¦ä¿å­˜å¿«é€Ÿåˆ†è¯å™¨
        if save_fast:
            # æ„é€ åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„
            tokenizer_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + TOKENIZER_FILE
            )
            # è°ƒç”¨åç«¯åˆ†è¯å™¨çš„ä¿å­˜æ–¹æ³•
            self.backend_tokenizer.save(tokenizer_file)
            file_names = file_names + (tokenizer_file,)

        # è¿”å›æ‰€æœ‰ä¿å­˜çš„æ–‡ä»¶ååˆ—è¡¨
        return file_names
```