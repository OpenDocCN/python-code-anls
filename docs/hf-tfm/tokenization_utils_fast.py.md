# `.\transformers\tokenization_utils_fast.py`

```
# å®šä¹‰äº†ä¸€ç»„å¸¸ç”¨çš„ç¼–ç å­—ç¬¦é›†ä¸º utf-8
# ç‰ˆæƒå£°æ˜ï¼ŒæŒ‡æ˜ä»£ç ç‰ˆæƒæ‰€æœ‰è€…å’Œè®¸å¯åè®®
"""
Tokenization classes for fast tokenizers (provided by HuggingFace's tokenizers library). For slow (python) tokenizers
see tokenization_utils.py
"""
# å¼•å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import copy  # å¼•å…¥ç”¨äºæ·±æ‹·è´å¯¹è±¡çš„æ¨¡å—
import json  # å¼•å…¥ç”¨äºå¤„ç† JSON æ•°æ®çš„æ¨¡å—
import os  # å¼•å…¥ç”¨äºå¤„ç†æ“ä½œç³»ç»Ÿç›¸å…³åŠŸèƒ½çš„æ¨¡å—
from collections import defaultdict  # å¼•å…¥ç”¨äºåˆ›å»ºé»˜è®¤å­—å…¸çš„æ¨¡å—
from typing import Any, Dict, List, Optional, Tuple, Union  # å¼•å…¥ç±»å‹æç¤ºç›¸å…³çš„æ¨¡å—

import tokenizers.pre_tokenizers as pre_tokenizers_fast  # å¼•å…¥ HuggingFace tokenizers åº“ä¸­çš„é¢„åˆ†è¯å™¨æ¨¡å—
from tokenizers import Encoding as EncodingFast  # å¼•å…¥ HuggingFace tokenizers åº“ä¸­çš„ç¼–ç ç±»
from tokenizers import Tokenizer as TokenizerFast  # å¼•å…¥ HuggingFace tokenizers åº“ä¸­çš„åˆ†è¯å™¨ç±»
from tokenizers.decoders import Decoder as DecoderFast  # å¼•å…¥ HuggingFace tokenizers åº“ä¸­çš„è§£ç å™¨ç±»
from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer  # å¼•å…¥ HuggingFace tokenizers åº“ä¸­çš„è®­ç»ƒå™¨ç±»

from .convert_slow_tokenizer import convert_slow_tokenizer  # ä»å½“å‰ç›®å½•ä¸‹çš„ convert_slow_tokenizer æ¨¡å—ä¸­å¼•å…¥æ…¢é€Ÿåˆ†è¯å™¨è½¬æ¢å‡½æ•°
from .tokenization_utils import PreTrainedTokenizer  # ä»å½“å‰ç›®å½•ä¸‹çš„ tokenization_utils æ¨¡å—ä¸­å¼•å…¥é¢„è®­ç»ƒåˆ†è¯å™¨ç±»
from .tokenization_utils_base import (  # ä»å½“å‰ç›®å½•ä¸‹çš„ tokenization_utils_base æ¨¡å—ä¸­å¼•å…¥ä¸€ç³»åˆ—åŸºç¡€åˆ†è¯å™¨ç›¸å…³ç±»å’Œå‡½æ•°
    INIT_TOKENIZER_DOCSTRING,
    AddedToken,
    BatchEncoding,
    PreTokenizedInput,
    PreTokenizedInputPair,
    PreTrainedTokenizerBase,
    SpecialTokensMixin,
    TextInput,
    TextInputPair,
    TruncationStrategy,
)
from .utils import PaddingStrategy, add_end_docstrings, logging  # ä»å½“å‰ç›®å½•ä¸‹çš„ utils æ¨¡å—ä¸­å¼•å…¥å¡«å……ç­–ç•¥ã€æ·»åŠ æ–‡æ¡£æœ«å°¾çš„å­—ç¬¦ä¸²å‡½æ•°å’Œæ—¥å¿—è®°å½•åŠŸèƒ½

# è·å–æ—¥å¿—è®°å½•å™¨å¯¹è±¡
logger = logging.get_logger(__name__)

# å¿«é€Ÿåˆ†è¯å™¨ï¼ˆç”± HuggingFace tokenizers åº“æä¾›ï¼‰å¯ä»¥ä¿å­˜åœ¨å•ä¸ªæ–‡ä»¶ä¸­
TOKENIZER_FILE = "tokenizer.json"  # å¿«é€Ÿåˆ†è¯å™¨æ¨¡å‹æ–‡ä»¶å
SPECIAL_TOKENS_MAP_FILE = "special_tokens_map.json"  # ç‰¹æ®Šæ ‡è®°æ˜ å°„æ–‡ä»¶å
TOKENIZER_CONFIG_FILE = "tokenizer_config.json"  # å¿«é€Ÿåˆ†è¯å™¨é…ç½®æ–‡ä»¶å

# æ…¢é€Ÿåˆ†è¯å™¨æœ‰ä¸€ä¸ªé¢å¤–çš„æ·»åŠ æ ‡è®°æ–‡ä»¶
ADDED_TOKENS_FILE = "added_tokens.json"  # æ·»åŠ æ ‡è®°æ–‡ä»¶å

INIT_TOKENIZER_DOCSTRING += """
        tokenizer_object ([`tokenizers.Tokenizer`]):
            A [`tokenizers.Tokenizer`] object from ğŸ¤— tokenizers to instantiate from. See [Using tokenizers from ğŸ¤—
            tokenizers](../fast_tokenizers) for more information.
        tokenizer_file ([`str`]):
            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from ğŸ¤—
            tokenizers.
"""

MODEL_TO_TRAINER_MAPPING = {
    "BPE": BpeTrainer,
    "Unigram": UnigramTrainer,
    "WordLevel": WordLevelTrainer,
    "WordPiece": WordPieceTrainer,
}

VOCAB_FILES_NAMES = {"tokenizer_file": TOKENIZER_FILE}


@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
# å¿«é€Ÿé¢„è®­ç»ƒåˆ†è¯å™¨ç±»ï¼Œç»§æ‰¿è‡ªåŸºç¡€é¢„è®­ç»ƒåˆ†è¯å™¨åŸºç±»
class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
    """
    Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).

    Inherits from [`~tokenization_utils_base.PreTrainedTokenizerBase`].
    """
    Handles all the shared methods for tokenization and special tokens, as well as methods for
    downloading/caching/loading pretrained tokenizers, as well as adding tokens to the vocabulary.

    This class also contains the added tokens in a unified way on top of all tokenizers so we don't have to handle the
    specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).
    """

    # å®šä¹‰ç±»å±æ€§ï¼Œå­˜å‚¨è¯æ±‡æ–‡ä»¶å
    vocab_files_names = VOCAB_FILES_NAMES
    # æ…¢é€Ÿåˆ†è¯å™¨ç±»ï¼Œé»˜è®¤ä¸º None
    slow_tokenizer_class: PreTrainedTokenizer = None

    @property
    def is_fast(self) -> bool:
        # è¿”å› Trueï¼Œè¡¨ç¤ºä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨
        return True

    @property
    def can_save_slow_tokenizer(self) -> bool:
        """
        `bool`: Whether or not the slow tokenizer can be saved. Usually for sentencepiece based slow tokenizer, this
        can only be `True` if the original `"sentencepiece.model"` was not deleted.
        """
        # è¿”å› Trueï¼Œè¡¨ç¤ºæ…¢é€Ÿåˆ†è¯å™¨å¯ä»¥ä¿å­˜
        return True

    @property
    def vocab_size(self) -> int:
        """
        `int`: Size of the base vocabulary (without the added tokens).
        """
        # è¿”å›åŸºç¡€è¯æ±‡è¡¨çš„å¤§å°ï¼ˆä¸åŒ…æ‹¬æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼‰
        return self._tokenizer.get_vocab_size(with_added_tokens=False)

    def get_vocab(self) -> Dict[str, int]:
        # è·å–è¯æ±‡è¡¨ï¼ŒåŒ…æ‹¬æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°
        return self._tokenizer.get_vocab(with_added_tokens=True)

    @property
    def vocab(self) -> Dict[str, int]:
        # è¿”å›è¯æ±‡è¡¨
        return self.get_vocab()

    @property
    def added_tokens_encoder(self) -> Dict[str, int]:
        """
        Returns the sorted mapping from string to index. The added tokens encoder is cached for performance
        optimisation in `self._added_tokens_encoder` for the slow tokenizers.
        """
        # è¿”å›ä»å­—ç¬¦ä¸²åˆ°ç´¢å¼•çš„æ’åºæ˜ å°„ï¼Œç”¨äºç¼“å­˜æ€§èƒ½ä¼˜åŒ–
        return {k.content: v for v, k in sorted(self.added_tokens_decoder.items(), key=lambda item: item[0])}

    @property
    def added_tokens_decoder(self) -> Dict[int, AddedToken]:
        """
        Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.

        Returns:
            `Dict[str, int]`: The added tokens.
        """
        # è¿”å›è¯æ±‡è¡¨ä¸­æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œä»¥ç´¢å¼•åˆ° AddedToken çš„å­—å…¸å½¢å¼
        return self._tokenizer.get_added_tokens_decoder()

    def get_added_vocab(self) -> Dict[str, int]:
        """
        Returns the added tokens in the vocabulary as a dictionary of token to index.

        Returns:
            `Dict[str, int]`: The added tokens.
        """
        # è¿”å›è¯æ±‡è¡¨ä¸­æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œä»¥æ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸å½¢å¼
        return {k.content: v for v, k in sorted(self.added_tokens_decoder.items(), key=lambda item: item[0])}

    def __len__(self) -> int:
        """
        Size of the full vocabulary with the added tokens.
        """
        # è¿”å›åŒ…æ‹¬æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°åœ¨å†…çš„å®Œæ•´è¯æ±‡è¡¨å¤§å°
        return self._tokenizer.get_vocab_size(with_added_tokens=True)

    @property
    def backend_tokenizer(self) -> TokenizerFast:
        """
        `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.
        """
        # è¿”å›ç”¨ä½œåç«¯çš„ Rust åˆ†è¯å™¨
        return self._tokenizer

    @property
    def decoder(self) -> DecoderFast:
        """
        `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.
        """
        # è¿”å›æ­¤åˆ†è¯å™¨çš„ Rust è§£ç å™¨
        return self._tokenizer.decoder
    def _convert_encoding(
        self,
        encoding: EncodingFast,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
    ) -> Tuple[Dict[str, Any], List[EncodingFast]]:
        """
        Convert the encoding representation (from low-level HuggingFace tokenizer output) to a python Dict and a list
        of encodings, take care of building a batch from overflowing tokens.

        Overflowing tokens are converted to additional examples (like batches) so the output values of the dict are
        lists (overflows) of lists (tokens).

        Output shape: (overflows, sequence length)
        """
        # æŒ‡å®šæ˜¯å¦è¿”å› token_type_idsï¼Œé»˜è®¤ä¸º None
        if return_token_type_ids is None:
            return_token_type_ids = "token_type_ids" in self.model_input_names
        # æŒ‡å®šæ˜¯å¦è¿”å› attention_maskï¼Œé»˜è®¤ä¸º None
        if return_attention_mask is None:
            return_attention_mask = "attention_mask" in self.model_input_names

        # å¦‚æœéœ€è¦è¿”å› overflowing_tokens å¹¶ä¸” encoding ä¸­æœ‰ overflowing tokensï¼Œåˆ™å°†å…¶åŠ å…¥ encodings åˆ—è¡¨
        if return_overflowing_tokens and encoding.overflowing is not None:
            encodings = [encoding] + encoding.overflowing
        else:
            encodings = [encoding]

        # åˆ›å»ºä¸€ä¸ª defaultdict ç”¨äºå­˜å‚¨ç¼–ç ç»“æœ
        encoding_dict = defaultdict(list)
        # éå† encodings åˆ—è¡¨
        for e in encodings:
            encoding_dict["input_ids"].append(e.ids)

            # å¦‚æœéœ€è¦è¿”å› token_type_idsï¼Œåˆ™æ·»åŠ åˆ° encoding_dict ä¸­
            if return_token_type_ids:
                encoding_dict["token_type_ids"].append(e.type_ids)
            # å¦‚æœéœ€è¦è¿”å› attention_maskï¼Œåˆ™æ·»åŠ åˆ° encoding_dict ä¸­
            if return_attention_mask:
                encoding_dict["attention_mask"].append(e.attention_mask)
            # å¦‚æœéœ€è¦è¿”å› special_tokens_maskï¼Œåˆ™æ·»åŠ åˆ° encoding_dict ä¸­
            if return_special_tokens_mask:
                encoding_dict["special_tokens_mask"].append(e.special_tokens_mask)
            # å¦‚æœéœ€è¦è¿”å› offsets_mappingï¼Œåˆ™æ·»åŠ åˆ° encoding_dict ä¸­
            if return_offsets_mapping:
                encoding_dict["offset_mapping"].append(e.offsets)
            # å¦‚æœéœ€è¦è¿”å› lengthï¼Œåˆ™æ·»åŠ åˆ° encoding_dict ä¸­
            if return_length:
                encoding_dict["length"].append(len(e.ids))

        # è¿”å›ç¼–ç ç»“æœå­—å…¸å’Œ encodings åˆ—è¡¨
        return encoding_dict, encodings

    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:
        """
        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
        vocabulary.

        Args:
            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).

        Returns:
            `int` or `List[int]`: The token id or list of token ids.
        """
        # å¦‚æœ tokens ä¸º Noneï¼Œåˆ™è¿”å› None
        if tokens is None:
            return None

        # å¦‚æœ tokens æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è°ƒç”¨ _convert_token_to_id_with_added_voc æ–¹æ³•è½¬æ¢ä¸º token id
        if isinstance(tokens, str):
            return self._convert_token_to_id_with_added_voc(tokens)

        # å¦‚æœ tokens æ˜¯åˆ—è¡¨ï¼Œåˆ™éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ª tokenï¼Œå¹¶è°ƒç”¨ _convert_token_to_id_with_added_voc æ–¹æ³•è½¬æ¢ä¸º token id
        return [self._convert_token_to_id_with_added_voc(token) for token in tokens]

    def _convert_token_to_id_with_added_voc(self, token: str) -> int:
        # ä½¿ç”¨ tokenizer å°† token è½¬æ¢ä¸ºå¯¹åº”çš„ id
        index = self._tokenizer.token_to_id(token)
        # å¦‚æœè¿”å›çš„ index ä¸º Noneï¼Œåˆ™è¿”å› unk_token_id
        if index is None:
            return self.unk_token_id
        # å¦åˆ™è¿”å›å¯¹åº”çš„ index
        return index
    # å°†ç´¢å¼•è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡è®°å­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›ç»“æœ
    def _convert_id_to_token(self, index: int) -> Optional[str]:
        return self._tokenizer.id_to_token(int(index))

    # æ·»åŠ æ–°æ ‡è®°åˆ°æ ‡è®°å™¨ä¸­
    def _add_tokens(self, new_tokens: List[Union[str, AddedToken]], special_tokens=False) -> int:
        # å¦‚æœéœ€è¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œåˆ™è°ƒç”¨æ ‡è®°å™¨çš„æ·»åŠ ç‰¹æ®Šæ ‡è®°æ–¹æ³•
        if special_tokens:
            return self._tokenizer.add_special_tokens(new_tokens)
        # å¦åˆ™è°ƒç”¨æ ‡è®°å™¨çš„æ·»åŠ æ ‡è®°æ–¹æ³•
        return self._tokenizer.add_tokens(new_tokens)

    # è®¡ç®—ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ç¼–ç åºåˆ—æ—¶æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ•°ç›®
    def num_special_tokens_to_add(self, pair: bool = False) -> int:
        """
        Returns the number of added tokens when encoding a sequence with special tokens.

        <Tip>

        This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put
        this inside your training loop.

        </Tip>

        Args:
            pair (`bool`, *optional*, defaults to `False`):
                Whether the number of added tokens should be computed in the case of a sequence pair or a single
                sequence.

        Returns:
            `int`: Number of special tokens added to sequences.
        """
        # è¿”å›ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ç¼–ç åºåˆ—æ—¶æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ•°ç›®
        return self._tokenizer.num_special_tokens_to_add(pair)

    # å°†æ ‡è®°ç´¢å¼•æˆ–ç´¢å¼•åˆ—è¡¨è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡è®°æˆ–æ ‡è®°åˆ—è¡¨
    def convert_ids_to_tokens(
        self, ids: Union[int, List[int]], skip_special_tokens: bool = False
    ) -> Union[str, List[str]]:
        """
        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
        added tokens.

        Args:
            ids (`int` or `List[int]`):
                The token id (or token ids) to convert to tokens.
            skip_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not to remove special tokens in the decoding.

        Returns:
            `str` or `List[str]`: The decoded token(s).
        """
        # å¦‚æœè¾“å…¥æ˜¯å•ä¸ªç´¢å¼•ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡è®°å¹¶è¿”å›
        if isinstance(ids, int):
            return self._tokenizer.id_to_token(ids)
        # å¦‚æœè¾“å…¥æ˜¯ç´¢å¼•åˆ—è¡¨ï¼Œåˆ™éå†åˆ—è¡¨ï¼Œå°†æ¯ä¸ªç´¢å¼•è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡è®°
        tokens = []
        for index in ids:
            index = int(index)
            # å¦‚æœè·³è¿‡ç‰¹æ®Šæ ‡è®°å¹¶ä¸”å½“å‰ç´¢å¼•æ˜¯ç‰¹æ®Šæ ‡è®°ä¹‹ä¸€ï¼Œåˆ™è·³è¿‡å½“å‰ç´¢å¼•
            if skip_special_tokens and index in self.all_special_ids:
                continue
            # å°†å½“å‰ç´¢å¼•è½¬æ¢ä¸ºå¯¹åº”çš„æ ‡è®°ï¼Œå¹¶æ·»åŠ åˆ°æ ‡è®°åˆ—è¡¨ä¸­
            tokens.append(self._tokenizer.id_to_token(index))
        # è¿”å›æ ‡è®°åˆ—è¡¨
        return tokens

    # å°†æ–‡æœ¬æ ‡è®°åŒ–ä¸ºæ ‡è®°åˆ—è¡¨
    def tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]:
        # è°ƒç”¨encode_plusæ–¹æ³•å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶è¿”å›æ ‡è®°åˆ—è¡¨
        return self.encode_plus(text=text, text_pair=pair, add_special_tokens=add_special_tokens, **kwargs).tokens()

    # è®¾ç½®æˆªæ–­å’Œå¡«å……ç­–ç•¥
    def set_truncation_and_padding(
        self,
        padding_strategy: PaddingStrategy,
        truncation_strategy: TruncationStrategy,
        max_length: int,
        stride: int,
        pad_to_multiple_of: Optional[int],
    ):
        """
        Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers
        library) and restore the tokenizer settings afterwards.

        The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a
        padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed
        section.

        Args:
            padding_strategy ([`~utils.PaddingStrategy`]):
                The kind of padding that will be applied to the input
            truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):
                The kind of truncation that will be applied to the input
            max_length (`int`):
                The maximum size of a sequence.
            stride (`int`):
                The stride to use when handling overflow.
            pad_to_multiple_of (`int`, *optional*):
                If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
                the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).
        """
        # Store the current truncation and padding settings
        _truncation = self._tokenizer.truncation
        _padding = self._tokenizer.padding
        # Set truncation and padding on the backend tokenizer
        if truncation_strategy == TruncationStrategy.DO_NOT_TRUNCATE:
            # If the truncation strategy is set to 'do not truncate', and there was a previous truncation strategy,
            # reset it to no truncation
            if _truncation is not None:
                self._tokenizer.no_truncation()
        else:
            # Define the target truncation settings
            target = {
                "max_length": max_length,
                "stride": stride,
                "strategy": truncation_strategy.value,
                "direction": self.truncation_side,
            }

            # Check if the current truncation settings match the target settings
            if _truncation is None:
                current = None
            else:
                current = {k: _truncation.get(k, None) for k in target}

            # If current truncation settings don't match target settings, enable truncation with target settings
            if current != target:
                self._tokenizer.enable_truncation(**target)

        if padding_strategy == PaddingStrategy.DO_NOT_PAD:
            # If the padding strategy is set to 'do not pad', and there was a previous padding strategy,
            # reset it to no padding
            if _padding is not None:
                self._tokenizer.no_padding()
        else:
            # Define the target padding settings
            length = max_length if padding_strategy == PaddingStrategy.MAX_LENGTH else None
            target = {
                "length": length,
                "direction": self.padding_side,
                "pad_id": self.pad_token_id,
                "pad_token": self.pad_token,
                "pad_type_id": self.pad_token_type_id,
                "pad_to_multiple_of": pad_to_multiple_of,
            }
            # If current padding settings don't match target settings, enable padding with target settings
            if _padding != target:
                self._tokenizer.enable_padding(**target)
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºæ‰¹é‡ç¼–ç æ–‡æœ¬æˆ–æ–‡æœ¬å¯¹
    def _batch_encode_plus(
        self,
        # è¾“å…¥å‚æ•°ä¸ºæ–‡æœ¬åˆ—è¡¨ã€æ–‡æœ¬å¯¹åˆ—è¡¨ã€é¢„åˆ†è¯è¾“å…¥åˆ—è¡¨æˆ–é¢„åˆ†è¯è¾“å…¥å¯¹åˆ—è¡¨
        batch_text_or_text_pairs: Union[
            List[TextInput], List[TextInputPair], List[PreTokenizedInput], List[PreTokenizedInputPair]
        ],
        # æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œé»˜è®¤ä¸ºTrue
        add_special_tokens: bool = True,
        # å¡«å……ç­–ç•¥ï¼Œé»˜è®¤ä¸ºä¸å¡«å……
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        # æˆªæ–­ç­–ç•¥ï¼Œé»˜è®¤ä¸ºä¸æˆªæ–­
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        # æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œé»˜è®¤ä¸ºNone
        max_length: Optional[int] = None,
        # æ­¥é•¿ï¼Œé»˜è®¤ä¸º0
        stride: int = 0,
        # æ˜¯å¦å·²åˆ†è¯ï¼Œé»˜è®¤ä¸ºFalse
        is_split_into_words: bool = False,
        # å¡«å……åˆ°æŒ‡å®šé•¿åº¦çš„å€æ•°ï¼Œé»˜è®¤ä¸ºNone
        pad_to_multiple_of: Optional[int] = None,
        # è¿”å›å¼ é‡ï¼Œé»˜è®¤ä¸ºNone
        return_tensors: Optional[str] = None,
        # è¿”å›tokenç±»å‹IDï¼Œé»˜è®¤ä¸ºNone
        return_token_type_ids: Optional[bool] = None,
        # è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œé»˜è®¤ä¸ºNone
        return_attention_mask: Optional[bool] = None,
        # è¿”å›æº¢å‡ºçš„tokenï¼Œé»˜è®¤ä¸ºFalse
        return_overflowing_tokens: bool = False,
        # è¿”å›ç‰¹æ®Šæ ‡è®°æ©ç ï¼Œé»˜è®¤ä¸ºFalse
        return_special_tokens_mask: bool = False,
        # è¿”å›åç§»æ˜ å°„ï¼Œé»˜è®¤ä¸ºFalse
        return_offsets_mapping: bool = False,
        # è¿”å›é•¿åº¦ï¼Œé»˜è®¤ä¸ºFalse
        return_length: bool = False,
        # æ˜¯å¦è¯¦ç»†è¾“å‡ºï¼Œé»˜è®¤ä¸ºTrue
        verbose: bool = True,
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºç¼–ç æ–‡æœ¬æˆ–æ–‡æœ¬å¯¹
    def _encode_plus(
        self,
        # è¾“å…¥æ–‡æœ¬ï¼Œå¯ä»¥æ˜¯æ–‡æœ¬æˆ–é¢„åˆ†è¯è¾“å…¥
        text: Union[TextInput, PreTokenizedInput],
        # ç¬¬äºŒä¸ªæ–‡æœ¬ï¼Œå¯é€‰ï¼Œå¯ä»¥æ˜¯æ–‡æœ¬æˆ–é¢„åˆ†è¯è¾“å…¥
        text_pair: Optional[Union[TextInput, PreTokenizedInput]] = None,
        # æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œé»˜è®¤ä¸ºTrue
        add_special_tokens: bool = True,
        # å¡«å……ç­–ç•¥ï¼Œé»˜è®¤ä¸ºä¸å¡«å……
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        # æˆªæ–­ç­–ç•¥ï¼Œé»˜è®¤ä¸ºä¸æˆªæ–­
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        # æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œé»˜è®¤ä¸ºNone
        max_length: Optional[int] = None,
        # æ­¥é•¿ï¼Œé»˜è®¤ä¸º0
        stride: int = 0,
        # æ˜¯å¦å·²åˆ†è¯ï¼Œé»˜è®¤ä¸ºFalse
        is_split_into_words: bool = False,
        # å¡«å……åˆ°æŒ‡å®šé•¿åº¦çš„å€æ•°ï¼Œé»˜è®¤ä¸ºNone
        pad_to_multiple_of: Optional[int] = None,
        # è¿”å›å¼ é‡ï¼Œé»˜è®¤ä¸ºNone
        return_tensors: Optional[bool] = None,
        # è¿”å›tokenç±»å‹IDï¼Œé»˜è®¤ä¸ºNone
        return_token_type_ids: Optional[bool] = None,
        # è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œé»˜è®¤ä¸ºNone
        return_attention_mask: Optional[bool] = None,
        # è¿”å›æº¢å‡ºçš„tokenï¼Œé»˜è®¤ä¸ºFalse
        return_overflowing_tokens: bool = False,
        # è¿”å›ç‰¹æ®Šæ ‡è®°æ©ç ï¼Œé»˜è®¤ä¸ºFalse
        return_special_tokens_mask: bool = False,
        # è¿”å›åç§»æ˜ å°„ï¼Œé»˜è®¤ä¸ºFalse
        return_offsets_mapping: bool = False,
        # è¿”å›é•¿åº¦ï¼Œé»˜è®¤ä¸ºFalse
        return_length: bool = False,
        # æ˜¯å¦è¯¦ç»†è¾“å‡ºï¼Œé»˜è®¤ä¸ºTrue
        verbose: bool = True,
        # å…¶ä»–å…³é”®å­—å‚æ•°
        **kwargs,
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ‰¹é‡ç¼–ç 
    def __call__(
        self,
        text: Union[str, List[str], List[int]],
        text_pair: Optional[Union[str, List[str], List[int]]] = None,
        is_split_into_words: bool = False,
        add_special_tokens: bool = True,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[str] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: Optional[bool] = None,
        return_special_tokens_mask: Optional[bool] = None,
        return_offsets_mapping: Optional[bool] = None,
        return_length: Optional[bool] = None,
        verbose: bool = True,
        **kwargs,
    ) -> BatchEncoding:
        # å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ‰¹é‡è¾“å…¥
        batched_input = [(text, text_pair)] if text_pair else [text]
        # è°ƒç”¨å†…éƒ¨æ–¹æ³•è¿›è¡Œæ‰¹é‡ç¼–ç 
        batched_output = self._batch_encode_plus(
            batched_input,
            is_split_into_words=is_split_into_words,
            add_special_tokens=add_special_tokens,
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            pad_to_multiple_of=pad_to_multiple_of,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            return_overflowing_tokens=return_overflowing_tokens,
            return_special_tokens_mask=return_special_tokens_mask,
            return_offsets_mapping=return_offsets_mapping,
            return_length=return_length,
            verbose=verbose,
            **kwargs,
        )

        # å¦‚æœè¿”å›çš„å¼ é‡ä¸ºNoneï¼Œåˆ™ç§»é™¤æ‰¹é‡ç»´åº¦
        # å¦‚æœæº¢å‡ºçš„æ ‡è®°ä½œä¸ºè¾“å‡ºçš„æ‰¹é‡è¿”å›ï¼Œåˆ™åœ¨è¿™ç§æƒ…å†µä¸‹ä¿ç•™å®ƒä»¬
        if return_tensors is None and not return_overflowing_tokens:
            batched_output = BatchEncoding(
                {
                    key: value[0] if len(value) > 0 and isinstance(value[0], list) else value
                    for key, value in batched_output.items()
                },
                batched_output.encodings,
            )

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è­¦å‘Šå…³äºåºåˆ—è¿‡é•¿
        self._eventual_warn_about_too_long_sequence(batched_output["input_ids"], max_length, verbose)

        # è¿”å›ï¿½ï¿½é‡ç¼–ç ç»“æœ
        return batched_output

    # å°†æ ‡è®°è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    def convert_tokens_to_string(self, tokens: List[str]) -> str:
        return self.backend_tokenizer.decoder.decode(tokens)

    # è§£ç æ–¹æ³•
    def _decode(
        self,
        token_ids: Union[int, List[int]],
        skip_special_tokens: bool = False,
        clean_up_tokenization_spaces: bool = None,
        **kwargs,
    ) -> str:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æºæ ‡è®°å™¨
        self._decode_use_source_tokenizer = kwargs.pop("use_source_tokenizer", False)

        # å¦‚æœtoken_idsæ˜¯æ•´æ•°ï¼Œåˆ™è½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(token_ids, int):
            token_ids = [token_ids]
        # è§£ç æ ‡è®°
        text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)

        # æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼
        clean_up_tokenization_spaces = (
            clean_up_tokenization_spaces
            if clean_up_tokenization_spaces is not None
            else self.clean_up_tokenization_spaces
        )
        if clean_up_tokenization_spaces:
            clean_text = self.clean_up_tokenization(text)
            return clean_text
        else:
            return text

    # ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹
    def _save_pretrained(
        self,
        save_directory: Union[str, os.PathLike],
        file_names: Tuple[str],
        legacy_format: Optional[bool] = None,
        filename_prefix: Optional[str] = None,
    ) -> Tuple[str]:
        """
        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens as well as in a unique JSON
        file containing {config + vocab + added-tokens}.
        """
        # å°†ä¿å­˜ç›®å½•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹
        save_directory = str(save_directory)

        # å¦‚æœæ…¢é€Ÿåˆ†è¯å™¨ç±»ä¸ºNoneä¸”legacy_formatä¸ºTrueï¼Œåˆ™å¼•å‘å€¼é”™è¯¯
        if self.slow_tokenizer_class is None and legacy_format is True:
            raise ValueError(
                "Your tokenizer does not have a legacy version defined and therefore cannot register this version. You"
                " might consider leaving the legacy_format at `None` or setting it to `False`."
            )

        # åˆ¤æ–­æ˜¯å¦ä¿å­˜æ…¢é€Ÿåˆ†è¯å™¨
        save_slow = (
            (legacy_format is None or legacy_format is True)
            and self.slow_tokenizer_class is not None
            and self.can_save_slow_tokenizer
        )
        # åˆ¤æ–­æ˜¯å¦ä¿å­˜å¿«é€Ÿåˆ†è¯å™¨
        save_fast = legacy_format is None or legacy_format is False

        # å¦‚æœéœ€è¦ä¿å­˜æ…¢é€Ÿåˆ†è¯å™¨
        if save_slow:
            # æ„å»ºæ·»åŠ çš„æ ‡è®°æ–‡ä»¶è·¯å¾„
            added_tokens_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + ADDED_TOKENS_FILE
            )
            # ç¡®ä¿å‘å‰å…¼å®¹
            added_vocab = {tok: index for tok, index in self.added_tokens_encoder.items() if index >= self.vocab_size}
            # å¦‚æœå­˜åœ¨æ·»åŠ çš„è¯æ±‡
            if added_vocab:
                with open(added_tokens_file, "w", encoding="utf-8") as f:
                    out_str = json.dumps(added_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
                    f.write(out_str)

            # ä¿å­˜è¯æ±‡æ–‡ä»¶
            vocab_files = self.save_vocabulary(save_directory, filename_prefix=filename_prefix)
            file_names = file_names + vocab_files + (added_tokens_file,)

        # å¦‚æœéœ€è¦ä¿å­˜å¿«é€Ÿåˆ†è¯å™¨
        if save_fast:
            # æ„å»ºåˆ†è¯å™¨æ–‡ä»¶è·¯å¾„
            tokenizer_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + TOKENIZER_FILE
            )
            # ä¿å­˜åˆ†è¯å™¨
            self.backend_tokenizer.save(tokenizer_file)
            file_names = file_names + (tokenizer_file,)

        # è¿”å›æ–‡ä»¶ååˆ—è¡¨
        return file_names

    def train_new_from_iterator(
        self,
        text_iterator,
        vocab_size,
        length=None,
        new_special_tokens=None,
        special_tokens_map=None,
        **kwargs,
```