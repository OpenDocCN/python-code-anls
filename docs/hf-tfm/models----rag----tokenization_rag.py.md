# `.\transformers\models\rag\tokenization_rag.py`

```
# è¯¥è„šæœ¬å®šä¹‰äº† RagTokenizer ç±»ï¼Œç”¨äº RAG æ¨¡å‹çš„è¾“å…¥/è¾“å‡ºæ•°æ®çš„ç¼–ç å’Œè§£ç ã€‚
# coding=utf-8
# Copyright 2020, The RAG Authors and The HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Tokenization classes for RAG."""
import os
import warnings
from typing import List, Optional

# å¯¼å…¥æ‰€éœ€çš„æ¨¡å—å’Œç±»
from ...tokenization_utils_base import BatchEncoding
from ...utils import logging
from .configuration_rag import RagConfig

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# RagTokenizer ç±»å®šä¹‰
class RagTokenizer:
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥æ”¶ question_encoder å’Œ generator ä¸¤ä¸ª Tokenizer å¯¹è±¡
    def __init__(self, question_encoder, generator):
        self.question_encoder = question_encoder
        self.generator = generator
        self.current_tokenizer = self.question_encoder

    # å°†æ¨¡å‹å’Œä¸¤ä¸ª Tokenizer ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    def save_pretrained(self, save_directory):
        # æ£€æŸ¥ä¿å­˜ç›®å½•æ˜¯å¦ä¸ºæ–‡ä»¶
        if os.path.isfile(save_directory):
            raise ValueError(f"Provided path ({save_directory}) should be a directory, not a file")
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_directory, exist_ok=True)
        # å®šä¹‰ question_encoder å’Œ generator çš„ä¿å­˜è·¯å¾„
        question_encoder_path = os.path.join(save_directory, "question_encoder_tokenizer")
        generator_path = os.path.join(save_directory, "generator_tokenizer")
        # åˆ†åˆ«ä¿å­˜ä¸¤ä¸ª Tokenizer
        self.question_encoder.save_pretrained(question_encoder_path)
        self.generator.save_pretrained(generator_path)

    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹åŠ è½½ RagTokenizer
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        # åŠ¨æ€å¯¼å…¥ AutoTokenizer ç±»
        from ..auto.tokenization_auto import AutoTokenizer

        # ä»ä¼ å…¥çš„å‚æ•°ä¸­è·å–é…ç½®å¯¹è±¡
        config = kwargs.pop("config", None)

        # å¦‚æœæœªä¼ å…¥é…ç½®å¯¹è±¡ï¼Œåˆ™ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½é…ç½®
        if config is None:
            config = RagConfig.from_pretrained(pretrained_model_name_or_path)

        # åˆ†åˆ«ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ question_encoder å’Œ generator ä¸¤ä¸ª Tokenizer
        question_encoder = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path, config=config.question_encoder, subfolder="question_encoder_tokenizer"
        )
        generator = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path, config=config.generator, subfolder="generator_tokenizer"
        )
        # è¿”å› RagTokenizer å®ä¾‹
        return cls(question_encoder=question_encoder, generator=generator)

    # è°ƒç”¨å½“å‰çš„ Tokenizer
    def __call__(self, *args, **kwargs):
        return self.current_tokenizer(*args, **kwargs)

    # è°ƒç”¨ generator Tokenizer çš„ batch_decode æ–¹æ³•
    def batch_decode(self, *args, **kwargs):
        return self.generator.batch_decode(*args, **kwargs)

    # è°ƒç”¨ generator Tokenizer çš„ decode æ–¹æ³•
    def decode(self, *args, **kwargs):
        return self.generator.decode(*args, **kwargs)

    # åˆ‡æ¢åˆ°è¾“å…¥æ¨¡å¼ï¼ˆä½¿ç”¨ question_encoder Tokenizerï¼‰
    def _switch_to_input_mode(self):
        self.current_tokenizer = self.question_encoder

    # åˆ‡æ¢åˆ°è¾“å‡ºæ¨¡å¼ï¼ˆä½¿ç”¨ generator Tokenizerï¼‰
    def _switch_to_target_mode(self):
        self.current_tokenizer = self.generator
    # å‘å‡ºè­¦å‘Šï¼Œæç¤º `prepare_seq2seq_batch` æ–¹æ³•å°†åœ¨ ğŸ¤— Transformers ç‰ˆæœ¬ 5 ä¸­è¢«ç§»é™¤ï¼Œå»ºè®®ä½¿ç”¨æ­£å¸¸çš„ `__call__` æ–¹æ³•å‡†å¤‡è¾“å…¥ï¼Œå¹¶åœ¨ `with_target_tokenizer` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸‹ä½¿ç”¨åˆ†è¯å™¨å‡†å¤‡ç›®æ ‡ã€‚æŸ¥çœ‹ç‰¹å®šåˆ†è¯å™¨çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šè¯¦æƒ…
    warnings.warn(
        "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the "
        "regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` "
        "context manager to prepare your targets. See the documentation of your specific tokenizer for more "
        "details",
        FutureWarning,
    )
    # å¦‚æœæœªæŒ‡å®šæœ€å¤§é•¿åº¦ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºå½“å‰åˆ†è¯å™¨çš„æ¨¡å‹æœ€å¤§é•¿åº¦
    if max_length is None:
        max_length = self.current_tokenizer.model_max_length
    # ä½¿ç”¨å½“å‰å¯¹è±¡ä½œä¸ºåˆ†è¯å™¨ï¼Œå‡†å¤‡æ¨¡å‹è¾“å…¥
    model_inputs = self(
        src_texts,  # æºæ–‡æœ¬åˆ—è¡¨
        add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        return_tensors=return_tensors,  # è¿”å›å¼ é‡ç±»å‹
        max_length=max_length,  # æœ€å¤§é•¿åº¦
        padding=padding,  # å¡«å……ç­–ç•¥
        truncation=truncation,  # æˆªæ–­ç­–ç•¥
        **kwargs,
    )
    # å¦‚æœç›®æ ‡æ–‡æœ¬åˆ—è¡¨æœªæä¾›ï¼Œåˆ™è¿”å›æ¨¡å‹è¾“å…¥
    if tgt_texts is None:
        return model_inputs
    # å¤„ç†ç›®æ ‡æ–‡æœ¬
    # å¦‚æœæœªæŒ‡å®šæœ€å¤§ç›®æ ‡é•¿åº¦ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºå½“å‰åˆ†è¯å™¨çš„æ¨¡å‹æœ€å¤§é•¿åº¦
    if max_target_length is None:
        max_target_length = self.current_tokenizer.model_max_length
    # ä½¿ç”¨å½“å‰å¯¹è±¡ä½œä¸ºåˆ†è¯å™¨ï¼Œå‡†å¤‡æ ‡ç­¾
    labels = self(
        text_target=tgt_texts,  # ç›®æ ‡æ–‡æœ¬åˆ—è¡¨
        add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        return_tensors=return_tensors,  # è¿”å›å¼ é‡ç±»å‹
        padding=padding,  # å¡«å……ç­–ç•¥
        max_length=max_target_length,  # æœ€å¤§é•¿åº¦
        truncation=truncation,  # æˆªæ–­ç­–ç•¥
        **kwargs,
    )
    # å°†æ ‡ç­¾çš„è¾“å…¥ ID å­˜å‚¨åœ¨æ¨¡å‹è¾“å…¥ä¸­çš„ "labels" é”®ä¸‹
    model_inputs["labels"] = labels["input_ids"]
    # è¿”å›æ¨¡å‹è¾“å…¥
    return model_inputs
```