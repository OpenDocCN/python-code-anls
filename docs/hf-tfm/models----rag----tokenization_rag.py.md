# `.\models\rag\tokenization_rag.py`

```
# coding=utf-8
# å£°æ˜æ–‡ä»¶ç¼–ç æ ¼å¼ä¸º UTF-8

# ç‰ˆæƒå£°æ˜å’Œè®¸å¯è¯ä¿¡æ¯

# å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œç±»
import os
import warnings
from typing import List, Optional

# å¯¼å…¥æ—¥å¿—è®°å½•å·¥å…·
from ...tokenization_utils_base import BatchEncoding
from ...utils import logging
from .configuration_rag import RagConfig

# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)


class RagTokenizer:
    def __init__(self, question_encoder, generator):
        # åˆå§‹åŒ– RAG Tokenizer ç±»ï¼Œæ¥å—é—®é¢˜ç¼–ç å™¨å’Œç”Ÿæˆå™¨ä½œä¸ºå‚æ•°
        self.question_encoder = question_encoder
        self.generator = generator
        self.current_tokenizer = self.question_encoder

    def save_pretrained(self, save_directory):
        # å°†å½“å‰ tokenizer å®ä¾‹ä¿å­˜åˆ°æŒ‡å®šç›®å½•ä¸‹
        if os.path.isfile(save_directory):
            # å¦‚æœä¿å­˜è·¯å¾„æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(f"Provided path ({save_directory}) should be a directory, not a file")
        # åˆ›å»ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
        os.makedirs(save_directory, exist_ok=True)
        # åˆ†åˆ«ä¿å­˜é—®é¢˜ç¼–ç å™¨å’Œç”Ÿæˆå™¨çš„ tokenizer åˆ°æŒ‡å®šç›®å½•ä¸‹çš„ä¸åŒå­ç›®å½•
        question_encoder_path = os.path.join(save_directory, "question_encoder_tokenizer")
        generator_path = os.path.join(save_directory, "generator_tokenizer")
        self.question_encoder.save_pretrained(question_encoder_path)
        self.generator.save_pretrained(generator_path)

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        # ä»é¢„è®­ç»ƒæ¨¡å‹æˆ–è·¯å¾„åŠ è½½ RAG Tokenizer å®ä¾‹
        # åŠ¨æ€å¯¼å…¥ AutoTokenizer ç±»
        from ..auto.tokenization_auto import AutoTokenizer

        # è·å–é…ç½®ä¿¡æ¯ï¼Œå¦‚æœæœªæä¾›åˆ™ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
        config = kwargs.pop("config", None)
        if config is None:
            config = RagConfig.from_pretrained(pretrained_model_name_or_path)

        # æ ¹æ®é…ç½®åŠ è½½é—®é¢˜ç¼–ç å™¨å’Œç”Ÿæˆå™¨çš„ tokenizer
        question_encoder = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path, config=config.question_encoder, subfolder="question_encoder_tokenizer"
        )
        generator = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path, config=config.generator, subfolder="generator_tokenizer"
        )
        return cls(question_encoder=question_encoder, generator=generator)

    def __call__(self, *args, **kwargs):
        # å®ç° __call__ æ–¹æ³•ï¼Œå…è®¸å®ä¾‹åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨
        return self.current_tokenizer(*args, **kwargs)

    def batch_decode(self, *args, **kwargs):
        # è°ƒç”¨ç”Ÿæˆå™¨çš„æ‰¹é‡è§£ç æ–¹æ³•
        return self.generator.batch_decode(*args, **kwargs)

    def decode(self, *args, **kwargs):
        # è°ƒç”¨ç”Ÿæˆå™¨çš„è§£ç æ–¹æ³•
        return self.generator.decode(*args, **kwargs)

    def _switch_to_input_mode(self):
        # åˆ‡æ¢å½“å‰ tokenizer åˆ°é—®é¢˜ç¼–ç å™¨æ¨¡å¼
        self.current_tokenizer = self.question_encoder

    def _switch_to_target_mode(self):
        # åˆ‡æ¢å½“å‰ tokenizer åˆ°ç”Ÿæˆå™¨æ¨¡å¼
        self.current_tokenizer = self.generator
    # è­¦å‘Šï¼š`prepare_seq2seq_batch`å·²è¢«å¼ƒç”¨ï¼Œå¹¶å°†åœ¨ğŸ¤— Transformersç‰ˆæœ¬5ä¸­ç§»é™¤ã€‚è¯·ä½¿ç”¨å¸¸è§„çš„`__call__`æ–¹æ³•å‡†å¤‡è¾“å…¥ï¼Œå¹¶åœ¨`with_target_tokenizer`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸‹ä½¿ç”¨åˆ†è¯å™¨å‡†å¤‡ç›®æ ‡ã€‚æŸ¥çœ‹ç‰¹å®šåˆ†è¯å™¨çš„æ–‡æ¡£è·å–æ›´å¤šè¯¦æƒ…
    warnings.warn(
        "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the "
        "regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` "
        "context manager to prepare your targets. See the documentation of your specific tokenizer for more "
        "details",
        FutureWarning,
    )
    
    # å¦‚æœæœªæä¾›æœ€å¤§é•¿åº¦å‚æ•°ï¼Œåˆ™ä½¿ç”¨å½“å‰åˆ†è¯å™¨çš„æ¨¡å‹æœ€å¤§é•¿åº¦
    if max_length is None:
        max_length = self.current_tokenizer.model_max_length
    
    # ä½¿ç”¨æ¨¡å‹çš„__call__æ–¹æ³•å‡†å¤‡è¾“å…¥ï¼ŒåŒ…æ‹¬æºæ–‡æœ¬ã€æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€è¿”å›çš„å¼ é‡ç±»å‹ã€æœ€å¤§é•¿åº¦ã€å¡«å……æ–¹å¼å’Œæˆªæ–­æ ‡å¿—
    model_inputs = self(
        src_texts,
        add_special_tokens=True,
        return_tensors=return_tensors,
        max_length=max_length,
        padding=padding,
        truncation=truncation,
        **kwargs,
    )
    
    # å¦‚æœæœªæä¾›ç›®æ ‡æ–‡æœ¬ï¼Œåˆ™ç›´æ¥è¿”å›æ¨¡å‹è¾“å…¥
    if tgt_texts is None:
        return model_inputs
    
    # å¤„ç†ç›®æ ‡æ–‡æœ¬
    # å¦‚æœæœªæä¾›æœ€å¤§ç›®æ ‡é•¿åº¦å‚æ•°ï¼Œåˆ™ä½¿ç”¨å½“å‰åˆ†è¯å™¨çš„æ¨¡å‹æœ€å¤§é•¿åº¦
    if max_target_length is None:
        max_target_length = self.current_tokenizer.model_max_length
    
    # ä½¿ç”¨æ¨¡å‹çš„__call__æ–¹æ³•å‡†å¤‡ç›®æ ‡æ ‡ç­¾ï¼ŒåŒ…æ‹¬ç›®æ ‡æ–‡æœ¬ã€æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€è¿”å›çš„å¼ é‡ç±»å‹ã€å¡«å……æ–¹å¼ã€æœ€å¤§é•¿åº¦å’Œæˆªæ–­æ ‡å¿—
    labels = self(
        text_target=tgt_texts,
        add_special_tokens=True,
        return_tensors=return_tensors,
        padding=padding,
        max_length=max_target_length,
        truncation=truncation,
        **kwargs,
    )
    
    # å°†å‡†å¤‡å¥½çš„ç›®æ ‡æ ‡ç­¾çš„è¾“å…¥IDå­˜å‚¨åœ¨æ¨¡å‹è¾“å…¥å­—å…¸ä¸­çš„"labels"é”®ä¸‹
    model_inputs["labels"] = labels["input_ids"]
    
    # è¿”å›æœ€ç»ˆçš„æ¨¡å‹è¾“å…¥å­—å…¸ï¼ŒåŒ…æ‹¬æºæ–‡æœ¬ã€å¯èƒ½çš„ç›®æ ‡æ–‡æœ¬æ ‡ç­¾
    return model_inputs
```