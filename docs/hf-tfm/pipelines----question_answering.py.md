# `.\transformers\pipelines\question_answering.py`

```
# 导入模块和库
import inspect
import types
import warnings
from collections.abc import Iterable
from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union
# 导入 numpy 库，并起别名 np
import numpy as np
# 导入相关模块和库
from ..data import SquadExample, SquadFeatures, squad_convert_examples_to_features
from ..modelcard import ModelCard
from ..tokenization_utils import PreTrainedTokenizer
from ..utils import (
    PaddingStrategy,
    add_end_docstrings,
    is_tf_available,
    is_tokenizers_available,
    is_torch_available,
    logging,
)
from .base import PIPELINE_INIT_ARGS, ArgumentHandler, ChunkPipeline
# 获取 logger 对象
logger = logging.get_logger(__name__)
# 如果是类型检查
if TYPE_CHECKING:
    from ..modeling_tf_utils import TFPreTrainedModel
    from ..modeling_utils import PreTrainedModel
    # 如果 tokenizers 可用，则导入相关模块
    if is_tokenizers_available():
        import tokenizers
# 如果是 TensorFlow 可用
if is_tf_available():
    # 导入 TensorFlow 模块
    import tensorflow as tf
    # 导入相关模块
    from ..models.auto.modeling_tf_auto import TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
    Dataset = None
# 如果是 PyTorch 可用
if is_torch_available():
    # 导入 PyTorch 模块
    import torch
    from torch.utils.data import Dataset
    from ..models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES

# 定义函数，传入 numpy 数组、整数、整数和 undesired_tokens 的 numpy 数组，返回元组
def decode_spans(
    start: np.ndarray, end: np.ndarray, topk: int, max_answer_len: int, undesired_tokens: np.ndarray
) -> Tuple:
    """
    Take the output of any `ModelForQuestionAnswering` and will generate probabilities for each span to be the actual
    answer.
    In addition, it filters out some unwanted/impossible cases like answer len being greater than max_answer_len or
    answer end position being before the starting position. The method supports output the k-best answer through the
    topk argument.

    Args:
        start (`np.ndarray`): Individual start probabilities for each token.
        end (`np.ndarray`): Individual end probabilities for each token.
        topk (`int`): Indicates how many possible answer span(s) to extract from the model output.
        max_answer_len (`int`): Maximum size of the answer to extract from the model's output.
        undesired_tokens (`np.ndarray`): Mask determining tokens that can be part of the answer
    """
    # 确保 start 数组有批次轴
    if start.ndim == 1:
        start = start[None]
    # 确保 end 数组有批次轴
    if end.ndim == 1:
        end = end[None]
    # 计算每个元组（start，end）成为实际答案的分数
    outer = np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))
    # 移除结束位置小于起始位置和结束位置 - 起始位置大于 max_answer_len 的候选答案
    candidates = np.tril(np.triu(outer), max_answer_len - 1)
    # 受 Chen & al. 启发（https://github.com/facebookresearch/DrQA）
    scores_flat = candidates.flatten()
    # 如果 topk 为 1
    if topk == 1:
        idx_sort = [np.argmax(scores_flat)]
    # 如果长度小于 topk
    elif len(scores_flat) < topk:
        idx_sort = np.argsort(-scores_flat)
    else:
        idx = np.argpartition(-scores_flat, topk)[0:topk]
        idx_sort = idx[np.argsort(-scores_flat[idx])]
    # 使用最终排序索引获取起始位置和结束位置
    starts, ends = np.unravel_index(idx_sort, candidates.shape)[1:]
    # 检查起始位置和结束位置是否在不希望的标记范围内，返回布尔数组
    desired_spans = np.isin(starts, undesired_tokens.nonzero()) & np.isin(ends, undesired_tokens.nonzero())
    # 根据布尔数组过滤起始位置数组
    starts = starts[desired_spans]
    # 根据布尔数组过滤结束位置数组
    ends = ends[desired_spans]
    # 从候选项中获取得分
    scores = candidates[0, starts, ends]
    
    # 返回过滤后的起始位置、结束位置和得分
    return starts, ends, scores
def select_starts_ends(
    start,
    end,
    p_mask,
    attention_mask,
    min_null_score=1000000,
    top_k=1,
    handle_impossible_answer=False,
    max_answer_len=15,
):
    """
    Takes the raw output of any `ModelForQuestionAnswering` and first normalizes its outputs and then uses
    `decode_spans()` to generate probabilities for each span to be the actual answer.

    Args:
        start (`np.ndarray`): Individual start logits for each token.
        end (`np.ndarray`): Individual end logits for each token.
        p_mask (`np.ndarray`): A mask with 1 for values that cannot be in the answer
        attention_mask (`np.ndarray`): The attention mask generated by the tokenizer
        min_null_score(`float`): The minimum null (empty) answer score seen so far.
        topk (`int`): Indicates how many possible answer span(s) to extract from the model output.
        handle_impossible_answer(`bool`): Whether to allow null (empty) answers
        max_answer_len (`int`): Maximum size of the answer to extract from the model's output.
    """
    # 确保填充的标记和问题标记不能属于候选答案集
    undesired_tokens = np.abs(np.array(p_mask) - 1)

    if attention_mask is not None:
        undesired_tokens = undesired_tokens & attention_mask

    # 生成掩码
    undesired_tokens_mask = undesired_tokens == 0.0

    # 确保张量中的非上下文索引不能贡献到 softmax
    start = np.where(undesired_tokens_mask, -10000.0, start)
    end = np.where(undesired_tokens_mask, -10000.0, end)

    # 规范化对数和跨度以检索答案
    start = np.exp(start - start.max(axis=-1, keepdims=True))
    start = start / start.sum()

    end = np.exp(end - end.max(axis=-1, keepdims=True))
    end = end / end.sum()

    if handle_impossible_answer:
        min_null_score = min(min_null_score, (start[0, 0] * end[0, 0]).item())

    # 掩码 CLS
    start[0, 0] = end[0, 0] = 0.0

    # 解码跨度以检索可能的答案
    starts, ends, scores = decode_spans(start, end, top_k, max_answer_len, undesired_tokens)
    return starts, ends, scores, min_null_score


class QuestionAnsweringArgumentHandler(ArgumentHandler):
    """
    QuestionAnsweringPipeline requires the user to provide multiple arguments (i.e. question & context) to be mapped to
    internal [`SquadExample`].

    QuestionAnsweringArgumentHandler manages all the possible to create a [`SquadExample`] from the command-line
    supplied arguments.
    """
    # 将输入的项目标准化为SquadExample对象
    def normalize(self, item):
        # 如果项目是SquadExample对象，则直接返回
        if isinstance(item, SquadExample):
            return item
        # 如果项目是字典，则检查其键和取值是否满足条件
        elif isinstance(item, dict):
            for k in ["question", "context"]:
                # 检查字典中是否包含必要的键
                if k not in item:
                    raise KeyError("You need to provide a dictionary with keys {question:..., context:...}")
                # 检查键对应的取值是否为None
                elif item[k] is None:
                    raise ValueError(f"`{k}` cannot be None")
                # 检查键对应的取值是否为字符串且不为空
                elif isinstance(item[k], str) and len(item[k]) == 0:
                    raise ValueError(f"`{k}` cannot be empty")
            # 创建QuestionAnsweringPipeline对象
            return QuestionAnsweringPipeline.create_sample(**item)
        # 如果不是SquadExample或者字典，则抛出异常
        raise ValueError(f"{item} argument needs to be of type (SquadExample, dict)")

    def __call__(self, *args, **kwargs):
        # 检测输入数据的位置
        if args is not None and len(args) > 0:
            # 如果只有一个参数，则将其作为输入
            if len(args) == 1:
                inputs = args[0]
            # 如果有两个参数且都是字符串，则构建包含问题和上下文的字典列表作为输入
            elif len(args) == 2 and {type(el) for el in args} == {str}:
                inputs = [{"question": args[0], "context": args[1]}]
            else:
                inputs = list(args)
        # 兼容性处理，兼容sklearn和Keras的批量数据输入
        elif "X" in kwargs:
            inputs = kwargs["X"]
        elif "data" in kwargs:
            inputs = kwargs["data"]
        elif "question" in kwargs and "context" in kwargs:
            # 根据不同情况构建包含问题和上下文的字典列表作为输入
            if isinstance(kwargs["question"], list) and isinstance(kwargs["context"], str):
                inputs = [{"question": Q, "context": kwargs["context"]} for Q in kwargs["question"]]
            elif isinstance(kwargs["question"], list) and isinstance(kwargs["context"], list):
                # 检查问题和上下文的列表长度是否一致
                if len(kwargs["question"]) != len(kwargs["context"]):
                    raise ValueError("Questions and contexts don't have the same lengths")
                inputs = [{"question": Q, "context": C} for Q, C in zip(kwargs["question"], kwargs["context"])]
            elif isinstance(kwargs["question"], str) and isinstance(kwargs["context"], str):
                inputs = [{"question": kwargs["question"], "context": kwargs["context"]}]
            else:
                raise ValueError("Arguments can't be understood")
        else:
            raise ValueError(f"Unknown arguments {kwargs}")

        # 当用户传入生成器时，需要直接返回该生成器
        generator_types = (types.GeneratorType, Dataset) if Dataset is not None else (types.GeneratorType,)
        if isinstance(inputs, generator_types):
            return inputs

        # 标准化输入
        if isinstance(inputs, dict):
            inputs = [inputs]
        elif isinstance(inputs, Iterable):
            # 复制一份以避免覆盖原始参数
            inputs = list(inputs)
        else:
            raise ValueError(f"Invalid arguments {kwargs}")

        # 对输入进行标准化处理
        for i, item in enumerate(inputs):
            inputs[i] = self.normalize(item)

        return inputs
# 添加文档字符串到 QuestionAnsweringPipeline 类，用于描述问题回答管道的使用和功能
@add_end_docstrings(PIPELINE_INIT_ARGS)
class QuestionAnsweringPipeline(ChunkPipeline):
    """
    Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question answering
    examples](../task_summary#question-answering) for more information.

    Example:

    ```python
    >>> from transformers import pipeline

    >>> oracle = pipeline(model="deepset/roberta-base-squad2")
    >>> oracle(question="Where do I live?", context="My name is Wolfgang and I live in Berlin")
    {'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}
    ```

    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

    This question answering pipeline can currently be loaded from [`pipeline`] using the following task identifier:
    `"question-answering"`.

    The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the
    up-to-date list of available models on
    [huggingface.co/models](https://huggingface.co/models?filter=question-answering).
    """

    # 设置默认的输入名称
    default_input_names = "question,context"
    # 设置是否处理不可能回答的答案
    handle_impossible_answer = False

    # 初始化 QuestionAnsweringPipeline 类
    def __init__(
        self,
        model: Union["PreTrainedModel", "TFPreTrainedModel"],
        tokenizer: PreTrainedTokenizer,
        modelcard: Optional[ModelCard] = None,
        framework: Optional[str] = None,
        task: str = "",
        **kwargs,
    ):
        # 调用父类的初始化方法
        super().__init__(
            model=model,
            tokenizer=tokenizer,
            modelcard=modelcard,
            framework=framework,
            task=task,
            **kwargs,
        )

        # 创建问题回答参数处理器
        self._args_parser = QuestionAnsweringArgumentHandler()
        # 检查模型类型
        self.check_model_type(
            TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
            if self.framework == "tf"
            else MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
        )

    # 创建样本的静态方法
    @staticmethod
    def create_sample(
        question: Union[str, List[str]], context: Union[str, List[str]]
    ) -> Union[SquadExample, List[SquadExample]]:
        """
        QuestionAnsweringPipeline leverages the [`SquadExample`] internally. This helper method encapsulate all the
        logic for converting question(s) and context(s) to [`SquadExample`].

        We currently support extractive question answering.

        Arguments:
            question (`str` or `List[str]`): The question(s) asked.
            context (`str` or `List[str]`): The context(s) in which we will look for the answer.

        Returns:
            One or a list of [`SquadExample`]: The corresponding [`SquadExample`] grouping question and context.
        """
        # 如果问题是列表，则返回问题和上下文的对应 SquadExample 组成的列表
        if isinstance(question, list):
            return [SquadExample(None, q, c, None, None, None) for q, c in zip(question, context)]
        # 否则返回单个 SquadExample
        else:
            return SquadExample(None, question, context, None, None, None)
    # 对参数进行清理和处理的私有方法
    def _sanitize_parameters(
        self,
        padding=None,
        topk=None,
        top_k=None,
        doc_stride=None,
        max_answer_len=None,
        max_seq_len=None,
        max_question_len=None,
        handle_impossible_answer=None,
        align_to_words=None,
        **kwargs,
    ):
        # 设置默认值参数字典
        preprocess_params = {}
        # 如果 padding 参数不为空，则将其添加到预处理参数字典中
        if padding is not None:
            preprocess_params["padding"] = padding
        # 如果 doc_stride 参数不为空，则将其添加到预处理参数字典中
        if doc_stride is not None:
            preprocess_params["doc_stride"] = doc_stride
        # 如果 max_question_len 参数不为空，则将其添加到预处理参数字典中
        if max_question_len is not None:
            preprocess_params["max_question_len"] = max_question_len
        # 如果 max_seq_len 参数不为空，则将其添加到预处理参数字典中
        if max_seq_len is not None:
            preprocess_params["max_seq_len"] = max_seq_len

        # 设置默认值参数字典
        postprocess_params = {}
        # 如果 topk 参数不为空且 top_k 参数为空，则发出警告并将 topk 参数赋值给 top_k
        if topk is not None and top_k is None:
            warnings.warn("topk parameter is deprecated, use top_k instead", UserWarning)
            top_k = topk
        # 如果 top_k 参数不为空，则将其添加到后处理参数字典中
        if top_k is not None:
            # 如果 top_k 参数小于 1，则抛出异常
            if top_k < 1:
                raise ValueError(f"top_k parameter should be >= 1 (got {top_k})")
            postprocess_params["top_k"] = top_k
        # 如果 max_answer_len 参数不为空，则将其添加到后处理参数字典中
        if max_answer_len is not None:
            # 如果 max_answer_len 参数小于 1，则抛出异常
            if max_answer_len < 1:
                raise ValueError(f"max_answer_len parameter should be >= 1 (got {max_answer_len}")
            postprocess_params["max_answer_len"] = max_answer_len
        # 如果 handle_impossible_answer 参数不为空，则将其添加到后处理参数字典中
        if handle_impossible_answer is not None:
            postprocess_params["handle_impossible_answer"] = handle_impossible_answer
        # 如果 align_to_words 参数不为空，则将其添加到后处理参数字典中
        if align_to_words is not None:
            postprocess_params["align_to_words"] = align_to_words
        # 返回预处理参数字典、空字典和后处理参数字典
        return preprocess_params, {}, postprocess_params

    # 执行前向传播的方法
    def _forward(self, inputs):
        # 获取输入中的例子
        example = inputs["example"]
        # 从输入中选择模型需要的输入特征
        model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}
        # 检查模型前向传播函数的签名，如果支持 use_cache 参数，则设置为 False
        model_forward = self.model.forward if self.framework == "pt" else self.model.call
        if "use_cache" in inspect.signature(model_forward).parameters.keys():
            model_inputs["use_cache"] = False
        # 调用模型进行前向传播
        output = self.model(**model_inputs)
        # 如果输出为字典，则返回 start_logits、end_logits、例子以及输入
        if isinstance(output, dict):
            return {"start": output["start_logits"], "end": output["end_logits"], "example": example, **inputs}
        # 如果输出不是字典，则返回输出的前两个元素作为开始和结束位置的 logit，例子以及输入
        else:
            start, end = output[:2]
            return {"start": start, "end": end, "example": example, **inputs}

    # 后处理方法，用于处理模型输出
    def postprocess(
        self,
        model_outputs,
        top_k=1,
        handle_impossible_answer=False,
        max_answer_len=15,
        align_to_words=True,
    ):
        # 该方法的实现需要在调用时完成

    # 获取索引的方法，用于从编码中获取起始和结束索引
    def get_indices(
        self, enc: "tokenizers.Encoding", s: int, e: int, sequence_index: int, align_to_words: bool
    # 定义带有类型提示的函数，参数为两个整数，返回一个元组
    def char_to_word_offset(self, s: int, e: int, align_to_words: bool = True, sequence_index: int = 0) -> Tuple[int, int]:
        if align_to_words:
            # 如果 align_to_words 为 True，则尝试将 token 转换为单词
            try:
                start_word = enc.token_to_word(s)
                end_word = enc.token_to_word(e)
                # 根据单词获取字符偏移量
                start_index = enc.word_to_chars(start_word, sequence_index=sequence_index)[0]
                end_index = enc.word_to_chars(end_word, sequence_index=sequence_index)[1]
            except Exception:
                # 捕获异常，对不太处理单词的分词器使用偏移量
                start_index = enc.offsets[s][0]
                end_index = enc.offsets[e][1]
        else:
            # 如果 align_to_words 为 False，则直接使用偏移量
            start_index = enc.offsets[s][0]
            end_index = enc.offsets[e][1]
        return start_index, end_index  # 返回开始字符偏移量和结束字符偏移量

    # 定义带有类型提示的方法，参数为字符串和两个整数，返回一个字典
    def span_to_answer(self, text: str, start: int, end: int) -> Dict[str, Union[str, int]]:
        """
        When decoding from token probabilities, this method maps token indexes to actual word in the initial context.

        Args:
            text (`str`): The actual context to extract the answer from.
            start (`int`): The answer starting token index.
            end (`int`): The answer end token index.

        Returns:
            Dictionary like `{'answer': str, 'start': int, 'end': int}`
        """
        words = []  # 初始化列表
        token_idx = char_start_idx = char_end_idx = chars_idx = 0  # 初始化变量

        # 遍历分隔后的 text
        for i, word in enumerate(text.split(" ")):
            token = self.tokenizer.tokenize(word)  # 根据分词器对单词进行标记化

            # 如果 token 在答案范围内，则将其添加到 words 中
            if start <= token_idx <= end:
                if token_idx == start:
                    char_start_idx = chars_idx  # 记录开始字符偏移量

                if token_idx == end:
                    char_end_idx = chars_idx + len(word)  # 记录结束字符偏移量

                words += [word]  # 将符合要求的单词添加到列表中

            # 如果 token 索引超过了结束索引，停止遍历
            if token_idx > end:
                break

            # 更新 token 索引和字符索引
            token_idx += len(token)
            chars_idx += len(word) + 1

        # 返回结果字典
        return {
            "answer": " ".join(words),  # 将列表中的单词用空格连接成字符串
            "start": max(0, char_start_idx),  # 计算开始偏移量
            "end": min(len(text), char_end_idx),  # 计算结束偏移量
        }
```