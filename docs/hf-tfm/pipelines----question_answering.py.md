# `.\pipelines\question_answering.py`

```
# 导入inspect模块，用于获取对象信息
import inspect
# 导入types模块，用于类型检查和动态类型创建
import types
# 导入warnings模块，用于处理警告信息
import warnings
# 从collections.abc导入Iterable，用于检查对象是否可迭代
from collections.abc import Iterable
# 导入typing模块中的类型标记，用于类型提示
from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union

# 导入numpy库，用于处理数值计算
import numpy as np

# 从相对路径导入特定模块
from ..data import SquadExample, SquadFeatures, squad_convert_examples_to_features
# 从相对路径导入模型卡片模块
from ..modelcard import ModelCard
# 从相对路径导入预训练分词器模块
from ..tokenization_utils import PreTrainedTokenizer
# 从相对路径导入工具模块
from ..utils import (
    PaddingStrategy,
    add_end_docstrings,
    is_tf_available,
    is_tokenizers_available,
    is_torch_available,
    logging,
)
# 从当前路径导入base模块中的具体内容
from .base import ArgumentHandler, ChunkPipeline, build_pipeline_init_args

# 获取当前模块的日志记录器
logger = logging.get_logger(__name__)

# 如果是类型检查环境，则导入相应的模型预训练模块
if TYPE_CHECKING:
    from ..modeling_tf_utils import TFPreTrainedModel
    from ..modeling_utils import PreTrainedModel
    # 如果支持tokenizers，则导入tokenizers模块
    if is_tokenizers_available():
        import tokenizers

# 如果支持TensorFlow，则导入TensorFlow模块和相关模型映射名称
if is_tf_available():
    import tensorflow as tf
    from ..models.auto.modeling_tf_auto import TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
    # 初始化Dataset为None
    Dataset = None

# 如果支持PyTorch，则导入PyTorch模块和相关模型映射名称，以及数据集类
if is_torch_available():
    import torch
    from torch.utils.data import Dataset
    from ..models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES

# 定义函数decode_spans，接受一系列参数并返回一个元组
def decode_spans(
    start: np.ndarray, end: np.ndarray, topk: int, max_answer_len: int, undesired_tokens: np.ndarray
) -> Tuple:
    """
    从任何ModelForQuestionAnswering的输出中获取每个可能答案区间的概率，并生成这些区间的概率。

    同时过滤掉一些不想要/不可能的情况，比如答案长度大于max_answer_len或者答案结束位置在起始位置之前。
    通过topk参数支持输出k个最佳答案区间。

    Args:
        start (`np.ndarray`): 每个token的起始概率
        end (`np.ndarray`): 每个token的结束概率
        topk (`int`): 指示从模型输出中提取多少个可能的答案区间
        max_answer_len (`int`): 从模型输出中提取的答案的最大长度
        undesired_tokens (`np.ndarray`): 掩码，确定可以成为答案一部分的token
    """
    # 确保start具有批处理轴
    if start.ndim == 1:
        start = start[None]

    # 确保end具有批处理轴
    if end.ndim == 1:
        end = end[None]

    # 计算每个元组(start, end)作为真实答案的得分
    outer = np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))

    # 删除end < start或end - start > max_answer_len的候选答案
    candidates = np.tril(np.triu(outer), max_answer_len - 1)

    # 受Chen & al.启发 (https://github.com/facebookresearch/DrQA)
    scores_flat = candidates.flatten()
    if topk == 1:
        idx_sort = [np.argmax(scores_flat)]
    elif len(scores_flat) < topk:
        idx_sort = np.argsort(-scores_flat)
    else:
        idx = np.argpartition(-scores_flat, topk)[0:topk]
        idx_sort = idx[np.argsort(-scores_flat[idx])]

    # 从flatten的scores中解码出starts和ends的索引
    starts, ends = np.unravel_index(idx_sort, candidates.shape)[1:]
    # 根据 undesired_tokens 中非零值的位置，筛选出 starts 和 ends 中对应位置也在 undesired_tokens 中非零值位置的元素
    desired_spans = np.isin(starts, undesired_tokens.nonzero()) & np.isin(ends, undesired_tokens.nonzero())
    
    # 根据 desired_spans 筛选出符合条件的 starts 和 ends 的元素
    starts = starts[desired_spans]
    ends = ends[desired_spans]
    
    # 从 candidates 中取出符合条件的分数，这些条件由 starts 和 ends 确定
    scores = candidates[0, starts, ends]
    
    # 返回经过筛选后的 starts、ends 和相应的 scores
    return starts, ends, scores
# 定义一个函数，用于处理模型问答输出的原始数据，并进行规范化处理，然后使用 `decode_spans()` 生成每个可能答案区间的概率。
def select_starts_ends(
    start,
    end,
    p_mask,
    attention_mask,
    min_null_score=1000000,
    top_k=1,
    handle_impossible_answer=False,
    max_answer_len=15,
):
    """
    Takes the raw output of any `ModelForQuestionAnswering` and first normalizes its outputs and then uses
    `decode_spans()` to generate probabilities for each span to be the actual answer.

    Args:
        start (`np.ndarray`): Individual start logits for each token.
            每个 token 的起始 logits。
        end (`np.ndarray`): Individual end logits for each token.
            每个 token 的结束 logits。
        p_mask (`np.ndarray`): A mask with 1 for values that cannot be in the answer
            不能作为答案的值的掩码，值为1。
        attention_mask (`np.ndarray`): The attention mask generated by the tokenizer
            由分词器生成的注意力掩码。
        min_null_score(`float`): The minimum null (empty) answer score seen so far.
            到目前为止看到的最小空答案分数。
        topk (`int`): Indicates how many possible answer span(s) to extract from the model output.
            指示从模型输出中提取多少个可能的答案区间。
        handle_impossible_answer(`bool`): Whether to allow null (empty) answers
            是否允许空答案。
        max_answer_len (`int`): Maximum size of the answer to extract from the model's output.
            从模型输出中提取的答案的最大长度。
    """
    # 确保填充的 token 和问题的 token 不能成为候选答案的一部分。
    undesired_tokens = np.abs(np.array(p_mask) - 1)

    if attention_mask is not None:
        undesired_tokens = undesired_tokens & attention_mask

    # 生成掩码
    undesired_tokens_mask = undesired_tokens == 0.0

    # 确保张量中非上下文索引不能对 softmax 贡献
    start = np.where(undesired_tokens_mask, -10000.0, start)
    end = np.where(undesired_tokens_mask, -10000.0, end)

    # 对 logits 和区间进行归一化，以检索答案
    start = np.exp(start - start.max(axis=-1, keepdims=True))
    start = start / start.sum()

    end = np.exp(end - end.max(axis=-1, keepdims=True))
    end = end / end.sum()

    if handle_impossible_answer:
        min_null_score = min(min_null_score, (start[0, 0] * end[0, 0]).item())

    # Mask CLS
    # 掩码 CLS token
    start[0, 0] = end[0, 0] = 0.0

    # 使用 decode_spans 解码生成答案区间的起始点、结束点和分数
    starts, ends, scores = decode_spans(start, end, top_k, max_answer_len, undesired_tokens)
    return starts, ends, scores, min_null_score


class QuestionAnsweringArgumentHandler(ArgumentHandler):
    """
    QuestionAnsweringPipeline requires the user to provide multiple arguments (i.e. question & context) to be mapped to
    internal [`SquadExample`].

    QuestionAnsweringArgumentHandler manages all the possible to create a [`SquadExample`] from the command-line
    supplied arguments.
    """
    # 定义一个方法用于规范化输入数据，确保符合预期的数据结构
    def normalize(self, item):
        # 如果输入是 SquadExample 类型，则直接返回
        if isinstance(item, SquadExample):
            return item
        # 如果输入是字典类型，则检查其包含的字段是否完整和有效，然后创建相应的样本对象
        elif isinstance(item, dict):
            for k in ["question", "context"]:
                # 检查字典中是否包含必需的键
                if k not in item:
                    raise KeyError("You need to provide a dictionary with keys {question:..., context:...}")
                # 检查对应的值是否为 None
                elif item[k] is None:
                    raise ValueError(f"`{k}` cannot be None")
                # 检查对应的字符串值是否为空
                elif isinstance(item[k], str) and len(item[k]) == 0:
                    raise ValueError(f"`{k}` cannot be empty")

            # 根据字典创建 QuestionAnsweringPipeline 的样本对象
            return QuestionAnsweringPipeline.create_sample(**item)
        # 如果输入既不是 SquadExample 类型也不是字典类型，则抛出异常
        raise ValueError(f"{item} argument needs to be of type (SquadExample, dict)")

    # 定义对象可调用方法，根据不同类型的输入进行处理并返回规范化后的数据
    def __call__(self, *args, **kwargs):
        # 检测实际输入数据的来源
        if args is not None and len(args) > 0:
            # 如果只有一个参数，则直接将其视为输入数据
            if len(args) == 1:
                inputs = args[0]
            # 如果有两个参数且都是字符串，则将其作为问题和上下文创建为字典格式的输入数据
            elif len(args) == 2 and {type(el) for el in args} == {str}:
                inputs = [{"question": args[0], "context": args[1]}]
            else:
                # 否则将所有参数作为列表
                inputs = list(args)
        # 如果通过关键字参数指定了 "X" 或 "data" 则将其作为输入数据
        elif "X" in kwargs:
            inputs = kwargs["X"]
        elif "data" in kwargs:
            inputs = kwargs["data"]
        # 如果指定了 "question" 和 "context" 则根据其类型构造相应的输入数据
        elif "question" in kwargs and "context" in kwargs:
            if isinstance(kwargs["question"], list) and isinstance(kwargs["context"], str):
                # 如果问题是列表而上下文是字符串，则创建多个包含问题和上下文的字典
                inputs = [{"question": Q, "context": kwargs["context"]} for Q in kwargs["question"]]
            elif isinstance(kwargs["question"], list) and isinstance(kwargs["context"], list):
                # 如果问题和上下文都是列表，则按索引逐一配对创建字典
                if len(kwargs["question"]) != len(kwargs["context"]):
                    raise ValueError("Questions and contexts don't have the same lengths")
                inputs = [{"question": Q, "context": C} for Q, C in zip(kwargs["question"], kwargs["context"])]
            elif isinstance(kwargs["question"], str) and isinstance(kwargs["context"], str):
                # 如果问题和上下文都是字符串，则创建一个包含问题和上下文的字典
                inputs = [{"question": kwargs["question"], "context": kwargs["context"]}]
            else:
                # 其他情况抛出异常
                raise ValueError("Arguments can't be understood")
        else:
            # 如果无法识别输入类型，则抛出异常
            raise ValueError(f"Unknown arguments {kwargs}")

        # 当输入为生成器类型时，直接返回输入数据
        generator_types = (types.GeneratorType, Dataset) if Dataset is not None else (types.GeneratorType,)
        if isinstance(inputs, generator_types):
            return inputs

        # 规范化输入数据
        if isinstance(inputs, dict):
            inputs = [inputs]
        elif isinstance(inputs, Iterable):
            # 复制以避免覆盖参数
            inputs = list(inputs)
        else:
            # 如果输入类型无效，则抛出异常
            raise ValueError(f"Invalid arguments {kwargs}")

        # 对每个输入项进行规范化处理
        for i, item in enumerate(inputs):
            inputs[i] = self.normalize(item)

        # 返回规范化后的输入数据
        return inputs
@add_end_docstrings(build_pipeline_init_args(has_tokenizer=True))
class QuestionAnsweringPipeline(ChunkPipeline):
    """
    Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question answering
    examples](../task_summary#question-answering) for more information.

    Example:

    ```python
    >>> from transformers import pipeline

    >>> oracle = pipeline(model="deepset/roberta-base-squad2")
    >>> oracle(question="Where do I live?", context="My name is Wolfgang and I live in Berlin")
    {'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}
    ```

    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

    This question answering pipeline can currently be loaded from [`pipeline`] using the following task identifier:
    `"question-answering"`.

    The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the
    up-to-date list of available models on
    [huggingface.co/models](https://huggingface.co/models?filter=question-answering).
    """

    default_input_names = "question,context"
    handle_impossible_answer = False

    def __init__(
        self,
        model: Union["PreTrainedModel", "TFPreTrainedModel"],
        tokenizer: PreTrainedTokenizer,
        modelcard: Optional[ModelCard] = None,
        framework: Optional[str] = None,
        task: str = "",
        **kwargs,
    ):
        """
        Initialize the QuestionAnsweringPipeline object.

        Args:
            model (Union["PreTrainedModel", "TFPreTrainedModel"]): The pre-trained model to use for question answering.
            tokenizer (PreTrainedTokenizer): Tokenizer associated with the model for tokenizing inputs.
            modelcard (Optional[ModelCard]): Model card containing details about the model (optional).
            framework (Optional[str]): The framework for the model, e.g., "tf" or "pt" (optional).
            task (str): Identifier for the task associated with the pipeline (optional).
            **kwargs: Additional keyword arguments passed to parent class constructor.
        """
        super().__init__(
            model=model,
            tokenizer=tokenizer,
            modelcard=modelcard,
            framework=framework,
            task=task,
            **kwargs,
        )

        # Initialize argument parser for question answering specific arguments
        self._args_parser = QuestionAnsweringArgumentHandler()

        # Check and set the appropriate mapping names based on the framework
        self.check_model_type(
            TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
            if self.framework == "tf"
            else MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES
        )

    @staticmethod
    def create_sample(
        question: Union[str, List[str]], context: Union[str, List[str]]
    ) -> Union[SquadExample, List[SquadExample]]:
        """
        Create a SquadExample or a list of SquadExamples from given questions and contexts.

        This helper method encapsulates the logic for converting questions and contexts into SquadExample instances.

        Args:
            question (Union[str, List[str]]): The question or list of questions.
            context (Union[str, List[str]]): The context or list of contexts.

        Returns:
            Union[SquadExample, List[SquadExample]]: The corresponding SquadExample or list of SquadExamples.
        """
        if isinstance(question, list):
            return [SquadExample(None, q, c, None, None, None) for q, c in zip(question, context)]
        else:
            return SquadExample(None, question, context, None, None, None)
    # 函数用于清理和设置参数，确保预处理和后处理的参数正确传递
    def _sanitize_parameters(
        self,
        padding=None,  # 设置填充值的参数，如果不为None，则将其加入预处理参数字典中
        topk=None,  # 设置topk的参数
        top_k=None,  # 设置top_k的参数，如果topk不为None，则发出警告并将其赋值给top_k
        doc_stride=None,  # 文档跨度参数，控制处理文本时的步长
        max_answer_len=None,  # 最大答案长度参数，用于后处理
        max_seq_len=None,  # 最大序列长度参数，用于预处理
        max_question_len=None,  # 最大问题长度参数，用于预处理
        handle_impossible_answer=None,  # 处理不可能答案的参数，用于后处理
        align_to_words=None,  # 对齐到单词的布尔参数，用于后处理
        **kwargs,  # 其他未指定的参数以关键字方式接收
    ):
        # 设置预处理参数的默认空字典
        preprocess_params = {}
        # 根据传入的参数，如果不为None则添加到预处理参数字典中
        if padding is not None:
            preprocess_params["padding"] = padding
        if doc_stride is not None:
            preprocess_params["doc_stride"] = doc_stride
        if max_question_len is not None:
            preprocess_params["max_question_len"] = max_question_len
        if max_seq_len is not None:
            preprocess_params["max_seq_len"] = max_seq_len

        # 设置后处理参数的默认空字典
        postprocess_params = {}
        # 如果topk不为None且top_k为None，则发出警告并将topk赋值给top_k
        if topk is not None and top_k is None:
            warnings.warn("topk parameter is deprecated, use top_k instead", UserWarning)
            top_k = topk
        # 如果top_k不为None，则检查其是否合法，若小于1则抛出异常
        if top_k is not None:
            if top_k < 1:
                raise ValueError(f"top_k parameter should be >= 1 (got {top_k})")
            postprocess_params["top_k"] = top_k
        # 如果max_answer_len不为None，则检查其是否合法，若小于1则抛出异常
        if max_answer_len is not None:
            if max_answer_len < 1:
                raise ValueError(f"max_answer_len parameter should be >= 1 (got {max_answer_len}")
            postprocess_params["max_answer_len"] = max_answer_len
        # 将handle_impossible_answer参数加入后处理参数字典中
        if handle_impossible_answer is not None:
            postprocess_params["handle_impossible_answer"] = handle_impossible_answer
        # 将align_to_words参数加入后处理参数字典中
        if align_to_words is not None:
            postprocess_params["align_to_words"] = align_to_words
        # 返回预处理参数字典、空字典和后处理参数字典
        return preprocess_params, {}, postprocess_params

    # 执行模型的前向传播
    def _forward(self, inputs):
        # 获取输入中的示例
        example = inputs["example"]
        # 从输入中选择与模型输入名称匹配的键值对，构建模型输入字典
        model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}
        # 根据框架类型选择正确的模型前向方法
        model_forward = self.model.forward if self.framework == "pt" else self.model.call
        # 如果模型前向方法支持"use_cache"参数，则设置为False
        if "use_cache" in inspect.signature(model_forward).parameters.keys():
            model_inputs["use_cache"] = False
        # 执行模型的前向传播，获取输出
        output = self.model(**model_inputs)
        # 如果输出是字典，则返回字典包含的"start"和"end" logits及示例和输入的其他键值对
        if isinstance(output, dict):
            return {"start": output["start_logits"], "end": output["end_logits"], "example": example, **inputs}
        # 否则，假设输出是一个元组，将第一和第二个元素作为"start"和"end" logits返回，同时返回示例和输入的其他键值对
        else:
            start, end = output[:2]
            return {"start": start, "end": end, "example": example, **inputs}

    # 后处理函数，处理模型输出
    def postprocess(
        self,
        model_outputs,  # 模型的输出
        top_k=1,  # 用于选择top_k个答案
        handle_impossible_answer=False,  # 是否处理不可能的答案
        max_answer_len=15,  # 最大答案长度限制
        align_to_words=True,  # 是否对齐到单词
    ):
        # 函数体未提供，未包含在此处的代码块中

    # 获取编码中指定范围内的索引
    def get_indices(
        self, enc: "tokenizers.Encoding", s: int, e: int, sequence_index: int, align_to_words: bool
    ):
        # 函数体未提供，未包含在此处的代码块中
    ) -> Tuple[int, int]:
        # 如果 align_to_words 为 True，则尝试使用编码器将 token 转换为单词，并获取单词在文本中的起始和结束字符索引
        if align_to_words:
            try:
                start_word = enc.token_to_word(s)
                end_word = enc.token_to_word(e)
                start_index = enc.word_to_chars(start_word, sequence_index=sequence_index)[0]
                end_index = enc.word_to_chars(end_word, sequence_index=sequence_index)[1]
            except Exception:
                # 如果某些分词器不能真正处理单词，则回退到使用偏移量
                start_index = enc.offsets[s][0]
                end_index = enc.offsets[e][1]
        else:
            # 否则直接使用偏移量来获取字符索引
            start_index = enc.offsets[s][0]
            end_index = enc.offsets[e][1]
        # 返回起始和结束字符索引
        return start_index, end_index

    def span_to_answer(self, text: str, start: int, end: int) -> Dict[str, Union[str, int]]:
        """
        从 token 概率解码时，将 token 索引映射回初始上下文中的实际单词。

        Args:
            text (`str`): 要从中提取答案的实际上下文。
            start (`int`): 答案起始 token 索引。
            end (`int`): 答案结束 token 索引。

        Returns:
            类似 `{'answer': str, 'start': int, 'end': int}` 的字典。
        """
        words = []
        token_idx = char_start_idx = char_end_idx = chars_idx = 0

        # 按空格分割文本，遍历每个单词及其 token
        for i, word in enumerate(text.split(" ")):
            token = self.tokenizer.tokenize(word)

            # 如果单词的 token 索引在答案范围内，则记录单词
            if start <= token_idx <= end:
                if token_idx == start:
                    char_start_idx = chars_idx  # 记录起始字符索引

                if token_idx == end:
                    char_end_idx = chars_idx + len(word)  # 记录结束字符索引

                words += [word]

            # 如果超过答案范围，停止追加
            if token_idx > end:
                break

            # 更新 token 索引和字符索引
            token_idx += len(token)
            chars_idx += len(word) + 1  # 加上空格的长度

        # 返回答案文本及其起始和结束字符索引
        return {
            "answer": " ".join(words),
            "start": max(0, char_start_idx),
            "end": min(len(text), char_end_idx),
        }
```