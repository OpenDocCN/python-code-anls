# `.\models\table_transformer\convert_table_transformer_to_hf_no_timm.py`

```
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Convert Table Transformer checkpoints with native (Transformers) backbone.

URL: https://github.com/microsoft/table-transformer
"""


import argparse
from pathlib import Path

import torch
from huggingface_hub import hf_hub_download
from PIL import Image
from torchvision.transforms import functional as F

from transformers import DetrImageProcessor, ResNetConfig, TableTransformerConfig, TableTransformerForObjectDetection
from transformers.utils import logging


logging.set_verbosity_info()  # è®¾ç½®æ—¥å¿—è®°å½•çº§åˆ«ä¸ºä¿¡æ¯çº§åˆ«
logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


def create_rename_keys(config):
    # here we list all keys to be renamed (original name on the left, our name on the right)
    rename_keys = []  # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨éœ€è¦é‡å‘½åçš„é”®å€¼å¯¹

    # stem
    # fmt: off
    rename_keys.append(("backbone.0.body.conv1.weight", "backbone.conv_encoder.model.embedder.embedder.convolution.weight"))
    rename_keys.append(("backbone.0.body.bn1.weight", "backbone.conv_encoder.model.embedder.embedder.normalization.weight"))
    rename_keys.append(("backbone.0.body.bn1.bias", "backbone.conv_encoder.model.embedder.embedder.normalization.bias"))
    rename_keys.append(("backbone.0.body.bn1.running_mean", "backbone.conv_encoder.model.embedder.embedder.normalization.running_mean"))
    rename_keys.append(("backbone.0.body.bn1.running_var", "backbone.conv_encoder.model.embedder.embedder.normalization.running_var"))
    # stages
    # fmt: on

    # convolutional projection + query embeddings + layernorm of decoder + class and bounding box heads
    # å°†æ—§çš„æ¨¡å‹å‚æ•°åç§°ä¸æ–°çš„æ¨¡å‹å‚æ•°åç§°å¯¹åº”èµ·æ¥ï¼Œç”¨äºé‡å‘½åæ¨¡å‹æƒé‡å’Œåç½®
    rename_keys.extend(
        [
            ("input_proj.weight", "input_projection.weight"),  # é‡å‘½åè¾“å…¥æŠ•å½±å±‚çš„æƒé‡å‚æ•°
            ("input_proj.bias", "input_projection.bias"),  # é‡å‘½åè¾“å…¥æŠ•å½±å±‚çš„åç½®å‚æ•°
            ("query_embed.weight", "query_position_embeddings.weight"),  # é‡å‘½åæŸ¥è¯¢ä½ç½®åµŒå…¥çš„æƒé‡å‚æ•°
            ("transformer.decoder.norm.weight", "decoder.layernorm.weight"),  # é‡å‘½åè§£ç å™¨å±‚å½’ä¸€åŒ–çš„æƒé‡å‚æ•°
            ("transformer.decoder.norm.bias", "decoder.layernorm.bias"),  # é‡å‘½åè§£ç å™¨å±‚å½’ä¸€åŒ–çš„åç½®å‚æ•°
            ("class_embed.weight", "class_labels_classifier.weight"),  # é‡å‘½åç±»æ ‡ç­¾åˆ†ç±»å™¨çš„æƒé‡å‚æ•°
            ("class_embed.bias", "class_labels_classifier.bias"),  # é‡å‘½åç±»æ ‡ç­¾åˆ†ç±»å™¨çš„åç½®å‚æ•°
            ("bbox_embed.layers.0.weight", "bbox_predictor.layers.0.weight"),  # é‡å‘½åè¾¹ç•Œæ¡†é¢„æµ‹å™¨ç¬¬ä¸€å±‚æƒé‡å‚æ•°
            ("bbox_embed.layers.0.bias", "bbox_predictor.layers.0.bias"),  # é‡å‘½åè¾¹ç•Œæ¡†é¢„æµ‹å™¨ç¬¬ä¸€å±‚åç½®å‚æ•°
            ("bbox_embed.layers.1.weight", "bbox_predictor.layers.1.weight"),  # é‡å‘½åè¾¹ç•Œæ¡†é¢„æµ‹å™¨ç¬¬äºŒå±‚æƒé‡å‚æ•°
            ("bbox_embed.layers.1.bias", "bbox_predictor.layers.1.bias"),  # é‡å‘½åè¾¹ç•Œæ¡†é¢„æµ‹å™¨ç¬¬äºŒå±‚åç½®å‚æ•°
            ("bbox_embed.layers.2.weight", "bbox_predictor.layers.2.weight"),  # é‡å‘½åè¾¹ç•Œæ¡†é¢„æµ‹å™¨ç¬¬ä¸‰å±‚æƒé‡å‚æ•°
            ("bbox_embed.layers.2.bias", "bbox_predictor.layers.2.bias"),  # é‡å‘½åè¾¹ç•Œæ¡†é¢„æµ‹å™¨ç¬¬ä¸‰å±‚åç½®å‚æ•°
            ("transformer.encoder.norm.weight", "encoder.layernorm.weight"),  # é‡å‘½åç¼–ç å™¨å±‚å½’ä¸€åŒ–çš„æƒé‡å‚æ•°
            ("transformer.encoder.norm.bias", "encoder.layernorm.bias"),  # é‡å‘½åç¼–ç å™¨å±‚å½’ä¸€åŒ–çš„åç½®å‚æ•°
        ]
    )
    
    # è¿”å›é‡å‘½ååçš„é”®åˆ—è¡¨
    return rename_keys
# é‡å‘½åçŠ¶æ€å­—å…¸ä¸­çš„é”®ï¼Œå°†æ—§é”®ï¼ˆoldï¼‰å¯¹åº”çš„å€¼å¼¹å‡ºï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨å˜é‡valä¸­ï¼Œç„¶åå°†æ–°é”®ï¼ˆnewï¼‰å’Œvalçš„å¯¹åº”å…³ç³»æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
def rename_key(state_dict, old, new):
    val = state_dict.pop(old)
    state_dict[new] = val

# ä»çŠ¶æ€å­—å…¸ä¸­è¯»å–æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeysï¼‰å’Œå€¼ï¼ˆvaluesï¼‰çš„æƒé‡å’Œåç½®ï¼Œå¹¶å°†å®ƒä»¬é‡æ–°ç»„ç»‡å­˜æ”¾åˆ°çŠ¶æ€å­—å…¸ä¸­
def read_in_q_k_v(state_dict, is_panoptic=False):
    prefix = ""
    if is_panoptic:
        prefix = "detr."

    # éå†å…­å±‚Transformerç¼–ç å™¨
    for i in range(6):
        # è¯»å–è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®ï¼ˆåœ¨PyTorchçš„MultiHeadAttentionä¸­ï¼Œè¿™æ˜¯å•ä¸ªçŸ©é˜µåŠ åç½®ï¼‰
        in_proj_weight = state_dict.pop(f"{prefix}transformer.encoder.layers.{i}.self_attn.in_proj_weight")
        in_proj_bias = state_dict.pop(f"{prefix}transformer.encoder.layers.{i}.self_attn.in_proj_bias")

        # å°†æŸ¥è¯¢æŠ•å½±çš„æƒé‡å’Œåç½®æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
        state_dict[f"encoder.layers.{i}.self_attn.q_proj.weight"] = in_proj_weight[:256, :]
        state_dict[f"encoder.layers.{i}.self_attn.q_proj.bias"] = in_proj_bias[:256]

        # å°†é”®æŠ•å½±çš„æƒé‡å’Œåç½®æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
        state_dict[f"encoder.layers.{i}.self_attn.k_proj.weight"] = in_proj_weight[256:512, :]
        state_dict[f"encoder.layers.{i}.self_attn.k_proj.bias"] = in_proj_bias[256:512]

        # å°†å€¼æŠ•å½±çš„æƒé‡å’Œåç½®æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
        state_dict[f"encoder.layers.{i}.self_attn.v_proj.weight"] = in_proj_weight[-256:, :]
        state_dict[f"encoder.layers.{i}.self_attn.v_proj.bias"] = in_proj_bias[-256:]

    # æ¥ä¸‹æ¥å¤„ç†Transformerè§£ç å™¨ï¼ˆç¨å¤æ‚ï¼Œå› ä¸ºå®ƒè¿˜æ¶‰åŠè·¨æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†ï¼‰
    # å¯¹äºæ¯ä¸ªå±‚æ¬¡ç´¢å¼• i åœ¨èŒƒå›´å†…ä» 0 åˆ° 5ï¼ˆå…±6ä¸ªå±‚æ¬¡ï¼‰
    for i in range(6):
        # è¯»å– self-attention çš„è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®
        in_proj_weight = state_dict.pop(f"{prefix}transformer.decoder.layers.{i}.self_attn.in_proj_weight")
        in_proj_bias = state_dict.pop(f"{prefix}transformer.decoder.layers.{i}.self_attn.in_proj_bias")
        
        # å°†æŸ¥è¯¢ã€é”®å’Œå€¼ï¼ˆæŒ‰é¡ºåºï¼‰æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­çš„ self-attention éƒ¨åˆ†
        state_dict[f"decoder.layers.{i}.self_attn.q_proj.weight"] = in_proj_weight[:256, :]
        state_dict[f"decoder.layers.{i}.self_attn.q_proj.bias"] = in_proj_bias[:256]
        state_dict[f"decoder.layers.{i}.self_attn.k_proj.weight"] = in_proj_weight[256:512, :]
        state_dict[f"decoder.layers.{i}.self_attn.k_proj.bias"] = in_proj_bias[256:512]
        state_dict[f"decoder.layers.{i}.self_attn.v_proj.weight"] = in_proj_weight[-256:, :]
        state_dict[f"decoder.layers.{i}.self_attn.v_proj.bias"] = in_proj_bias[-256:]
        
        # è¯»å– cross-attention çš„è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®
        in_proj_weight_cross_attn = state_dict.pop(
            f"{prefix}transformer.decoder.layers.{i}.multihead_attn.in_proj_weight"
        )
        in_proj_bias_cross_attn = state_dict.pop(f"{prefix}transformer.decoder.layers.{i}.multihead_attn.in_proj_bias")
        
        # å°†æŸ¥è¯¢ã€é”®å’Œå€¼ï¼ˆæŒ‰é¡ºåºï¼‰æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­çš„ cross-attention éƒ¨åˆ†
        state_dict[f"decoder.layers.{i}.encoder_attn.q_proj.weight"] = in_proj_weight_cross_attn[:256, :]
        state_dict[f"decoder.layers.{i}.encoder_attn.q_proj.bias"] = in_proj_bias_cross_attn[:256]
        state_dict[f"decoder.layers.{i}.encoder_attn.k_proj.weight"] = in_proj_weight_cross_attn[256:512, :]
        state_dict[f"decoder.layers.{i}.encoder_attn.k_proj.bias"] = in_proj_bias_cross_attn[256:512]
        state_dict[f"decoder.layers.{i}.encoder_attn.v_proj.weight"] = in_proj_weight_cross_attn[-256:, :]
        state_dict[f"decoder.layers.{i}.encoder_attn.v_proj.bias"] = in_proj_bias_cross_attn[-256:]
# è°ƒæ•´å›¾åƒå¤§å°å‡½æ•°ï¼Œæ ¹æ®æŒ‡å®šçš„æ£€æŸ¥ç‚¹ URL åˆ¤æ–­ç›®æ ‡å¤§å°ï¼Œå°†å›¾åƒè°ƒæ•´ä¸ºé€‚å½“çš„å°ºå¯¸
def resize(image, checkpoint_url):
    # è·å–å›¾åƒçš„å®½åº¦å’Œé«˜åº¦
    width, height = image.size
    # è®¡ç®—å½“å‰å›¾åƒçš„æœ€å¤§å°ºå¯¸
    current_max_size = max(width, height)
    # æ ¹æ®æ£€æŸ¥ç‚¹ URL å†³å®šç›®æ ‡æœ€å¤§å°ºå¯¸ï¼Œæ£€æµ‹æ¨¡å‹ä½¿ç”¨ 800ï¼Œå…¶ä»–æƒ…å†µä½¿ç”¨ 1000
    target_max_size = 800 if "detection" in checkpoint_url else 1000
    # è®¡ç®—è°ƒæ•´æ¯”ä¾‹
    scale = target_max_size / current_max_size
    # è°ƒæ•´å›¾åƒå¤§å°
    resized_image = image.resize((int(round(scale * width)), int(round(scale * height))))

    return resized_image


# æ ‡å‡†åŒ–å›¾åƒå‡½æ•°ï¼Œä½¿ç”¨ PyTorch çš„è½¬æ¢å·¥å…·è¿›è¡Œå›¾åƒæ ‡å‡†åŒ–å¤„ç†
def normalize(image):
    # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
    image = F.to_tensor(image)
    # æ ¹æ®æŒ‡å®šçš„å‡å€¼å’Œæ ‡å‡†å·®å¯¹å›¾åƒè¿›è¡Œæ ‡å‡†åŒ–
    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    return image


# è½¬æ¢è¡¨æ ¼ Transformer æ¨¡å‹çš„æ£€æŸ¥ç‚¹å‡½æ•°ï¼ŒåŠ è½½æ¨¡å‹æƒé‡å¹¶è¿›è¡Œè½¬æ¢æ“ä½œ
@torch.no_grad()
def convert_table_transformer_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to_hub):
    """
    Copy/paste/tweak model's weights to our DETR structure.
    """

    logger.info("Converting model...")

    # åˆ›å»º HuggingFace æ¨¡å‹å¹¶åŠ è½½çŠ¶æ€å­—å…¸
    backbone_config = ResNetConfig.from_pretrained(
        "microsoft/resnet-18", out_features=["stage1", "stage2", "stage3", "stage4"]
    )

    # ä½¿ç”¨ç»™å®šé…ç½®åˆ›å»º TableTransformerConfig å¯¹è±¡
    config = TableTransformerConfig(
        backbone_config=backbone_config,
        use_timm_backbone=False,
        mask_loss_coefficient=1,
        dice_loss_coefficient=1,
        ce_loss_coefficient=1,
        bbox_loss_coefficient=5,
        giou_loss_coefficient=2,
        eos_coefficient=0.4,
        class_cost=1,
        bbox_cost=5,
        giou_cost=2,
    )

    # åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸
    state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location="cpu")

    # é‡å‘½åé”®å
    for src, dest in create_rename_keys(config):
        rename_key(state_dict, src, dest)
    # å¤„ç†æŸ¥è¯¢ã€é”®å’Œå€¼çŸ©é˜µéœ€è¦ç‰¹æ®Šå¤„ç†
    read_in_q_k_v(state_dict)
    # é‡è¦ï¼šå¯¹åŸºç¡€æ¨¡å‹é”®åæ·»åŠ å‰ç¼€ï¼Œå› ä¸ºå¤´éƒ¨æ¨¡å‹ä½¿ç”¨ä¸åŒçš„å±æ€§
    prefix = "model."
    for key in state_dict.copy().keys():
        if not key.startswith("class_labels_classifier") and not key.startswith("bbox_predictor"):
            val = state_dict.pop(key)
            state_dict[prefix + key] = val

    # æ ¹æ®æ£€æŸ¥ç‚¹ URL è®¾ç½®ä¸åŒçš„é…ç½®å‚æ•°
    if "detection" in checkpoint_url:
        config.num_queries = 15
        config.num_labels = 2
        id2label = {0: "table", 1: "table rotated"}
        config.id2label = id2label
        config.label2id = {v: k for k, v in id2label.items()}
    else:
        config.num_queries = 125
        config.num_labels = 6
        id2label = {
            0: "table",
            1: "table column",
            2: "table row",
            3: "table column header",
            4: "table projected row header",
            5: "table spanning cell",
        }
        config.id2label = id2label
        config.label2id = {v: k for k, v in id2label.items()}

    # åˆ›å»ºå›¾åƒå¤„ç†å™¨å¯¹è±¡ï¼ŒæŒ‡å®šè¾“å‡ºæ ¼å¼ä¸º coco_detectionï¼Œæœ€é•¿è¾¹å°ºå¯¸ä¸º 800
    image_processor = DetrImageProcessor(format="coco_detection", size={"longest_edge": 800})
    # åˆ›å»º TableTransformerForObjectDetection æ¨¡å‹å¯¹è±¡
    model = TableTransformerForObjectDetection(config)
    # åŠ è½½è½¬æ¢åçš„çŠ¶æ€å­—å…¸
    model.load_state_dict(state_dict)
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # éªŒè¯è½¬æ¢ç»“æœ
    filename = "example_pdf.png" if "detection" in checkpoint_url else "example_table.png"
    # ä½¿ç”¨ hf_hub_download å‡½æ•°ä¸‹è½½æŒ‡å®š repository ID çš„æ–‡ä»¶ï¼Œå¹¶è¿”å›æ–‡ä»¶è·¯å¾„
    file_path = hf_hub_download(repo_id="nielsr/example-pdf", repo_type="dataset", filename=filename)
    # ä½¿ç”¨ PIL åº“æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶è½¬æ¢ä¸º RGB æ¨¡å¼çš„å›¾åƒå¯¹è±¡
    image = Image.open(file_path).convert("RGB")
    # è°ƒç”¨ resize å‡½æ•°å¯¹å›¾åƒè¿›è¡Œç¼©æ”¾å¹¶æ ‡å‡†åŒ–åƒç´ å€¼ï¼Œç„¶åæ·»åŠ ä¸€ä¸ªç»´åº¦ä»¥é€‚åº”æ¨¡å‹è¾“å…¥è¦æ±‚
    pixel_values = normalize(resize(image, checkpoint_url)).unsqueeze(0)

    # å°†å¤„ç†åçš„å›¾åƒæ•°æ®è¾“å…¥æ¨¡å‹è¿›è¡Œæ¨æ–­
    outputs = model(pixel_values)

    # æ ¹æ® checkpoint_url æ˜¯å¦åŒ…å« "detection" å­—ç¬¦ä¸²æ¥è®¾ç½®é¢„æœŸçš„è¾“å‡ºå½¢çŠ¶ã€logits å’Œ boxes
    if "detection" in checkpoint_url:
        expected_shape = (1, 15, 3)
        expected_logits = torch.tensor(
            [[-6.7897, -16.9985, 6.7937], [-8.0186, -22.2192, 6.9677], [-7.3117, -21.0708, 7.4055]]
        )
        expected_boxes = torch.tensor([[0.4867, 0.1767, 0.6732], [0.6718, 0.4479, 0.3830], [0.4716, 0.1760, 0.6364]])
    else:
        expected_shape = (1, 125, 7)
        expected_logits = torch.tensor(
            [[-18.1430, -8.3214, 4.8274], [-18.4685, -7.1361, -4.2667], [-26.3693, -9.3429, -4.9962]]
        )
        expected_boxes = torch.tensor([[0.4983, 0.5595, 0.9440], [0.4916, 0.6315, 0.5954], [0.6108, 0.8637, 0.1135]])

    # ä½¿ç”¨æ–­è¨€æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„å½¢çŠ¶æ˜¯å¦ç¬¦åˆé¢„æœŸ
    assert outputs.logits.shape == expected_shape
    # ä½¿ç”¨æ–­è¨€æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„ logits æ˜¯å¦ä¸é¢„æœŸçš„å€¼åœ¨æŒ‡å®šå®¹å·®èŒƒå›´å†…ç›¸ä¼¼
    assert torch.allclose(outputs.logits[0, :3, :3], expected_logits, atol=1e-4)
    # ä½¿ç”¨æ–­è¨€æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„ pred_boxes æ˜¯å¦ä¸é¢„æœŸçš„å€¼åœ¨æŒ‡å®šå®¹å·®èŒƒå›´å†…ç›¸ä¼¼
    assert torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4)
    # è¾“å‡ºç¡®è®¤ä¿¡æ¯ï¼Œè¡¨æ˜æ£€æŸ¥é€šè¿‡
    print("Looks ok!")

    # å¦‚æœ pytorch_dump_folder_path ä¸ä¸º Noneï¼Œåˆ™ä¿å­˜æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨åˆ°æŒ‡å®šè·¯å¾„
    if pytorch_dump_folder_path is not None:
        logger.info(f"Saving PyTorch model and image processor to {pytorch_dump_folder_path}...")
        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
        model.save_pretrained(pytorch_dump_folder_path)
        image_processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœ push_to_hub ä¸º Trueï¼Œåˆ™å°†æ¨¡å‹æ¨é€åˆ° HF hub
    if push_to_hub:
        logger.info("Pushing model to the hub...")
        # æ ¹æ® checkpoint_url ä¸­æ˜¯å¦åŒ…å« "detection" å­—ç¬¦ä¸²é€‰æ‹©ä¸åŒçš„æ¨¡å‹åç§°
        model_name = (
            "microsoft/table-transformer-detection"
            if "detection" in checkpoint_url
            else "microsoft/table-transformer-structure-recognition"
        )
        # è°ƒç”¨æ¨¡å‹å¯¹è±¡çš„ push_to_hub æ–¹æ³•å°†æ¨¡å‹æ¨é€åˆ° HF hub
        model.push_to_hub(model_name, revision="no_timm")
        # åŒæ ·å°†å›¾åƒå¤„ç†å™¨å¯¹è±¡æ¨é€åˆ° HF hub
        image_processor.push_to_hub(model_name, revision="no_timm")
# å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œï¼ˆè€Œä¸æ˜¯è¢«å¯¼å…¥åˆ°å…¶ä»–æ¨¡å—ï¼‰ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼š--checkpoint_urlï¼Œç”¨äºæŒ‡å®šæ¨¡å‹æ£€æŸ¥ç‚¹çš„ä¸‹è½½é“¾æ¥ï¼Œé»˜è®¤ä¸ºå…¬å…±æ¨¡å‹çš„æ£€æŸ¥ç‚¹é“¾æ¥
    parser.add_argument(
        "--checkpoint_url",
        default="https://pubtables1m.blob.core.windows.net/model/pubtables1m_detection_detr_r18.pth",
        type=str,
        choices=[
            "https://pubtables1m.blob.core.windows.net/model/pubtables1m_detection_detr_r18.pth",
            "https://pubtables1m.blob.core.windows.net/model/pubtables1m_structure_detr_r18.pth",
        ],
        help="URL of the Table Transformer checkpoint you'd like to convert."
    )

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼š--pytorch_dump_folder_pathï¼Œç”¨äºæŒ‡å®šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º None
    parser.add_argument(
        "--pytorch_dump_folder_path",
        default=None,
        type=str,
        help="Path to the folder to output PyTorch model."
    )

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼š--push_to_hubï¼Œä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ° ğŸ¤— hub
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨ args å¯¹è±¡ä¸­
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•° convert_table_transformer_checkpointï¼Œä¼ å…¥è§£æåçš„å‘½ä»¤è¡Œå‚æ•°
    convert_table_transformer_checkpoint(args.checkpoint_url, args.pytorch_dump_folder_path, args.push_to_hub)
```