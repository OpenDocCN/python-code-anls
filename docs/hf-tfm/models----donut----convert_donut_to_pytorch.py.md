# `.\models\donut\convert_donut_to_pytorch.py`

```
# æŒ‡å®šè„šæœ¬çš„ç¼–ç æ ¼å¼ä¸º UTF-8

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import torch  # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from datasets import load_dataset  # ä» Hugging Face datasets åº“åŠ è½½æ•°æ®é›†
from donut import DonutModel  # å¯¼å…¥ DonutModel æ¨¡å‹
from transformers import (  # å¯¼å…¥ Transformers åº“ä¸­çš„æ¨¡å‹å’Œé…ç½®
    DonutImageProcessor,  # Donut å›¾åƒå¤„ç†å™¨
    DonutProcessor,  # Donut å¤„ç†å™¨
    DonutSwinConfig,  # Donut Swin æ¨¡å‹çš„é…ç½®
    DonutSwinModel,  # Donut Swin æ¨¡å‹
    MBartConfig,  # MBart æ¨¡å‹çš„é…ç½®
    MBartForCausalLM,  # MBart ç”¨äºå› æœè¯­è¨€å»ºæ¨¡
    VisionEncoderDecoderModel,  # è§†è§‰ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
    XLMRobertaTokenizerFast,  # XLM-Roberta å¿«é€Ÿåˆ†è¯å™¨
)


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºä»ç»™å®šæ¨¡å‹è·å– Donut å’Œ MBart çš„é…ç½®
def get_configs(model):
    # è·å–åŸå§‹æ¨¡å‹çš„é…ç½®
    original_config = model.config

    # åˆ›å»º DonutSwin æ¨¡å‹çš„é…ç½®
    encoder_config = DonutSwinConfig(
        image_size=original_config.input_size,  # å›¾åƒå°ºå¯¸
        patch_size=4,  # è¡¥ä¸å°ºå¯¸
        depths=original_config.encoder_layer,  # ç¼–ç å™¨å±‚æ•°
        num_heads=[4, 8, 16, 32],  # æ³¨æ„åŠ›å¤´çš„æ•°é‡
        window_size=original_config.window_size,  # çª—å£å¤§å°
        embed_dim=128,  # åµŒå…¥ç»´åº¦
    )
    # åˆ›å»º MBart æ¨¡å‹çš„é…ç½®
    decoder_config = MBartConfig(
        is_decoder=True,  # æ˜¯å¦ä¸ºè§£ç å™¨
        is_encoder_decoder=False,  # æ˜¯å¦ä¸ºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
        add_cross_attention=True,  # æ˜¯å¦æ·»åŠ äº¤å‰æ³¨æ„åŠ›
        decoder_layers=original_config.decoder_layer,  # è§£ç å™¨å±‚æ•°
        max_position_embeddings=original_config.max_position_embeddings,  # æœ€å¤§ä½ç½®åµŒå…¥
        vocab_size=len(
            model.decoder.tokenizer
        ),  # è¯æ±‡è¡¨å¤§å°ï¼Œæ³¨æ„æœ‰ä¸€äº›ç‰¹æ®Šçš„ä»¤ç‰Œè¢«æ·»åŠ åˆ° XLMRobertaTokenizer çš„è¯æ±‡è¡¨ä¸­
        scale_embedding=True,  # æ˜¯å¦ç¼©æ”¾åµŒå…¥
        add_final_layer_norm=True,  # æ˜¯å¦æ·»åŠ æœ€ç»ˆå±‚å½’ä¸€åŒ–
    )

    return encoder_config, decoder_config  # è¿”å›ç¼–ç å™¨å’Œè§£ç å™¨çš„é…ç½®


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºé‡å‘½åæ¨¡å‹ä¸­çš„é”®å
def rename_key(name):
    # å¦‚æœé”®åä¸­åŒ…å« 'encoder.model'ï¼Œåˆ™æ›¿æ¢ä¸º 'encoder'
    if "encoder.model" in name:
        name = name.replace("encoder.model", "encoder")
    # å¦‚æœé”®åä¸­åŒ…å« 'decoder.model'ï¼Œåˆ™æ›¿æ¢ä¸º 'decoder'
    if "decoder.model" in name:
        name = name.replace("decoder.model", "decoder")
    # å¦‚æœé”®åä¸­åŒ…å« 'patch_embed.proj'ï¼Œåˆ™æ›¿æ¢ä¸º 'embeddings.patch_embeddings.projection'
    if "patch_embed.proj" in name:
        name = name.replace("patch_embed.proj", "embeddings.patch_embeddings.projection")
    # å¦‚æœé”®åä¸­åŒ…å« 'patch_embed.norm'ï¼Œåˆ™æ›¿æ¢ä¸º 'embeddings.norm'
    if "patch_embed.norm" in name:
        name = name.replace("patch_embed.norm", "embeddings.norm")
    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä»¥"encoder"å¼€å¤´
    if name.startswith("encoder"):
        # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"layers"ï¼Œåˆ™æ·»åŠ å‰ç¼€"encoder."
        if "layers" in name:
            name = "encoder." + name
        # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"attn.proj"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"attention.output.dense"
        if "attn.proj" in name:
            name = name.replace("attn.proj", "attention.output.dense")
        # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"attn"ä¸”ä¸åŒ…å«"mask"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"attention.self"
        if "attn" in name and "mask" not in name:
            name = name.replace("attn", "attention.self")
        # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"norm1"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"layernorm_before"
        if "norm1" in name:
            name = name.replace("norm1", "layernorm_before")
        # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"norm2"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"layernorm_after"
        if "norm2" in name:
            name = name.replace("norm2", "layernorm_after")
        # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"mlp.fc1"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"intermediate.dense"
        if "mlp.fc1" in name:
            name = name.replace("mlp.fc1", "intermediate.dense")
        # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"mlp.fc2"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"output.dense"
        if "mlp.fc2" in name:
            name = name.replace("mlp.fc2", "output.dense")

        # å¦‚æœæ–‡ä»¶åä¸º"encoder.norm.weight"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"encoder.layernorm.weight"
        if name == "encoder.norm.weight":
            name = "encoder.layernorm.weight"
        # å¦‚æœæ–‡ä»¶åä¸º"encoder.norm.bias"ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸º"encoder.layernorm.bias"
        if name == "encoder.norm.bias":
            name = "encoder.layernorm.bias"

    # è¿”å›ä¿®æ”¹åçš„æ–‡ä»¶å
    return name
# å°†åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºæ–°æ¨¡å‹çš„çŠ¶æ€å­—å…¸
def convert_state_dict(orig_state_dict, model):
    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„å‰¯æœ¬ä¸­çš„æ‰€æœ‰é”®
    for key in orig_state_dict.copy().keys():
        # ç§»é™¤åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„é”®ï¼Œå¹¶è·å–å¯¹åº”çš„å€¼
        val = orig_state_dict.pop(key)

        # æ£€æŸ¥é”®ä¸­æ˜¯å¦åŒ…å«"qkv"
        if "qkv" in key:
            # æ‹†åˆ†é”®ï¼Œå¹¶æå–å±‚å·å’Œå—å·
            key_split = key.split(".")
            layer_num = int(key_split[3])
            block_num = int(key_split[5])
            dim = model.encoder.encoder.layers[layer_num].blocks[block_num].attention.self.all_head_size

            # æ£€æŸ¥æ˜¯å¦ä¸ºæƒé‡å‚æ•°
            if "weight" in key:
                # æ›´æ–°æ–°çŠ¶æ€å­—å…¸ä¸­çš„æƒé‡å‚æ•°
                orig_state_dict[
                    f"encoder.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.weight"
                ] = val[:dim, :]
                orig_state_dict[
                    f"encoder.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.weight"
                ] = val[dim : dim * 2, :]
                orig_state_dict[
                    f"encoder.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.weight"
                ] = val[-dim:, :]
            else:
                # æ›´æ–°æ–°çŠ¶æ€å­—å…¸ä¸­çš„åç½®å‚æ•°
                orig_state_dict[
                    f"encoder.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.bias"
                ] = val[:dim]
                orig_state_dict[
                    f"encoder.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.bias"
                ] = val[dim : dim * 2]
                orig_state_dict[
                    f"encoder.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.bias"
                ] = val[-dim:]
        elif "attn_mask" in key or key in ["encoder.model.norm.weight", "encoder.model.norm.bias"]:
            # HuggingFace å®ç°ä¸ä½¿ç”¨ attn_mask ç¼“å†²åŒº
            # å¹¶ä¸”æ¨¡å‹ä¸ä½¿ç”¨ç¼–ç å™¨çš„æœ€ç»ˆ LayerNorm
            pass
        else:
            # æ›´æ–°æ–°çŠ¶æ€å­—å…¸ä¸­çš„å…¶ä»–å‚æ•°
            orig_state_dict[rename_key(key)] = val

    return orig_state_dict


# å°† Donut æ¨¡å‹æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºé€‚ç”¨äº HuggingFace çš„æ ¼å¼
def convert_donut_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):
    # åŠ è½½åŸå§‹æ¨¡å‹
    original_model = DonutModel.from_pretrained(model_name).eval()

    # åŠ è½½ HuggingFace æ¨¡å‹
    encoder_config, decoder_config = get_configs(original_model)
    encoder = DonutSwinModel(encoder_config)
    decoder = MBartForCausalLM(decoder_config)
    model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)
    model.eval()

    # è·å–åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    state_dict = original_model.state_dict()
    # è½¬æ¢åŸå§‹æ¨¡å‹çš„çŠ¶æ€å­—å…¸
    new_state_dict = convert_state_dict(state_dict, model)
    # åŠ è½½æ–°çš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­
    model.load_state_dict(new_state_dict)

    # åœ¨æ‰«ææ–‡æ¡£ä¸ŠéªŒè¯ç»“æœ
    dataset = load_dataset("hf-internal-testing/example-documents")
    image = dataset["test"][0]["image"].convert("RGB")

    # åŠ è½½æ¨¡å‹ä½¿ç”¨çš„ tokenizer
    tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name, from_slow=True)
    # åˆ›å»º Donut å›¾åƒå¤„ç†å™¨
    image_processor = DonutImageProcessor(
        do_align_long_axis=original_model.config.align_long_axis, size=original_model.config.input_size[::-1]
    )
    processor = DonutProcessor(image_processor, tokenizer)
    # å¤„ç†å›¾åƒå¹¶è·å–åƒç´ å€¼
    pixel_values = processor(image, return_tensors="pt").pixel_values
    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦ä¸º "naver-clova-ix/donut-base-finetuned-docvqa"
    if model_name == "naver-clova-ix/donut-base-finetuned-docvqa":
        # å¦‚æœæ˜¯ä¸Šè¿°æ¨¡å‹åç§°ï¼Œåˆ™è®¾ç½®ä»»åŠ¡æç¤ºä¸ºç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        task_prompt = "<s_docvqa><s_question>{user_input}</s_question><s_answer>"
        # è®¾ç½®ä¸€ä¸ªç¤ºä¾‹é—®é¢˜
        question = "When is the coffee break?"
        # å°†ç”¨æˆ·è¾“å…¥çš„é—®é¢˜å¡«å……åˆ°ä»»åŠ¡æç¤ºä¸­
        task_prompt = task_prompt.replace("{user_input}", question)
    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦ä¸º "naver-clova-ix/donut-base-finetuned-rvlcdip"
    elif model_name == "naver-clova-ix/donut-base-finetuned-rvlcdip":
        # å¦‚æœæ˜¯ä¸Šè¿°æ¨¡å‹åç§°ï¼Œåˆ™è®¾ç½®ä»»åŠ¡æç¤ºä¸ºç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²
        task_prompt = "<s_rvlcdip>"
    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åœ¨ç»™å®šçš„æ¨¡å‹åˆ—è¡¨ä¸­
    elif model_name in [
        "naver-clova-ix/donut-base-finetuned-cord-v1",
        "naver-clova-ix/donut-base-finetuned-cord-v1-2560",
    ]:
        # å¦‚æœæ˜¯åˆ—è¡¨ä¸­çš„æŸä¸ªæ¨¡å‹åç§°ï¼Œåˆ™è®¾ç½®ä»»åŠ¡æç¤ºä¸ºç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²
        task_prompt = "<s_cord>"
    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦ä¸º "naver-clova-ix/donut-base-finetuned-cord-v2"
    elif model_name == "naver-clova-ix/donut-base-finetuned-cord-v2":
        # å¦‚æœæ˜¯ä¸Šè¿°æ¨¡å‹åç§°ï¼Œåˆ™è®¾ç½®ä»»åŠ¡æç¤ºä¸ºç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²
        task_prompt = "s_cord-v2>"
    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦ä¸º "naver-clova-ix/donut-base-finetuned-zhtrainticket"
    elif model_name == "naver-clova-ix/donut-base-finetuned-zhtrainticket":
        # å¦‚æœæ˜¯ä¸Šè¿°æ¨¡å‹åç§°ï¼Œåˆ™è®¾ç½®ä»»åŠ¡æç¤ºä¸ºç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²
        task_prompt = "<s_zhtrainticket>"
    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åœ¨ç»™å®šçš„æ¨¡å‹åˆ—è¡¨ä¸­
    elif model_name in ["naver-clova-ix/donut-proto", "naver-clova-ix/donut-base"]:
        # å¦‚æœæ˜¯åˆ—è¡¨ä¸­çš„æŸä¸ªæ¨¡å‹åç§°ï¼Œåˆ™è®¾ç½®ä»»åŠ¡æç¤ºä¸ºå›ºå®šå­—ç¬¦ä¸²
        # è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªéšæœºçš„å­—ç¬¦ä¸²ä½œä¸ºç¤ºä¾‹
        task_prompt = "hello world"
    else:
        # å¦‚æœæ¨¡å‹åç§°ä¸åœ¨æ”¯æŒçš„åˆ—è¡¨ä¸­ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        raise ValueError("Model name not supported")

    # ä½¿ç”¨åŸå§‹æ¨¡å‹çš„è§£ç å™¨å¯¹ä»»åŠ¡æç¤ºè¿›è¡Œç¼–ç ï¼Œè¿”å›è¾“å…¥çš„ token IDs
    prompt_tensors = original_model.decoder.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt")[
        "input_ids"
    ]

    # ä½¿ç”¨åŸå§‹æ¨¡å‹çš„ç¼–ç å™¨å¯¹åƒç´ å€¼è¿›è¡Œç¼–ç ï¼Œè·å–åŸå§‹çš„ patch embeddings
    original_patch_embed = original_model.encoder.model.patch_embed(pixel_values)
    # ä½¿ç”¨æ–°æ¨¡å‹çš„ç¼–ç å™¨å¯¹åƒç´ å€¼è¿›è¡Œç¼–ç ï¼Œè·å–æ–°çš„ patch embeddings
    patch_embeddings, _ = model.encoder.embeddings(pixel_values)
    # æ£€æŸ¥ä¸¤ä¸ª patch embeddings æ˜¯å¦åœ¨ä¸€å®šçš„è¯¯å·®èŒƒå›´å†…ç›¸ç­‰
    assert torch.allclose(original_patch_embed, patch_embeddings, atol=1e-3)

    # éªŒè¯ç¼–ç å™¨çš„éšè—çŠ¶æ€æ˜¯å¦ä¸€è‡´
    # ä½¿ç”¨åŸå§‹æ¨¡å‹çš„ç¼–ç å™¨è·å–æœ€åçš„éšè—çŠ¶æ€
    original_last_hidden_state = original_model.encoder(pixel_values)
    # ä½¿ç”¨æ–°æ¨¡å‹çš„ç¼–ç å™¨è·å–æœ€åçš„éšè—çŠ¶æ€
    last_hidden_state = model.encoder(pixel_values).last_hidden_state
    # æ£€æŸ¥ä¸¤ä¸ªéšè—çŠ¶æ€æ˜¯å¦åœ¨ä¸€å®šçš„è¯¯å·®èŒƒå›´å†…ç›¸ç­‰
    assert torch.allclose(original_last_hidden_state, last_hidden_state, atol=1e-2)

    # éªŒè¯è§£ç å™¨çš„éšè—çŠ¶æ€æ˜¯å¦ä¸€è‡´
    # ä½¿ç”¨åŸå§‹æ¨¡å‹ç”ŸæˆåŸå§‹çš„ logits
    original_logits = original_model(pixel_values, prompt_tensors, None).logits
    # ä½¿ç”¨æ–°æ¨¡å‹ç”Ÿæˆæ–°çš„ logits
    logits = model(pixel_values, decoder_input_ids=prompt_tensors).logits
    # æ£€æŸ¥ä¸¤ä¸ª logits æ˜¯å¦åœ¨ä¸€å®šçš„è¯¯å·®èŒƒå›´å†…ç›¸ç­‰
    assert torch.allclose(original_logits, logits, atol=1e-3)
    # æ‰“å°éªŒè¯æˆåŠŸä¿¡æ¯
    print("Looks ok!")

    # å¦‚æœæä¾›äº† PyTorch æ¨¡å‹ä¿å­˜è·¯å¾„
    if pytorch_dump_folder_path is not None:
        # æ‰“å°ä¿å­˜æ¨¡å‹å’Œå¤„ç†å™¨çš„ä¿¡æ¯
        print(f"Saving model and processor to {pytorch_dump_folder_path}")
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        model.save_pretrained(pytorch_dump_folder_path)
        # å°†å¤„ç†å™¨ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        processor.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœéœ€è¦æ¨é€åˆ° Hub
    if push_to_hub:
        # æ¨é€æ¨¡å‹åˆ° Hubï¼Œä½¿ç”¨æ¨¡å‹åç§°çš„ä¸€éƒ¨åˆ†ä½œä¸ºè·¯å¾„
        model.push_to_hub("nielsr/" + model_name.split("/")[-1], commit_message="Update model")
        # æ¨é€å¤„ç†å™¨åˆ° Hubï¼Œä½¿ç”¨æ¨¡å‹åç§°çš„ä¸€éƒ¨åˆ†ä½œä¸ºè·¯å¾„
        processor.push_to_hub("nielsr/" + model_name.split("/")[-1], commit_message="Update model")
# å¦‚æœä»£ç è¢«ç›´æ¥è¿è¡Œè€Œä¸æ˜¯è¢«å¼•å…¥ä½œä¸ºæ¨¡å—ï¼Œä»¥ä¸‹å†…å®¹ä¼šè¢«æ‰§è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    
    # æ·»åŠ å¿…éœ€çš„å‚æ•°ï¼šæ¨¡å‹åç§°ï¼Œé»˜è®¤æ•°å€¼ã€ç±»å‹å’Œå¸®åŠ©ä¿¡æ¯
    parser.add_argument(
        "--model_name",
        default="naver-clova-ix/donut-base-finetuned-docvqa",
        required=False,
        type=str,
        help="Name of the original model you'd like to convert.",
    )
    
    # æ·»åŠ å¿…éœ€çš„å‚æ•°ï¼šPyTorchæ¨¡å‹è¾“å‡ºç›®å½•è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path",
        default=None,
        required=False,
        type=str,
        help="Path to the output PyTorch model directory.",
    )
    
    # æ·»åŠ å‚æ•°ï¼šæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹å’Œå¤„ç†å™¨æ¨é€åˆ°ğŸ¤— hub
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="Whether or not to push the converted model and processor to the ğŸ¤— hub.",
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # è°ƒç”¨å‡½æ•°ï¼Œå°†Donutæ¨¡å‹æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºPyTorchæ¨¡å‹ï¼Œå¹¶å¯é€‰æ‹©æ¨é€åˆ°hub
    convert_donut_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)
```  
```