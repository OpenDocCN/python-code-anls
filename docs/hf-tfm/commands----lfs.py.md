# `.\transformers\commands\lfs.py`

```py
"""
Implementation of a custom transfer agent for the transfer type "multipart" for git-lfs.

Inspired by: github.com/cbartz/git-lfs-swift-transfer-agent/blob/master/git_lfs_swift_transfer.py

Spec is: github.com/git-lfs/git-lfs/blob/master/docs/custom-transfers.md


To launch debugger while developing:

``` [lfs "customtransfer.multipart"]
path = /path/to/transformers/.env/bin/python args = -m debugpy --listen 5678 --wait-for-client
/path/to/transformers/src/transformers/commands/transformers_cli.py lfs-multipart-upload ```"""

import json
import os
import subprocess
import sys
import warnings
from argparse import ArgumentParser
from contextlib import AbstractContextManager
from typing import Dict, List, Optional

import requests

from ..utils import logging
from . import BaseTransformersCLICommand

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name

# å®šä¹‰ lfs-multipart-upload å‘½ä»¤
LFS_MULTIPART_UPLOAD_COMMAND = "lfs-multipart-upload"

# è‡ªå®šä¹‰ LfsCommands ç±»ï¼Œå®ç° git-lfs çš„ "multipart" ä¼ è¾“ç±»å‹çš„è‡ªå®šä¹‰ä¼ è¾“ä»£ç†
class LfsCommands(BaseTransformersCLICommand):
    """
    Implementation of a custom transfer agent for the transfer type "multipart" for git-lfs. This lets users upload
    large files >5GB ğŸ”¥. Spec for LFS custom transfer agent is:
    https://github.com/git-lfs/git-lfs/blob/master/docs/custom-transfers.md

    This introduces two commands to the CLI:

    1. $ transformers-cli lfs-enable-largefiles

    This should be executed once for each model repo that contains a model file >5GB. It's documented in the error
    message you get if you just try to git push a 5GB file without having enabled it before.

    2. $ transformers-cli lfs-multipart-upload

    This command is called by lfs directly and is not meant to be called by the user.
    """

    # æ³¨å†Œå­å‘½ä»¤
    @staticmethod
    def register_subcommand(parser: ArgumentParser):
        # æ·»åŠ  lfs-enable-largefiles å‘½ä»¤
        enable_parser = parser.add_parser(
            "lfs-enable-largefiles",
            help=(
                "Deprecated: use `huggingface-cli` instead. Configure your repository to enable upload of files > 5GB."
            ),
        )
        enable_parser.add_argument("path", type=str, help="Local path to repository you want to configure.")
        enable_parser.set_defaults(func=lambda args: LfsEnableCommand(args))

        # æ·»åŠ  lfs-multipart-upload å‘½ä»¤
        upload_parser = parser.add_parser(
            LFS_MULTIPART_UPLOAD_COMMAND,
            help=(
                "Deprecated: use `huggingface-cli` instead. "
                "Command will get called by git-lfs, do not call it directly."
            ),
        )
        upload_parser.set_defaults(func=lambda args: LfsUploadCommand(args))

# å®šä¹‰ LfsEnableCommand ç±»
class LfsEnableCommand:
    def __init__(self, args):
        self.args = args
    # åœ¨è¿è¡Œæ–¹æ³•ä¸­å‘å‡ºè­¦å‘Šï¼Œæç¤ºé€šè¿‡ transformers-cli ç®¡ç†ä»“åº“å·²ä¸æ¨èï¼Œå»ºè®®ä½¿ç”¨ `huggingface-cli` ä»£æ›¿
    warnings.warn(
        "Managing repositories through transformers-cli is deprecated. Please use `huggingface-cli` instead."
    )
    # è·å–æœ¬åœ°è·¯å¾„å¹¶ç¡®ä¿å…¶ä¸ºæœ‰æ•ˆçš„ Git ä»“åº“è·¯å¾„
    local_path = os.path.abspath(self.args.path)
    if not os.path.isdir(local_path):
        # è‹¥è·¯å¾„ä¸æ˜¯æœ‰æ•ˆçš„ Git ä»“åº“ï¼Œåˆ™æ‰“å°é”™è¯¯ä¿¡æ¯å¹¶é€€å‡º
        print("This does not look like a valid git repo.")
        exit(1)
    # è®¾ç½® Git LFS çš„è‡ªå®šä¹‰ä¼ è¾“é…ç½®ï¼Œä½¿ç”¨ transformers-cli
    subprocess.run(
        "git config lfs.customtransfer.multipart.path transformers-cli".split(), check=True, cwd=local_path
    )
    # è®¾ç½® Git LFS çš„è‡ªå®šä¹‰ä¼ è¾“å‚æ•°ï¼ŒæŒ‡å®šä¸Šä¼ å‘½ä»¤
    subprocess.run(
        f"git config lfs.customtransfer.multipart.args {LFS_MULTIPART_UPLOAD_COMMAND}".split(),
        check=True,
        cwd=local_path,
    )
    # æ‰“å°æç¤ºä¿¡æ¯ï¼Œè¯´æ˜æœ¬åœ°ä»“åº“å·²ç»è®¾ç½®å¥½ä»¥å¤„ç†å¤§æ–‡ä»¶
    print("Local repo set up for largefiles")
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†æ¶ˆæ¯ä»¥è¡Œåˆ†éš”çš„ JSON æ ¼å¼å†™å…¥æ ‡å‡†è¾“å‡º
def write_msg(msg: Dict):
    # å°†æ¶ˆæ¯å­—å…¸è½¬æ¢æˆ JSON æ ¼å¼å¹¶æ·»åŠ æ¢è¡Œç¬¦
    msg = json.dumps(msg) + "\n"
    # å°†æ¶ˆæ¯å†™å…¥æ ‡å‡†è¾“å‡º
    sys.stdout.write(msg)
    # ç«‹å³åˆ·æ–°æ ‡å‡†è¾“å‡ºç¼“å†²åŒº
    sys.stdout.flush()

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»æ ‡å‡†è¾“å…¥è¯»å–è¡Œåˆ†éš”çš„ JSON æ ¼å¼æ¶ˆæ¯
def read_msg() -> Optional[Dict]:
    # ä»æ ‡å‡†è¾“å…¥è¯»å–ä¸€è¡Œå¹¶å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦ï¼Œè§£ææˆ JSON æ ¼å¼æ¶ˆæ¯
    msg = json.loads(sys.stdin.readline().strip())

    # å¦‚æœæ¶ˆæ¯ä¸­å«æœ‰ "terminate" å­—æ®µï¼Œè¡¨ç¤ºæ¥æ”¶åˆ°ç»ˆæ­¢æ¶ˆæ¯ï¼Œåˆ™è¿”å› None
    if "terminate" in (msg.get("type"), msg.get("event")):
        # æ”¶åˆ°ç»ˆæ­¢æ¶ˆæ¯
        return None

    # å¦‚æœæ¶ˆæ¯ä¸­çš„ "event" å­—æ®µä¸æ˜¯ "download" æˆ– "upload"ï¼Œåˆ™è®°å½•æ—¥å¿—å¹¶é€€å‡ºç¨‹åº
    if msg.get("event") not in ("download", "upload"):
        logger.critical("Received unexpected message")
        sys.exit(1)

    # è¿”å›è§£æåçš„æ¶ˆæ¯å­—å…¸
    return msg

# å®šä¹‰ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ç±»ï¼Œå®ç°ä»…è¯»å–æ–‡ä»¶çš„æŒ‡å®šéƒ¨åˆ†
class FileSlice(AbstractContextManager):
    """
    File-like object that only reads a slice of a file

    Inspired by stackoverflow.com/a/29838711/593036
    """

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—æ–‡ä»¶è·¯å¾„ã€èµ·å§‹åç§»å’Œè¯»å–é™åˆ¶å‚æ•°
    def __init__(self, filepath: str, seek_from: int, read_limit: int):
        self.filepath = filepath
        self.seek_from = seek_from
        self.read_limit = read_limit
        self.n_seen = 0

    # è¿›å…¥ä¸Šä¸‹æ–‡æ—¶æ‰§è¡Œçš„æ–¹æ³•
    def __enter__(self):
        # æ‰“å¼€æ–‡ä»¶å¹¶å°†æ–‡ä»¶æŒ‡é’ˆç§»åŠ¨åˆ°æŒ‡å®šä½ç½®
        self.f = open(self.filepath, "rb")
        self.f.seek(self.seek_from)
        return self

    # è¿”å›æ–‡ä»¶å†…å®¹çš„é•¿åº¦
    def __len__(self):
        # è·å–æ–‡ä»¶æ€»é•¿åº¦
        total_length = os.fstat(self.f.fileno()).st_size
        # è¿”å›å®é™…å¯è¯»å–çš„é•¿åº¦ï¼Œä¸è¶…è¿‡è¯»å–é™åˆ¶
        return min(self.read_limit, total_length - self.seek_from)

    # è¯»å–æ–‡ä»¶å†…å®¹çš„æ–¹æ³•
    def read(self, n=-1):
        # å¦‚æœå·²ç»è¯»å–äº†æŒ‡å®šé•¿åº¦çš„å†…å®¹ï¼Œåˆ™è¿”å›ç©ºå­—èŠ‚ä¸²
        if self.n_seen >= self.read_limit:
            return b""
        # è®¡ç®—å‰©ä½™å¯è¯»å–çš„å­—èŠ‚æ•°
        remaining_amount = self.read_limit - self.n_seen
        # è¯»å–æ–‡ä»¶å†…å®¹ï¼Œä¸è¶…è¿‡å‰©ä½™å¯è¯»å–çš„å­—èŠ‚æ•°
        data = self.f.read(remaining_amount if n < 0 else min(n, remaining_amount))
        # æ›´æ–°å·²è¯»å–çš„å­—èŠ‚æ•°
        self.n_seen += len(data)
        return data

    # è¿­ä»£å™¨æ–¹æ³•ï¼Œæ¯æ¬¡è¿­ä»£è¿”å›è¯»å–çš„æ•°æ®
    def __iter__(self):
        yield self.read(n=4 * 1024 * 1024)

    # é€€å‡ºä¸Šä¸‹æ–‡æ—¶æ‰§è¡Œçš„æ–¹æ³•ï¼Œå…³é—­æ–‡ä»¶
    def __exit__(self, *args):
        self.f.close()

# å®šä¹‰ä¸€ä¸ªç±»ï¼Œè¡¨ç¤º LFS ä¸Šä¼ å‘½ä»¤
class LfsUploadCommand:
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—å‚æ•°å¹¶ä¿å­˜åˆ°å®ä¾‹å±æ€§ä¸­
    def __init__(self, args):
        self.args = args
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºè¿è¡Œè‡ªå®šä¹‰çš„ä¼ è¾“è¿‡ç¨‹
    def run(self):
        # ä»æ ‡å‡†è¾“å…¥è¯»å–ä¸€è¡Œæ•°æ®ï¼Œå¹¶å°†å…¶è§£æä¸º JSON æ ¼å¼ï¼Œè·å–åˆå§‹åŒ–ä¿¡æ¯
        init_msg = json.loads(sys.stdin.readline().strip())
        # æ£€æŸ¥åˆå§‹åŒ–ä¿¡æ¯æ˜¯å¦æ­£ç¡®ï¼Œå¦‚æœä¸æ­£ç¡®åˆ™å‘é€é”™è¯¯æ¶ˆæ¯å¹¶é€€å‡ºç¨‹åº
        if not (init_msg.get("event") == "init" and init_msg.get("operation") == "upload"):
            write_msg({"error": {"code": 32, "message": "Wrong lfs init operation"}})
            sys.exit(1)

        # å“åº”åˆå§‹åŒ–ä¿¡æ¯ï¼Œå‘é€ä¸€ä¸ªç©ºçš„ç¡®è®¤æ¶ˆæ¯åˆ°æ ‡å‡†è¾“å‡º
        write_msg({})

        # åœ¨åˆå§‹åŒ–äº¤æ¢ä¹‹åï¼Œgit-lfs å°†ä¼šå‘é€ä»»æ„æ•°é‡çš„ä¼ è¾“è¯·æ±‚åˆ°ä¼ è¾“è¿‡ç¨‹çš„æ ‡å‡†è¾“å…¥ä¸­ï¼ŒæŒ‰é¡ºåºè¿›è¡Œå¤„ç†
        while True:
            # è¯»å–ä¼ è¾“è¯·æ±‚æ¶ˆæ¯
            msg = read_msg()
            if msg is None:
                # å½“æ‰€æœ‰ä¼ è¾“è¯·æ±‚éƒ½è¢«å¤„ç†å®Œæ¯•æ—¶ï¼Œgit-lfs å°†ä¼šå‘é€ä¸€ä¸ªç»ˆæ­¢äº‹ä»¶åˆ°ä¼ è¾“è¿‡ç¨‹çš„æ ‡å‡†è¾“å…¥ä¸­
                # æ¥æ”¶åˆ°æ­¤æ¶ˆæ¯åï¼Œä¼ è¾“è¿‡ç¨‹åº”è¯¥è¿›è¡Œæ¸…ç†å¹¶ç»ˆæ­¢ï¼Œä¸éœ€è¦å“åº”
                sys.exit(0)

            # è·å–ä¼ è¾“è¯·æ±‚ä¸­çš„ç›¸å…³ä¿¡æ¯
            oid = msg["oid"]
            filepath = msg["path"]
            completion_url = msg["action"]["href"]
            header = msg["action"]["header"]
            chunk_size = int(header.pop("chunk_size"))
            presigned_urls: List[str] = list(header.values())

            parts = []
            # éå†é¢„ç­¾å URL åˆ—è¡¨ï¼ŒæŒ‰ç…§æŒ‡å®šçš„å—å¤§å°ä¸Šä¼ æ•°æ®
            for i, presigned_url in enumerate(presigned_urls):
                with FileSlice(filepath, seek_from=i * chunk_size, read_limit=chunk_size) as data:
                    r = requests.put(presigned_url, data=data)
                    r.raise_for_status()
                    # å°†ä¸Šä¼ ç»“æœæ·»åŠ åˆ° parts åˆ—è¡¨ä¸­
                    parts.append(
                        {
                            "etag": r.headers.get("etag"),
                            "partNumber": i + 1,
                        }
                    )
                    # ä¸ºäº†æ”¯æŒæ•°æ®ä¸Šä¼ /ä¸‹è½½æ—¶çš„è¿›åº¦æŠ¥å‘Šï¼Œä¼ è¾“è¿‡ç¨‹åº”è¯¥å‘æ ‡å‡†è¾“å‡ºå‘é€æ¶ˆæ¯
                    write_msg(
                        {
                            "event": "progress",
                            "oid": oid,
                            "bytesSoFar": (i + 1) * chunk_size,
                            "bytesSinceLast": chunk_size,
                        }
                    )
                    # ä¸æ˜¯ç²¾ç¡®çš„ï¼Œä½†å¯ä»¥æ¥å—

            # å‘å®Œæˆ URL å‘é€ POST è¯·æ±‚ï¼ŒåŒ…å«ä¸Šä¼ å®Œæˆçš„ä¿¡æ¯
            r = requests.post(
                completion_url,
                json={
                    "oid": oid,
                    "parts": parts,
                },
            )
            r.raise_for_status()

            # å‘é€å®Œæˆäº‹ä»¶æ¶ˆæ¯åˆ°æ ‡å‡†è¾“å‡º
            write_msg({"event": "complete", "oid": oid})
```