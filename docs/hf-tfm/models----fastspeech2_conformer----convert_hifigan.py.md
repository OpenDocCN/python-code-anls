# `.\models\fastspeech2_conformer\convert_hifigan.py`

```py
# coding=utf-8
# ç‰ˆæƒ 2023 å¹´ HuggingFace Inc. å›¢é˜Ÿä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ç‰ˆæœ¬ 2.0 æˆæƒä½¿ç”¨æ­¤æ–‡ä»¶ï¼›é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™è½¯ä»¶æŒ‰â€œåŸæ ·â€åˆ†å‘ï¼Œæ— ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚
# è¯·æŸ¥é˜…è®¸å¯è¯äº†è§£è¯¦ç»†çš„è®¸å¯æ¡ä»¶åŠé™åˆ¶ã€‚
"""å°† FastSpeech2Conformer HiFi-GAN çš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºæ¨¡å‹ã€‚"""

import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
from pathlib import Path  # å¯¼å…¥å¤„ç†è·¯å¾„çš„æ¨¡å—

import torch  # å¯¼å…¥ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
import yaml  # å¯¼å…¥å¤„ç† YAML æ ¼å¼çš„æ¨¡å—

from transformers import FastSpeech2ConformerHifiGan, FastSpeech2ConformerHifiGanConfig, logging  # å¯¼å…¥æ¨¡å‹ç›¸å…³ç±»å’Œæ—¥å¿—è®°å½•

logging.set_verbosity_info()  # è®¾ç½®æ—¥å¿—è®°å½•çº§åˆ«ä¸ºä¿¡æ¯
logger = logging.get_logger("transformers.models.FastSpeech2Conformer")  # è·å–æ¨¡å‹æ—¥å¿—è®°å½•å™¨


def load_weights(checkpoint, hf_model, config):
    """åŠ è½½æƒé‡åˆ°æ¨¡å‹ä¸­ã€‚

    Args:
        checkpoint (dict): æ£€æŸ¥ç‚¹ä¸­çš„æƒé‡å­—å…¸
        hf_model (FastSpeech2ConformerHifiGan): éœ€è¦åŠ è½½æƒé‡çš„æ¨¡å‹å®ä¾‹
        config (FastSpeech2ConformerHifiGanConfig): æ¨¡å‹çš„é…ç½®ä¿¡æ¯
    """
    vocoder_key_prefix = "tts.generator.vocoder."
    checkpoint = {k.replace(vocoder_key_prefix, ""): v for k, v in checkpoint.items() if vocoder_key_prefix in k}

    hf_model.apply_weight_norm()

    hf_model.conv_pre.weight_g.data = checkpoint["input_conv.weight_g"]
    hf_model.conv_pre.weight_v.data = checkpoint["input_conv.weight_v"]
    hf_model.conv_pre.bias.data = checkpoint["input_conv.bias"]

    for i in range(len(config.upsample_rates)):
        hf_model.upsampler[i].weight_g.data = checkpoint[f"upsamples.{i}.1.weight_g"]
        hf_model.upsampler[i].weight_v.data = checkpoint[f"upsamples.{i}.1.weight_v"]
        hf_model.upsampler[i].bias.data = checkpoint[f"upsamples.{i}.1.bias"]

    for i in range(len(config.upsample_rates) * len(config.resblock_kernel_sizes)):
        for j in range(len(config.resblock_dilation_sizes)):
            hf_model.resblocks[i].convs1[j].weight_g.data = checkpoint[f"blocks.{i}.convs1.{j}.1.weight_g"]
            hf_model.resblocks[i].convs1[j].weight_v.data = checkpoint[f"blocks.{i}.convs1.{j}.1.weight_v"]
            hf_model.resblocks[i].convs1[j].bias.data = checkpoint[f"blocks.{i}.convs1.{j}.1.bias"]

            hf_model.resblocks[i].convs2[j].weight_g.data = checkpoint[f"blocks.{i}.convs2.{j}.1.weight_g"]
            hf_model.resblocks[i].convs2[j].weight_v.data = checkpoint[f"blocks.{i}.convs2.{j}.1.weight_v"]
            hf_model.resblocks[i].convs2[j].bias.data = checkpoint[f"blocks.{i}.convs2.{j}.1.bias"]

    hf_model.conv_post.weight_g.data = checkpoint["output_conv.1.weight_g"]
    hf_model.conv_post.weight_v.data = checkpoint["output_conv.1.weight_v"]
    hf_model.conv_post.bias.data = checkpoint["output_conv.1.bias"]

    hf_model.remove_weight_norm()


def remap_hifigan_yaml_config(yaml_config_path):
    """é‡æ–°æ˜ å°„ HiFi-GAN çš„ YAML é…ç½®ã€‚

    Args:
        yaml_config_path (str): YAML é…ç½®æ–‡ä»¶çš„è·¯å¾„
    """
    with Path(yaml_config_path).open("r", encoding="utf-8") as f:
        args = yaml.safe_load(f)
        args = argparse.Namespace(**args)

    vocoder_type = args.tts_conf["vocoder_type"]
    # æ£€æŸ¥å£°ç å™¨ç±»å‹æ˜¯å¦ä¸º "hifigan_generator"ï¼Œå¦‚æœä¸æ˜¯åˆ™å¼•å‘ç±»å‹é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
    if vocoder_type != "hifigan_generator":
        raise TypeError(f"Vocoder config must be for `hifigan_generator`, but got {vocoder_type}")

    # åˆ›å»ºä¸€ä¸ªç©ºçš„é‡æ˜ å°„å­—å…¸
    remapped_dict = {}

    # è·å–å£°ç å™¨å‚æ•°å­—å…¸
    vocoder_params = args.tts_conf["vocoder_params"]

    # å®šä¹‰é”®æ˜ å°„å…³ç³»å­—å…¸ï¼Œå°† espnet é…ç½®é”®æ˜ å°„åˆ° huggingface é…ç½®é”®
    key_mappings = {
        "channels": "upsample_initial_channel",
        "in_channels": "model_in_dim",
        "resblock_dilations": "resblock_dilation_sizes",
        "resblock_kernel_sizes": "resblock_kernel_sizes",
        "upsample_kernel_sizes": "upsample_kernel_sizes",
        "upsample_scales": "upsample_rates",
    }

    # éå†é”®æ˜ å°„å­—å…¸ï¼Œå°† espnet é…ç½®ä¸­å¯¹åº”é”®çš„å€¼æ˜ å°„åˆ° remapped_dict ä¸­çš„å¯¹åº” huggingface é”®
    for espnet_config_key, hf_config_key in key_mappings.items():
        remapped_dict[hf_config_key] = vocoder_params[espnet_config_key]

    # å°†é‡‡æ ·ç‡ä»å‚æ•°ä¸­çš„ TTS é…ç½®å¤åˆ¶åˆ° remapped_dict
    remapped_dict["sampling_rate"] = args.tts_conf["sampling_rate"]
    
    # è®¾ç½® normalize_before ä¸º False
    remapped_dict["normalize_before"] = False
    
    # ä»å£°ç å™¨å‚æ•°ä¸­çš„éçº¿æ€§æ¿€æ´»å‚æ•°ä¸­è·å– leaky ReLU çš„è´Ÿæ–œç‡ï¼Œå¹¶è®¾ç½®åˆ° remapped_dict
    remapped_dict["leaky_relu_slope"] = vocoder_params["nonlinear_activation_params"]["negative_slope"]

    # è¿”å›é‡æ˜ å°„åçš„é…ç½®å­—å…¸
    return remapped_dict
# ä½¿ç”¨è£…é¥°å™¨ @torch.no_grad() æ¥ç¡®ä¿åœ¨æ­¤å‡½æ•°ä¸­ä¸ä¼šè®¡ç®—æ¢¯åº¦
@torch.no_grad()
# å®šä¹‰å‡½æ•° convert_hifigan_checkpointï¼Œç”¨äºè½¬æ¢ HiFi-GAN æ¨¡å‹çš„æ£€æŸ¥ç‚¹
def convert_hifigan_checkpoint(
    checkpoint_path,  # è¾“å…¥å‚æ•°ï¼šåŸå§‹æ£€æŸ¥ç‚¹çš„æ–‡ä»¶è·¯å¾„
    pytorch_dump_folder_path,  # è¾“å…¥å‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„
    yaml_config_path=None,  # è¾“å…¥å‚æ•°ï¼šå¯é€‰çš„æ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆYAMLï¼‰è·¯å¾„ï¼Œé»˜è®¤ä¸º None
    repo_id=None,  # è¾“å…¥å‚æ•°ï¼šå¯é€‰çš„ ğŸ¤— hub ä¸Šæ¨¡å‹çš„ repo_idï¼Œé»˜è®¤ä¸º None
):
    # å¦‚æœæä¾›äº† yaml_config_pathï¼Œåˆ™ä½¿ç”¨ remap_hifigan_yaml_config å‡½æ•°å¤„ç†é…ç½®æ–‡ä»¶å¹¶åˆ›å»ºé…ç½®å¯¹è±¡
    if yaml_config_path is not None:
        config_kwargs = remap_hifigan_yaml_config(yaml_config_path)
        config = FastSpeech2ConformerHifiGanConfig(**config_kwargs)
    else:
        # å¦åˆ™ï¼Œä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºé…ç½®å¯¹è±¡
        config = FastSpeech2ConformerHifiGanConfig()

    # ä½¿ç”¨é…ç½®å¯¹è±¡åˆ›å»º FastSpeech2ConformerHifiGan æ¨¡å‹
    model = FastSpeech2ConformerHifiGan(config)

    # åŠ è½½åŸå§‹æ£€æŸ¥ç‚¹æ–‡ä»¶å†…å®¹åˆ° orig_checkpoint
    orig_checkpoint = torch.load(checkpoint_path)
    # è°ƒç”¨ load_weights å‡½æ•°ï¼Œå°† orig_checkpoint ä¸­çš„æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­
    load_weights(orig_checkpoint, model, config)

    # å°†æ¨¡å‹ä¿å­˜ä¸º PyTorch æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœæä¾›äº† repo_idï¼Œåˆ™æ‰“å°æ¶ˆæ¯å¹¶å°†æ¨¡å‹æ¨é€åˆ° ğŸ¤— hub ä¸Šçš„æŒ‡å®š repo_id
    if repo_id:
        print("Pushing to the hub...")
        model.push_to_hub(repo_id)


# å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹å†…å®¹
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šåŸå§‹æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼Œå¿…éœ€å‚æ•°
    parser.add_argument("--checkpoint_path", required=True, default=None, type=str, help="Path to original checkpoint")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆYAMLï¼‰è·¯å¾„ï¼Œå¯é€‰å‚æ•°
    parser.add_argument("--yaml_config_path", default=None, type=str, help="Path to config.yaml of model to convert")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¿…éœ€å‚æ•°
    parser.add_argument(
        "--pytorch_dump_folder_path", required=True, default=None, type=str, help="Path to the output PyTorch model."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ˜¯å¦æ¨é€åˆ° ğŸ¤— hub çš„ repo_idï¼Œå¯é€‰å‚æ•°
    parser.add_argument(
        "--push_to_hub", default=None, type=str, help="Where to upload the converted model on the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨ convert_hifigan_checkpoint å‡½æ•°ï¼Œä¼ å…¥å‘½ä»¤è¡Œè§£æå¾—åˆ°çš„å‚æ•°
    convert_hifigan_checkpoint(
        args.checkpoint_path,
        args.pytorch_dump_folder_path,
        args.yaml_config_path,
        args.push_to_hub,
    )
```