# `.\transformers\models\videomae\convert_videomae_to_pytorch.py`

```
# è®¾ç½®è„šæœ¬çš„å­—ç¬¦ç¼–ç ä¸º utf-8
# ç‰ˆæƒå£°æ˜
# 2022 å¹´ç‰ˆæƒæ‰€æœ‰ The HuggingFace Inc. å›¢é˜Ÿ
#
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰çš„è§„å®š
# é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™è¯¥è½¯ä»¶æ˜¯åŸºäº"åŸæ ·"åˆ†å‘çš„
# æ²¡æœ‰ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶
# æœ‰å…³ç‰¹å®šçš„è¯­è¨€ç‰ˆæœ¬çš„æƒé™å’Œé™åˆ¶ï¼Œè¯·å‚è§è®¸å¯è¯

# å¯¼å…¥æ‰€éœ€çš„åº“
import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å— argparse
import json  # å¯¼å…¥ json æ¨¡å—ç”¨äºå¤„ç† JSON æ ¼å¼çš„æ•°æ®

import gdown  # å¯¼å…¥ gdown æ¨¡å—è¿›è¡Œ Google Drive æ–‡ä»¶çš„ä¸‹è½½
import numpy as np  # å¯¼å…¥ numpy åº“
import torch  # å¯¼å…¥ PyTorch åº“
from huggingface_hub import hf_hub_download  # ä» huggingface_hub æ¨¡å—ä¸­å¯¼å…¥ hf_hub_download å‡½æ•°

# ä» transformers åº“ä¸­å¯¼å…¥ä»¥ä¸‹æ¨¡å‹ç›¸å…³ç»„ä»¶
from transformers import (
    VideoMAEConfig,  # å¯¼å…¥ VideoMAEConfig ç±»
    VideoMAEForPreTraining,  # å¯¼å…¥ VideoMAEForPreTraining ç±»
    VideoMAEForVideoClassification,  # å¯¼å…¥ VideoMAEForVideoClassification ç±»
    VideoMAEImageProcessor,  # å¯¼å…¥ VideoMAEImageProcessor ç±»
)


# æ ¹æ®æ¨¡å‹åç§°è·å– VideoMAEConfig é…ç½®ä¿¡æ¯
def get_videomae_config(model_name):
    # åˆ›å»º VideoMAEConfig å¯¹è±¡
    config = VideoMAEConfig()

    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®æ¶æ„é…ç½®
    set_architecture_configs(model_name, config)

    # å¦‚æœæ¨¡å‹åç§°ä¸­ä¸åŒ…å« 'finetuned'ï¼Œåˆ™å°† use_mean_pooling è®¾ä¸º False
    if "finetuned" not in model_name:
        config.use_mean_pooling = False

    # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å« 'finetuned'ï¼Œå¤„ç†æ ‡ç­¾ä¿¡æ¯
    if "finetuned" in model_name:
        repo_id = "huggingface/label-files"
        # æ ¹æ®æ¨¡å‹åç§°è®¾å®šæ ‡ç­¾æ•°å’Œæ–‡ä»¶å
        if "kinetics" in model_name:
            config.num_labels = 400
            filename = "kinetics400-id2label.json"
        elif "ssv2" in model_name:
            config.num_labels = 174
            filename = "something-something-v2-id2label.json"
        else:
            raise ValueError("Model name should either contain 'kinetics' or 'ssv2' in case it's fine-tuned.")
        # åŠ è½½æ ‡ç­¾æ•°æ®
        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
        id2label = {int(k): v for k, v in id2label.items()}
        config.id2label = id2label
        config.label2id = {v: k for k, v in id2label.items()}

    return config


# æ ¹æ®æ¨¡å‹åç§°è®¾ç½®æ¶æ„é…ç½®ä¿¡æ¯
def set_architecture_configs(model_name, config):
    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®ä¸åŒçš„æ¶æ„å‚æ•°
    if "small" in model_name:
        config.hidden_size = 384
        config.intermediate_size = 1536
        config.num_hidden_layers = 12
        config.num_attention_heads = 16
        config.decoder_num_hidden_layers = 12
        config.decoder_num_attention_heads = 3
        config.decoder_hidden_size = 192
        config.decoder_intermediate_size = 768
    elif "large" in model_name:
        config.hidden_size = 1024
        config.intermediate_size = 4096
        config.num_hidden_layers = 24
        config.num_attention_heads = 16
        config.decoder_num_hidden_layers = 12
        config.decoder_num_attention_heads = 8
        config.decoder_hidden_size = 512
        config.decoder_intermediate_size = 2048
    # å¦‚æœæ¨¡å‹åç§°åŒ…å«"huge"å…³é”®è¯ï¼Œåˆ™è®¾ç½®é…ç½®å‚æ•°ä¸ºå¤§å‹æ¨¡å‹çš„æ•°å€¼
    elif "huge" in model_name:
        # è®¾ç½®éšè—å±‚å¤§å°ä¸º1280
        config.hidden_size = 1280
        # è®¾ç½®ä¸­é—´å±‚å¤§å°ä¸º5120
        config.intermediate_size = 5120
        # è®¾ç½®éšè—å±‚æ•°ä¸º32
        config.num_hidden_layers = 32
        # è®¾ç½®æ³¨æ„åŠ›å¤´æ•°ä¸º16
        config.num_attention_heads = 16
        # è®¾ç½®è§£ç å™¨éšè—å±‚æ•°ä¸º12
        config.decoder_num_hidden_layers = 12
        # è®¾ç½®è§£ç å™¨æ³¨æ„åŠ›å¤´æ•°ä¸º8
        config.decoder_num_attention_heads = 8
        # è®¾ç½®è§£ç å™¨éšè—å±‚å¤§å°ä¸º640
        config.decoder_hidden_size = 640
        # è®¾ç½®è§£ç å™¨ä¸­é—´å±‚å¤§å°ä¸º2560
        config.decoder_intermediate_size = 2560
    # å¦‚æœæ¨¡å‹åç§°ä¸åŒ…å«"base"å…³é”®è¯ï¼Œåˆ™è§¦å‘å€¼é”™è¯¯å¼‚å¸¸ï¼Œè¦æ±‚æ¨¡å‹åç§°åŒ…å«"small", "base", "large", æˆ– "huge"
    elif "base" not in model_name:
        raise ValueError('Model name should include either "small", "base", "large", or "huge"')
# é‡å‘½åç»™å®šçš„å‚æ•°åï¼Œæ ¹æ®ä¸åŒçš„è§„åˆ™è¿›è¡Œæ›¿æ¢
def rename_key(name):
    # å¦‚æœå‚æ•°åä¸­åŒ…å«"encoder."
    if "encoder." in name:
        # å°†"encoder."æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        name = name.replace("encoder.", "")
    # å¦‚æœå‚æ•°åä¸­åŒ…å«"cls_token"
    if "cls_token" in name:
        # å°†"cls_token"æ›¿æ¢ä¸º"videomae.embeddings.cls_token"
        name = name.replace("cls_token", "videomae.embeddings.cls_token")
    # ...
    # å…¶ä»–æ¡ä»¶ä¸‹çš„æ›¿æ¢è§„åˆ™åŒä¸Š
    # ...
    # æœ€åå°†ä¿®æ”¹åçš„å‚æ•°åè¿”å›
    return name


# æ ¹æ®ç»™å®šçš„åŸå§‹çŠ¶æ€å’Œé…ç½®è½¬æ¢çŠ¶æ€å­—å…¸
def convert_state_dict(orig_state_dict, config):
    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®é›†åˆçš„æ‹·è´ï¼Œä»¥ä¾¿åœ¨è¿­ä»£æ—¶å¯ä»¥å®‰å…¨åœ°ä¿®æ”¹åŸå§‹å­—å…¸
    for key in orig_state_dict.copy().keys():
        # å¼¹å‡ºå½“å‰é”®å¯¹åº”çš„å€¼ï¼Œå¹¶èµ‹å€¼ç»™å˜é‡ val
        val = orig_state_dict.pop(key)
    
        # æ£€æŸ¥å½“å‰é”®æ˜¯å¦ä»¥ "encoder." å¼€å¤´ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å»é™¤è¯¥å‰ç¼€
        if key.startswith("encoder."):
            key = key.replace("encoder.", "")
    
        # æ£€æŸ¥å½“å‰é”®æ˜¯å¦åŒ…å« "qkv"ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿›ä¸€æ­¥å¤„ç†
        if "qkv" in key:
            # ä½¿ç”¨ "." åˆ†å‰²é”®åï¼Œä»¥ä¾¿æå–å±‚å·ç­‰ä¿¡æ¯
            key_split = key.split(".")
    
            # æ£€æŸ¥é”®åæ˜¯å¦ä»¥ "decoder.blocks" å¼€å¤´ï¼Œæ ¹æ®ä¸åŒæƒ…å†µè®¾ç½®ä¸åŒçš„ç»´åº¦å’Œå‰ç¼€
            if key.startswith("decoder.blocks"):
                dim = config.decoder_hidden_size
                layer_num = int(key_split[2])
                prefix = "decoder.decoder_layers."
    
                # å¦‚æœé”®åä¸­åŒ…å« "weight"ï¼Œåˆ™æŒ‰ç…§ç‰¹å®šè§„åˆ™é‡å‘½åï¼Œå¹¶è®¾ç½®å¯¹åº”çš„å€¼
                if "weight" in key:
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.query.weight"] = val[:dim, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.key.weight"] = val[dim : dim * 2, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.value.weight"] = val[-dim:, :]
            else:
                dim = config.hidden_size
                layer_num = int(key_split[1])
                prefix = "videomae.encoder.layer."
    
                # å¦‚æœé”®åä¸­åŒ…å« "weight"ï¼Œåˆ™æŒ‰ç…§ç‰¹å®šè§„åˆ™é‡å‘½åï¼Œå¹¶è®¾ç½®å¯¹åº”çš„å€¼
                if "weight" in key:
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.query.weight"] = val[:dim, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.key.weight"] = val[dim : dim * 2, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.value.weight"] = val[-dim:, :]
        else:
            # å¦‚æœä¸ç¬¦åˆä¸Šè¿°æ¡ä»¶ï¼Œåˆ™è°ƒç”¨å‡½æ•° rename_key() é‡å‘½åé”®ï¼Œå¹¶è®¾ç½®å¯¹åº”çš„å€¼
            orig_state_dict[rename_key(key)] = val
    
    # è¿”å›å¤„ç†åçš„åŸå§‹çŠ¶æ€å­—å…¸
    return orig_state_dict
# ä¸‹é¢çš„ä»£ç å‡†å¤‡åœ¨åƒæ„å¤§åˆ©é¢è§†é¢‘ä¸ŠéªŒè¯æˆ‘ä»¬çš„ç»“æœ
# ä½¿ç”¨çš„å¸§ç´¢å¼•ï¼š[164 168 172 176 181 185 189 193 198 202 206 210 215 219 223 227]
def prepare_video():
    # ä»æ•°æ®é›†ä¸­ä¸‹è½½è§†é¢‘
    file = hf_hub_download(
        repo_id="hf-internal-testing/spaghetti-video", filename="eating_spaghetti.npy", repo_type="dataset"
    )
    # åŠ è½½è§†é¢‘æ•°æ®
    video = np.load(file)
    return list(video)


def convert_videomae_checkpoint(checkpoint_url, pytorch_dump_folder_path, model_name, push_to_hub):
    # æ ¹æ®æ¨¡å‹åè·å–Videomaeçš„é…ç½®
    config = get_videomae_config(model_name)

    if "finetuned" in model_name:
        # å¦‚æœæ¨¡å‹åä¸­åŒ…å«finetunedï¼Œåˆ™ä½¿ç”¨VideoMAEForVideoClassification
        model = VideoMAEForVideoClassification(config)
    else:
        # å¦åˆ™ä½¿ç”¨VideoMAEForPreTraining
        model = VideoMAEForPreTraining(config)

    # ä¸‹è½½æ‰˜ç®¡åœ¨Google Driveä¸Šçš„åŸå§‹æ£€æŸ¥ç‚¹
    output = "pytorch_model.bin"
    gdown.cached_download(checkpoint_url, output, quiet=False)
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡
    files = torch.load(output, map_location="cpu")
    if "model" in files:
        state_dict = files["model"]
    else:
        state_dict = files["module"]
    # è½¬æ¢æ¨¡å‹æƒé‡æˆé€‚é…å½“å‰é…ç½®çš„æ ¼å¼
    new_state_dict = convert_state_dict(state_dict, config)

    model.load_state_dict(new_state_dict)
    model.eval()

    # åœ¨åŸºæœ¬è¾“å…¥ä¸ŠéªŒè¯æ¨¡å‹
    image_processor = VideoMAEImageProcessor(image_mean=[0.5, 0.5, 0.5], image_std=[0.5, 0.5, 0.5])
    video = prepare_video()
    inputs = image_processor(video, return_tensors="pt")

    if "finetuned" not in model_name:
        # å¦‚æœæ¨¡å‹åä¸­ä¸åŒ…å«finetunedï¼Œä¸‹è½½bool-masked-posæ–‡ä»¶
        local_path = hf_hub_download(repo_id="hf-internal-testing/bool-masked-pos", filename="bool_masked_pos.pt")
        inputs["bool_masked_pos"] = torch.load(local_path)

    outputs = model(**inputs)
    logits = outputs.logits

    model_names = [
        "videomae-small-finetuned-kinetics",
        "videomae-small-finetuned-ssv2",
        # Kinetics-400 checkpoints (short = pretrained only for 800 epochs instead of 1600)
        "videomae-base-short",
        "videomae-base-short-finetuned-kinetics",
        "videomae-base",
        "videomae-base-finetuned-kinetics",
        "videomae-large",
        "videomae-large-finetuned-kinetics",
        "videomae-huge-finetuned-kinetics",
        # Something-Something-v2 checkpoints (short = pretrained only for 800 epochs instead of 2400)
        "videomae-base-short-ssv2",
        "videomae-base-short-finetuned-ssv2",
        "videomae-base-ssv2",
        "videomae-base-finetuned-ssv2",
    ]

    # æ³¨æ„ï¼šlogitsä¸image_meanå’Œimage_stdéƒ½ç­‰äº[0.5, 0.5, 0.5]å’Œ[0.5, 0.5, 0.5]æ—¶è¿›è¡Œäº†æµ‹è¯•
    if model_name == "videomae-small-finetuned-kinetics":
        expected_shape = torch.Size([1, 400])
        expected_slice = torch.tensor([-0.9291, -0.4061, -0.9307])
    elif model_name == "videomae-small-finetuned-ssv2":
        expected_shape = torch.Size([1, 174])
        expected_slice = torch.tensor([0.2671, -0.4689, -0.8235])
    elif model_name == "videomae-base":
        expected_shape = torch.Size([1, 1408, 1536])
        expected_slice = torch.tensor([[0.7739, 0.7968, 0.7089], [0.6701, 0.7487, 0.6209], [0.4287, 0.5158, 0.4773]])
    elif model_name == "videomae-base-short":
        # è®¾ç½®é¢„æœŸè¾“å‡ºå½¢çŠ¶ä¸º [1, 1408, 1536]
        expected_shape = torch.Size([1, 1408, 1536])
        # è®¾ç½®é¢„æœŸè¾“å‡ºåˆ‡ç‰‡ä¸ºæŒ‡å®šçš„å¼ é‡
        expected_slice = torch.tensor([[0.7994, 0.9612, 0.8508], [0.7401, 0.8958, 0.8302], [0.5862, 0.7468, 0.7325]])
        # å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬éªŒè¯äº†æ ‡å‡†åƒç´ æŸå¤±å’Œéæ ‡å‡†åŒ–ç›®æ ‡çš„æŸå¤±
        expected_loss = torch.tensor([0.5142]) if config.norm_pix_loss else torch.tensor([0.6469])
    elif model_name == "videomae-large":
        # è®¾ç½®é¢„æœŸè¾“å‡ºå½¢çŠ¶ä¸º [1, 1408, 1536]
        expected_shape = torch.Size([1, 1408, 1536])
        # è®¾ç½®é¢„æœŸè¾“å‡ºåˆ‡ç‰‡ä¸ºæŒ‡å®šçš„å¼ é‡
        expected_slice = torch.tensor([[0.7149, 0.7997, 0.6966], [0.6768, 0.7869, 0.6948], [0.5139, 0.6221, 0.5605]])
    # ... å…¶ä»–æ¨¡å‹çš„è®¾ç½®é¢„æœŸè¾“å‡ºå½¢çŠ¶å’Œåˆ‡ç‰‡
    else:
        # å¦‚æœæ¨¡å‹åç§°ä¸æ”¯æŒï¼Œåˆ™å¼•å‘å€¼é”™è¯¯
        raise ValueError(f"Model name not supported. Should be one of {model_names}")
    
    # éªŒè¯ logits
    assert logits.shape == expected_shape
    if "finetuned" in model_name:
        # å¦‚æœæ¨¡å‹æ˜¯å¾®è°ƒçš„ï¼Œåˆ™é€šè¿‡ allclose å‡½æ•°éªŒè¯æŒ‡å®šåˆ‡ç‰‡æ˜¯å¦æ¥è¿‘é¢„æœŸå€¼
        assert torch.allclose(logits[0, :3], expected_slice, atol=1e-4)
    else:
        # å¦‚æœæ¨¡å‹ä¸æ˜¯å¾®è°ƒçš„ï¼Œæ‰“å° logits çš„éƒ¨åˆ†å†…å®¹ï¼Œå¹¶é€šè¿‡ allclose å‡½æ•°éªŒè¯å…¶åˆ‡ç‰‡æ˜¯å¦æ¥è¿‘é¢„æœŸå€¼
        print("Logits:", logits[0, :3, :3])
        assert torch.allclose(logits[0, :3, :3], expected_slice, atol=1e-4)
    print("Logits ok!")

    # éªŒè¯æŸå¤±ï¼Œå¦‚æœé€‚ç”¨
    if model_name == "videomae-base-short":
        loss = outputs.loss
        # é€šè¿‡ allclose å‡½æ•°éªŒè¯æŸå¤±æ˜¯å¦æ¥è¿‘é¢„æœŸå€¼
        assert torch.allclose(loss, expected_loss, atol=1e-4)
        print("Loss ok!")

    if pytorch_dump_folder_path is not None:
        # å¦‚æœå­˜åœ¨ pytorch_dump_folder_pathï¼Œåˆ™ä¿å­˜æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨
        print(f"Saving model and image processor to {pytorch_dump_folder_path}")
        image_processor.save_pretrained(pytorch_dump_folder_path)
        model.save_pretrained(pytorch_dump_folder_path)
    # å¦‚æœéœ€è¦æ¨é€åˆ°hubï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if push_to_hub:
        # æ‰“å°æç¤ºä¿¡æ¯ï¼Œè¡¨ç¤ºæ­£åœ¨æ¨é€åˆ°hub
        print("Pushing to the hub...")
        # å°†æ¨¡å‹æ¨é€åˆ°hubï¼ŒæŒ‡å®šæ¨¡å‹åç§°å’Œç»„ç»‡
        model.push_to_hub(model_name, organization="nielsr")
# å¦‚æœå½“å‰è„šæœ¬è¢«ä½œä¸ºä¸»ç¨‹åºè¿è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    
    # æ·»åŠ å¿…é€‰å‚æ•°
    parser.add_argument(
        "--checkpoint_url",
        default="https://drive.google.com/u/1/uc?id=1tEhLyskjb755TJ65ptsrafUG2llSwQE1&amp;export=download&amp;confirm=t&amp;uuid=aa3276eb-fb7e-482a-adec-dc7171df14c4",
        type=str,
        help=(
            "URL of the original PyTorch checkpoint (on Google Drive) you'd like to convert. Should be a direct"
            " download link."
        ),
    )
    parser.add_argument(
        "--pytorch_dump_folder_path",
        default="/Users/nielsrogge/Documents/VideoMAE/Test",
        type=str,
        help="Path to the output PyTorch model directory.",
    )
    parser.add_argument("--model_name", default="videomae-base", type=str, help="Name of the model.")
    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )

    # è§£æä¼ å…¥çš„å‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°æ¥è½¬æ¢ VideoMAE æ£€æŸ¥ç‚¹
    convert_videomae_checkpoint(args.checkpoint_url, args.pytorch_dump_folder_path, args.model_name, args.push_to_hub)
```