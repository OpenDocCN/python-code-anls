# `.\models\videomae\convert_videomae_to_pytorch.py`

```py
# coding=utf-8
# å£°æ˜æ–‡ä»¶ç¼–ç æ ¼å¼ä¸º UTF-8

# Copyright 2022 The HuggingFace Inc. team.
# ç‰ˆæƒå£°æ˜

# Licensed under the Apache License, Version 2.0 (the "License");
# ä¾æ® Apache License, Version 2.0 æˆæƒè®¸å¯

# you may not use this file except in compliance with the License.
# é™¤éç¬¦åˆ Apache License, Version 2.0 çš„æˆæƒè®¸å¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æœ¬æ–‡ä»¶

# You may obtain a copy of the License at
# å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# åœ¨é€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„çš„æƒ…å†µä¸‹ï¼Œä¾æ®â€œåŸæ ·â€æä¾›ï¼Œè½¯ä»¶åˆ†å‘

# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# æ— è®ºæ˜¯æ˜ç¤ºè¿˜æ˜¯æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶

# See the License for the specific language governing permissions and
# è¯¦ç»†äº†è§£è®¸å¯è¯å¯å‚é˜…ç‰¹å®šçš„è¯­è¨€å’Œæƒé™
# limitations under the License.
# åœ¨è®¸å¯è¯ä¸‹çš„é™åˆ¶

"""Convert VideoMAE checkpoints from the original repository: https://github.com/MCG-NJU/VideoMAE"""
# æ–‡æ¡£å­—ç¬¦ä¸²ï¼ŒæŒ‡æ˜ä»£ç ç”¨é€”æ˜¯å°† VideoMAE æ£€æŸ¥ç‚¹ä»åŸå§‹ä»“åº“è½¬æ¢è¿‡æ¥

import argparse  # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import json  # å¯¼å…¥ JSON æ•°æ®å¤„ç†æ¨¡å—

import gdown  # å¯¼å…¥ gdown ç”¨äºä¸‹è½½å·¥å…·
import numpy as np  # å¯¼å…¥ NumPy æ¨¡å—
import torch  # å¯¼å…¥ PyTorch æ¨¡å—
from huggingface_hub import hf_hub_download  # ä» huggingface_hub å¯¼å…¥æ¨¡å‹ä¸‹è½½å‡½æ•°

from transformers import (  # å¯¼å…¥ transformers æ¨¡å—ä¸­çš„å¤šä¸ªç±»
    VideoMAEConfig,  # VideoMAE æ¨¡å‹é…ç½®ç±»
    VideoMAEForPreTraining,  # ç”¨äºé¢„è®­ç»ƒçš„ VideoMAE æ¨¡å‹ç±»
    VideoMAEForVideoClassification,  # ç”¨äºè§†é¢‘åˆ†ç±»çš„ VideoMAE æ¨¡å‹ç±»
    VideoMAEImageProcessor,  # VideoMAE å›¾åƒå¤„ç†å™¨ç±»
)


def get_videomae_config(model_name):
    # è·å– VideoMAE æ¨¡å‹é…ç½®çš„å‡½æ•°å®šä¹‰ï¼Œå‚æ•°ä¸ºæ¨¡å‹åç§°
    config = VideoMAEConfig()  # åˆ›å»º VideoMAEConfig å®ä¾‹

    set_architecture_configs(model_name, config)  # è°ƒç”¨è®¾ç½®æ¶æ„é…ç½®çš„å‡½æ•°

    if "finetuned" not in model_name:
        # å¦‚æœæ¨¡å‹åç§°ä¸­ä¸åŒ…å« "finetuned"
        config.use_mean_pooling = False  # ç¦ç”¨å¹³å‡æ± åŒ–

    if "finetuned" in model_name:
        # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å« "finetuned"
        repo_id = "huggingface/label-files"  # è®¾ç½®ä»“åº“ ID
        if "kinetics" in model_name:
            # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å« "kinetics"
            config.num_labels = 400  # è®¾ç½®æ ‡ç­¾æ•°é‡ä¸º 400
            filename = "kinetics400-id2label.json"  # è®¾ç½®æ–‡ä»¶å
        elif "ssv2" in model_name:
            # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å« "ssv2"
            config.num_labels = 174  # è®¾ç½®æ ‡ç­¾æ•°é‡ä¸º 174
            filename = "something-something-v2-id2label.json"  # è®¾ç½®æ–‡ä»¶å
        else:
            # å¦‚æœæ¨¡å‹åç§°æ—¢ä¸åŒ…å« "kinetics" ä¹Ÿä¸åŒ…å« "ssv2"
            raise ValueError("Model name should either contain 'kinetics' or 'ssv2' in case it's fine-tuned.")
            # æŠ›å‡ºæ•°å€¼é”™è¯¯ï¼Œè¦æ±‚æ¨¡å‹åç§°ä¸­åº”åŒ…å« 'kinetics' æˆ– 'ssv2'ï¼Œä»¥è¡¨æ˜å…¶æ˜¯å¦è¿›è¡Œäº†å¾®è°ƒ
        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type="dataset"), "r"))
        # ä½¿ç”¨ huggingface_hub ä¸‹è½½å¹¶åŠ è½½æ ‡ç­¾æ–‡ä»¶å†…å®¹åˆ° id2label å­—å…¸ä¸­
        id2label = {int(k): v for k, v in id2label.items()}  # å°†é”®è½¬æ¢ä¸ºæ•´æ•°ç±»å‹
        config.id2label = id2label  # è®¾ç½®é…ç½®å¯¹è±¡çš„ id2label å±æ€§
        config.label2id = {v: k for k, v in id2label.items()}  # è®¾ç½®é…ç½®å¯¹è±¡çš„ label2id å±æ€§

    return config  # è¿”å›é…ç½®å¯¹è±¡


def set_architecture_configs(model_name, config):
    # è®¾ç½®æ¶æ„é…ç½®çš„å‡½æ•°å®šä¹‰ï¼Œå‚æ•°ä¸ºæ¨¡å‹åç§°å’Œé…ç½®å¯¹è±¡
    if "small" in model_name:
        # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å« "small"
        config.hidden_size = 384  # è®¾ç½®éšè—å±‚å¤§å°ä¸º 384
        config.intermediate_size = 1536  # è®¾ç½®ä¸­é—´å±‚å¤§å°ä¸º 1536
        config.num_hidden_layers = 12  # è®¾ç½®éšè—å±‚å±‚æ•°ä¸º 12
        config.num_attention_heads = 16  # è®¾ç½®æ³¨æ„åŠ›å¤´æ•°ä¸º 16
        config.decoder_num_hidden_layers = 12  # è®¾ç½®è§£ç å™¨éšè—å±‚å±‚æ•°ä¸º 12
        config.decoder_num_attention_heads = 3  # è®¾ç½®è§£ç å™¨æ³¨æ„åŠ›å¤´æ•°ä¸º 3
        config.decoder_hidden_size = 192  # è®¾ç½®è§£ç å™¨éšè—å±‚å¤§å°ä¸º 192
        config.decoder_intermediate_size = 768  # è®¾ç½®è§£ç å™¨ä¸­é—´å±‚å¤§å°ä¸º 768
    elif "large" in model_name:
        # å¦‚æœæ¨¡å‹åç§°ä¸­åŒ…å« "large"
        config.hidden_size = 1024  # è®¾ç½®éšè—å±‚å¤§å°ä¸º 1024
        config.intermediate_size = 4096  # è®¾ç½®ä¸­é—´å±‚å¤§å°ä¸º 4096
        config.num_hidden_layers = 24  # è®¾ç½®éšè—å±‚å±‚æ•°ä¸º 24
        config.num_attention_heads = 16  # è®¾ç½®æ³¨æ„åŠ›å¤´æ•°ä¸º 16
        config.decoder_num_hidden_layers = 12  # è®¾ç½®è§£ç å™¨éšè—å±‚å±‚æ•°ä¸º 12
        config.decoder_num_attention_heads = 8  # è®¾ç½®è§£ç å™¨æ³¨æ„åŠ›å¤´æ•°ä¸º 8
        config.decoder_hidden_size = 512  # è®¾ç½®è§£ç å™¨éšè—å±‚å¤§å°ä¸º 512
        config.decoder_intermediate_size = 2048  # è®¾ç½®è§£ç å™¨ä¸­é—´å±‚å¤§å°ä¸º 2048
    # å¦‚æœæ¨¡å‹åä¸­åŒ…å« "huge"
    elif "huge" in model_name:
        # è®¾ç½®éšè—å±‚å¤§å°ä¸º 1280
        config.hidden_size = 1280
        # è®¾ç½®ä¸­é—´å±‚å¤§å°ä¸º 5120
        config.intermediate_size = 5120
        # è®¾ç½®éšè—å±‚çš„æ•°é‡ä¸º 32
        config.num_hidden_layers = 32
        # è®¾ç½®æ³¨æ„åŠ›å¤´çš„æ•°é‡ä¸º 16
        config.num_attention_heads = 16
        # è®¾ç½®è§£ç å™¨éšè—å±‚çš„æ•°é‡ä¸º 12
        config.decoder_num_hidden_layers = 12
        # è®¾ç½®è§£ç å™¨æ³¨æ„åŠ›å¤´çš„æ•°é‡ä¸º 8
        config.decoder_num_attention_heads = 8
        # è®¾ç½®è§£ç å™¨éšè—å±‚å¤§å°ä¸º 640
        config.decoder_hidden_size = 640
        # è®¾ç½®è§£ç å™¨ä¸­é—´å±‚å¤§å°ä¸º 2560
        config.decoder_intermediate_size = 2560
    # å¦‚æœæ¨¡å‹åä¸­ä¸åŒ…å« "base"
    elif "base" not in model_name:
        # æŠ›å‡ºæ•°å€¼é”™è¯¯ï¼Œæç¤ºæ¨¡å‹ååº”åŒ…å« "small", "base", "large", æˆ– "huge"
        raise ValueError('Model name should include either "small", "base", "large", or "huge"')
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºé‡å‘½åç»™å®šçš„é”®å
def rename_key(name):
    # å¦‚æœé”®åä¸­åŒ…å« "encoder."ï¼Œåˆ™æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
    if "encoder." in name:
        name = name.replace("encoder.", "")
    # å¦‚æœé”®åä¸­åŒ…å« "cls_token"ï¼Œåˆ™æ›¿æ¢ä¸º "videomae.embeddings.cls_token"
    if "cls_token" in name:
        name = name.replace("cls_token", "videomae.embeddings.cls_token")
    # å¦‚æœé”®åä¸­åŒ…å« "decoder_pos_embed"ï¼Œåˆ™æ›¿æ¢ä¸º "decoder.decoder_pos_embed"
    if "decoder_pos_embed" in name:
        name = name.replace("decoder_pos_embed", "decoder.decoder_pos_embed")
    # å¦‚æœé”®åä¸­åŒ…å« "pos_embed" ä¸”ä¸åŒ…å« "decoder"ï¼Œåˆ™æ›¿æ¢ä¸º "videomae.embeddings.position_embeddings"
    if "pos_embed" in name and "decoder" not in name:
        name = name.replace("pos_embed", "videomae.embeddings.position_embeddings")
    # å¦‚æœé”®åä¸­åŒ…å« "patch_embed.proj"ï¼Œåˆ™æ›¿æ¢ä¸º "videomae.embeddings.patch_embeddings.projection"
    if "patch_embed.proj" in name:
        name = name.replace("patch_embed.proj", "videomae.embeddings.patch_embeddings.projection")
    # å¦‚æœé”®åä¸­åŒ…å« "patch_embed.norm"ï¼Œåˆ™æ›¿æ¢ä¸º "videomae.embeddings.norm"
    if "patch_embed.norm" in name:
        name = name.replace("patch_embed.norm", "videomae.embeddings.norm")
    # å¦‚æœé”®åä¸­åŒ…å« "decoder.blocks"ï¼Œåˆ™æ›¿æ¢ä¸º "decoder.decoder_layers"
    if "decoder.blocks" in name:
        name = name.replace("decoder.blocks", "decoder.decoder_layers")
    # å¦‚æœé”®åä¸­åŒ…å« "blocks"ï¼Œåˆ™æ›¿æ¢ä¸º "videomae.encoder.layer"
    if "blocks" in name:
        name = name.replace("blocks", "videomae.encoder.layer")
    # å¦‚æœé”®åä¸­åŒ…å« "attn.proj"ï¼Œåˆ™æ›¿æ¢ä¸º "attention.output.dense"
    if "attn.proj" in name:
        name = name.replace("attn.proj", "attention.output.dense")
    # å¦‚æœé”®åä¸­åŒ…å« "attn" ä¸”ä¸åŒ…å« "bias"ï¼Œåˆ™æ›¿æ¢ä¸º "attention.self"
    if "attn" in name and "bias" not in name:
        name = name.replace("attn", "attention.self")
    # å¦‚æœé”®åä¸­åŒ…å« "attn"ï¼Œåˆ™æ›¿æ¢ä¸º "attention.attention"
    if "attn" in name:
        name = name.replace("attn", "attention.attention")
    # å¦‚æœé”®åä¸­åŒ…å« "norm1"ï¼Œåˆ™æ›¿æ¢ä¸º "layernorm_before"
    if "norm1" in name:
        name = name.replace("norm1", "layernorm_before")
    # å¦‚æœé”®åä¸­åŒ…å« "norm2"ï¼Œåˆ™æ›¿æ¢ä¸º "layernorm_after"
    if "norm2" in name:
        name = name.replace("norm2", "layernorm_after")
    # å¦‚æœé”®åä¸­åŒ…å« "mlp.fc1"ï¼Œåˆ™æ›¿æ¢ä¸º "intermediate.dense"
    if "mlp.fc1" in name:
        name = name.replace("mlp.fc1", "intermediate.dense")
    # å¦‚æœé”®åä¸­åŒ…å« "mlp.fc2"ï¼Œåˆ™æ›¿æ¢ä¸º "output.dense"
    if "mlp.fc2" in name:
        name = name.replace("mlp.fc2", "output.dense")
    # å¦‚æœé”®åä¸­åŒ…å« "decoder_embed"ï¼Œåˆ™æ›¿æ¢ä¸º "decoder.decoder_embed"
    if "decoder_embed" in name:
        name = name.replace("decoder_embed", "decoder.decoder_embed")
    # å¦‚æœé”®åä¸­åŒ…å« "decoder_norm"ï¼Œåˆ™æ›¿æ¢ä¸º "decoder.decoder_norm"
    if "decoder_norm" in name:
        name = name.replace("decoder_norm", "decoder.decoder_norm")
    # å¦‚æœé”®åä¸­åŒ…å« "decoder_pred"ï¼Œåˆ™æ›¿æ¢ä¸º "decoder.decoder_pred"
    if "decoder_pred" in name:
        name = name.replace("decoder_pred", "decoder.decoder_pred")
    # å¦‚æœé”®åä¸­åŒ…å« "norm.weight" ä¸”ä¸åŒ…å« "decoder" å’Œ "fc"ï¼Œåˆ™æ›¿æ¢ä¸º "videomae.layernorm.weight"
    if "norm.weight" in name and "decoder" not in name and "fc" not in name:
        name = name.replace("norm.weight", "videomae.layernorm.weight")
    # å¦‚æœé”®åä¸­åŒ…å« "norm.bias" ä¸”ä¸åŒ…å« "decoder" å’Œ "fc"ï¼Œåˆ™æ›¿æ¢ä¸º "videomae.layernorm.bias"
    if "norm.bias" in name and "decoder" not in name and "fc" not in name:
        name = name.replace("norm.bias", "videomae.layernorm.bias")
    # å¦‚æœé”®åä¸­åŒ…å« "head" ä¸”ä¸åŒ…å« "decoder"ï¼Œåˆ™æ›¿æ¢ä¸º "classifier"
    if "head" in name and "decoder" not in name:
        name = name.replace("head", "classifier")

    # è¿”å›å¤„ç†åçš„é”®å
    return name
    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®çš„å‰¯æœ¬
    for key in orig_state_dict.copy().keys():
        # å¼¹å‡ºå½“å‰é”®å¯¹åº”çš„å€¼
        val = orig_state_dict.pop(key)

        # å¦‚æœé”®ä»¥"encoder."å¼€å¤´ï¼Œåˆ™ç§»é™¤è¯¥å‰ç¼€
        if key.startswith("encoder."):
            key = key.replace("encoder.", "")

        # å¦‚æœé”®ä¸­åŒ…å«"qkv"
        if "qkv" in key:
            # æ ¹æ®"."åˆ†å‰²é”®
            key_split = key.split(".")
            # å¦‚æœé”®ä»¥"decoder.blocks"å¼€å¤´
            if key.startswith("decoder.blocks"):
                # è®¾ç½®ç»´åº¦å’Œå±‚å·
                dim = config.decoder_hidden_size
                layer_num = int(key_split[2])
                prefix = "decoder.decoder_layers."
                # å¦‚æœé”®åŒ…å«"weight"
                if "weight" in key:
                    # æ›´æ–°åŸå§‹çŠ¶æ€å­—å…¸ï¼Œæ›¿æ¢æˆç‰¹å®šæ ¼å¼çš„é”®å’Œå¯¹åº”çš„å€¼
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.query.weight"] = val[:dim, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.key.weight"] = val[dim : dim * 2, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.value.weight"] = val[-dim:, :]
            else:
                # è®¾ç½®ç»´åº¦å’Œå±‚å·
                dim = config.hidden_size
                layer_num = int(key_split[1])
                prefix = "videomae.encoder.layer."
                # å¦‚æœé”®åŒ…å«"weight"
                if "weight" in key:
                    # æ›´æ–°åŸå§‹çŠ¶æ€å­—å…¸ï¼Œæ›¿æ¢æˆç‰¹å®šæ ¼å¼çš„é”®å’Œå¯¹åº”çš„å€¼
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.query.weight"] = val[:dim, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.key.weight"] = val[dim : dim * 2, :]
                    orig_state_dict[f"{prefix}{layer_num}.attention.attention.value.weight"] = val[-dim:, :]
        else:
            # å¯¹é”®è¿›è¡Œé‡å‘½åå¤„ç†å¹¶æ›´æ–°åŸå§‹çŠ¶æ€å­—å…¸
            orig_state_dict[rename_key(key)] = val

    # è¿”å›æ›´æ–°åçš„åŸå§‹çŠ¶æ€å­—å…¸
    return orig_state_dict
# æˆ‘ä»¬å°†åœ¨åƒæ„å¤§åˆ©é¢è§†é¢‘ä¸ŠéªŒè¯æˆ‘ä»¬çš„ç»“æœ
# ä½¿ç”¨çš„å¸§ç´¢å¼•ï¼š[164 168 172 176 181 185 189 193 198 202 206 210 215 219 223 227]
def prepare_video():
    # ä»æŒ‡å®šçš„æ•°æ®é›†ä»“åº“ä¸‹è½½åä¸º 'eating_spaghetti.npy' çš„æ–‡ä»¶
    file = hf_hub_download(
        repo_id="hf-internal-testing/spaghetti-video", filename="eating_spaghetti.npy", repo_type="dataset"
    )
    # åŠ è½½.npyæ–‡ä»¶ä¸­çš„è§†é¢‘æ•°æ®
    video = np.load(file)
    return list(video)


def convert_videomae_checkpoint(checkpoint_url, pytorch_dump_folder_path, model_name, push_to_hub):
    # è·å–VideoMAEæ¨¡å‹é…ç½®
    config = get_videomae_config(model_name)

    if "finetuned" in model_name:
        # å¦‚æœæ¨¡å‹åä¸­åŒ…å«'finetuned'ï¼Œåˆ™ä½¿ç”¨VideoMAEForVideoClassificationè¿›è¡Œåˆå§‹åŒ–
        model = VideoMAEForVideoClassification(config)
    else:
        # å¦åˆ™ä½¿ç”¨VideoMAEForPreTrainingè¿›è¡Œåˆå§‹åŒ–
        model = VideoMAEForPreTraining(config)

    # ä¸‹è½½æ‰˜ç®¡åœ¨Google Driveä¸Šçš„åŸå§‹æ£€æŸ¥ç‚¹
    output = "pytorch_model.bin"
    gdown.cached_download(checkpoint_url, output, quiet=False)
    # åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶å¹¶æ˜ å°„åˆ°CPU
    files = torch.load(output, map_location="cpu")
    if "model" in files:
        state_dict = files["model"]
    else:
        state_dict = files["module"]
    # è½¬æ¢æ£€æŸ¥ç‚¹çš„çŠ¶æ€å­—å…¸
    new_state_dict = convert_state_dict(state_dict, config)

    # åŠ è½½æ–°çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­
    model.load_state_dict(new_state_dict)
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # ä½¿ç”¨å›¾åƒå¤„ç†å™¨VideoMAEImageProcessorè¿›è¡Œè§†é¢‘å¸§çš„é¢„å¤„ç†
    image_processor = VideoMAEImageProcessor(image_mean=[0.5, 0.5, 0.5], image_std=[0.5, 0.5, 0.5])
    # å‡†å¤‡è§†é¢‘æ•°æ®ï¼Œè½¬æ¢ä¸ºPyTorchå¼ é‡åˆ—è¡¨
    video = prepare_video()
    inputs = image_processor(video, return_tensors="pt")

    # å¦‚æœæ¨¡å‹åä¸­ä¸åŒ…å«'finetuned'
    if "finetuned" not in model_name:
        # ä»æŒ‡å®šçš„æ•°æ®é›†ä»“åº“ä¸‹è½½åä¸º 'bool_masked_pos.pt' çš„æœ¬åœ°æ–‡ä»¶
        local_path = hf_hub_download(repo_id="hf-internal-testing/bool-masked-pos", filename="bool_masked_pos.pt")
        # åŠ è½½æœ¬åœ°æ–‡ä»¶åˆ°inputså­—å…¸ä¸­çš„ 'bool_masked_pos' é”®
        inputs["bool_masked_pos"] = torch.load(local_path)

    # ä½¿ç”¨æ¨¡å‹å¤„ç†inputsï¼Œå¾—åˆ°è¾“å‡ºç»“æœ
    outputs = model(**inputs)
    logits = outputs.logits

    # å®šä¹‰ä¸åŒæ¨¡å‹åç§°å¯¹åº”çš„é¢„æœŸè¾“å‡ºå½¢çŠ¶å’Œåˆ‡ç‰‡
    model_names = [
        "videomae-small-finetuned-kinetics",
        "videomae-small-finetuned-ssv2",
        # Kinetics-400æ£€æŸ¥ç‚¹ï¼ˆshort = ä»…é¢„è®­ç»ƒ800ä¸ªå‘¨æœŸï¼Œè€Œä¸æ˜¯1600ä¸ªå‘¨æœŸï¼‰
        "videomae-base-short",
        "videomae-base-short-finetuned-kinetics",
        "videomae-base",
        "videomae-base-finetuned-kinetics",
        "videomae-large",
        "videomae-large-finetuned-kinetics",
        "videomae-huge-finetuned-kinetics",
        # Something-Something-v2æ£€æŸ¥ç‚¹ï¼ˆshort = ä»…é¢„è®­ç»ƒ800ä¸ªå‘¨æœŸï¼Œè€Œä¸æ˜¯2400ä¸ªå‘¨æœŸï¼‰
        "videomae-base-short-ssv2",
        "videomae-base-short-finetuned-ssv2",
        "videomae-base-ssv2",
        "videomae-base-finetuned-ssv2",
    ]

    # æ³¨æ„ï¼šlogitsä½¿ç”¨çš„å›¾åƒå‡å€¼å’Œæ ‡å‡†å·®åˆ†åˆ«ä¸º[0.5, 0.5, 0.5]å’Œ[0.5, 0.5, 0.5]è¿›è¡Œäº†æµ‹è¯•
    if model_name == "videomae-small-finetuned-kinetics":
        expected_shape = torch.Size([1, 400])
        expected_slice = torch.tensor([-0.9291, -0.4061, -0.9307])
    elif model_name == "videomae-small-finetuned-ssv2":
        expected_shape = torch.Size([1, 174])
        expected_slice = torch.tensor([0.2671, -0.4689, -0.8235])
    elif model_name == "videomae-base":
        expected_shape = torch.Size([1, 1408, 1536])
        expected_slice = torch.tensor([[0.7739, 0.7968, 0.7089], [0.6701, 0.7487, 0.6209], [0.4287, 0.5158, 0.4773]])
    elif model_name == "videomae-base-short":
        expected_shape = torch.Size([1, 1408, 1536])
        expected_slice = torch.tensor([[0.7994, 0.9612, 0.8508], [0.7401, 0.8958, 0.8302], [0.5862, 0.7468, 0.7325]])
        # å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬éªŒè¯äº†å½’ä¸€åŒ–å’Œéå½’ä¸€åŒ–ç›®æ ‡çš„æŸå¤±
        expected_loss = torch.tensor([0.5142]) if config.norm_pix_loss else torch.tensor([0.6469])
    elif model_name == "videomae-large":
        expected_shape = torch.Size([1, 1408, 1536])
        expected_slice = torch.tensor([[0.7149, 0.7997, 0.6966], [0.6768, 0.7869, 0.6948], [0.5139, 0.6221, 0.5605]])
    elif model_name == "videomae-large-finetuned-kinetics":
        expected_shape = torch.Size([1, 400])
        expected_slice = torch.tensor([0.0771, 0.0011, -0.3625])
    elif model_name == "videomae-huge-finetuned-kinetics":
        expected_shape = torch.Size([1, 400])
        expected_slice = torch.tensor([0.2433, 0.1632, -0.4894])
    elif model_name == "videomae-base-short-finetuned-kinetics":
        expected_shape = torch.Size([1, 400])
        expected_slice = torch.tensor([0.6588, 0.0990, -0.2493])
    elif model_name == "videomae-base-finetuned-kinetics":
        expected_shape = torch.Size([1, 400])
        expected_slice = torch.tensor([0.3669, -0.0688, -0.2421])
    elif model_name == "videomae-base-short-ssv2":
        expected_shape = torch.Size([1, 1408, 1536])
        expected_slice = torch.tensor([[0.4712, 0.5296, 0.5786], [0.2278, 0.2729, 0.4026], [0.0352, 0.0730, 0.2506]])
    elif model_name == "videomae-base-short-finetuned-ssv2":
        expected_shape = torch.Size([1, 174])
        expected_slice = torch.tensor([-0.0537, -0.1539, -0.3266])
    elif model_name == "videomae-base-ssv2":
        expected_shape = torch.Size([1, 1408, 1536])
        expected_slice = torch.tensor([[0.8131, 0.8727, 0.8546], [0.7366, 0.9377, 0.8870], [0.5935, 0.8874, 0.8564]])
    elif model_name == "videomae-base-finetuned-ssv2":
        expected_shape = torch.Size([1, 174])
        expected_slice = torch.tensor([0.1961, -0.8337, -0.6389])
    else:
        raise ValueError(f"Model name not supported. Should be one of {model_names}")

    # éªŒè¯è¾“å‡ºçš„å½¢çŠ¶æ˜¯å¦ç¬¦åˆé¢„æœŸ
    assert logits.shape == expected_shape
    # å¦‚æœæ¨¡å‹åç§°åŒ…å«â€œfinetunedâ€ï¼Œåˆ™éªŒè¯å‰ä¸‰ä¸ªè¾“å‡ºå€¼æ˜¯å¦æ¥è¿‘é¢„æœŸåˆ‡ç‰‡å€¼
    if "finetuned" in model_name:
        assert torch.allclose(logits[0, :3], expected_slice, atol=1e-4)
    else:
        print("Logits:", logits[0, :3, :3])
        assert torch.allclose(logits[0, :3, :3], expected_slice, atol=1e-4)
    print("Logits ok!")

    # å¦‚æœé€‚ç”¨ï¼ŒéªŒè¯æŸå¤±å€¼
    if model_name == "videomae-base-short":
        loss = outputs.loss
        assert torch.allclose(loss, expected_loss, atol=1e-4)
        print("Loss ok!")

    # å¦‚æœæŒ‡å®šäº† PyTorch æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œåˆ™ä¿å­˜æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨
    if pytorch_dump_folder_path is not None:
        print(f"Saving model and image processor to {pytorch_dump_folder_path}")
        image_processor.save_pretrained(pytorch_dump_folder_path)
        model.save_pretrained(pytorch_dump_folder_path)
    # å¦‚æœ push_to_hub ä¸ºçœŸï¼Œåˆ™æ‰§è¡Œä¸‹é¢çš„ä»£ç å—
    if push_to_hub:
        # æ‰“å°ä¿¡æ¯ï¼šæ­£åœ¨æ¨é€åˆ°hub...
        print("Pushing to the hub...")
        # è°ƒç”¨ model å¯¹è±¡çš„ push_to_hub æ–¹æ³•ï¼Œå°†æ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ hub
        model.push_to_hub(model_name, organization="nielsr")
if __name__ == "__main__":
    # å¦‚æœè„šæœ¬ç›´æ¥è¿è¡Œè€Œéè¢«å¯¼å…¥ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç 
    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡

    # Required parameters
    parser.add_argument(
        "--checkpoint_url",
        default="https://drive.google.com/u/1/uc?id=1tEhLyskjb755TJ65ptsrafUG2llSwQE1&amp;export=download&amp;confirm=t&amp;uuid=aa3276eb-fb7e-482a-adec-dc7171df14c4",
        type=str,
        help=(
            "URL of the original PyTorch checkpoint (on Google Drive) you'd like to convert. Should be a direct"
            " download link."
        ),
    )
    # æ·»åŠ å¿…éœ€çš„å‚æ•°ï¼šåŸå§‹ PyTorch æ£€æŸ¥ç‚¹çš„ä¸‹è½½é“¾æ¥

    parser.add_argument(
        "--pytorch_dump_folder_path",
        default="/Users/nielsrogge/Documents/VideoMAE/Test",
        type=str,
        help="Path to the output PyTorch model directory.",
    )
    # æ·»åŠ å¿…éœ€çš„å‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„ç›®å½•è·¯å¾„

    parser.add_argument("--model_name", default="videomae-base", type=str, help="Name of the model.")
    # æ·»åŠ å‚æ•°ï¼šæ¨¡å‹çš„åç§°ï¼Œé»˜è®¤ä¸º "videomae-base"

    parser.add_argument(
        "--push_to_hub", action="store_true", help="Whether or not to push the converted model to the ğŸ¤— hub."
    )
    # æ·»åŠ å‚æ•°ï¼šæ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ° ğŸ¤— hub

    args = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ä¸€ä¸ªå‘½åç©ºé—´

    convert_videomae_checkpoint(args.checkpoint_url, args.pytorch_dump_folder_path, args.model_name, args.push_to_hub)
    # è°ƒç”¨å‡½æ•° convert_videomae_checkpointï¼Œä¼ é€’è§£æåçš„å‚æ•°è¿›è¡Œæ¨¡å‹æ£€æŸ¥ç‚¹è½¬æ¢
```