# `.\transformers\models\mask2former\convert_mask2former_original_pytorch_checkpoint_to_pytorch.py`

```
# å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œç±»
import json
import sys
from argparse import ArgumentParser
from dataclasses import dataclass
from pathlib import Path
from pprint import pformat
from typing import Any, Dict, Iterator, List, Set, Tuple

import requests
import torch
import torchvision.transforms as T
from detectron2.checkpoint import DetectionCheckpointer
from detectron2.config import get_cfg
from detectron2.projects.deeplab import add_deeplab_config
from huggingface_hub import hf_hub_download
from PIL import Image
from torch import Tensor, nn

from transformers import (
    Mask2FormerConfig,
    Mask2FormerForUniversalSegmentation,
    Mask2FormerImageProcessor,
    Mask2FormerModel,
    SwinConfig,
)
from transformers.models.mask2former.modeling_mask2former import (
    Mask2FormerForUniversalSegmentationOutput,
    Mask2FormerModelOutput,
)
from transformers.utils import logging

# å®šä¹‰ä¸€äº›ç±»å‹åˆ«å
StateDict = Dict[str, Tensor]

# è®¾ç½®æ—¥å¿—è¾“å‡ºçº§åˆ«ä¸ºinfo
logging.set_verbosity_info()
logger = logging.get_logger()

# è®¾ç½®éšæœºç§å­
torch.manual_seed(0)

# å®šä¹‰ä¸€ä¸ªTrackedStateDictç±»ï¼Œç”¨äºè·Ÿè¸ªå­—å…¸çš„è®¿é—®æƒ…å†µ
class TrackedStateDict:
    def __init__(self, to_track: Dict):
        """This class "tracks" a python dictionary by keeping track of which item is accessed.

        Args:
            to_track (Dict): The dictionary we wish to track
        """
        self.to_track = to_track
        self._seen: Set[str] = set()

    def __getitem__(self, key: str) -> Any:
        return self.to_track[key]

    def __setitem__(self, key: str, item: Any):
        self._seen.add(key)
        self.to_track[key] = item

    def diff(self) -> List[str]:
        """This method returns a set difference between the keys in the tracked state dict and the one we have access so far.
        This is an effective method to check if we have update all the keys

        Returns:
            List[str]: List of keys not yet updated
        """
        return set(self.to_track.keys()) - self._seen

    def copy(self) -> Dict:
        # proxy the call to the internal dictionary
        return self.to_track.copy()

# å®šä¹‰ä¸€ä¸ªprepare_imgå‡½æ•°ï¼Œç”¨äºå‡†å¤‡ä¸€å¼ çŒ«å’ªå›¾åƒ
def prepare_img():
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    img_data = requests.get(url, stream=True).raw
    im = Image.open(img_data)
    return im

# å®šä¹‰ä¸€ä¸ªArgsç±»ï¼Œç”¨äºå­˜å‚¨å‘½ä»¤è¡Œå‚æ•°
@dataclass
class Args:
    """Fake command line arguments needed by mask2former/detectron implementation"""
    config_file: str
def setup_cfg(args: Args):
    # ä»æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°åŠ è½½é…ç½®
    cfg = get_cfg()
    add_deeplab_config(cfg)  # æ·»åŠ  DeepLab é…ç½®
    add_maskformer2_config(cfg)  # æ·»åŠ  Maskformer2 é…ç½®
    cfg.merge_from_file(args.config_file)  # ä»æ–‡ä»¶ä¸­åˆå¹¶é…ç½®
    cfg.freeze()  # å†»ç»“é…ç½®ï¼Œé˜²æ­¢ä¿®æ”¹
    return cfg


class OriginalMask2FormerConfigToOursConverter:
    # å°†åŸå§‹ Mask2Former é…ç½®è½¬æ¢ä¸ºæˆ‘ä»¬çš„é…ç½®
    class OriginalMask2FormerConfigToImageProcessorConverter:
        def __call__(self, original_config: object) -> Mask2FormerImageProcessor:
            # æå–æ¨¡å‹å’Œè¾“å…¥é…ç½®
            model = original_config.MODEL
            model_input = original_config.INPUT

            return Mask2FormerImageProcessor(
                # è®¾ç½®å›¾åƒå‡å€¼ä¸ºæ¨¡å‹åƒç´ å‡å€¼çš„æ ‡å‡†åŒ–åˆ—è¡¨
                image_mean=(torch.tensor(model.PIXEL_MEAN) / 255).tolist(),
                # è®¾ç½®å›¾åƒæ ‡å‡†å·®ä¸ºæ¨¡å‹åƒç´ æ ‡å‡†å·®çš„æ ‡å‡†åŒ–åˆ—è¡¨
                image_std=(torch.tensor(model.PIXEL_STD) / 255).tolist(),
                # è®¾ç½®æµ‹è¯•æ—¶çš„æœ€å°å°ºå¯¸
                size=model_input.MIN_SIZE_TEST,
                # è®¾ç½®æµ‹è¯•æ—¶çš„æœ€å¤§å°ºå¯¸
                max_size=model_input.MAX_SIZE_TEST,
                # è®¾ç½®ç±»åˆ«æ•°ç›®
                num_labels=model.SEM_SEG_HEAD.NUM_CLASSES,
                # è®¾ç½®å¿½ç•¥å€¼
                ignore_index=model.SEM_SEG_HEAD.IGNORE_VALUE,
                # è®¾ç½®å°ºå¯¸å¯åˆ†å‰²æ€§
                size_divisibility=32,
            )


class OriginalMask2FormerCheckpointToOursConverter:
    def __init__(self, original_model: nn.Module, config: Mask2FormerConfig):
        self.original_model = original_model
        self.config = config

    # å°†æ‰€æœ‰é”®ä»æºçŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºå¹¶æ’å…¥ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­ï¼ŒåŒæ—¶è¿›è¡Œé‡å‘½å
    def pop_all(self, renamed_keys: List[Tuple[str, str]], dst_state_dict: StateDict, src_state_dict: StateDict):
        for src_key, dst_key in renamed_keys:
            dst_state_dict[dst_key] = src_state_dict.pop(src_key)

    # æ›¿æ¢ Maskformer Swin éª¨å¹²éƒ¨åˆ†
    # æ›¿æ¢ Transformer è§£ç å™¨éƒ¨åˆ†
    def replace_masked_attention_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        dst_prefix: str = "transformer_module.decoder"
        src_prefix: str = "sem_seg_head.predictor"

        renamed_keys = self.rename_keys_in_masked_attention_decoder(dst_state_dict, src_state_dict)

        # æ·»åŠ æ›´å¤šé”®å€¼å¯¹
        renamed_keys.extend(
            [
                # æ›¿æ¢å½’ä¸€åŒ–å±‚æƒé‡
                (f"{src_prefix}.decoder_norm.weight", f"{dst_prefix}.layernorm.weight"),
                # æ›¿æ¢å½’ä¸€åŒ–å±‚åç½®
                (f"{src_prefix}.decoder_norm.bias", f"{dst_prefix}.layernorm.bias"),
            ]
        )

        mlp_len = 3
        for i in range(mlp_len):
            renamed_keys.extend(
                [
                    # æ›¿æ¢æ©ç åµŒå…¥å±‚æƒé‡
                    (
                        f"{src_prefix}.mask_embed.layers.{i}.weight",
                        f"{dst_prefix}.mask_predictor.mask_embedder.{i}.0.weight",
                    ),
                    # æ›¿æ¢æ©ç åµŒå…¥å±‚åç½®
                    (
                        f"{src_prefix}.mask_embed.layers.{i}.bias",
                        f"{dst_prefix}.mask_predictor.mask_embedder.{i}.0.bias",
                    ),
                ]
            )

        # å¼¹å‡ºæºçŠ¶æ€å­—å…¸ä¸­çš„é”®å¹¶æ’å…¥ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
    # ç”¨äºå°†æºçŠ¶æ€å­—å…¸ä¸­çš„è§£ç å™¨è‡ªæ³¨æ„åŠ›å±‚çš„æƒé‡å’Œåç½®æ›¿æ¢ä¸ºç›®æ ‡çŠ¶æ€å­—å…¸ä¸­çš„å¯¹åº”é¡¹
    def replace_keys_qkv_transformer_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­è§£ç å™¨è‡ªæ³¨æ„åŠ›å±‚çš„å‰ç¼€
        dst_prefix: str = "transformer_module.decoder.layers"
        # æºçŠ¶æ€å­—å…¸ä¸­è¯­ä¹‰åˆ†å‰²å¤´é¢„æµ‹å™¨çš„å‰ç¼€
        src_prefix: str = "sem_seg_head.predictor"
        # å¯¹è§£ç å™¨çš„æ¯ä¸€å±‚è¿›è¡Œè¿­ä»£
        for i in range(self.config.decoder_layers - 1):
            # ä»æºçŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºè§£ç å™¨è‡ªæ³¨æ„åŠ›å±‚è¾“å…¥æŠ•å½±å±‚çš„æƒé‡å’Œåç½®
            in_proj_weight = src_state_dict.pop(
                f"{src_prefix}.transformer_self_attention_layers.{i}.self_attn.in_proj_weight"
            )
            in_proj_bias = src_state_dict.pop(
                f"{src_prefix}.transformer_self_attention_layers.{i}.self_attn.in_proj_bias"
            )
            # æ¥ä¸‹æ¥ï¼ŒæŒ‰é¡ºåºæ·»åŠ æŸ¥è¯¢ã€é”®å’Œå€¼åˆ°çŠ¶æ€å­—å…¸
            # æŸ¥è¯¢æƒé‡
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.q_proj.weight"] = in_proj_weight[:256, :]
            # æŸ¥è¯¢åç½®
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.q_proj.bias"] = in_proj_bias[:256]
            # é”®æƒé‡
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.k_proj.weight"] = in_proj_weight[256:512, :]
            # é”®åç½®
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.k_proj.bias"] = in_proj_bias[256:512]
            # å€¼æƒé‡
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.v_proj.weight"] = in_proj_weight[-256:, :]
            # å€¼åç½®
            dst_state_dict[f"{dst_prefix}.{i}.self_attn.v_proj.bias"] = in_proj_bias[-256:]

    # ç”¨äºæ›¿æ¢çŠ¶æ€å­—å…¸ä¸­çš„è½¬æ¢å™¨æ¨¡å—
    def replace_transformer_module(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸ä¸­è½¬æ¢å™¨æ¨¡å—çš„å‰ç¼€
        dst_prefix: str = "transformer_module"
        # æºçŠ¶æ€å­—å…¸ä¸­è¯­ä¹‰åˆ†å‰²å¤´é¢„æµ‹å™¨çš„å‰ç¼€
        src_prefix: str = "sem_seg_head.predictor"

        # æ›¿æ¢æ©ç æ³¨æ„åŠ›è§£ç å™¨
        self.replace_masked_attention_decoder(dst_state_dict, src_state_dict)

        # é‡å‘½åçš„é”®å¯¹
        renamed_keys = [
            (f"{src_prefix}.query_embed.weight", f"{dst_prefix}.queries_embedder.weight"),
            (f"{src_prefix}.query_feat.weight", f"{dst_prefix}.queries_features.weight"),
            (f"{src_prefix}.level_embed.weight", f"{dst_prefix}.level_embed.weight"),
        ]

        # ä»çŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºæ‰€æœ‰çš„é”®å¯¹ï¼Œå¹¶æ›¿æ¢ç›¸å…³é”®
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
        # æ›¿æ¢è§£ç å™¨è‡ªæ³¨æ„åŠ›å±‚çš„é”®
        self.replace_keys_qkv_transformer_decoder(dst_state_dict, src_state_dict)

    # ç”¨äºæ›¿æ¢é€šç”¨åˆ†å‰²æ¨¡å—
    def replace_universal_segmentation_module(self, dst_state_dict: StateDict, src_state_dict: StateDict):
        # ç›®æ ‡çŠ¶æ€å­—å…¸çš„å‰ç¼€
        dst_prefix: str = ""
        # æºçŠ¶æ€å­—å…¸ä¸­è¯­ä¹‰åˆ†å‰²å¤´é¢„æµ‹å™¨çš„å‰ç¼€
        src_prefix: str = "sem_seg_head.predictor"

        # é‡å‘½åçš„é”®å¯¹
        renamed_keys = [
            (f"{src_prefix}.class_embed.weight", f"{dst_prefix}class_predictor.weight"),
            (f"{src_prefix}.class_embed.bias", f"{dst_prefix}class_predictor.bias"),
        ]

        # è®°å½•æ—¥å¿—ï¼Œæ˜¾ç¤ºå°†è¦æ›¿æ¢çš„é”®å¯¹
        logger.info(f"Replacing keys {pformat(renamed_keys)}")
        # ä»çŠ¶æ€å­—å…¸ä¸­å¼¹å‡ºæ‰€æœ‰çš„é”®å¯¹ï¼Œå¹¶æ›¿æ¢ç›¸å…³é”®
        self.pop_all(renamed_keys, dst_state_dict, src_state_dict)
    # å°†è¾“å…¥çš„ Mask2FormerModel è½¬æ¢ä¸º Mask2FormerModel ç±»å‹
    def convert(self, mask2former: Mask2FormerModel) -> Mask2FormerModel:
        # åˆ›å»ºç›®æ ‡çŠ¶æ€å­—å…¸å¹¶æ‹·è´è¾“å…¥æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        dst_state_dict = TrackedStateDict(mask2former.state_dict())
        src_state_dict = self.original_model.state_dict()
    
        # æ›¿æ¢åƒç´ æ¨¡å—
        self.replace_pixel_module(dst_state_dict, src_state_dict)
        # æ›¿æ¢å˜æ¢å™¨æ¨¡å—
        self.replace_transformer_module(dst_state_dict, src_state_dict)
    
        # æ‰“å°ç¼ºå¤±çš„é”®å€¼å¯¹
        logger.info(f"Missed keys are {pformat(dst_state_dict.diff())}")
        # æ‰“å°æœªæ‹·è´çš„é”®å€¼
        logger.info(f"Not copied keys are {pformat(src_state_dict.keys())}")
        # è¾“å‡ºå®Œæˆä¿¡æ¯
        logger.info("ğŸ™Œ Done")
    
        # æ ¹æ®éœ€è¦è¿½è¸ªçš„é”®å€¼å¯¹åˆ›å»ºçŠ¶æ€å­—å…¸
        state_dict = {key: dst_state_dict[key] for key in dst_state_dict.to_track.keys()}
        # åŠ è½½æ–°çš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹
        mask2former.load_state_dict(state_dict)
        return mask2former
    
    # å°†è¾“å…¥çš„ Mask2FormerForUniversalSegmentation è½¬æ¢ä¸º Mask2FormerForUniversalSegmentation ç±»å‹
    def convert_universal_segmentation(
        self, mask2former: Mask2FormerForUniversalSegmentation
    ) -> Mask2FormerForUniversalSegmentation:
        # åˆ›å»ºç›®æ ‡çŠ¶æ€å­—å…¸å¹¶æ‹·è´è¾“å…¥æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        dst_state_dict = TrackedStateDict(mask2former.state_dict())
        src_state_dict = self.original_model.state_dict()
    
        # æ›¿æ¢é€šç”¨åˆ†å‰²æ¨¡å—
        self.replace_universal_segmentation_module(dst_state_dict, src_state_dict)
    
        # æ ¹æ®éœ€è¦è¿½è¸ªçš„é”®å€¼å¯¹åˆ›å»ºçŠ¶æ€å­—å…¸
        state_dict = {key: dst_state_dict[key] for key in dst_state_dict.to_track.keys()}
        # åŠ è½½æ–°çš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹
        mask2former.load_state_dict(state_dict)
    
        return mask2former
    
    # é™æ€æ–¹æ³•ï¼Œæ ¹æ®æ£€æŸ¥ç‚¹å’Œé…ç½®ç›®å½•ç”Ÿæˆè·¯å¾„ä¿¡æ¯çš„è¿­ä»£å™¨
    @staticmethod
    def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[Tuple[object, Path, Path]]:
        # è·å–æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
        checkpoints: List[Path] = checkpoints_dir.glob("**/*.pkl")
    
        # éå†æ¯ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶
        for checkpoint in checkpoints:
            logger.info(f"ğŸ’ª Converting {checkpoint.stem}")
            # æŸ¥æ‰¾å…³è”çš„é…ç½®æ–‡ä»¶
    
            # æ•°æ®é›†åç§°ï¼Œä¾‹å¦‚ 'coco'
            dataset_name = checkpoint.parents[2].stem
            if dataset_name == "ade":
                dataset_name = dataset_name.replace("ade", "ade20k")
    
            # ä»»åŠ¡ç±»å‹ï¼Œä¾‹å¦‚ 'instance-segmentation'
            segmentation_task = checkpoint.parents[1].stem
    
            # ä¸æ£€æŸ¥ç‚¹å¯¹åº”çš„é…ç½®æ–‡ä»¶å
            config_file_name = f"{checkpoint.parents[0].stem}.yaml"
    
            # é…ç½®æ–‡ä»¶è·¯å¾„
            config: Path = config_dir / dataset_name / segmentation_task / "swin" / config_file_name
            yield config, checkpoint
# æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹æ˜¯å¦åœ¨ç»™å®šå®¹å·®ä¸‹è¾“å‡ºç›¸åŒç»“æœ
def test(
    original_model,  # åŸå§‹æ¨¡å‹
    our_model: Mask2FormerForUniversalSegmentation,  # æˆ‘ä»¬çš„æ¨¡å‹
    image_processor: Mask2FormerImageProcessor,  # å›¾åƒå¤„ç†å™¨
    tolerance: float,  # å®¹å·®å€¼
):
    # ç¦ç”¨æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        # å°†åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        original_model = original_model.eval()
        our_model = our_model.eval()

        # å‡†å¤‡å›¾åƒæ•°æ®
        im = prepare_img()
        x = image_processor(images=im, return_tensors="pt")["pixel_values"]

        # è·å–åŸå§‹æ¨¡å‹çš„ä¸»å¹²ç‰¹å¾
        original_model_backbone_features = original_model.backbone(x.clone())
        # è·å–æˆ‘ä»¬çš„æ¨¡å‹çš„è¾“å‡ºï¼ŒåŒ…æ‹¬éšè—çŠ¶æ€
        our_model_output: Mask2FormerModelOutput = our_model.model(x.clone(), output_hidden_states=True)

        # æµ‹è¯•ä¸»å¹²
        for original_model_feature, our_model_feature in zip(
            original_model_backbone_features.values(), our_model_output.encoder_hidden_states
        ):
            assert torch.allclose(
                original_model_feature, our_model_feature, atol=tolerance
            ), "The backbone features are not the same."

        # æµ‹è¯•åƒç´ è§£ç å™¨
        mask_features, _, multi_scale_features = original_model.sem_seg_head.pixel_decoder.forward_features(
            original_model_backbone_features
        )

        for original_model_feature, our_model_feature in zip(
            multi_scale_features, our_model_output.pixel_decoder_hidden_states
        ):
            assert torch.allclose(
                original_model_feature, our_model_feature, atol=tolerance
            ), "The pixel decoder feature are not the same"

        # æµ‹è¯•å®Œæ•´æ¨¡å‹
        tr_complete = T.Compose(
            [T.Resize((384, 384)), T.ToTensor()],
        )
        y = (tr_complete(im) * 255.0).to(torch.int).float()

        # ä¿®æ”¹åŸå§‹ Mask2Former ä»£ç ä»¥è¿”å›æ©ç å’Œç±»åˆ« logits
        original_class_logits, original_mask_logits = original_model([{"image": y.clone().squeeze(0)}])

        # è·å–æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡º
        our_model_out: Mask2FormerForUniversalSegmentationOutput = our_model(x.clone())
        our_mask_logits = our_model_out.masks_queries_logits
        our_class_logits = our_model_out.class_queries_logits

        # æ–­è¨€åŸå§‹æ¨¡å‹å’Œæˆ‘ä»¬çš„æ¨¡å‹è¾“å‡ºå½¢çŠ¶ç›¸åŒ
        assert original_mask_logits.shape == our_mask_logits.shape, "Output masks shapes are not matching."
        assert original_class_logits.shape == our_class_logits.shape, "Output class logits shapes are not matching."
        # æ–­è¨€ç±»åˆ« logits å’Œé¢„æµ‹çš„æ©ç ç›¸åŒ
        assert torch.allclose(
            original_class_logits, our_class_logits, atol=tolerance
        ), "The class logits are not the same."
        assert torch.allclose(
            original_mask_logits, our_mask_logits, atol=tolerance
        ), "The predicted masks are not the same."

        # è®°å½•æµ‹è¯•é€šè¿‡ä¿¡æ¯
        logger.info("âœ… Test passed!")


# ä»æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­è·å–æ¨¡å‹åç§°
def get_model_name(checkpoint_file: Path):
    # model_name_raw æ˜¯å½¢å¦‚ maskformer2_swin_small_bs16_50ep çš„å­—ç¬¦ä¸²
    model_name_raw: str = checkpoint_file.parents[0].stem

    # `segmentation_task_type` å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: `instance-segmentation`, `panoptic-segmentation`, `semantic-segmentation`
    segmentation_task_name: str = checkpoint_file.parents[1].stem
    # æ£€æŸ¥segmentation_task_nameæ˜¯å¦åœ¨æŒ‡å®šçš„ä¸‰ç§åˆ†å‰²ä»»åŠ¡åç§°ä¹‹ä¸€ï¼Œå¦åˆ™æŠ›å‡ºæ•°å€¼é”™è¯¯å¼‚å¸¸
    if segmentation_task_name not in ["instance-segmentation", "panoptic-segmentation", "semantic-segmentation"]:
        raise ValueError(
            f"{segmentation_task_name} must be wrong since acceptable values are: instance-segmentation,"
            " panoptic-segmentation, semantic-segmentation."
        )

    # ä»checkpoint_fileçˆ¶ç›®å½•çš„çˆ¶ç›®å½•ä¸­è·å–æ•°æ®é›†åç§°ï¼Œå¿…é¡»æ˜¯"coco", "ade", "cityscapes", "mapillary-vistas"ä¹‹ä¸€ï¼Œå¦åˆ™æŠ›å‡ºæ•°å€¼é”™è¯¯å¼‚å¸¸
    dataset_name: str = checkpoint_file.parents[2].stem
    if dataset_name not in ["coco", "ade", "cityscapes", "mapillary-vistas"]:
        raise ValueError(
            f"{dataset_name} must be wrong since we didn't find 'coco' or 'ade' or 'cityscapes' or 'mapillary-vistas'"
            " in it "
        )

    # è®¾ç½®backboneä¸º"swin"ï¼Œå®šä¹‰backbone_typesåˆ—è¡¨å’Œå½“å‰æ¨¡å‹çš„backboneç±»å‹
    backbone = "swin"
    backbone_types = ["tiny", "small", "base_IN21k", "base", "large"]
    backbone_type = list(filter(lambda x: x in model_name_raw, backbone_types))[0].replace("_", "-")

    # æ ¹æ®backboneã€backbone_typeã€dataset_nameå’Œsegmentation_task_nameç»„åˆæˆæ¨¡å‹åç§°
    model_name = f"mask2former-{backbone}-{backbone_type}-{dataset_name}-{segmentation_task_name.split('-')[0]}"

    # è¿”å›æ„å»ºå¥½çš„æ¨¡å‹åç§°
    return model_name
# å½“è¯¥è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œæ—¶æ‰§è¡Œä»¥ä¸‹ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªArgumentParserå¯¹è±¡ï¼Œç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
    parser = ArgumentParser(
        # è®¾ç½®ç¨‹åºçš„æè¿°ä¿¡æ¯
        description="Command line to convert the original mask2formers (with swin backbone) to our implementations."
    )

    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼ŒæŒ‡å®šåŒ…å«æ¨¡å‹checkpointçš„ç›®å½•
    parser.add_argument(
        "--checkpoints_dir",
        type=Path,
        help=(
            "A directory containing the model's checkpoints. The directory has to have the following structure:"
            " <DIR_NAME>/<DATASET_NAME>/<SEGMENTATION_TASK_NAME>/<CONFIG_NAME>.pkl"
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼ŒæŒ‡å®šåŒ…å«æ¨¡å‹é…ç½®æ–‡ä»¶çš„ç›®å½•
    parser.add_argument(
        "--configs_dir",
        type=Path,
        help=(
            "A directory containing the model's configs, see detectron2 doc. The directory has to have the following"
            " structure: <DIR_NAME>/<DATASET_NAME>/<SEGMENTATION_TASK_NAME>/<CONFIG_NAME>.yaml"
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå¿…éœ€å‚æ•°ï¼ŒæŒ‡å®šMask2Formerçš„åŸå§‹å®ç°ç›®å½•
    parser.add_argument(
        "--mask2former_dir",
        required=True,
        type=Path,
        help=(
            "A path to Mask2Former's original implementation directory. You can download from here:"
            " https://github.com/facebookresearch/Mask2Former"
        ),
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œè·å–ç»“æœ
    args = parser.parse_args()

    # ä»è§£æçš„å‚æ•°ä¸­è·å–å„ä¸ªç›®å½•çš„è·¯å¾„
    checkpoints_dir: Path = args.checkpoints_dir
    config_dir: Path = args.configs_dir
    mask2former_dir: Path = args.mask2former_dir
    # å°†Mask2FormeråŸå§‹å®ç°ç›®å½•çš„çˆ¶ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­
    sys.path.append(str(mask2former_dir.parent))
    # ä»Mask2Formerçš„åŸå§‹æºä»£ç ä¸­å¯¼å…¥é…ç½®å’Œæ¨¡å‹ç±»
    from Mask2Former.mask2former.config import add_maskformer2_config
    from Mask2Former.mask2former.maskformer_model import MaskFormer as OriginalMask2Former

    # éå†checkpoints_dirå’Œconfig_dirä¸­çš„æ–‡ä»¶ï¼Œå¹¶è½¬æ¢ä¸ºæˆ‘ä»¬è‡ªå·±çš„å®ç°
    for config_file, checkpoint_file in OriginalMask2FormerCheckpointToOursConverter.using_dirs(
        checkpoints_dir, config_dir
    ):
        # è·å–æ¨¡å‹åç§°
        model_name = get_model_name(checkpoint_file)
        # åˆ›å»ºå›¾åƒå¤„ç†å™¨å¯¹è±¡ï¼Œå¹¶å°†åŸå§‹çš„é…ç½®æ–‡ä»¶è½¬æ¢æˆå›¾åƒå¤„ç†å™¨çš„é…ç½®
        image_processor = OriginalMask2FormerConfigToImageProcessorConverter()(
            setup_cfg(Args(config_file=config_file))
        )
        # è®¾ç½®å›¾åƒå¤„ç†å™¨çš„å°ºå¯¸ä¸º384x384
        image_processor.size = {"height": 384, "width": 384}

        # æ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºåŸå§‹çš„Mask2Formeræ¨¡å‹
        original_config = setup_cfg(Args(config_file=config_file))
        mask2former_kwargs = OriginalMask2Former.from_config(original_config)
        original_model = OriginalMask2Former(**mask2former_kwargs).eval()

        # åŠ è½½checkpointæ–‡ä»¶ä¸­çš„æ¨¡å‹å‚æ•°åˆ°åŸå§‹æ¨¡å‹ä¸­
        DetectionCheckpointer(original_model).load(str(checkpoint_file))

        # å°†åŸå§‹æ¨¡å‹çš„é…ç½®è½¬æ¢æˆæˆ‘ä»¬çš„é…ç½®
        config: Mask2FormerConfig = OriginalMask2FormerConfigToOursConverter()(original_config)
        # åˆ›å»ºæˆ‘ä»¬çš„Mask2Formeræ¨¡å‹
        mask2former = Mask2FormerModel(config=config).eval()

        # å°†åŸå§‹æ¨¡å‹çš„å‚æ•°è½¬æ¢æˆæˆ‘ä»¬çš„æ¨¡å‹
        converter = OriginalMask2FormerCheckpointToOursConverter(original_model, config)
        mask2former = converter.convert(mask2former)

        # åˆ›å»ºç”¨äºé€šç”¨åˆ†å‰²çš„Mask2Formeræ¨¡å‹
        mask2former_for_segmentation = Mask2FormerForUniversalSegmentation(config=config).eval()
        # å°†æˆ‘ä»¬çš„æ¨¡å‹è®¾ç½®ä¸ºé€šç”¨åˆ†å‰²æ¨¡å‹çš„å­æ¨¡å‹
        mask2former_for_segmentation.model = mask2former

        # å°†é€šç”¨åˆ†å‰²æ¨¡å‹çš„å‚æ•°è½¬æ¢æˆæˆ‘ä»¬çš„æ¨¡å‹çš„å‚æ•°
        mask2former_for_segmentation = converter.convert_universal_segmentation(mask2former_for_segmentation)

        # è®¾ç½®å®¹å·®å€¼
        tolerance = 3e-1
        # é«˜å®¹å·®çš„æ¨¡å‹åˆ—è¡¨
        high_tolerance_models = [
            "mask2former-swin-base-IN21k-coco-instance",
            "mask2former-swin-base-coco-instance",
            "mask2former-swin-small-cityscapes-semantic",
        ]

        if model_name in high_tolerance_models:
            # å¦‚æœæ¨¡å‹åœ¨é«˜å®¹å·®æ¨¡å‹åˆ—è¡¨ä¸­ï¼Œåˆ™å°†å®¹å·®å€¼è®¾ç½®ä¸º3e-1
            tolerance = 3e-1

        # è®°å½•æ—¥å¿—ï¼Œæµ‹è¯•æ¨¡å‹
        logger.info(f"ğŸª„ Testing {model_name}...")
        test(original_model, mask2former_for_segmentation, image_processor, tolerance)
        # è®°å½•æ—¥å¿—ï¼Œå°†æ¨¡å‹æ¨é€åˆ°hub
        logger.info(f"ğŸª„ Pushing {model_name} to hub...")

        # å°†å›¾åƒå¤„ç†å™¨å¯¹è±¡ä¸Šä¼ åˆ°hub
        image_processor.push_to_hub(model_name)
        # å°†é€šç”¨åˆ†å‰²æ¨¡å‹ä¸Šä¼ åˆ°hub
        mask2former_for_segmentation.push_to_hub(model_name)
```