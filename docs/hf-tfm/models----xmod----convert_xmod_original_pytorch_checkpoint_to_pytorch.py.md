# `.\models\xmod\convert_xmod_original_pytorch_checkpoint_to_pytorch.py`

```
# è®¾ç½®ç¼–ç æ ¼å¼ä¸º UTF-8
# ç‰ˆæƒå£°æ˜åŠè®¸å¯ä¿¡æ¯
# å¼•å…¥ argparse ç”¨äºå‘½ä»¤è¡Œå‚æ•°è§£æ
# ä» pathlib æ¨¡å—ä¸­å¼•å…¥ Path ç±»
# å¼•å…¥ fairseq åº“
# å¼•å…¥ torch åº“
# ä» fairseq çš„ xmod æ¨¡å—ä¸­å¼•å…¥ XMODModel ç±»åˆ«åä¸º FairseqXmodModel
# ä» packaging æ¨¡å—ä¸­å¼•å…¥ version å‡½æ•°
# ä» transformers åº“ä¸­å¼•å…¥ XmodConfig, XmodForMaskedLM, XmodForSequenceClassification ç±»
# ä» transformers.utils ä¸­å¼•å…¥ logging æ¨¡å—

if version.parse(fairseq.__version__) < version.parse("0.12.2"):
    # å¦‚æœ fairseq ç‰ˆæœ¬å°äº 0.12.2ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
    raise Exception("requires fairseq >= 0.12.2")
if version.parse(fairseq.__version__) > version.parse("2"):
    # å¦‚æœ fairseq ç‰ˆæœ¬å¤§äº 2ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
    raise Exception("requires fairseq < v2")

# è®¾ç½®æ—¥å¿—è¾“å‡ºç­‰çº§ä¸º INFO
logging.set_verbosity_info()
# è·å–æ—¥å¿—è®°å½•å™¨å¯¹è±¡
logger = logging.get_logger(__name__)

# ç¤ºä¾‹æ–‡æœ¬
SAMPLE_TEXT = "Hello, World!"
# ç¤ºä¾‹è¯­è¨€æ ‡è¯†
SAMPLE_LANGUAGE = "en_XX"

def convert_xmod_checkpoint_to_pytorch(
    xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool
):
    # æ•°æ®ç›®å½•
    data_dir = Path("data_bin")
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½ FairseqXmodModel æ¨¡å‹
    xmod = FairseqXmodModel.from_pretrained(
        model_name_or_path=str(Path(xmod_checkpoint_path).parent),
        checkpoint_file=Path(xmod_checkpoint_path).name,
        _name="xmod_base",
        arch="xmod_base",
        task="multilingual_masked_lm",
        data_name_or_path=str(data_dir),
        bpe="sentencepiece",
        sentencepiece_model=str(Path(xmod_checkpoint_path).parent / "sentencepiece.bpe.model"),
        src_dict=str(data_dir / "dict.txt"),
    )
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨ dropout
    xmod.eval()
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(xmod)

    # è·å– xmod æ¨¡å‹çš„å¥å­ç¼–ç å™¨
    xmod_sent_encoder = xmod.model.encoder.sentence_encoder
    # æ ¹æ® xmod æ¨¡å‹çš„é…ç½®åˆ›å»º XmodConfig å¯¹è±¡
    config = XmodConfig(
        vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings,
        hidden_size=xmod.cfg.model.encoder_embed_dim,
        num_hidden_layers=xmod.cfg.model.encoder_layers,
        num_attention_heads=xmod.cfg.model.encoder_attention_heads,
        intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim,
        max_position_embeddings=514,
        type_vocab_size=1,
        layer_norm_eps=1e-5,  # PyTorch é»˜è®¤å€¼ï¼Œä¸ fairseq å…¼å®¹
        pre_norm=xmod.cfg.model.encoder_normalize_before,
        adapter_reduction_factor=getattr(xmod.cfg.model, "bottleneck", 2),
        adapter_layer_norm=xmod.cfg.model.adapter_layer_norm,
        adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm,
        ln_before_adapter=xmod.cfg.model.ln_before_adapter,
        languages=xmod.cfg.model.languages,
    )
    # å¦‚æœéœ€è¦åˆ†ç±»å¤´éƒ¨ï¼Œåˆ™è®¾ç½®é…ç½®å¯¹è±¡çš„æ ‡ç­¾æ•°é‡ä¸ºæ¨¡å‹ç‰¹å®šåˆ†ç±»å¤´çš„è¾“å‡ºæƒé‡è¡Œæ•°
    if classification_head:
        config.num_labels = xmod.model.classification_heads["mnli"].out_proj.weight.shape[0]
    # æ‰“å° X-MOD çš„é…ç½®ä¿¡æ¯
    print("Our X-MOD config:", config)

    # æ ¹æ®æ˜¯å¦æœ‰åˆ†ç±»å¤´é€‰æ‹©æ¨¡å‹ç±»å‹ï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)
    model.eval()

    # å¤åˆ¶æ‰€æœ‰æƒé‡
    # åµŒå…¥å±‚æƒé‡
    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight
    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight
    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.roberta.embeddings.token_type_embeddings.weight
    )  # å°†å…¶ç½®é›¶å› ä¸º xmod ä¸ä½¿ç”¨å®ƒä»¬

    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight
    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias

    # å¦‚æœå­˜åœ¨å±‚å½’ä¸€åŒ–ï¼Œåˆ™å¤åˆ¶ç¼–ç å™¨å±‚å½’ä¸€åŒ–çš„æƒé‡å’Œåç½®
    if xmod_sent_encoder.layer_norm is not None:
        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight
        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias

    # å¦‚æœæ˜¯åˆ†ç±»å¤´ï¼Œå¤åˆ¶åˆ†ç±»å™¨çš„æƒé‡å’Œåç½®
    if classification_head:
        model.classifier.dense.weight = xmod.model.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = xmod.model.classification_heads["mnli"].dense.bias
        model.classifier.out_proj.weight = xmod.model.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = xmod.model.classification_heads["mnli"].out_proj.bias
    else:
        # å¦‚æœæ˜¯è¯­è¨€æ¨¡å‹å¤´ï¼Œå¤åˆ¶è¯­è¨€æ¨¡å‹å¤´çš„æƒé‡å’Œåç½®
        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight
        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias
        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias
        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight
        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias

    # æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦ä¸€è‡´
    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)  # æ‰¹é‡å¤§å°ä¸º 1
    model.roberta.set_default_language(SAMPLE_LANGUAGE)

    our_output = model(input_ids)[0]
    if classification_head:
        their_output = xmod.model.classification_heads["mnli"](xmod.extract_features(input_ids))
    else:
        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]
    print(our_output.shape, their_output.shape)
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    print(f"max_absolute_diff = {max_absolute_diff}")  # çº¦ä¸º 1e-7
    success = torch.allclose(our_output, their_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")
    if not success:
        raise Exception("Something went wRoNg")

    # åˆ›å»ºç›®å½•ä»¥ä¿å­˜ PyTorch æ¨¡å‹
    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
    print(f"Saving model to {pytorch_dump_folder_path}")
    model.save_pretrained(pytorch_dump_folder_path)
if __name__ == "__main__":
    # å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡

    # å¿…é€‰å‚æ•°
    parser.add_argument(
        "--xmod_checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼ŒæŒ‡å®šå®˜æ–¹ PyTorch æ¨¡å‹çš„è·¯å¾„ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œå¿…é€‰é¡¹

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼ŒæŒ‡å®šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œå¿…é€‰é¡¹

    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼Œè¡¨ç¤ºæ˜¯å¦è¦è½¬æ¢æœ€ç»ˆçš„åˆ†ç±»å¤´éƒ¨ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼å‚æ•°

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•°ï¼Œå°† xmod æ¨¡å‹è½¬æ¢ä¸º PyTorch æ¨¡å‹
    convert_xmod_checkpoint_to_pytorch(
        args.xmod_checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
```