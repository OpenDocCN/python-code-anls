# `.\transformers\models\xmod\convert_xmod_original_pytorch_checkpoint_to_pytorch.py`

```
# coding=utf-8
# ç‰ˆæƒå£°æ˜

"""Convert X-MOD checkpoint."""

# å¯¼å…¥éœ€è¦çš„æ¨¡å—
import argparse
from pathlib import Path
import fairseq
import torch
from fairseq.models.xmod import XMODModel as FairseqXmodModel
from packaging import version
from transformers import XmodConfig, XmodForMaskedLM, XmodForSequenceClassification
from transformers.utils import logging

# æ£€æŸ¥ fairseq ç‰ˆæœ¬æ˜¯å¦ç¬¦åˆè¦æ±‚
if version.parse(fairseq.__version__) < version.parse("0.12.2"):
    raise Exception("requires fairseq >= 0.12.2")
if version.parse(fairseq.__version__) > version.parse("2"):
    raise Exception("requires fairseq < v2")

# è®¾ç½®æ—¥å¿—ä¿¡æ¯
logging.set_verbosity_info()
logger = logging.get_logger(__name__)

# å®šä¹‰ç¤ºä¾‹æ–‡æœ¬å’Œè¯­è¨€
SAMPLE_TEXT = "Hello, World!"
SAMPLE_LANGUAGE = "en_XX"

# å°† X-MOD æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
def convert_xmod_checkpoint_to_pytorch(
    xmod_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool
):
    # å®šä¹‰æ•°æ®ç›®å½•
    data_dir = Path("data_bin")
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ X-MOD æ¨¡å‹
    xmod = FairseqXmodModel.from_pretrained(
        model_name_or_path=str(Path(xmod_checkpoint_path).parent),
        checkpoint_file=Path(xmod_checkpoint_path).name,
        _name="xmod_base",
        arch="xmod_base",
        task="multilingual_masked_lm",
        data_name_or_path=str(data_dir),
        bpe="sentencepiece",
        sentencepiece_model=str(Path(xmod_checkpoint_path).parent / "sentencepiece.bpe.model"),
        src_dict=str(data_dir / "dict.txt"),
    )
    # å…³é—­ dropout
    xmod.eval()
    print(xmod)

    # è·å– X-MOD æ¨¡å‹çš„å¥å­ç¼–ç å™¨
    xmod_sent_encoder = xmod.model.encoder.sentence_encoder
    # å®šä¹‰ç”¨äºè½¬æ¢çš„é…ç½®
    config = XmodConfig(
        vocab_size=xmod_sent_encoder.embed_tokens.num_embeddings,
        hidden_size=xmod.cfg.model.encoder_embed_dim,
        num_hidden_layers=xmod.cfg.model.encoder_layers,
        num_attention_heads=xmod.cfg.model.encoder_attention_heads,
        intermediate_size=xmod.cfg.model.encoder_ffn_embed_dim,
        max_position_embeddings=514,
        type_vocab_size=1,
        layer_norm_eps=1e-5,  # PyTorch é»˜è®¤å€¼
        pre_norm=xmod.cfg.model.encoder_normalize_before,
        adapter_reduction_factor=getattr(xmod.cfg.model, "bottleneck", 2),
        adapter_layer_norm=xmod.cfg.model.adapter_layer_norm,
        adapter_reuse_layer_norm=xmod.cfg.model.adapter_reuse_layer_norm,
        ln_before_adapter=xmod.cfg.model.ln_before_adapter,
        languages=xmod.cfg.model.languages,
    )
    # å¦‚æœæœ‰åˆ†ç±»å¤´ï¼Œåˆ™è®¾ç½®æ ‡ç­¾æ•°é‡
    if classification_head:
        config.num_labels = xmod.model.classification_heads["mnli"].out_proj.weight.shape[0]
    # è¾“å‡ºé…ç½®ä¿¡æ¯
    print("Our X-MOD config:", config)

    # æ ¹æ®æ˜¯å¦æ˜¯åˆ†ç±»ä»»åŠ¡é€‰æ‹©ä¸åŒçš„æ¨¡å‹
    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)
    model.eval()

    # å¤åˆ¶æ‰€æœ‰æƒé‡
    # åµŒå…¥å±‚æƒé‡
    model.roberta.embeddings.word_embeddings.weight = xmod_sent_encoder.embed_tokens.weight
    model.roberta.embeddings.position_embeddings.weight = xmod_sent_encoder.embed_positions.weight
    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.roberta.embeddings.token_type_embeddings.weight
    )  # å°†å…¶å½’é›¶ï¼Œå› ä¸º xmod ä¸ä½¿ç”¨å®ƒä»¬

    model.roberta.embeddings.LayerNorm.weight = xmod_sent_encoder.layernorm_embedding.weight
    model.roberta.embeddings.LayerNorm.bias = xmod_sent_encoder.layernorm_embedding.bias

    if xmod_sent_encoder.layer_norm is not None:
        model.roberta.encoder.LayerNorm.weight = xmod_sent_encoder.layer_norm.weight
        model.roberta.encoder.LayerNorm.bias = xmod_sent_encoder.layer_norm.bias

    if classification_head:
        # åˆ†ç±»å¤´æƒé‡
        model.classifier.dense.weight = xmod.model.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = xmod.model.classification_heads["mnli"].dense.bias
        model.classifier.out_proj.weight = xmod.model.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = xmod.model.classification_heads["mnli"].out_proj.bias
    else:
        # è¯­è¨€æ¨¡å‹å¤´æƒé‡
        model.lm_head.dense.weight = xmod.model.encoder.lm_head.dense.weight
        model.lm_head.dense.bias = xmod.model.encoder.lm_head.dense.bias
        model.lm_head.layer_norm.weight = xmod.model.encoder.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = xmod.model.encoder.lm_head.layer_norm.bias
        model.lm_head.decoder.weight = xmod.model.encoder.lm_head.weight
        model.lm_head.decoder.bias = xmod.model.encoder.lm_head.bias

    # æ£€éªŒä¸¤ä¸ªæ¨¡å‹è¾“å‡ºæ˜¯å¦ç›¸åŒ
    input_ids = xmod.encode(SAMPLE_TEXT).unsqueeze(0)  # æ‰¹æ¬¡å¤§å°ä¸º 1
    model.roberta.set_default_language(SAMPLE_LANGUAGE)

    our_output = model(input_ids)[0]
    if classification_head:
        their_output = xmod.model.classification_heads["mnli"](xmod.extract_features(input_ids))
    else:
        their_output = xmod.model(input_ids, lang_id=[SAMPLE_LANGUAGE])[0]
    print(our_output.shape, their_output.shape)
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    print(f"max_absolute_diff = {max_absolute_diff}")  # å¤§çº¦ä¸º 1e-7
    success = torch.allclose(our_output, their_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")
    if not success:
        raise Exception("Something went wRoNg")

    # åˆ›å»ºå­˜å‚¨è·¯å¾„
    Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
    print(f"Saving model to {pytorch_dump_folder_path}")
    model.save_pretrained(pytorch_dump_folder_path)
# å¦‚æœå½“å‰è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€å‚æ•°
    parser.add_argument(
        "--xmod_checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°å°†XMODæ£€æŸ¥ç‚¹è½¬æ¢ä¸ºPyTorchæ ¼å¼
    convert_xmod_checkpoint_to_pytorch(
        args.xmod_checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
```