# `.\models\xlm_roberta_xl\convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py`

```
# æŒ‡å®š Python æ–‡ä»¶çš„ç¼–ç æ ¼å¼ä¸º UTF-8

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import argparse  # è§£æå‘½ä»¤è¡Œå‚æ•°çš„åº“
import pathlib   # å¤„ç†è·¯å¾„çš„åº“

import fairseq   # å¼•å…¥ fairseq åº“
import torch     # å¼•å…¥ PyTorch åº“
from fairseq.models.roberta import RobertaModel as FairseqRobertaModel  # å¯¼å…¥ fairseq ä¸­çš„ RoBERTa æ¨¡å‹
from fairseq.modules import TransformerSentenceEncoderLayer  # å¯¼å…¥ fairseq ä¸­çš„ TransformerSentenceEncoderLayer æ¨¡å—
from packaging import version  # ç”¨äºå¤„ç†ç‰ˆæœ¬å·çš„åº“

# ä» transformers åº“ä¸­å¯¼å…¥ç›¸å…³æ¨¡å—å’Œç±»
from transformers import XLMRobertaConfig, XLMRobertaXLForMaskedLM, XLMRobertaXLForSequenceClassification
from transformers.models.bert.modeling_bert import (
    BertIntermediate,   # å¯¼å…¥ BERT æ¨¡å‹ä¸­çš„ BertIntermediate ç±»
    BertLayer,          # å¯¼å…¥ BERT æ¨¡å‹ä¸­çš„ BertLayer ç±»
    BertOutput,         # å¯¼å…¥ BERT æ¨¡å‹ä¸­çš„ BertOutput ç±»
    BertSelfAttention,  # å¯¼å…¥ BERT æ¨¡å‹ä¸­çš„ BertSelfAttention ç±»
    BertSelfOutput,     # å¯¼å…¥ BERT æ¨¡å‹ä¸­çš„ BertSelfOutput ç±»
)
from transformers.models.roberta.modeling_roberta import RobertaAttention  # å¯¼å…¥ RoBERTa æ¨¡å‹ä¸­çš„ RobertaAttention ç±»
from transformers.utils import logging  # å¯¼å…¥ transformers åº“ä¸­çš„æ—¥å¿—è®°å½•æ¨¡å—

# æ£€æŸ¥ fairseq ç‰ˆæœ¬æ˜¯å¦ç¬¦åˆè¦æ±‚
if version.parse(fairseq.__version__) < version.parse("1.0.0a"):
    raise Exception("requires fairseq >= 1.0.0a")

# è®¾ç½®æ—¥å¿—è®°å½•çš„è¯¦ç»†ç¨‹åº¦ä¸º info çº§åˆ«
logging.set_verbosity_info()
# è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨
logger = logging.get_logger(__name__)

# å®šä¹‰ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬
SAMPLE_TEXT = "Hello world! cÃ©cÃ© herlolip"

# å®šä¹‰å‡½æ•°ï¼šå°† XLM-RoBERTa XL çš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
def convert_xlm_roberta_xl_checkpoint_to_pytorch(
    roberta_checkpoint_path: str,  # RoBERTa æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    pytorch_dump_folder_path: str,  # è½¬æ¢åçš„ PyTorch æ¨¡å‹ä¿å­˜è·¯å¾„
    classification_head: bool  # æ˜¯å¦åŒ…å«åˆ†ç±»å¤´
):
    """
    å¤åˆ¶/ç²˜è´´/è°ƒæ•´ RoBERTa çš„æƒé‡åˆ°æˆ‘ä»¬çš„ BERT ç»“æ„ã€‚
    """
    # ä»é¢„è®­ç»ƒçš„ RoBERTa æ¨¡å‹åŠ è½½æƒé‡
    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨ dropout
    roberta.eval()
    # è·å– RoBERTa æ¨¡å‹ä¸­çš„å¥å­ç¼–ç å™¨
    roberta_sent_encoder = roberta.model.encoder.sentence_encoder
    # æ ¹æ® RoBERTa æ¨¡å‹çš„é…ç½®åˆ›å»º XLM-RoBERTa çš„é…ç½®
    config = XLMRobertaConfig(
        vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings,  # è¯æ±‡è¡¨å¤§å°
        hidden_size=roberta.cfg.model.encoder_embed_dim,  # éšè—å±‚å¤§å°
        num_hidden_layers=roberta.cfg.model.encoder_layers,  # ç¼–ç å™¨å±‚æ•°
        num_attention_heads=roberta.cfg.model.encoder_attention_heads,  # æ³¨æ„åŠ›å¤´æ•°
        intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim,  # ä¸­é—´å±‚å¤§å°
        max_position_embeddings=514,  # æœ€å¤§ä½ç½®åµŒå…¥
        type_vocab_size=1,  # ç±»å‹è¯æ±‡è¡¨å¤§å°
        layer_norm_eps=1e-5,  # å±‚å½’ä¸€åŒ–çš„ epsilon å€¼ï¼Œä¸ fairseq ä½¿ç”¨çš„ PyTorch é»˜è®¤å€¼ç›¸åŒ
    )
    # å¦‚æœåŒ…å«åˆ†ç±»å¤´ï¼Œåˆ™è®¾ç½®é…ç½®ä¸­çš„æ ‡ç­¾æ•°ç›®
    if classification_head:
        config.num_labels = roberta.model.classification_heads["mnli"].out_proj.weight.shape[0]

    # æ‰“å°é…ç½®ä¿¡æ¯
    print("Our RoBERTa config:", config)

    # æ ¹æ®æ˜¯å¦åŒ…å«åˆ†ç±»å¤´é€‰æ‹©ç›¸åº”çš„ XLM-RoBERTa æ¨¡å‹
    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # å¼€å§‹å¤åˆ¶æ‰€æœ‰æƒé‡ã€‚
    # å¤åˆ¶åµŒå…¥å±‚çš„æƒé‡
    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight
    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight
    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.roberta.embeddings.token_type_embeddings.weight
    )  # å°† RoBERTa æ¨¡å‹çš„ token_type_embeddings æƒé‡ç½®é›¶ï¼Œå› ä¸º RoBERTa ä¸ä½¿ç”¨å®ƒä»¬ã€‚

    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight
    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias

    for i in range(config.num_hidden_layers):
        # å¾ªç¯éå†æ¯ä¸€å±‚çš„ç¼–ç å™¨

        # è·å–å½“å‰å±‚çš„ BertLayer å¯¹è±¡å’Œå¯¹åº”çš„ TransformerSentenceEncoderLayer å¯¹è±¡
        layer: BertLayer = model.roberta.encoder.layer[i]
        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]

        # è®¾ç½®æ³¨æ„åŠ›å±‚çš„æƒé‡å’Œåç½®
        attention: RobertaAttention = layer.attention
        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight
        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias

        # è®¾ç½®è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æƒé‡å’Œåç½®
        self_attn: BertSelfAttention = layer.attention.self
        assert (
            roberta_layer.self_attn.k_proj.weight.data.shape
            == roberta_layer.self_attn.q_proj.weight.data.shape
            == roberta_layer.self_attn.v_proj.weight.data.shape
            == torch.Size((config.hidden_size, config.hidden_size))
        )
        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight
        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias
        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight
        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias
        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight
        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias

        # è®¾ç½®è‡ªæ³¨æ„åŠ›æœºåˆ¶è¾“å‡ºçš„æƒé‡å’Œåç½®
        self_output: BertSelfOutput = layer.attention.output
        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape
        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight
        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias

        # è®¾ç½®æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–çš„æƒé‡å’Œåç½®
        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight
        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias

        # è®¾ç½®ä¸­é—´å±‚çš„å…¨è¿æ¥å±‚çš„æƒé‡å’Œåç½®
        intermediate: BertIntermediate = layer.intermediate
        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape
        intermediate.dense.weight = roberta_layer.fc1.weight
        intermediate.dense.bias = roberta_layer.fc1.bias

        # è®¾ç½®è¾“å‡ºå±‚çš„æƒé‡å’Œåç½®
        bert_output: BertOutput = layer.output
        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape
        bert_output.dense.weight = roberta_layer.fc2.weight
        bert_output.dense.bias = roberta_layer.fc2.bias
        # å±‚ç»“æŸ
    # å¦‚æœæœ‰åˆ†ç±»å¤´ï¼Œåˆ™å¤åˆ¶ RoBERTa æ¨¡å‹çš„åˆ†ç±»å¤´å‚æ•°åˆ°å½“å‰æ¨¡å‹çš„åˆ†ç±»å™¨ä¸­
    if classification_head:
        # å¤åˆ¶æƒé‡å’Œåç½®
        model.classifier.dense.weight = roberta.model.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = roberta.model.classification_heads["mnli"].dense.bias
        # å¤åˆ¶è¾“å‡ºæŠ•å½±çš„æƒé‡å’Œåç½®
        model.classifier.out_proj.weight = roberta.model.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = roberta.model.classification_heads["mnli"].out_proj.bias
    else:
        # å¦‚æœæ²¡æœ‰åˆ†ç±»å¤´ï¼Œåˆ™å¤åˆ¶ RoBERTa æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´å‚æ•°åˆ°å½“å‰æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´ä¸­
        # å¤åˆ¶æƒé‡å’Œåç½®
        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight
        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias
        # å¤åˆ¶ LayerNorm çš„æƒé‡å’Œåç½®
        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias
        # å¤åˆ¶è§£ç å™¨çš„æƒé‡å’Œåç½®
        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight
        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias

    # æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦ä¸€è‡´
    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)  # å°†è¾“å…¥ç¼–ç ä¸ºå¼ é‡ï¼Œå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ä½œä¸ºæ‰¹å¤„ç†çš„å¤§å°ä¸º1
    our_output = model(input_ids)[0]  # è·å–å½“å‰æ¨¡å‹çš„è¾“å‡º
    if classification_head:
        their_output = roberta.model.classification_heads["mnli"](roberta.extract_features(input_ids))
    else:
        their_output = roberta.model(input_ids)[0]  # è·å– RoBERTa æ¨¡å‹çš„è¾“å‡º
    print(our_output.shape, their_output.shape)  # æ‰“å°ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºçš„å½¢çŠ¶
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()  # è®¡ç®—è¾“å‡ºä¹‹é—´çš„æœ€å¤§ç»å¯¹å·®å¼‚
    print(f"max_absolute_diff = {max_absolute_diff}")  # æ‰“å°æœ€å¤§ç»å¯¹å·®å¼‚ï¼Œé¢„æœŸçº¦ä¸º 1e-7
    success = torch.allclose(our_output, their_output, atol=1e-3)  # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºæ˜¯å¦åœ¨æŒ‡å®šè¯¯å·®èŒƒå›´å†…ä¸€è‡´
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")  # æ‰“å°æ˜¯å¦ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºç›¸åŒ
    if not success:
        raise Exception("Something went wRoNg")  # å¦‚æœè¾“å‡ºä¸ä¸€è‡´ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

    # ç¡®ä¿è·¯å¾„å­˜åœ¨å¹¶åˆ›å»º PyTorch æ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹
    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
    print(f"Saving model to {pytorch_dump_folder_path}")  # æ‰“å°æ¨¡å‹ä¿å­˜çš„è·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)  # å°†å½“å‰æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
if __name__ == "__main__":
    # å¦‚æœè„šæœ¬è¢«ç›´æ¥æ‰§è¡Œè€Œéä½œä¸ºæ¨¡å—å¯¼å…¥ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—

    parser = argparse.ArgumentParser()
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡

    # å¿…å¡«å‚æ•°
    parser.add_argument(
        "--roberta_checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    # æ·»åŠ ä¸€ä¸ªå¿…å¡«å‚æ•°ï¼šRoBERTa æ¨¡å‹çš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œç”¨æˆ·å¿…é¡»æä¾›ï¼Œå¸®åŠ©ä¿¡æ¯æŒ‡æ˜å®ƒæ˜¯å®˜æ–¹ PyTorch dump çš„è·¯å¾„

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    # æ·»åŠ ä¸€ä¸ªå¿…å¡«å‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œç”¨æˆ·å¿…é¡»æä¾›ï¼Œå¸®åŠ©ä¿¡æ¯æŒ‡æ˜å®ƒæ˜¯è¾“å‡º PyTorch æ¨¡å‹çš„è·¯å¾„

    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # æ·»åŠ ä¸€ä¸ªæ ‡å¿—å‚æ•°ï¼šæ˜¯å¦è½¬æ¢æœ€ç»ˆçš„åˆ†ç±»å¤´éƒ¨ï¼Œå½“å­˜åœ¨è¯¥å‚æ•°æ—¶è®¾ç½®å…¶å€¼ä¸º Trueï¼Œå¸®åŠ©ä¿¡æ¯è¯´æ˜äº†è¿™ä¸ªå‚æ•°çš„ä½œç”¨

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•°ï¼Œå°† XLM-RoBERTa XL æ¨¡å‹çš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ ¼å¼
    convert_xlm_roberta_xl_checkpoint_to_pytorch(
        args.roberta_checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
```