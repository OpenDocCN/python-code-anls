# `.\transformers\models\xlm_roberta_xl\convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8

# å¯¼å…¥éœ€è¦çš„æ¨¡å—å’Œåº“
import argparse  # å¯¼å…¥ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°çš„æ¨¡å—
import pathlib  # æä¾›å¤„ç†æ–‡ä»¶è·¯å¾„çš„ç±»å’Œå‡½æ•°

import fairseq  # å¯¼å…¥ fairseq åº“
import torch  # å¯¼å…¥ PyTorch åº“
from fairseq.models.roberta import RobertaModel as FairseqRobertaModel  # å¯¼å…¥ fairseq ä¸­çš„ RoBERTa æ¨¡å‹
from fairseq.modules import TransformerSentenceEncoderLayer  # å¯¼å…¥ fairseq ä¸­çš„ TransformerSentenceEncoderLayer æ¨¡å—
from packaging import version  # å¯¼å…¥ç‰ˆæœ¬ä¿¡æ¯æ¨¡å—

from transformers import (  # å¯¼å…¥ transformers åº“ä¸­çš„ä¸€äº›ç±»å’Œå‡½æ•°
    XLMRobertaConfig,  # å¯¼å…¥ XLMRobertaConfig ç±»
    XLMRobertaXLForMaskedLM,  # å¯¼å…¥ XLMRobertaXLForMaskedLM ç±»
    XLMRobertaXLForSequenceClassification,  # å¯¼å…¥ XLMRobertaXLForSequenceClassification ç±»
)
from transformers.models.bert.modeling_bert import (  # å¯¼å…¥ transformers åº“ä¸­çš„ä¸€äº› BERT æ¨¡å‹ç›¸å…³çš„ç±»
    BertIntermediate,  # å¯¼å…¥ BertIntermediate ç±»
    BertLayer,  # å¯¼å…¥ BertLayer ç±»
    BertOutput,  # å¯¼å…¥ BertOutput ç±»
    BertSelfAttention,  # å¯¼å…¥ BertSelfAttention ç±»
    BertSelfOutput,  # å¯¼å…¥ BertSelfOutput ç±»
)
from transformers.models.roberta.modeling_roberta import RobertaAttention  # å¯¼å…¥ RoBERTaAttention ç±»
from transformers.utils import logging  # å¯¼å…¥ logging æ¨¡å—

# æ£€æŸ¥ fairseq ç‰ˆæœ¬æ˜¯å¦ç¬¦åˆè¦æ±‚
if version.parse(fairseq.__version__) < version.parse("1.0.0a"):
    raise Exception("requires fairseq >= 1.0.0a")  # å¦‚æœç‰ˆæœ¬ä¸ç¬¦åˆè¦æ±‚åˆ™æŠ›å‡ºå¼‚å¸¸

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º INFO
logging.set_verbosity_info()
# è·å– logger å¯¹è±¡
logger = logging.get_logger(__name__)

# å®šä¹‰ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬
SAMPLE_TEXT = "Hello world! cÃ©cÃ© herlolip"


def convert_xlm_roberta_xl_checkpoint_to_pytorch(
    roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool
):
    """
    å¤åˆ¶/ç²˜è´´/è°ƒæ•´ RoBERTa çš„æƒé‡åˆ°æˆ‘ä»¬çš„ BERT ç»“æ„ã€‚
    """
    # åŠ è½½é¢„è®­ç»ƒçš„ RoBERTa æ¨¡å‹
    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå…³é—­ dropout
    roberta.eval()
    # è·å– RoBERTa çš„å¥å­ç¼–ç å™¨
    roberta_sent_encoder = roberta.model.encoder.sentence_encoder
    # åˆ›å»ºä¸€ä¸ª XLMRobertaConfig é…ç½®å¯¹è±¡
    config = XLMRobertaConfig(
        vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings,  # è¯æ±‡è¡¨å¤§å°
        hidden_size=roberta.cfg.model.encoder_embed_dim,  # éšè—å±‚å¤§å°
        num_hidden_layers=roberta.cfg.model.encoder_layers,  # éšè—å±‚å±‚æ•°
        num_attention_heads=roberta.cfg.model.encoder_attention_heads,  # æ³¨æ„åŠ›å¤´æ•°
        intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim,  # ä¸­é—´å±‚å¤§å°
        max_position_embeddings=514,  # æœ€å¤§ä½ç½®ç¼–ç 
        type_vocab_size=1,  # ç±»å‹è¯æ±‡è¡¨å¤§å°
        layer_norm_eps=1e-5,  # å±‚å½’ä¸€åŒ– epsilon å€¼
    )
    # å¦‚æœæœ‰åˆ†ç±»å¤´ï¼Œåˆ™è®¾ç½®åˆ†ç±»æ ‡ç­¾æ•°
    if classification_head:
        config.num_labels = roberta.model.classification_heads["mnli"].out_proj.weight.shape[0]

    # è¾“å‡ºæˆ‘ä»¬çš„ RoBERTa é…ç½®ä¿¡æ¯
    print("Our RoBERTa config:", config)

    # æ ¹æ®æ˜¯å¦æœ‰åˆ†ç±»å¤´é€‰æ‹©åˆ›å»º XLMRobertaXLForSequenceClassification æˆ– XLMRobertaXLForMaskedLM
    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # å¼€å§‹å¤åˆ¶æƒé‡

    # å¤åˆ¶è¯åµŒå…¥æƒé‡
    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight
    # å¤åˆ¶ä½ç½®ç¼–ç æƒé‡
    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight
    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.roberta.embeddings.token_type_embeddings.weight
    )  # just zero them out b/c RoBERTa doesn't use them.

    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight
    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias

    for i in range(config.num_hidden_layers):
        # Encoder: start of layer
        layer: BertLayer = model.roberta.encoder.layer[i]  # è·å–å½“å‰å±‚çš„ BERT å±‚å¯¹è±¡
        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]  # è·å–å¯¹åº”çš„ RoBERTa å¥å­ç¼–ç å™¨å±‚å¯¹è±¡

        attention: RobertaAttention = layer.attention  # è·å–å½“å‰å±‚çš„ RoBERTa æ³¨æ„åŠ›å¯¹è±¡
        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight  # å°† RoBERTa ä¸­çš„è‡ªæ³¨æ„åŠ›å±‚æƒé‡èµ‹å€¼ç»™å½“å‰å±‚çš„æ³¨æ„åŠ›çš„æƒé‡
        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias  # å°† RoBERTa ä¸­çš„è‡ªæ³¨æ„åŠ›å±‚åç½®èµ‹å€¼ç»™å½“å‰å±‚çš„æ³¨æ„åŠ›çš„åç½®

        # self attention
        self_attn: BertSelfAttention = layer.attention.self  # è·å–å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›å¯¹è±¡
        assert (
            roberta_layer.self_attn.k_proj.weight.data.shape  # æ–­è¨€ç¡®ä¿ç»´åº¦ç›¸ç­‰
            == roberta_layer.self_attn.q_proj.weight.data.shape
            == roberta_layer.self_attn.v_proj.weight.data.shape
            == torch.Size((config.hidden_size, config.hidden_size))
        )

        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„æŸ¥è¯¢å‘é‡æƒé‡ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›å±‚çš„æŸ¥è¯¢å‘é‡æƒé‡
        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„æŸ¥è¯¢å‘é‡åç½®ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›å±‚çš„æŸ¥è¯¢å‘é‡åç½®
        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„é”®å‘é‡æƒé‡ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›å±‚çš„é”®å‘é‡æƒé‡
        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„é”®å‘é‡åç½®ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›å±‚çš„é”®å‘é‡åç½®
        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„å€¼å‘é‡æƒé‡ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›å±‚çš„å€¼å‘é‡æƒé‡
        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„å€¼å‘é‡åç½®ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›å±‚çš„å€¼å‘é‡åç½®

        # self-attention output
        self_output: BertSelfOutput = layer.attention.output  # è·å–å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›è¾“å‡ºå¯¹è±¡
        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape  # æ–­è¨€ç¡®ä¿ç»´åº¦ç›¸ç­‰
        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºæŠ•å½±æƒé‡ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›è¾“å‡ºçš„æƒé‡
        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias  # èµ‹å€¼ RoBERTa ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºæŠ•å½±åç½®ç»™å½“å‰å±‚çš„è‡ªæ³¨æ„åŠ›è¾“å‡ºçš„åç½®

        # this one is final layer norm
        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight  # èµ‹å€¼ RoBERTa ä¸­æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–æƒé‡ç»™å½“å‰å±‚çš„å½’ä¸€åŒ–çš„æƒé‡
        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias  # èµ‹å€¼ RoBERTa ä¸­æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–åç½®ç»™å½“å‰å±‚çš„å½’ä¸€åŒ–çš„åç½®

        # intermediate
        intermediate: BertIntermediate = layer.intermediate  # è·å–å½“å‰å±‚çš„ä¸­é—´å±‚å¯¹è±¡
        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape  # æ–­è¨€ç¡®ä¿ç»´åº¦ç›¸ç­‰
        intermediate.dense.weight = roberta_layer.fc1.weight  # èµ‹å€¼ RoBERTa ä¸­å…¨è¿æ¥å±‚ 1 çš„æƒé‡ç»™å½“å‰å±‚çš„ä¸­é—´å±‚çš„æƒé‡
        intermediate.dense.bias = roberta_layer.fc1.bias  # èµ‹å€¼ RoBERTa ä¸­å…¨è¿æ¥å±‚ 1 çš„åç½®ç»™å½“å‰å±‚çš„ä¸­é—´å±‚çš„åç½®

        # output
        bert_output: BertOutput = layer.output  # è·å–å½“å‰å±‚çš„è¾“å‡ºå¯¹è±¡
        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape  # æ–­è¨€ç¡®ä¿ç»´åº¦ç›¸ç­‰
        bert_output.dense.weight = roberta_layer.fc2.weight  # èµ‹å€¼ RoBERTa ä¸­å…¨è¿æ¥å±‚ 2 çš„æƒé‡ç»™å½“å‰å±‚çš„è¾“å‡ºçš„æƒé‡
        bert_output.dense.bias = roberta_layer.fc2.bias  # èµ‹å€¼ RoBERTa ä¸­å…¨è¿æ¥å±‚ 2 çš„åç½®ç»™å½“å‰å±‚çš„è¾“å‡ºçš„åç½®
        # end of layer
    å¦‚æœæœ‰åˆ†ç±»å¤´
    if classification_head:
        å°†æ¨¡å‹çš„åˆ†ç±»å™¨çš„æƒé‡è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„åˆ†ç±»å¤´mnliçš„denseå±‚çš„æƒé‡
        model.classifier.dense.weight = roberta.model.classification_heads["mnli"].dense.weight
        å°†æ¨¡å‹çš„åˆ†ç±»å™¨çš„åç½®è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„åˆ†ç±»å¤´mnliçš„denseå±‚çš„åç½®
        model.classifier.dense.bias = roberta.model.classification_heads["mnli"].dense.bias
        å°†æ¨¡å‹çš„åˆ†ç±»å™¨çš„è¾“å‡ºæŠ•å½±çš„æƒé‡è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„åˆ†ç±»å¤´mnliçš„æŠ•å½±å±‚çš„æƒé‡
        model.classifier.out_proj.weight = roberta.model.classification_heads["mnli"].out_proj.weight
        å°†æ¨¡å‹çš„åˆ†ç±»å™¨çš„è¾“å‡ºæŠ•å½±çš„åç½®è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„åˆ†ç±»å¤´mnliçš„æŠ•å½±å±‚çš„åç½®
        model.classifier.out_proj.bias = roberta.model.classification_heads["mnli"].out_proj.bias
    å¦åˆ™
    else:
        # è¯­è¨€æ¨¡å‹å¤´
        å°†æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´çš„denseå±‚çš„æƒé‡è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„ç¼–ç å™¨lm_headçš„denseå±‚çš„æƒé‡
        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight
        å°†æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´çš„denseå±‚çš„åç½®è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„ç¼–ç å™¨lm_headçš„denseå±‚çš„åç½®
        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias
        å°†æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´çš„layer_normçš„æƒé‡è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„ç¼–ç å™¨lm_headçš„layer_normçš„æƒé‡
        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight
        å°†æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´çš„layer_normçš„åç½®è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„ç¼–ç å™¨lm_headçš„layer_normçš„åç½®
        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias
        å°†æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´çš„è§£ç å±‚çš„æƒé‡è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„ç¼–ç å™¨lm_headçš„æƒé‡
        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight
        å°†æ¨¡å‹çš„è¯­è¨€æ¨¡å‹å¤´çš„è§£ç å±‚çš„åç½®è®¾ç½®ä¸ºrobertaæ¨¡å‹çš„ç¼–ç å™¨lm_headçš„åç½®
        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias

    # æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚
    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)  # æ‰¹æ¬¡å¤§å°ä¸º1

    æˆ‘ä»¬çš„è¾“å‡º = æ¨¡å‹ï¼ˆè¾“å…¥idsï¼‰[0]
    å¦‚æœæœ‰åˆ†ç±»å¤´
        ä»–ä»¬çš„è¾“å‡º = roberta.model.classification_heads["mnli"](roberta.extract_features(input_ids))
    å¦åˆ™
        ä»–ä»¬çš„è¾“å‡º = roberta.model(input_ids)[0]
    æ‰“å°æˆ‘ä»¬çš„è¾“å‡ºå½¢çŠ¶å’Œä»–ä»¬çš„è¾“å‡ºå½¢çŠ¶
    print(our_output.shape, their_output.shape)
    æœ€å¤§ç»å¯¹å·® = torch.max(torch.abs(our_output - their_output)).item()
    æ‰“å°æœ€å¤§ç»å¯¹å·®
    print(f"max_absolute_diff = {max_absolute_diff}")  # ~ 1e-7
    æˆåŠŸ = torch.allclose(our_output, their_output, atol=1e-3)
    æ‰“å°"ä¸¤ä¸ªæ¨¡å‹æ˜¯å¦è¾“å‡ºç›¸åŒçš„å¼ é‡ï¼Ÿ"ï¼Œå¦‚æœæˆåŠŸåˆ™è¾“å‡ºğŸ”¥ï¼Œå¦åˆ™è¾“å‡ºğŸ’©
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")
    å¦‚æœä¸æˆåŠŸ
        å¼•å‘å¼‚å¸¸
        raise Exception("Something went wRoNg")

    åˆ›å»ºè·¯å¾„ä¸ºpytorch_dump_folder_pathçš„æ–‡ä»¶å¤¹ï¼Œå¦‚æœçˆ¶æ–‡ä»¶å¤¹ä¸å­˜åœ¨ä¹Ÿè¿›è¡Œåˆ›å»º
    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
    æ‰“å°å°†æ¨¡å‹ä¿å­˜åˆ°pytorch_dump_folder_path
    print(f"Saving model to {pytorch_dump_folder_path}")
    å°†æ¨¡å‹ä¿å­˜åˆ°pytorch_dump_folder_path
    model.save_pretrained(pytorch_dump_folder_path)
# å¦‚æœå½“å‰è„šæœ¬ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€å‚æ•°ï¼šRoBERTa æ¨¡å‹çš„æ£€æŸ¥ç‚¹è·¯å¾„
    parser.add_argument(
        "--roberta_checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    # æ·»åŠ å¿…éœ€å‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    # æ·»åŠ å¯é€‰å‚æ•°ï¼šæ˜¯å¦è½¬æ¢æœ€ç»ˆçš„åˆ†ç±»å¤´
    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°å°† XLM-RoBERTa XL æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
    convert_xlm_roberta_xl_checkpoint_to_pytorch(
        args.roberta_checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
```