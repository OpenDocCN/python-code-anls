# `.\models\fastspeech2_conformer\convert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py`

```
# è®¾ç½®ä»£ç æ–‡ä»¶çš„ç¼–ç æ ¼å¼ä¸º UTF-8

# å¯¼å…¥ argparse æ¨¡å—ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°
import argparse

# å¯¼å…¥ json æ¨¡å—ï¼Œç”¨äºå¤„ç† JSON æ ¼å¼æ•°æ®
import json

# å¯¼å…¥ re æ¨¡å—ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œ
import re

# ä» pathlib æ¨¡å—ä¸­å¯¼å…¥ Path ç±»ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„
from pathlib import Path

# ä» tempfile æ¨¡å—ä¸­å¯¼å…¥ TemporaryDirectory ç±»ï¼Œç”¨äºåˆ›å»ºä¸´æ—¶ç›®å½•
from tempfile import TemporaryDirectory

# å¯¼å…¥ torch åº“ï¼Œç”¨äºå¤„ç† PyTorch ç›¸å…³åŠŸèƒ½
import torch

# å¯¼å…¥ yaml æ¨¡å—ï¼Œç”¨äºå¤„ç† YAML æ ¼å¼æ•°æ®
import yaml

# ä» transformers åº“ä¸­å¯¼å…¥ä»¥ä¸‹ç±»å’Œå‡½æ•°
from transformers import (
    FastSpeech2ConformerConfig,        # FastSpeech2ConformerConfig ç±»ï¼Œç”¨äºé…ç½® FastSpeech2Conformer æ¨¡å‹
    FastSpeech2ConformerModel,        # FastSpeech2ConformerModel ç±»ï¼ŒFastSpeech2Conformer æ¨¡å‹
    FastSpeech2ConformerTokenizer,    # FastSpeech2ConformerTokenizer ç±»ï¼ŒFastSpeech2Conformer æ¨¡å‹çš„åˆ†è¯å™¨
    logging                           # logging æ¨¡å—ï¼Œç”¨äºæ—¥å¿—è®°å½•
)

# è®¾ç½® logging æ¨¡å—çš„è¯¦ç»†ç¨‹åº¦ä¸º info
logging.set_verbosity_info()

# è·å–æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½• FastSpeech2Conformer æ¨¡å‹ç›¸å…³çš„æ—¥å¿—ä¿¡æ¯
logger = logging.get_logger("transformers.models.FastSpeech2Conformer")

# å®šä¹‰ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå°†é…ç½®å‚æ•°æ˜ å°„åˆ°æ–°çš„å‘½åæ ¼å¼ï¼Œç”¨äºå…¼å®¹æ—§çš„é…ç½®
CONFIG_MAPPING = {
    "adim": "hidden_size",                                    # adim æ˜ å°„åˆ° hidden_size
    "aheads": "num_attention_heads",                          # aheads æ˜ å°„åˆ° num_attention_heads
    "conformer_dec_kernel_size": "decoder_kernel_size",       # conformer_dec_kernel_size æ˜ å°„åˆ° decoder_kernel_size
    "conformer_enc_kernel_size": "encoder_kernel_size",       # conformer_enc_kernel_size æ˜ å°„åˆ° encoder_kernel_size
    "decoder_normalize_before": "decoder_normalize_before",   # decoder_normalize_before æ˜ å°„åˆ° decoder_normalize_before
    "dlayers": "decoder_layers",                              # dlayers æ˜ å°„åˆ° decoder_layers
    "dunits": "decoder_linear_units",                         # dunits æ˜ å°„åˆ° decoder_linear_units
    "duration_predictor_chans": "duration_predictor_channels",# duration_predictor_chans æ˜ å°„åˆ° duration_predictor_channels
    "duration_predictor_kernel_size": "duration_predictor_kernel_size",  # duration_predictor_kernel_size æ˜ å°„åˆ° duration_predictor_kernel_size
    "duration_predictor_layers": "duration_predictor_layers",# duration_predictor_layers æ˜ å°„åˆ° duration_predictor_layers
    "elayers": "encoder_layers",                              # elayers æ˜ å°„åˆ° encoder_layers
    "encoder_normalize_before": "encoder_normalize_before",   # encoder_normalize_before æ˜ å°„åˆ° encoder_normalize_before
    "energy_embed_dropout": "energy_embed_dropout",           # energy_embed_dropout æ˜ å°„åˆ° energy_embed_dropout
    "energy_embed_kernel_size": "energy_embed_kernel_size",   # energy_embed_kernel_size æ˜ å°„åˆ° energy_embed_kernel_size
    "energy_predictor_chans": "energy_predictor_channels",    # energy_predictor_chans æ˜ å°„åˆ° energy_predictor_channels
    "energy_predictor_dropout": "energy_predictor_dropout",   # energy_predictor_dropout æ˜ å°„åˆ° energy_predictor_dropout
    "energy_predictor_kernel_size": "energy_predictor_kernel_size",  # energy_predictor_kernel_size æ˜ å°„åˆ° energy_predictor_kernel_size
    "energy_predictor_layers": "energy_predictor_layers",     # energy_predictor_layers æ˜ å°„åˆ° energy_predictor_layers
    "eunits": "encoder_linear_units",                         # eunits æ˜ å°„åˆ° encoder_linear_units
    "pitch_embed_dropout": "pitch_embed_dropout",             # pitch_embed_dropout æ˜ å°„åˆ° pitch_embed_dropout
    "pitch_embed_kernel_size": "pitch_embed_kernel_size",     # pitch_embed_kernel_size æ˜ å°„åˆ° pitch_embed_kernel_size
    "pitch_predictor_chans": "pitch_predictor_channels",      # pitch_predictor_chans æ˜ å°„åˆ° pitch_predictor_channels
    "pitch_predictor_dropout": "pitch_predictor_dropout",     # pitch_predictor_dropout æ˜ å°„åˆ° pitch_predictor_dropout
    "pitch_predictor_kernel_size": "pitch_predictor_kernel_size",  # pitch_predictor_kernel_size æ˜ å°„åˆ° pitch_predictor_kernel_size
    "pitch_predictor_layers": "pitch_predictor_layers",       # pitch_predictor_layers æ˜ å°„åˆ° pitch_predictor_layers
    "positionwise_conv_kernel_size": "positionwise_conv_kernel_size",  # positionwise_conv_kernel_size æ˜ å°„åˆ° positionwise_conv_kernel_size
    "postnet_chans": "speech_decoder_postnet_units",          # postnet_chans æ˜ å°„åˆ° speech_decoder_postnet_units
    "postnet_filts": "speech_decoder_postnet_kernel",         # postnet_filts æ˜ å°„åˆ° speech_decoder_postnet_kernel
    "postnet_layers": "speech_decoder_postnet_layers",        # postnet_layers æ˜ å°„åˆ° speech_decoder_postnet_layers
    "reduction_factor": "reduction_factor",                   # reduction_factor æ˜ å°„åˆ° reduction_factor
    "stop_gradient_from_energy_predictor": "stop_gradient_from_energy_predictor",  # stop_gradient_from_energy_predictor æ˜ å°„åˆ° stop_gradient_from_energy_predictor
    "stop_gradient_from_pitch_predictor": "stop_gradient_from_pitch_predictor",    # stop_gradient_from_pitch_predictor æ˜ å°„åˆ° stop_gradient_from_pitch_predictor
    "transformer_dec_attn_dropout_rate": "decoder_attention_dropout_rate",  # transformer_dec_attn_dropout_rate æ˜ å°„åˆ° decoder_attention_dropout_rate
    "transformer_dec_dropout_rate": "decoder_dropout_rate",   # transformer_dec_dropout_rate æ˜ å°„åˆ° decoder_dropout_rate
    "transformer_dec_positional_dropout_rate": "decoder_positional_dropout_rate",
    # å°†é…ç½®ä¸­çš„ "transformer_dec_positional_dropout_rate" æ˜ å°„ä¸º "decoder_positional_dropout_rate"

    "transformer_enc_attn_dropout_rate": "encoder_attention_dropout_rate",
    # å°†é…ç½®ä¸­çš„ "transformer_enc_attn_dropout_rate" æ˜ å°„ä¸º "encoder_attention_dropout_rate"

    "transformer_enc_dropout_rate": "encoder_dropout_rate",
    # å°†é…ç½®ä¸­çš„ "transformer_enc_dropout_rate" æ˜ å°„ä¸º "encoder_dropout_rate"

    "transformer_enc_positional_dropout_rate": "encoder_positional_dropout_rate",
    # å°†é…ç½®ä¸­çš„ "transformer_enc_positional_dropout_rate" æ˜ å°„ä¸º "encoder_positional_dropout_rate"

    "use_cnn_in_conformer": "use_cnn_in_conformer",
    # æŒ‡ç¤ºæ˜¯å¦åœ¨ Conformer æ¨¡å‹ä¸­ä½¿ç”¨ CNN

    "use_macaron_style_in_conformer": "use_macaron_style_in_conformer",
    # æŒ‡ç¤ºæ˜¯å¦åœ¨ Conformer æ¨¡å‹ä¸­ä½¿ç”¨ Macaron é£æ ¼çš„ç»“æ„

    "use_masking": "use_masking",
    # æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨æ©ç æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒ

    "use_weighted_masking": "use_weighted_masking",
    # æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨åŠ æƒæ©ç è¿›è¡Œæ¨¡å‹è®­ç»ƒ

    "idim": "input_dim",
    # è¾“å…¥æ•°æ®çš„ç»´åº¦

    "odim": "num_mel_bins",
    # æ¢…å°”é¢‘è°±å›¾çš„é¢‘é“æ•°

    "spk_embed_dim": "speaker_embed_dim",
    # è¯´è¯äººåµŒå…¥å‘é‡çš„ç»´åº¦

    "langs": "num_languages",
    # è¯­è¨€çš„æ•°é‡

    "spks": "num_speakers",
    # è¯´è¯äººçš„æ•°é‡
}

# é‡æ–°æ˜ å°„æ¨¡å‹çš„ YAML é…ç½®æ–‡ä»¶
def remap_model_yaml_config(yaml_config_path):
    # æ‰“å¼€å¹¶è¯»å– YAML é…ç½®æ–‡ä»¶
    with Path(yaml_config_path).open("r", encoding="utf-8") as f:
        # ä½¿ç”¨ yaml.safe_load å°† YAML æ–‡ä»¶å†…å®¹åŠ è½½ä¸º Python å¯¹è±¡
        args = yaml.safe_load(f)
        # å°†åŠ è½½çš„å‚æ•°è½¬æ¢ä¸º argparse.Namespace å¯¹è±¡
        args = argparse.Namespace(**args)

    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„é‡æ–°æ˜ å°„é…ç½®å­—å…¸
    remapped_config = {}

    # è·å–æ¨¡å‹å‚æ•°ä¸­çš„æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢å™¨å‚æ•°
    model_params = args.tts_conf["text2mel_params"]
    # ä½¿ç”¨ CONFIG_MAPPING å­—å…¸è¿›è¡Œå‚æ•°é‡æ˜ å°„ï¼ŒæœªåŒ…å«çš„é”®ä¼šè¢«å¿½ç•¥
    for espnet_config_key, hf_config_key in CONFIG_MAPPING.items():
        # å¦‚æœ espnet_config_key å­˜åœ¨äºæ¨¡å‹å‚æ•°ä¸­
        if espnet_config_key in model_params:
            # å°†æ˜ å°„åçš„å‚æ•°åŠ å…¥åˆ° remapped_config å­—å…¸ä¸­
            remapped_config[hf_config_key] = model_params[espnet_config_key]

    # è¿”å›é‡æ–°æ˜ å°„åçš„é…ç½®å­—å…¸ï¼Œä»¥åŠ args å¯¹è±¡ä¸­çš„ g2p å’Œ token_list å±æ€§
    return remapped_config, args.g2p, args.token_list


def convert_espnet_state_dict_to_hf(state_dict):
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„æ–°çŠ¶æ€å­—å…¸
    new_state_dict = {}
    # éå†ç»™å®šçš„çŠ¶æ€å­—å…¸ï¼ˆstate_dictï¼‰ä¸­çš„æ¯ä¸ªé”®
    for key in state_dict:
        # å¦‚æœé”®ååŒ…å«ç‰¹å®šå­ä¸² "tts.generator.text2mel."
        if "tts.generator.text2mel." in key:
            # å»é™¤é”®åä¸­çš„ "tts.generator.text2mel."ï¼Œå¾—åˆ°æ–°çš„é”®å
            new_key = key.replace("tts.generator.text2mel.", "")
            
            # å¦‚æœé”®ååŒ…å« "postnet"
            if "postnet" in key:
                # ä¿®æ”¹æ–°é”®åï¼Œå°† "postnet.postnet" æ›¿æ¢ä¸º "speech_decoder_postnet.layers"
                new_key = new_key.replace("postnet.postnet", "speech_decoder_postnet.layers")
                # æ ¹æ®é”®åç»“å°¾ä¸åŒçš„åç¼€ï¼Œè°ƒæ•´ä¸ºç‰¹å®šçš„å‘½åæ ¼å¼
                new_key = new_key.replace(".0.weight", ".conv.weight")
                new_key = new_key.replace(".1.weight", ".batch_norm.weight")
                new_key = new_key.replace(".1.bias", ".batch_norm.bias")
                new_key = new_key.replace(".1.running_mean", ".batch_norm.running_mean")
                new_key = new_key.replace(".1.running_var", ".batch_norm.running_var")
                new_key = new_key.replace(".1.num_batches_tracked", ".batch_norm.num_batches_tracked")
            
            # å¦‚æœé”®ååŒ…å« "feat_out"
            if "feat_out" in key:
                # æ ¹æ®é”®åæ˜¯å¦åŒ…å« "weight" æˆ– "bias"ï¼Œç¡®å®šæ–°é”®å
                if "weight" in key:
                    new_key = "speech_decoder_postnet.feat_out.weight"
                if "bias" in key:
                    new_key = "speech_decoder_postnet.feat_out.bias"
            
            # å¦‚æœé”®åä¸º "encoder.embed.0.weight"
            if "encoder.embed.0.weight" in key:
                # å°† "0." æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå¾—åˆ°æ–°é”®å
                new_key = new_key.replace("0.", "")
            
            # å¦‚æœé”®ååŒ…å« "w_1"
            if "w_1" in key:
                # å°† "w_1" æ›¿æ¢ä¸º "conv1"
                new_key = new_key.replace("w_1", "conv1")
            
            # å¦‚æœé”®ååŒ…å« "w_2"
            if "w_2" in key:
                # å°† "w_2" æ›¿æ¢ä¸º "conv2"
                new_key = new_key.replace("w_2", "conv2")
            
            # å¦‚æœé”®ååŒ…å« "predictor.conv"
            if "predictor.conv" in key:
                # å°† ".conv" æ›¿æ¢ä¸º ".conv_layers"
                new_key = new_key.replace(".conv", ".conv_layers")
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼å’Œæ›¿æ¢è§„åˆ™æ¥è°ƒæ•´æ–°é”®åçš„æ ¼å¼
                pattern = r"(\d)\.(\d)"
                replacement = (
                    r"\1.conv" if ("2.weight" not in new_key) and ("2.bias" not in new_key) else r"\1.layer_norm"
                )
                new_key = re.sub(pattern, replacement, new_key)
            
            # å¦‚æœé”®åä¸­åŒ…å« "pitch_embed" æˆ– "energy_embed"
            if "pitch_embed" in key or "energy_embed" in key:
                # å°† "0" æ›¿æ¢ä¸º "conv"
                new_key = new_key.replace("0", "conv")
            
            # å¦‚æœé”®åä¸­åŒ…å« "encoders"
            if "encoders" in key:
                # æ›¿æ¢é”®åä¸­çš„ "encoders" ä¸º "conformer_layers"
                new_key = new_key.replace("encoders", "conformer_layers")
                # æ›¿æ¢å…¶ä»–ç‰¹å®šçš„åç¼€éƒ¨åˆ†ä¸ºå¯¹åº”çš„æ–°å‘½åæ ¼å¼
                new_key = new_key.replace("norm_final", "final_layer_norm")
                new_key = new_key.replace("norm_mha", "self_attn_layer_norm")
                new_key = new_key.replace("norm_ff_macaron", "ff_macaron_layer_norm")
                new_key = new_key.replace("norm_ff", "ff_layer_norm")
                new_key = new_key.replace("norm_conv", "conv_layer_norm")
            
            # å¦‚æœé”®åä¸­åŒ…å« "lid_emb"
            if "lid_emb" in key:
                # å°† "lid_emb" æ›¿æ¢ä¸º "language_id_embedding"
                new_key = new_key.replace("lid_emb", "language_id_embedding")
            
            # å¦‚æœé”®åä¸­åŒ…å« "sid_emb"
            if "sid_emb" in key:
                # å°† "sid_emb" æ›¿æ¢ä¸º "speaker_id_embedding"
                new_key = new_key.replace("sid_emb", "speaker_id_embedding")
            
            # å°†æ–°çš„é”®åä¸åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„å€¼å¯¹åº”èµ·æ¥ï¼Œæ·»åŠ åˆ°æ–°çš„çŠ¶æ€å­—å…¸ä¸­
            new_state_dict[new_key] = state_dict[key]

    # è¿”å›ç»è¿‡ä¿®æ”¹åçš„æ–°çŠ¶æ€å­—å…¸
    return new_state_dict
# ä½¿ç”¨è£…é¥°å™¨ @torch.no_grad() æ¥ç¡®ä¿åœ¨æ­¤å‡½æ•°ä¸­ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—
@torch.no_grad()
# å®šä¹‰å‡½æ•°ï¼Œå°† FastSpeech2Conformer æ¨¡å‹çš„æ£€æŸ¥ç‚¹è½¬æ¢ä¸º PyTorch æ¨¡å‹
def convert_FastSpeech2ConformerModel_checkpoint(
    checkpoint_path,  # åŸå§‹æ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„
    yaml_config_path,  # æ¨¡å‹é…ç½®æ–‡ä»¶ config.yaml çš„è·¯å¾„
    pytorch_dump_folder_path,  # è¾“å‡ºçš„ PyTorch æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
    repo_id=None,  # å¯é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šä¸Šä¼ åˆ° ğŸ¤— hub çš„ repo ID
):
    # è°ƒç”¨å‡½æ•° remap_model_yaml_config è¯»å–æ¨¡å‹å‚æ•°ã€åˆ†è¯å™¨åç§°åŠè¯æ±‡è¡¨
    model_params, tokenizer_name, vocab = remap_model_yaml_config(yaml_config_path)
    
    # æ ¹æ®è¯»å–çš„æ¨¡å‹å‚æ•°åˆ›å»º FastSpeech2ConformerConfig é…ç½®å¯¹è±¡
    config = FastSpeech2ConformerConfig(**model_params)

    # æ ¹æ®é…ç½®å¯¹è±¡åˆ›å»º FastSpeech2ConformerModel æ¨¡å‹
    model = FastSpeech2ConformerModel(config)

    # åŠ è½½ ESPnet æ¨¡å‹çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
    espnet_checkpoint = torch.load(checkpoint_path)
    # å°† ESPnet æ¨¡å‹çš„çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºé€‚ç”¨äº Hugging Face çš„çŠ¶æ€å­—å…¸æ ¼å¼
    hf_compatible_state_dict = convert_espnet_state_dict_to_hf(espnet_checkpoint)

    # å°†è½¬æ¢åçš„çŠ¶æ€å­—å…¸åŠ è½½åˆ°æ¨¡å‹ä¸­
    model.load_state_dict(hf_compatible_state_dict)

    # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šçš„ PyTorch æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ä¸­
    model.save_pretrained(pytorch_dump_folder_path)

    # å‡†å¤‡åˆ†è¯å™¨
    with TemporaryDirectory() as tempdir:
        # åˆ›å»ºè¯æ±‡è¡¨çš„ç´¢å¼•æ˜ å°„
        vocab = {token: id for id, token in enumerate(vocab)}
        # åˆ›å»ºè¯æ±‡è¡¨æ–‡ä»¶çš„è·¯å¾„
        vocab_file = Path(tempdir) / "vocab.json"
        # å°†è¯æ±‡è¡¨å†™å…¥åˆ° JSON æ–‡ä»¶ä¸­
        with open(vocab_file, "w") as f:
            json.dump(vocab, f)
        
        # ç¡®å®šæ˜¯å¦éœ€è¦å»é™¤ç©ºæ ¼
        should_strip_spaces = "no_space" in tokenizer_name
        # ä½¿ç”¨ FastSpeech2ConformerTokenizer åˆ›å»ºåˆ†è¯å™¨å¯¹è±¡
        tokenizer = FastSpeech2ConformerTokenizer(str(vocab_file), should_strip_spaces=should_strip_spaces)

    # å°†åˆ†è¯å™¨ä¿å­˜åˆ°æŒ‡å®šçš„ PyTorch æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ä¸­
    tokenizer.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœæä¾›äº† repo_idï¼Œå°†æ¨¡å‹å’Œåˆ†è¯å™¨æ¨é€åˆ° ğŸ¤— hub ä¸Š
    if repo_id:
        print("Pushing to the hub...")
        model.push_to_hub(repo_id)
        tokenizer.push_to_hub(repo_id)


if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šåŸå§‹æ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„
    parser.add_argument("--checkpoint_path", required=True, default=None, type=str, help="Path to original checkpoint")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ¨¡å‹é…ç½®æ–‡ä»¶ config.yaml çš„è·¯å¾„
    parser.add_argument("--yaml_config_path", required=True, default=None, type=str, help="Path to config.yaml of model to convert")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè¾“å‡ºçš„ PyTorch æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument("--pytorch_dump_folder_path", required=True, default=None, type=str, help="Path to the output PyTorch model.")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šå¯é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šä¸Šä¼ åˆ° ğŸ¤— hub çš„ repo ID
    parser.add_argument("--push_to_hub", default=None, type=str, help="Where to upload the converted model on the ğŸ¤— hub.")

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨è½¬æ¢å‡½æ•°ï¼Œä¼ å…¥è§£æåçš„å‘½ä»¤è¡Œå‚æ•°
    convert_FastSpeech2ConformerModel_checkpoint(
        args.checkpoint_path,
        args.yaml_config_path,
        args.pytorch_dump_folder_path,
        args.push_to_hub,
    )
```