# `.\models\fastspeech2_conformer\convert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜
# è¯¥ä»£ç ç‰ˆæƒå½’ 2023 å¹´çš„ HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰
# æ ¹æ® Apache è®¸å¯è¯ 2.0 è¿›è¡Œè®¸å¯
# é™¤éç¬¦åˆè®¸å¯è¯è§„å®šï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬
# http://www.apache.org/licenses/LICENSE-2.0
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æœ¬è½¯ä»¶æŒ‰"åŸæ ·"åˆ†å‘ï¼Œ
# æ— ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä»»ä½•éšå«çš„æ‹…ä¿
# æˆ–é€‚é”€æ€§ã€ç‰¹å®šç”¨é€”é€‚ç”¨æ€§å’Œéä¾µæƒæ€§çš„æ‹…ä¿
"""å°† FastSpeech2Conformer æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºå…¶ä»–æ ¼å¼ã€‚"""

# å¯¼å…¥æ¨¡å—
import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°çš„åº“
import json  # ç”¨äºå¤„ç† JSON æ ¼å¼çš„æ•°æ®çš„åº“
import re  # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„åº“
from pathlib import Path  # ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„çš„åº“
from tempfile import TemporaryDirectory  # ç”¨äºåˆ›å»ºä¸´æ—¶ç›®å½•çš„åº“

import torch  # PyTorch æ·±åº¦å­¦ä¹ åº“

import yaml  # ç”¨äºå¤„ç† YAML æ ¼å¼çš„æ•°æ®çš„åº“

from transformers import (  # ä» transformers åº“ä¸­å¯¼å…¥ä»¥ä¸‹ç±»å’Œå‡½æ•°
    FastSpeech2ConformerConfig,  # FastSpeech2Conformer æ¨¡å‹é…ç½®ç±»
    FastSpeech2ConformerModel,  # FastSpeech2Conformer æ¨¡å‹ç±»
    FastSpeech2ConformerTokenizer,  # FastSpeech2Conformer æ¨¡å‹åˆ†è¯å™¨ç±»
    logging,  # transformers åº“ä¸­çš„æ—¥å¿—è®°å½•æ¨¡å—
)

# è®¾ç½®æ—¥å¿—è®°å½•çº§åˆ«ä¸º info
logging.set_verbosity_info()
# è·å– logger å¯¹è±¡
logger = logging.get_logger("transformers.models.FastSpeech2Conformer")

# å®šä¹‰é…ç½®æ˜ å°„å…³ç³»ï¼Œå°†æ—§é…ç½®åç§°æ˜ å°„åˆ°æ–°é…ç½®åç§°
CONFIG_MAPPING = {
    "adim": "hidden_size",  # éšè—å±‚å¤§å°
    "aheads": "num_attention_heads",  # æ³¨æ„åŠ›å¤´æ•°
    "conformer_dec_kernel_size": "decoder_kernel_size",  # è§£ç å™¨å†…æ ¸å¤§å°
    "conformer_enc_kernel_size": "encoder_kernel_size",  # ç¼–ç å™¨å†…æ ¸å¤§å°
    "decoder_normalize_before": "decoder_normalize_before",  # è§£ç å™¨å‰å½’ä¸€åŒ–
    "dlayers": "decoder_layers",  # è§£ç å™¨å±‚æ•°
    "dunits": "decoder_linear_units",  # è§£ç å™¨çº¿æ€§å•å…ƒæ•°
    "duration_predictor_chans": "duration_predictor_channels",  # æ—¶é•¿é¢„æµ‹å™¨é€šé“æ•°
    "duration_predictor_kernel_size": "duration_predictor_kernel_size",  # æ—¶é•¿é¢„æµ‹å™¨å†…æ ¸å¤§å°
    "duration_predictor_layers": "duration_predictor_layers",  # æ—¶é•¿é¢„æµ‹å™¨å±‚æ•°
    "elayers": "encoder_layers",  # ç¼–ç å™¨å±‚æ•°
    "encoder_normalize_before": "encoder_normalize_before",  # ç¼–ç å™¨å‰å½’ä¸€åŒ–
    "energy_embed_dropout": "energy_embed_dropout",  # èƒ½é‡åµŒå…¥ä¸¢å¤±ç‡
    "energy_embed_kernel_size": "energy_embed_kernel_size",  # èƒ½é‡åµŒå…¥å†…æ ¸å¤§å°
    "energy_predictor_chans": "energy_predictor_channels",  # èƒ½é‡é¢„æµ‹å™¨é€šé“æ•°
    "energy_predictor_dropout": "energy_predictor_dropout",  # èƒ½é‡é¢„æµ‹å™¨ä¸¢å¤±ç‡
    "energy_predictor_kernel_size": "energy_predictor_kernel_size",  # èƒ½é‡é¢„æµ‹å™¨å†…æ ¸å¤§å°
    "energy_predictor_layers": "energy_predictor_layers",  # èƒ½é‡é¢„æµ‹å™¨å±‚æ•°
    "eunits": "encoder_linear_units",  # ç¼–ç å™¨çº¿æ€§å•å…ƒæ•°
    "pitch_embed_dropout": "pitch_embed_dropout",  # éŸ³é«˜åµŒå…¥ä¸¢å¤±ç‡
    "pitch_embed_kernel_size": "pitch_embed_kernel_size",  # éŸ³é«˜åµŒå…¥å†…æ ¸å¤§å°
    "pitch_predictor_chans": "pitch_predictor_channels",  # éŸ³é«˜é¢„æµ‹å™¨é€šé“æ•°
    "pitch_predictor_dropout": "pitch_predictor_dropout",  # éŸ³é«˜é¢„æµ‹å™¨ä¸¢å¤±ç‡
    "pitch_predictor_kernel_size": "pitch_predictor_kernel_size",  # éŸ³é«˜é¢„æµ‹å™¨å†…æ ¸å¤§å°
    "pitch_predictor_layers": "pitch_predictor_layers",  # éŸ³é«˜é¢„æµ‹å™¨å±‚æ•°
    "positionwise_conv_kernel_size": "positionwise_conv_kernel_size",  # ä½ç½®å·ç§¯å†…æ ¸å¤§å°
    "postnet_chans": "speech_decoder_postnet_units",  # åç½‘ç»œé€šé“æ•°
    "postnet_filts": "speech_decoder_postnet_kernel",  # åç½‘ç»œå†…æ ¸å¤§å°
    "postnet_layers": "speech_decoder_postnet_layers",  # åç½‘ç»œå±‚æ•°
    "reduction_factor": "reduction_factor",  # ç¼©å°å› å­
    "stop_gradient_from_energy_predictor": "stop_gradient_from_energy_predictor",  # ä»èƒ½é‡é¢„æµ‹å™¨åœæ­¢æ¢¯åº¦
    "stop_gradient_from_pitch_predictor": "stop_gradient_from_pitch_predictor",  # ä»éŸ³é«˜é¢„æµ‹å™¨åœæ­¢æ¢¯åº¦
    "transformer_dec_attn_dropout_rate": "decoder_attention_dropout_rate",  # è§£ç å™¨æ³¨æ„åŠ›ä¸¢å¤±ç‡
    "transformer_dec_dropout_rate": "decoder_dropout_rate",  # è§£ç å™¨ä¸¢å¤±ç‡
}
    "transformer_dec_positional_dropout_rate": "decoder_positional_dropout_rate",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"transformer_dec_positional_dropout_rate"æ˜ å°„ä¸º"decoder_positional_dropout_rate"
    
    "transformer_enc_attn_dropout_rate": "encoder_attention_dropout_rate",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"transformer_enc_attn_dropout_rate"æ˜ å°„ä¸º"encoder_attention_dropout_rate"
    
    "transformer_enc_dropout_rate": "encoder_dropout_rate",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"transformer_enc_dropout_rate"æ˜ å°„ä¸º"encoder_dropout_rate"
    
    "transformer_enc_positional_dropout_rate": "encoder_positional_dropout_rate",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"transformer_enc_positional_dropout_rate"æ˜ å°„ä¸º"encoder_positional_dropout_rate"
    
    "use_cnn_in_conformer": "use_cnn_in_conformer",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"use_cnn_in_conformer"æ˜ å°„ä¸º"use_cnn_in_conformer"
    
    "use_macaron_style_in_conformer": "use_macaron_style_in_conformer",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"use_macaron_style_in_conformer"æ˜ å°„ä¸º"use_macaron_style_in_conformer"
    
    "use_masking": "use_masking",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"use_masking"æ˜ å°„ä¸º"use_masking"
    
    "use_weighted_masking": "use_weighted_masking",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"use_weighted_masking"æ˜ å°„ä¸º"use_weighted_masking"
    
    "idim": "input_dim",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"idim"æ˜ å°„ä¸º"input_dim"
    
    "odim": "num_mel_bins",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"odim"æ˜ å°„ä¸º"num_mel_bins"
    
    "spk_embed_dim": "speaker_embed_dim",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"spk_embed_dim"æ˜ å°„ä¸º"speaker_embed_dim"
    
    "langs": "num_languages",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"langs"æ˜ å°„ä¸º"num_languages"
    
    "spks": "num_speakers",  
    # å°†é…ç½®å‚æ•°ä¸­çš„"spks"æ˜ å°„ä¸º"num_speakers"
# é‡æ–°æ˜ å°„ ESPnet æ¨¡å‹çš„ YAML é…ç½®æ–‡ä»¶
def remap_model_yaml_config(yaml_config_path):
    # ä½¿ç”¨ UTF-8 ç¼–ç æ‰“å¼€ YAML é…ç½®æ–‡ä»¶
    with Path(yaml_config_path).open("r", encoding="utf-8") as f:
        # ä½¿ç”¨ PyYAML åŠ è½½ YAML æ–‡ä»¶å†…å®¹ï¼Œå¹¶è½¬æ¢ä¸ºå­—å…¸ç±»å‹çš„å‚æ•°
        args = yaml.safe_load(f)
        # å°†å‚æ•°å­—å…¸è½¬æ¢ä¸º argparse.Namespace å¯¹è±¡
        args = argparse.Namespace(**args)

    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸æ¥å­˜å‚¨é‡æ–°æ˜ å°„åçš„é…ç½®
    remapped_config = {}

    # ä»å‚æ•°ä¸­è·å– TTS é…ç½®ä¸­çš„ text2mel_params
    model_params = args.tts_conf["text2mel_params"]
    # éå† CONFIG_MAPPING ä¸­çš„æ˜ å°„å…³ç³»ï¼Œå°† ESPnet é…ç½®é”®æ˜ å°„åˆ° Hugging Face é…ç½®é”®
    for espnet_config_key, hf_config_key in CONFIG_MAPPING.items():
        # å¦‚æœ ESPnet é…ç½®é”®åœ¨æ¨¡å‹å‚æ•°ä¸­å­˜åœ¨
        if espnet_config_key in model_params:
            # å°† ESPnet çš„é…ç½®é”®åŠå…¶å¯¹åº”å€¼æ˜ å°„åˆ° Hugging Face çš„é…ç½®é”®åŠå…¶å¯¹åº”å€¼
            remapped_config[hf_config_key] = model_params[espnet_config_key]

    # è¿”å›é‡æ–°æ˜ å°„åçš„é…ç½®ã€args.g2p å’Œ args.token_list
    return remapped_config, args.g2p, args.token_list


# å°† ESPnet çš„çŠ¶æ€å­—å…¸è½¬æ¢ä¸º Hugging Face çš„çŠ¶æ€å­—å…¸
def convert_espnet_state_dict_to_hf(state_dict):
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸æ¥å­˜å‚¨æ–°çš„çŠ¶æ€å­—å…¸
    new_state_dict = {}
    # éå†ç»™å®šçš„ state_dict å­—å…¸
    for key in state_dict:
        # æ£€æŸ¥å½“å‰ key æ˜¯å¦åŒ…å«ç‰¹å®šå­—ç¬¦ä¸²
        if "tts.generator.text2mel." in key:
            # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
            new_key = key.replace("tts.generator.text2mel.", "")
            # å¦‚æœ key ä¸­åŒ…å« "postnet"
            if "postnet" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²ï¼Œå¹¶è°ƒæ•´æ ¼å¼
                new_key = new_key.replace("postnet.postnet", "speech_decoder_postnet.layers")
                new_key = new_key.replace(".0.weight", ".conv.weight")
                new_key = new_key.replace(".1.weight", ".batch_norm.weight")
                new_key = new_key.replace(".1.bias", ".batch_norm.bias")
                new_key = new_key.replace(".1.running_mean", ".batch_norm.running_mean")
                new_key = new_key.replace(".1.running_var", ".batch_norm.running_var")
                new_key = new_key.replace(".1.num_batches_tracked", ".batch_norm.num_batches_tracked")
            # å¦‚æœ key ä¸­åŒ…å« "feat_out"
            if "feat_out" in key:
                # æ ¹æ® key ä¸­çš„ç‰¹å®šæ–‡æœ¬è®¾ç½®æ–°çš„ key
                if "weight" in key:
                    new_key = "speech_decoder_postnet.feat_out.weight"
                if "bias" in key:
                    new_key = "speech_decoder_postnet.feat_out.bias"
            # å¦‚æœ key ä¸­åŒ…å« "encoder.embed.0.weight"
            if "encoder.embed.0.weight" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
                new_key = new_key.replace("0.", "")
            # å¦‚æœ key ä¸­åŒ…å« "w_1"
            if "w_1" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
                new_key = new_key.replace("w_1", "conv1")
            # å¦‚æœ key ä¸­åŒ…å« "w_2"
            if "w_2" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
                new_key = new_key.replace("w_2", "conv2")
            # å¦‚æœ key ä¸­åŒ…å« "predictor.conv"
            if "predictor.conv" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²ï¼Œå¹¶æ ¹æ®æ¡ä»¶é€‰æ‹©æ›¿æ¢æ ¼å¼
                new_key = new_key.replace(".conv", ".conv_layers")
                pattern = r"(\d)\.(\d)"
                replacement = (
                    r"\1.conv" if ("2.weight" not in new_key) and ("2.bias" not in new_key) else r"\1.layer_norm"
                )
                new_key = re.sub(pattern, replacement, new_key)
            # å¦‚æœ key ä¸­åŒ…å« "pitch_embed" æˆ– "energy_embed"
            if "pitch_embed" in key or "energy_embed" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
                new_key = new_key.replace("0", "conv")
            # å¦‚æœ key ä¸­åŒ…å« "encoders"
            if "encoders" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
                new_key = new_key.replace("encoders", "conformer_layers")
                new_key = new_key.replace("norm_final", "final_layer_norm")
                new_key = new_key.replace("norm_mha", "self_attn_layer_norm")
                new_key = new_key.replace("norm_ff_macaron", "ff_macaron_layer_norm")
                new_key = new_key.replace("norm_ff", "ff_layer_norm")
                new_key = new_key.replace("norm_conv", "conv_layer_norm")
            # å¦‚æœ key ä¸­åŒ…å« "lid_emb"
            if "lid_emb" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
                new_key = new_key.replace("lid_emb", "language_id_embedding")
            # å¦‚æœ key ä¸­åŒ…å« "sid_emb"
            if "sid_emb" in key:
                # æ›¿æ¢ key ä¸­çš„ç‰¹å®šå­—ç¬¦ä¸²
                new_key = new_key.replace("sid_emb", "speaker_id_embedding")

            # å°†æ–°çš„ key å’Œå¯¹åº”çš„æ•°å€¼å­˜å…¥æ–°çš„ state_dict ä¸­
            new_state_dict[new_key] = state_dict[key]

    # è¿”å›å¤„ç†åçš„æ–° state_dict
    return new_state_dict
# å¯¼å…¥ç›¸å…³çš„åº“
@torch.no_grad()
# å¿½ç•¥æ¢¯åº¦è®¡ç®—
def convert_FastSpeech2ConformerModel_checkpoint(
    checkpoint_path, # è¦è½¬æ¢çš„åŸå§‹æ£€æŸ¥ç‚¹è·¯å¾„
    yaml_config_path, # è¦è½¬æ¢çš„æ¨¡å‹çš„é…ç½®æ–‡ä»¶è·¯å¾„
    pytorch_dump_folder_path, # è¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„
    repo_id=None, # è¦ä¸Šä¼ åˆ°æ¨¡å‹åº“çš„ IDï¼Œå¯é€‰
):
    # åŸºäºæ¨¡å‹é…ç½®æ–‡ä»¶é‡å»ºæ¨¡å‹å‚æ•°ã€åˆ†è¯å™¨åç§°å’Œè¯æ±‡è¡¨
    model_params, tokenizer_name, vocab = remap_model_yaml_config(yaml_config_path)
    # åŸºäºæ¨¡å‹å‚æ•°åˆ›å»º FastSpeech2ConformerConfig å¯¹è±¡
    config = FastSpeech2ConformerConfig(**model_params)

    # å‡†å¤‡æ¨¡å‹
    model = FastSpeech2ConformerModel(config)

    # åŠ è½½ ESPnet æ£€æŸ¥ç‚¹
    espnet_checkpoint = torch.load(checkpoint_path)
    # å°† ESPnet æ£€æŸ¥ç‚¹çš„å‚æ•°è½¬æ¢ä¸ºé€‚é… Hugging Face æ¨¡å‹çš„æ ¼å¼
    hf_compatible_state_dict = convert_espnet_state_dict_to_hf(espnet_checkpoint)

    # åŠ è½½æ¨¡å‹å‚æ•°
    model.load_state_dict(hf_compatible_state_dict)

    # ä¿å­˜ PyTorch æ¨¡å‹
    model.save_pretrained(pytorch_dump_folder_path)

    # å‡†å¤‡åˆ†è¯å™¨
    with TemporaryDirectory() as tempdir:
        # å°†è¯æ±‡è¡¨è½¬æ¢ä¸º ID åˆ°æ ‡è®°çš„æ˜ å°„
        vocab = {token: id for id, token in enumerate(vocab)}
        # åœ¨ä¸´æ—¶ç›®å½•ä¸‹åˆ›å»ºè¯æ±‡è¡¨æ–‡ä»¶
        vocab_file = Path(tempdir) / "vocab.json"
        with open(vocab_file, "w") as f:
            json.dump(vocab, f)
        # æ ¹æ®è¯æ±‡è¡¨æ–‡ä»¶å’Œåˆ†è¯å™¨åç§°åˆ›å»ºåˆ†è¯å™¨
        should_strip_spaces = "no_space" in tokenizer_name
        tokenizer = FastSpeech2ConformerTokenizer(str(vocab_file), should_strip_spaces=should_strip_spaces)

    # ä¿å­˜åˆ†è¯å™¨
    tokenizer.save_pretrained(pytorch_dump_folder_path)

    # å¦‚æœæœ‰æä¾› repo_idï¼Œåˆ™å°†æ¨¡å‹å’Œåˆ†è¯å™¨ä¸Šä¼ åˆ°æ¨¡å‹åº“
    if repo_id:
        # æ‰“å°æç¤ºä¿¡æ¯
        print("Pushing to the hub...")
        # å°†æ¨¡å‹ä¸Šä¼ åˆ°æ¨¡å‹åº“
        model.push_to_hub(repo_id)
        # å°†åˆ†è¯å™¨ä¸Šä¼ åˆ°æ¨¡å‹åº“
        tokenizer.push_to_hub(repo_id)


if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šåŸå§‹æ£€æŸ¥ç‚¹è·¯å¾„
    parser.add_argument("--checkpoint_path", required=True, default=None, type=str, help="Path to original checkpoint")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè¦è½¬æ¢çš„æ¨¡å‹çš„é…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument(
        "--yaml_config_path", required=True, default=None, type=str, help="Path to config.yaml of model to convert"
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„
    parser.add_argument(
        "--pytorch_dump_folder_path", required=True, default=None, type=str, help="Path to the output PyTorch model."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šä¸Šä¼ è½¬æ¢åçš„æ¨¡å‹åˆ°æ¨¡å‹åº“çš„ä½ç½®
    parser.add_argument(
        "--push_to_hub", default=None, type=str, help="Where to upload the converted model on the ğŸ¤— hub."
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    # è°ƒç”¨å‡½æ•°è¿›è¡Œæ¨¡å‹è½¬æ¢
    convert_FastSpeech2ConformerModel_checkpoint(
        args.checkpoint_path,
        args.yaml_config_path,
        args.pytorch_dump_folder_path,
        args.push_to_hub,
    )
```