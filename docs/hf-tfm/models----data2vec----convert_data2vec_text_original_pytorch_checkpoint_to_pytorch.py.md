# `.\models\data2vec\convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py`

```py
# å¼•å…¥æ‰€éœ€çš„æ¨¡å—å’Œåº“
import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import os  # ç”¨äºæ“ä½œç³»ç»Ÿç›¸å…³çš„åŠŸèƒ½
import pathlib  # æä¾›å¤„ç†æ–‡ä»¶å’Œç›®å½•è·¯å¾„çš„ç±»

import fairseq  # å¼•å…¥fairseqåº“
import torch  # å¼•å…¥PyTorchåº“
from fairseq.modules import TransformerSentenceEncoderLayer  # ä»fairseqæ¨¡å—ä¸­å¼•å…¥TransformerSentenceEncoderLayer
from packaging import version  # ç”¨äºç‰ˆæœ¬æ¯”è¾ƒçš„åŒ…

from transformers import (  # ä»transformersåº“ä¸­å¼•å…¥å¤šä¸ªç±»å’Œå‡½æ•°
    Data2VecTextConfig,  # ç”¨äºé…ç½®Data2VecTextæ¨¡å‹çš„ç±»
    Data2VecTextForMaskedLM,  # ç”¨äºData2VecTextçš„MLMä»»åŠ¡çš„ç±»
    Data2VecTextForSequenceClassification,  # ç”¨äºData2VecTextçš„åºåˆ—åˆ†ç±»ä»»åŠ¡çš„ç±»
    Data2VecTextModel,  # Data2VecTextæ¨¡å‹çš„ä¸»ç±»
)
from transformers.models.bert.modeling_bert import (  # ä»BERTæ¨¡å‹ä¸­å¼•å…¥å¤šä¸ªç±»
    BertIntermediate,  # BERTä¸­é—´å±‚çš„ç±»
    BertLayer,  # BERTå±‚çš„ç±»
    BertOutput,  # BERTè¾“å‡ºå±‚çš„ç±»
    BertSelfAttention,  # BERTè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç±»
    BertSelfOutput,  # BERTè‡ªæ³¨æ„åŠ›è¾“å‡ºçš„ç±»
)

# é‡è¦æç¤ºï¼šä¸ºäº†è¿è¡Œæœ¬è„šæœ¬ï¼Œè¯·ç¡®ä¿ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½å­—å…¸ï¼š`dict.txt` https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz
# æ–‡ä»¶æ¥æºäº https://github.com/pytorch/fairseq/blob/main/examples/data2vec/models/data2vec_text.py
from transformers.utils import logging  # ä»transformerså·¥å…·æ¨¡å—ä¸­å¼•å…¥æ—¥å¿—è®°å½•åŠŸèƒ½


if version.parse(fairseq.__version__) < version.parse("0.9.0"):
    raise Exception("requires fairseq >= 0.9.0")  # å¦‚æœfairseqç‰ˆæœ¬ä½äº0.9.0ï¼ŒæŠ›å‡ºå¼‚å¸¸

logging.set_verbosity_info()  # è®¾ç½®æ—¥å¿—è¯¦ç»†ç¨‹åº¦ä¸ºinfo
logger = logging.get_logger(__name__)  # è·å–å½“å‰è„šæœ¬çš„æ—¥å¿—è®°å½•å™¨

SAMPLE_TEXT = "Hello world! cÃ©cÃ© herlolip"  # ç¤ºä¾‹æ–‡æœ¬

def convert_data2vec_checkpoint_to_pytorch(
    data2vec_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool
):
    """
    å°†data2vecçš„æ£€æŸ¥ç‚¹æƒé‡å¤åˆ¶/ç²˜è´´/è°ƒæ•´åˆ°æˆ‘ä»¬çš„BERTç»“æ„ä¸­ã€‚
    """
    # è·å–data2vecæ£€æŸ¥ç‚¹çš„è·¯å¾„ä¿¡æ¯
    data2vec_checkpoint_dir, data2vec_checkpoint_file_name = os.path.split(data2vec_checkpoint_path)
    # ä»é¢„è®­ç»ƒçš„data2vecæ¨¡å‹åŠ è½½æ¨¡å‹
    data2vec = Data2VecTextModel.from_pretrained(
        data2vec_checkpoint_dir, checkpoint_file=data2vec_checkpoint_file_name
    )
    data2vec.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨dropout
    data2vec_model = data2vec.models[0]  # è·å–data2vecæ¨¡å‹çš„ä¸»ä½“éƒ¨åˆ†
    data2vec_sent_encoder = data2vec_model.encoder.sentence_encoder  # è·å–data2vecæ¨¡å‹çš„å¥å­ç¼–ç å™¨
    # åˆ›å»ºData2VecTextConfigé…ç½®å¯¹è±¡ï¼Œç”¨äºåç»­çš„BERTæ¨¡å‹
    config = Data2VecTextConfig(
        vocab_size=data2vec_sent_encoder.embed_tokens.num_embeddings,  # è¯æ±‡è¡¨å¤§å°
        hidden_size=data2vec_model.args.encoder_embed_dim,  # éšè—å±‚å¤§å°
        num_hidden_layers=data2vec_model.args.encoder_layers,  # éšè—å±‚å±‚æ•°
        num_attention_heads=data2vec_model.args.encoder_attention_heads,  # æ³¨æ„åŠ›å¤´æ•°
        intermediate_size=data2vec_model.args.encoder_ffn_embed_dim,  # ä¸­é—´å±‚å¤§å°
        max_position_embeddings=514,  # æœ€å¤§ä½ç½®ç¼–ç 
        type_vocab_size=1,  # ç±»å‹è¯æ±‡è¡¨å¤§å°
        layer_norm_eps=1e-5,  # å±‚å½’ä¸€åŒ–epsilonå€¼ï¼Œä¸fairseqé»˜è®¤ç›¸åŒ
    )
    if classification_head:
        config.num_labels = data2vec.model.classification_heads["mnli"].out_proj.weight.shape[0]  # å¦‚æœæœ‰åˆ†ç±»å¤´ï¼Œè®¾ç½®æ ‡ç­¾æ•°ç›®
    print("Our BERT config:", config)  # æ‰“å°é…ç½®ä¿¡æ¯
    # æ ¹æ®æ˜¯å¦éœ€è¦åˆ†ç±»å¤´é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼šå¦‚æœéœ€è¦åˆ†ç±»å¤´ï¼Œåˆ™ä½¿ç”¨Data2VecTextForSequenceClassificationï¼Œå¦åˆ™ä½¿ç”¨Data2VecTextForMaskedLM
    model = Data2VecTextForSequenceClassification(config) if classification_head else Data2VecTextForMaskedLM(config)
    model.eval()

    # ç°åœ¨è®©æˆ‘ä»¬å¤åˆ¶æ‰€æœ‰çš„æƒé‡ã€‚

    # å¤åˆ¶åµŒå…¥å±‚æƒé‡
    model.data2vec_text.embeddings.word_embeddings.weight = data2vec_sent_encoder.embed_tokens.weight
    model.data2vec_text.embeddings.position_embeddings.weight = data2vec_sent_encoder.embed_positions.weight
    model.data2vec_text.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.data2vec_text.embeddings.token_type_embeddings.weight
    )  # å°†å…¶ç½®é›¶ï¼Œå› ä¸ºdata2vecä¸ä½¿ç”¨è¿™äº›
    model.data2vec_text.embeddings.LayerNorm.weight = data2vec_sent_encoder.layernorm_embedding.weight
    model.data2vec_text.embeddings.LayerNorm.bias = data2vec_sent_encoder.layernorm_embedding.bias

    if classification_head:
        # å¦‚æœå­˜åœ¨åˆ†ç±»å¤´ï¼Œå¤åˆ¶åˆ†ç±»å™¨æƒé‡
        model.classifier.dense.weight = data2vec.model.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = data2vec.model.classification_heads["mnli"].dense.bias
        model.classifier.out_proj.weight = data2vec.model.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = data2vec.model.classification_heads["mnli"].out_proj.bias
    else:
        # å¦åˆ™ï¼Œå¤åˆ¶è¯­è¨€æ¨¡å‹å¤´æƒé‡
        model.lm_head.dense.weight = data2vec_model.encoder.lm_head.dense.weight
        model.lm_head.dense.bias = data2vec_model.encoder.lm_head.dense.bias
        model.lm_head.layer_norm.weight = data2vec_model.encoder.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = data2vec_model.encoder.lm_head.layer_norm.bias
        model.lm_head.decoder.weight = data2vec_model.encoder.lm_head.weight
        model.lm_head.decoder.bias = data2vec_model.encoder.lm_head.bias

    # æ£€æŸ¥æ˜¯å¦è¾“å‡ºç›¸åŒçš„ç»“æœã€‚

    # ä½¿ç”¨data2vecå¯¹æ ·æœ¬æ–‡æœ¬ç¼–ç å¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    input_ids: torch.Tensor = data2vec.encode(SAMPLE_TEXT).unsqueeze(0)

    # è®¡ç®—æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡º
    our_output = model(input_ids)[0]

    if classification_head:
        # å¦‚æœä½¿ç”¨åˆ†ç±»å¤´ï¼Œè®¡ç®—data2vecæ¨¡å‹çš„è¾“å‡º
        their_output = data2vec.model.classification_heads["mnli"](data2vec.extract_features(input_ids))
    else:
        # å¦åˆ™ï¼Œè®¡ç®—data2vecæ¨¡å‹çš„è¾“å‡º
        their_output = data2vec_model(input_ids)[0]

    # æ‰“å°ä¸¤ä¸ªè¾“å‡ºçš„å½¢çŠ¶
    print(our_output.shape, their_output.shape)

    # è®¡ç®—ä¸¤è€…ä¹‹é—´çš„æœ€å¤§ç»å¯¹å·®
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    print(f"max_absolute_diff = {max_absolute_diff}")  # å¤§çº¦ä¸º1e-7

    # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºçš„å¼ é‡æ˜¯å¦å‡ ä¹ç›¸åŒ
    success = torch.allclose(our_output, their_output, atol=1e-3)
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")

    # å¦‚æœä¸¤è€…è¾“å‡ºä¸å‡ ä¹ç›¸åŒï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
    if not success:
        raise Exception("Something went wRoNg")

    # åˆ›å»ºç›®å½•ä»¥ä¿å­˜PyTorchæ¨¡å‹
    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)
    print(f"Saving model to {pytorch_dump_folder_path}")

    # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)
if __name__ == "__main__":
    # å¦‚æœè¿™ä¸ªè„šæœ¬æ˜¯ä½œä¸ºä¸»ç¨‹åºè¿è¡Œæ—¶æ‰§è¡Œä»¥ä¸‹æ“ä½œ

    parser = argparse.ArgumentParser()
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡

    # Required parameters
    parser.add_argument(
        "--checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    # æ·»åŠ ä¸€ä¸ªå¿…é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šå®˜æ–¹ PyTorch è½¬å‚¨æ–‡ä»¶çš„è·¯å¾„

    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    # æ·»åŠ ä¸€ä¸ªå¿…é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šè¾“å‡º PyTorch æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„

    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # æ·»åŠ ä¸€ä¸ªé€‰é¡¹å‚æ•°ï¼Œè¡¨ç¤ºæ˜¯å¦è½¬æ¢æœ€ç»ˆçš„åˆ†ç±»å¤´éƒ¨

    args = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ args å˜é‡ä¸­

    convert_data2vec_checkpoint_to_pytorch(
        args.checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
    # è°ƒç”¨å‡½æ•° convert_data2vec_checkpoint_to_pytorchï¼Œä¼ å…¥å‘½ä»¤è¡Œå‚æ•°ä¸­è§£æçš„è·¯å¾„å’Œé€‰é¡¹


è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªå…¸å‹çš„å‘½ä»¤è¡Œå·¥å…·çš„å…¥å£ç‚¹ï¼Œå®ƒä½¿ç”¨ argparse æ¨¡å—è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è°ƒç”¨ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†è¿™äº›å‚æ•°æŒ‡å®šçš„ä»»åŠ¡ã€‚
```