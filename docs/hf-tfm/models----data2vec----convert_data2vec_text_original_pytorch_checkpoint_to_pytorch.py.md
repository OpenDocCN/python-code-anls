# `.\models\data2vec\convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py`

```
# æŒ‡æ˜ä»£ç ä»¥ UTF-8 ç¼–ç æ ¼å¼ç¼–å†™
# ç‰ˆæƒå£°æ˜
# éµå¾ª Apache åè®®ï¼Œå¼•å…¥åŒ…ã€æ¨¡å—å’Œç±»
# å¯¼å…¥ argparse æ¨¡å—ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
# å¯¼å…¥ os å’Œ pathlib æ¨¡å—ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„
# å¯¼å…¥ fairseq æ¨¡å—
# å¯¼å…¥ torch æ¨¡å—
# å¯¼å…¥ fairseq çš„ TransformerSentenceEncoderLayer ç±»
# å¯¼å…¥ packaging çš„ version ç±»
# å¯¼å…¥ transformers ä¸­çš„ä¸åŒç±»å’Œå‡½æ•°
# å¯¼å…¥ logging æ¨¡å—
# æ£€æŸ¥ fairseq ç‰ˆæœ¬æ˜¯å¦å¤§äºç­‰äº 0.9.0
# è®¾ç½®æ—¥å¿—è¾“å‡ºçº§åˆ«
# è·å– logger å¯¹è±¡
# å®šä¹‰ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬
# å®šä¹‰è½¬æ¢æ•°æ®2vecæ£€æŸ¥ç‚¹åˆ°PyTorchçš„å‡½æ•°
    ï¼ƒå°†data2vecæ¨¡å‹çš„æƒé‡å¤åˆ¶/ç²˜è´´/è°ƒæ•´åˆ°æˆ‘ä»¬çš„BERTç»“æ„ä¸­
# åˆ†å‰²æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œè·å–æ£€æŸ¥ç‚¹ç›®å½•å’Œæ–‡ä»¶å
# åŠ è½½é¢„è®­ç»ƒçš„data2vecæ¨¡å‹
# å°†data2vecæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼
# è·å–data2vecæ¨¡å‹çš„encoderå’Œsentence_encoder
# åˆ›å»ºData2VecTextConfigå¯¹è±¡ï¼Œå¹¶è®¾ç½®ç›¸å…³å‚æ•°
# ï¼ˆè¿™é‡Œå¯èƒ½æœ‰ä¸€ä¸ªé”™è¯¯ï¼Œdata2vec.modelåº”è¯¥æ˜¯data2vec_modelï¼‰
# è¾“å‡ºBERTçš„é…ç½®ä¿¡æ¯
    # æ ¹æ®éœ€è¦é€‰æ‹©æ¨¡å‹ç±»å‹ï¼Œå¹¶è¿›è¡Œè¯„ä¼°
    model = Data2VecTextForSequenceClassification(config) if classification_head else Data2VecTextForMaskedLM(config)
    model.eval()
    
    # å¤åˆ¶æ‰€æœ‰çš„æƒé‡
    # åµŒå…¥å±‚æƒé‡
    model.data2vec_text.embeddings.word_embeddings.weight = data2vec_sent_encoder.embed_tokens.weight  # å¤åˆ¶è¯åµŒå…¥æƒé‡
    model.data2vec_text.embeddings.position_embeddings.weight = data2vec_sent_encoder.embed_positions.weight  # å¤åˆ¶ä½ç½®åµŒå…¥æƒé‡
    model.data2vec_text.embeddings.token_type_embeddings.weight.data = torch.zeros_like(
        model.data2vec_text.embeddings.token_type_embeddings.weight
    )  # å°†æ ‡è®°ç±»å‹åµŒå…¥æƒé‡æ¸…é›¶ï¼Œå› ä¸ºdata2vecä¸ä½¿ç”¨å®ƒä»¬
    model.data2vec_text.embeddings.LayerNorm.weight = data2vec_sent_encoder.layernorm_embedding.weight  # å¤åˆ¶å½’ä¸€åŒ–å±‚æƒé‡
    model.data2vec_text.embeddings.LayerNorm.bias = data2vec_sent_encoder.layernorm_embedding.bias  # å¤åˆ¶å½’ä¸€åŒ–å±‚åç½®
    
    if classification_head:
        # åˆ†ç±»å™¨æƒé‡
        model.classifier.dense.weight = data2vec.model.classification_heads["mnli"].dense.weight
        model.classifier.dense.bias = data2vec.model.classification_heads["mnli"].dense.bias
        model.classifier.out_proj.weight = data2vec.model.classification_heads["mnli"].out_proj.weight
        model.classifier.out_proj.bias = data2vec.model.classification_heads["mnli"].out_proj.bias
    else:
        # è¯­è¨€æ¨¡å‹å¤´æƒé‡
        model.lm_head.dense.weight = data2vec_model.encoder.lm_head.dense.weight
        model.lm_head.dense.bias = data2vec_model.encoder.lm_head.dense.bias
        model.lm_head.layer_norm.weight = data2vec_model.encoder.lm_head.layer_norm.weight
        model.lm_head.layer_norm.bias = data2vec_model.encoder.lm_head.layer_norm.bias
        model.lm_head.decoder.weight = data2vec_model.encoder.lm_head.weight
        model.lm_head.decoder.bias = data2vec_model.encoder.lm_head.bias
    
    # æ£€æŸ¥æ˜¯å¦å¾—åˆ°ç›¸åŒçš„ç»“æœ
    input_ids: torch.Tensor = data2vec.encode(SAMPLE_TEXT).unsqueeze(0)  # å¤§å°ä¸º1çš„æ‰¹é‡è¾“å…¥
    
    our_output = model(input_ids)[0]  # æˆ‘ä»¬çš„æ¨¡å‹è¾“å‡º
    if classification_head:
        their_output = data2vec.model.classification_heads["mnli"](data2vec.extract_features(input_ids))  # å¯¹æ¯”æ¨¡å‹çš„è¾“å‡º
    else:
        their_output = data2vec_model(input_ids)[0]
    print(our_output.shape, their_output.shape)
    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()
    print(f"max_absolute_diff = {max_absolute_diff}")  # æœ€å¤§çš„ç»å¯¹å·®å€¼
    success = torch.allclose(our_output, their_output, atol=1e-3)  # æŸ¥çœ‹ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºçš„å¼ é‡æ˜¯å¦ç›¸åŒ
    print("Do both models output the same tensors?", "ğŸ”¥" if success else "ğŸ’©")  # æ‰“å°ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºçš„å¼ é‡æ˜¯å¦ç›¸åŒ
    if not success:
        raise Exception("Something went wRoNg")  # å¦‚æœä¸ç›¸åŒï¼ŒæŠ›å‡ºå¼‚å¸¸
    
    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)  # åˆ›å»ºå­˜å‚¨è·¯å¾„
    print(f"Saving model to {pytorch_dump_folder_path}")  # ä¿å­˜æ¨¡å‹å¹¶æ‰“å°ä¿å­˜è·¯å¾„
    model.save_pretrained(pytorch_dump_folder_path)  # ä¿å­˜æ¨¡å‹
# å¦‚æœè¯¥è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ å¿…éœ€çš„å‚æ•°
    parser.add_argument(
        "--checkpoint_path", default=None, type=str, required=True, help="Path the official PyTorch dump."
    )
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, required=True, help="Path to the output PyTorch model."
    )
    parser.add_argument(
        "--classification_head", action="store_true", help="Whether to convert a final classification head."
    )
    # è§£æä¼ é€’ç»™è„šæœ¬çš„å‚æ•°
    args = parser.parse_args()
    # å°†å‚æ•°ä¼ é€’ç»™è½¬æ¢å‡½æ•°ï¼Œæ‰§è¡Œæ•°æ®è½¬æ¢
    convert_data2vec_checkpoint_to_pytorch(
        args.checkpoint_path, args.pytorch_dump_folder_path, args.classification_head
    )
```