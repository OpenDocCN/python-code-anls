# `.\trainer.py`

```
# coding=utf-8
# Copyright 2020-present the HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
The Trainer class, to easily train a ğŸ¤— Transformers from scratch or finetune it on a new task.
"""

import contextlib
import copy
import functools
import glob
import importlib.metadata
import inspect
import math
import os
import random
import re
import shutil
import sys
import tempfile
import time
import warnings
from collections.abc import Mapping
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union

# Integrations must be imported before ML frameworks:
# isort: off
from .integrations import (
    get_reporting_integration_callbacks,
    hp_params,
)

# isort: on

import huggingface_hub.utils as hf_hub_utils
import numpy as np
import torch
import torch.distributed as dist
from huggingface_hub import ModelCard, create_repo, upload_folder
from packaging import version
from torch import nn
from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler

from . import __version__
from .configuration_utils import PretrainedConfig
from .data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator
from .debug_utils import DebugOption, DebugUnderflowOverflow
from .hyperparameter_search import ALL_HYPERPARAMETER_SEARCH_BACKENDS, default_hp_search_backend
from .integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available
from .integrations.tpu import tpu_spmd_dataloader
from .modelcard import TrainingSummary
from .modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model
from .models.auto.modeling_auto import (
    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,
    MODEL_MAPPING_NAMES,
)
from .optimization import Adafactor, get_scheduler
from .pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_13
from .tokenization_utils_base import PreTrainedTokenizerBase
from .trainer_callback import (
    CallbackHandler,
    DefaultFlowCallback,
    PrinterCallback,
    ProgressCallback,
    TrainerCallback,
    TrainerControl,
    TrainerState,
)
from .trainer_pt_utils import (
    DistributedTensorGatherer,
    IterableDatasetShard,
    LabelSmoother,
    LayerWiseDummyOptimizer,
    LengthGroupedSampler,
    SequentialDistributedSampler,
    distributed_broadcast_scalars,
    distributed_concat,
    find_batch_size,
    get_dataloader_sampler,
    get_model_param_count,
)

# Importing integration modules and utilities for reporting and hyperparameters
from .integrations import get_reporting_integration_callbacks, hp_params

# Importing utility functions and classes from the huggingface_hub library
import huggingface_hub.utils as hf_hub_utils

# Importing essential modules from Python's standard library
import numpy as np  # NumPy for numerical computing
import torch  # PyTorch for deep learning framework
import torch.distributed as dist  # PyTorch distributed for parallel computing

# Importing specific functions and classes from huggingface_hub library
from huggingface_hub import ModelCard, create_repo, upload_folder

# Importing version comparison utility from packaging module
from packaging import version

# Importing neural network related modules from PyTorch
from torch import nn  # Neural network module
from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler  # Data handling utilities

# Importing essential internal modules from Transformers library
from . import __version__  # Current version of Transformers library
from .configuration_utils import PretrainedConfig  # Configuration utilities for pretrained models
from .data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator  # Data collation utilities
from .debug_utils import DebugOption, DebugUnderflowOverflow  # Debugging utilities
from .hyperparameter_search import ALL_HYPERPARAMETER_SEARCH_BACKENDS, default_hp_search_backend  # Hyperparameter search utilities
from .integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available  # DeepSpeed integration utilities
from .integrations.tpu import tpu_spmd_dataloader  # TPU integration for data loading
from .modelcard import TrainingSummary  # ModelCard for model documentation
from .modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model  # Model utilities
from .models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES  # Auto model mapping names
from .optimization import Adafactor, get_scheduler  # Optimization utilities
from .pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_13  # PyTorch utility functions
from .tokenization_utils_base import PreTrainedTokenizerBase  # Base class for tokenization utilities
from .trainer_callback import CallbackHandler, DefaultFlowCallback, PrinterCallback, ProgressCallback, TrainerCallback, TrainerControl, TrainerState  # Trainer callback utilities
from .trainer_pt_utils import DistributedTensorGatherer, IterableDatasetShard, LabelSmoother, LayerWiseDummyOptimizer, LengthGroupedSampler, SequentialDistributedSampler, distributed_broadcast_scalars, distributed_concat, find_batch_size, get_dataloader_sampler, get_model_param_count  # PyTorch specific trainer utilities
    # ä»ç»™å®šçš„åç§°ä¸­è·å–æ¨¡å—ç±»
    get_module_class_from_name,
    
    # è·å–å‚æ•°çš„åç§°åˆ—è¡¨
    get_parameter_names,
    
    # é€’å½’è¿æ¥ï¼ˆconcatenateï¼‰æ“ä½œï¼Œå¯èƒ½æ˜¯è¿æ¥åµŒå¥—ç»“æ„çš„å‡½æ•°
    nested_concat,
    
    # é€’å½’åˆ†ç¦»ï¼ˆdetachï¼‰æ“ä½œï¼Œå¯èƒ½æ˜¯å°†æ¢¯åº¦ä¿¡æ¯åˆ†ç¦»å‡ºæ¥çš„å‡½æ•°
    nested_detach,
    
    # å°†åµŒå¥—ç»“æ„è½¬æ¢ä¸º NumPy æ•°ç»„çš„å‡½æ•°
    nested_numpify,
    
    # å¯¹ XLA ç½‘æ ¼è¿›è¡Œé€’å½’é™ç»´ï¼ˆreduceï¼‰çš„å‡½æ•°
    nested_xla_mesh_reduce,
    
    # é‡æ–°å‘å‡º PyTorch è­¦å‘Šä¿¡æ¯çš„å‡½æ•°
    reissue_pt_warnings,
    
    # ç§»é™¤è™šæ‹Ÿæ£€æŸ¥ç‚¹çš„å‡½æ•°
    remove_dummy_checkpoint,
# ä» `trainer_utils` æ¨¡å—ä¸­å¯¼å…¥å¤šä¸ªå·¥å…·ç±»å’Œå‡½æ•°
from .trainer_utils import (
    PREFIX_CHECKPOINT_DIR,  # å¯¼å…¥æ£€æŸ¥ç‚¹ç›®å½•å‰ç¼€
    BestRun,  # å¯¼å…¥æœ€ä½³è¿è¡Œç»“æœç±»
    EvalLoopOutput,  # å¯¼å…¥è¯„ä¼°å¾ªç¯è¾“å‡ºç±»
    EvalPrediction,  # å¯¼å…¥è¯„ä¼°é¢„æµ‹ç±»
    HPSearchBackend,  # å¯¼å…¥è¶…å‚æ•°æœç´¢åç«¯ç±»
    HubStrategy,  # å¯¼å…¥Hubç­–ç•¥ç±»
    IntervalStrategy,  # å¯¼å…¥é—´éš”ç­–ç•¥ç±»
    PredictionOutput,  # å¯¼å…¥é¢„æµ‹è¾“å‡ºç±»
    RemoveColumnsCollator,  # å¯¼å…¥ç§»é™¤åˆ—çš„é›†åˆç±»
    TrainerMemoryTracker,  # å¯¼å…¥è®­ç»ƒå™¨å†…å­˜è¿½è¸ªç±»
    TrainOutput,  # å¯¼å…¥è®­ç»ƒè¾“å‡ºç±»
    check_target_module_exists,  # å¯¼å…¥æ£€æŸ¥ç›®æ ‡æ¨¡å—æ˜¯å¦å­˜åœ¨çš„å‡½æ•°
    default_compute_objective,  # å¯¼å…¥é»˜è®¤è®¡ç®—ç›®æ ‡çš„å‡½æ•°
    denumpify_detensorize,  # å¯¼å…¥å»é™¤NumPy arrayæˆ–tensoråŒ–çš„å‡½æ•°
    enable_full_determinism,  # å¯¼å…¥å¯ç”¨å®Œå…¨ç¡®å®šæ€§çš„å‡½æ•°
    find_executable_batch_size,  # å¯¼å…¥æŸ¥æ‰¾å¯æ‰§è¡Œæ‰¹é‡å¤§å°çš„å‡½æ•°
    get_last_checkpoint,  # å¯¼å…¥è·å–æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹çš„å‡½æ•°
    has_length,  # å¯¼å…¥åˆ¤æ–­å¯¹è±¡æ˜¯å¦å…·æœ‰é•¿åº¦çš„å‡½æ•°
    neftune_post_forward_hook,  # å¯¼å…¥Neftuneåå‘é’©å­å‡½æ•°
    number_of_arguments,  # å¯¼å…¥è·å–å‚æ•°ä¸ªæ•°çš„å‡½æ•°
    seed_worker,  # å¯¼å…¥ç§å­å·¥ä½œå™¨å‡½æ•°
    set_seed,  # å¯¼å…¥è®¾ç½®ç§å­çš„å‡½æ•°
    speed_metrics,  # å¯¼å…¥é€Ÿåº¦åº¦é‡æŒ‡æ ‡å‡½æ•°
)

# ä» `training_args` æ¨¡å—ä¸­å¯¼å…¥ä¼˜åŒ–å™¨åç§°ã€å¹¶è¡Œæ¨¡å¼ã€è®­ç»ƒå‚æ•°ç±»
from .training_args import OptimizerNames, ParallelMode, TrainingArguments

# ä» `utils` æ¨¡å—ä¸­å¯¼å…¥å¤šä¸ªå¸¸é‡ã€ç±»å’Œå‡½æ•°
from .utils import (
    ADAPTER_CONFIG_NAME,  # å¯¼å…¥é€‚é…å™¨é…ç½®åç§°
    ADAPTER_SAFE_WEIGHTS_NAME,  # å¯¼å…¥é€‚é…å™¨å®‰å…¨æƒé‡åç§°
    ADAPTER_WEIGHTS_NAME,  # å¯¼å…¥é€‚é…å™¨æƒé‡åç§°
    CONFIG_NAME,  # å¯¼å…¥é…ç½®åç§°
    SAFE_WEIGHTS_INDEX_NAME,  # å¯¼å…¥å®‰å…¨æƒé‡ç´¢å¼•åç§°
    SAFE_WEIGHTS_NAME,  # å¯¼å…¥å®‰å…¨æƒé‡åç§°
    WEIGHTS_INDEX_NAME,  # å¯¼å…¥æƒé‡ç´¢å¼•åç§°
    WEIGHTS_NAME,  # å¯¼å…¥æƒé‡åç§°
    PushInProgress,  # å¯¼å…¥æ¨é€è¿›è¡Œä¸­ç±»
    PushToHubMixin,  # å¯¼å…¥æ¨é€åˆ°Hubæ··åˆç±»
    can_return_loss,  # å¯¼å…¥èƒ½å¦è¿”å›æŸå¤±çš„å‡½æ•°
    find_labels,  # å¯¼å…¥æŸ¥æ‰¾æ ‡ç­¾çš„å‡½æ•°
    is_accelerate_available,  # å¯¼å…¥åŠ é€Ÿåº“æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_apex_available,  # å¯¼å…¥APEXæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_bitsandbytes_available,  # å¯¼å…¥BitsAndBytesæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_datasets_available,  # å¯¼å…¥æ•°æ®é›†æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_galore_torch_available,  # å¯¼å…¥Galore Torchæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_in_notebook,  # å¯¼å…¥æ˜¯å¦åœ¨ç¬”è®°æœ¬ä¸­çš„å‡½æ•°
    is_ipex_available,  # å¯¼å…¥IPExæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_peft_available,  # å¯¼å…¥PEFTæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_safetensors_available,  # å¯¼å…¥å®‰å…¨å¼ é‡æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_sagemaker_dp_enabled,  # å¯¼å…¥SageMakeråˆ†å¸ƒå¼è®­ç»ƒæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_sagemaker_mp_enabled,  # å¯¼å…¥SageMakeræ¨¡å‹å¹¶è¡Œæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_torch_compile_available,  # å¯¼å…¥Torchç¼–è¯‘æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_torch_neuroncore_available,  # å¯¼å…¥Torch NeuronCoreæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_torch_npu_available,  # å¯¼å…¥Torch NPUæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_torch_xla_available,  # å¯¼å…¥Torch XLAæ˜¯å¦å¯ç”¨çš„å‡½æ•°
    logging,  # å¯¼å…¥æ—¥å¿—åŠŸèƒ½
    strtobool,  # å¯¼å…¥å­—ç¬¦ä¸²è½¬å¸ƒå°”å€¼çš„å‡½æ•°
)

# ä» `utils.quantization_config` æ¨¡å—ä¸­å¯¼å…¥é‡åŒ–æ–¹æ³•
from .utils.quantization_config import QuantizationMethod

# é»˜è®¤å›è°ƒå‡½æ•°åˆ—è¡¨
DEFAULT_CALLBACKS = [DefaultFlowCallback]

# é»˜è®¤è¿›åº¦å›è°ƒå‡½æ•°
DEFAULT_PROGRESS_CALLBACK = ProgressCallback

# å¦‚æœåœ¨ç¬”è®°æœ¬ä¸­ï¼Œåˆ™å¯¼å…¥ç¬”è®°æœ¬è¿›åº¦å›è°ƒå‡½æ•°
if is_in_notebook():
    from .utils.notebook import NotebookProgressCallback
    DEFAULT_PROGRESS_CALLBACK = NotebookProgressCallback

# å¦‚æœAPEXå¯ç”¨ï¼Œåˆ™å¯¼å…¥AMP
if is_apex_available():
    from apex import amp

# å¦‚æœæ•°æ®é›†å¯ç”¨ï¼Œåˆ™å¯¼å…¥datasetsæ¨¡å—
if is_datasets_available():
    import datasets

# å¦‚æœTorch XLAå¯ç”¨ï¼Œåˆ™å¯¼å…¥ç›¸å…³æ¨¡å—
if is_torch_xla_available():
    import torch_xla.core.xla_model as xm
    import torch_xla.debug.metrics as met
    import torch_xla.distributed.spmd as xs
    import torch_xla.runtime as xr

# å¦‚æœSageMakeræ¨¡å‹å¹¶è¡Œå¯ç”¨ï¼Œåˆ™å¯¼å…¥ç›¸å…³æ¨¡å—å’Œç‰ˆæœ¬æ£€æŸ¥
if is_sagemaker_mp_enabled():
    import smdistributed.modelparallel.torch as smp
    from smdistributed.modelparallel import __version__ as SMP_VERSION

    IS_SAGEMAKER_MP_POST_1_10 = version.parse(SMP_VERSION) >= version.parse("1.10")

    from .trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat
else:
    IS_SAGEMAKER_MP_POST_1_10 = False

# å¦‚æœå®‰å…¨å¼ é‡å¯ç”¨ï¼Œåˆ™å¯¼å…¥ç›¸å…³æ¨¡å—
if is_safetensors_available():
    import safetensors.torch

# å¦‚æœPEFTå¯ç”¨ï¼Œåˆ™å¯¼å…¥PeftModel
if is_peft_available():
    from peft import PeftModel

# å¦‚æœåŠ é€Ÿåº“å¯ç”¨ï¼Œåˆ™å¯¼å…¥åŠ é€Ÿåº“ç›¸å…³æ¨¡å—å’Œå‡½æ•°
if is_accelerate_available():
    from accelerate import Accelerator, skip_first_batches
    from accelerate import __version__ as accelerate_version
    from accelerate.utils import (
        DistributedDataParallelKwargs,
        DistributedType,
        GradientAccumulationPlugin,
        load_fsdp_model,
        load_fsdp_optimizer,
        save_fsdp_model,
        save_fsdp_optimizer,
    )

    DATA_SAMPLERS = [RandomSampler]  # æ•°æ®é‡‡æ ·å™¨åˆ—è¡¨
    # æ£€æŸ¥åŠ é€Ÿåº“çš„ç‰ˆæœ¬æ˜¯å¦å¤§äº "0.23.0"
    if version.parse(accelerate_version) > version.parse("0.23.0"):
        # å¦‚æœæ»¡è¶³æ¡ä»¶ï¼Œå¯¼å…¥ SeedableRandomSampler ç±»ä» accelerate.data_loader æ¨¡å—
        from accelerate.data_loader import SeedableRandomSampler
    
        # å°† SeedableRandomSampler ç±»åŠ å…¥åˆ° DATA_SAMPLERS åˆ—è¡¨ä¸­
        DATA_SAMPLERS += [SeedableRandomSampler]
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ DeepSpeed åº“
    if is_deepspeed_available():
        # å¦‚æœ DeepSpeed å¯ç”¨ï¼Œä» accelerate.utils æ¨¡å—å¯¼å…¥ DeepSpeedSchedulerWrapper ç±»
        from accelerate.utils import DeepSpeedSchedulerWrapper
# æ£€æŸ¥ç»™å®šçš„æ¨¡å‹æ˜¯å¦å±äº PEFT æ¨¡å‹ç±»æˆ–å…¶æ··åˆç±»
def _is_peft_model(model):
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† PEFT
    if is_peft_available():
        # å¦‚æœå®‰è£…äº† PEFTï¼Œå…ˆå°†åŸºç¡€ç±»è®¾ç½®ä¸º PeftModel
        classes_to_check = (PeftModel,)
        # æ£€æŸ¥ PEFT çš„ç‰ˆæœ¬æ˜¯å¦å¤§äºç­‰äº 0.7.0
        if version.parse(importlib.metadata.version("peft")) >= version.parse("0.7.0"):
            from peft import PeftMixedModel

            # å¦‚æœç‰ˆæœ¬å…è®¸ï¼Œæ·»åŠ  PeftMixedModel åˆ°å¾…æ£€æŸ¥ç±»åˆ—è¡¨
            classes_to_check = (*classes_to_check, PeftMixedModel)
        # è¿”å›æ¨¡å‹æ˜¯å¦å±äºå¾…æ£€æŸ¥ç±»åˆ—è¡¨ä¸­çš„ä»»ä½•ä¸€ä¸ª
        return isinstance(model, classes_to_check)
    # å¦‚æœæœªå®‰è£… PEFTï¼Œåˆ™è¿”å› False
    return False


def _get_fsdp_ckpt_kwargs():
    # TODO: @AjayP13, @younesbelkada åœ¨ä¸‹ä¸€ä¸ª `accelerate` å‘å¸ƒä¸­ï¼Œä½¿ç”¨ç‰ˆæœ¬æ£€æŸ¥æ›¿æ¢æ­¤æ£€æŸ¥
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† accelerate å¹¶ä¸” save_fsdp_model çš„å‚æ•°åˆ—è¡¨ä¸­åŒ…å« "adapter_only"
    if is_accelerate_available() and "adapter_only" in list(inspect.signature(save_fsdp_model).parameters):
        # å¦‚æœæ¡ä»¶æˆç«‹ï¼Œè¿”å›é€‚åˆ FSDP æ£€æŸ¥ç‚¹çš„å‚æ•°å­—å…¸
        return {"adapter_only": True}
    else:
        # å¦åˆ™è¿”å›ç©ºå­—å…¸
        return {}


if TYPE_CHECKING:
    import optuna  # å¯¼å…¥ç±»å‹æ£€æŸ¥æ—¶æ‰€éœ€çš„ optuna æ¨¡å—


logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


# ç”¨äºä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶çš„æ–‡ä»¶åå¸¸é‡
TRAINING_ARGS_NAME = "training_args.bin"
TRAINER_STATE_NAME = "trainer_state.json"
OPTIMIZER_NAME = "optimizer.pt"
OPTIMIZER_NAME_BIN = "optimizer.bin"
SCHEDULER_NAME = "scheduler.pt"
SCALER_NAME = "scaler.pt"
FSDP_MODEL_NAME = "pytorch_model_fsdp"


class Trainer:
    """
    Trainer æ˜¯ä¸€ä¸ªç®€å•ä½†åŠŸèƒ½é½å…¨çš„ PyTorch è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ï¼Œä¸“ä¸º ğŸ¤— Transformers ä¼˜åŒ–ã€‚

    é‡è¦å±æ€§:

        - **model** -- å§‹ç»ˆæŒ‡å‘æ ¸å¿ƒæ¨¡å‹ã€‚å¦‚æœä½¿ç”¨çš„æ˜¯ transformers æ¨¡å‹ï¼Œå®ƒå°†æ˜¯ [`PreTrainedModel`] çš„å­ç±»ã€‚
        - **model_wrapped** -- å§‹ç»ˆæŒ‡å‘æœ€å¤–å±‚çš„æ¨¡å‹ã€‚å¦‚æœä½¿ç”¨ `DeepSpeed`ï¼Œå†…éƒ¨æ¨¡å‹ä¼šè¢«åŒ…è£…æˆ `DeepSpeed` å’Œ `torch.nn.DistributedDataParallel`ã€‚
          å¦‚æœå†…éƒ¨æ¨¡å‹æ²¡æœ‰è¢«åŒ…è£…ï¼Œåˆ™ `self.model_wrapped` ä¸ `self.model` ç›¸åŒã€‚
        - **is_model_parallel** -- æ˜¯å¦å°†æ¨¡å‹åˆ‡æ¢åˆ°æ¨¡å‹å¹¶è¡Œæ¨¡å¼ï¼ˆä¸åŒäºæ•°æ®å¹¶è¡Œï¼Œæ„å‘³ç€ä¸€äº›æ¨¡å‹å±‚åœ¨ä¸åŒçš„ GPU ä¸Šæ‹†åˆ†ï¼‰ã€‚
        - **place_model_on_device** -- æ˜¯å¦è‡ªåŠ¨å°†æ¨¡å‹æ”¾ç½®åœ¨è®¾å¤‡ä¸Šã€‚å¦‚æœä½¿ç”¨æ¨¡å‹å¹¶è¡Œæˆ– DeepSpeedï¼Œæˆ–è€…é»˜è®¤çš„ `TrainingArguments.place_model_on_device`
          è¢«è¦†ç›–ä¸ºè¿”å› `False`ï¼Œå®ƒå°†è®¾ç½®ä¸º `False`ã€‚
        - **is_in_train** -- å½“å‰æ¨¡å‹æ˜¯å¦æ­£åœ¨æ‰§è¡Œ `train`ï¼ˆä¾‹å¦‚ï¼Œåœ¨ `train` è¿è¡Œæ—¶è°ƒç”¨ `evaluate`ï¼‰ã€‚

    """

    # ä¸‹é¢çš„æ–¹æ³•æ˜¯ Trainer çš„ç¤ºä¾‹æ–¹æ³•ï¼Œä¿å­˜åœ¨ trainer_pt_utils ä¸­ã€‚
    from .trainer_pt_utils import _get_learning_rate, log_metrics, metrics_format, save_metrics, save_state
    def __init__(
        self,
        model: Union[PreTrainedModel, nn.Module] = None,
        args: TrainingArguments = None,
        data_collator: Optional[DataCollator] = None,
        train_dataset: Optional[Dataset] = None,
        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        model_init: Optional[Callable[[], PreTrainedModel]] = None,
        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
        callbacks: Optional[List[TrainerCallback]] = None,
        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
    ):
        """
        åˆå§‹åŒ–ä¸€ä¸ª `Trainer` å¯¹è±¡ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚

        Args:
            model (Union[PreTrainedModel, nn.Module], optional): è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
            args (TrainingArguments, optional): è®­ç»ƒå’Œè¯„ä¼°çš„å‚æ•°è®¾ç½®ã€‚
            data_collator (Optional[DataCollator], optional): ç”¨äºæ‰¹å¤„ç†æ•°æ®çš„æ•°æ®æ”¶é›†å™¨ã€‚
            train_dataset (Optional[Dataset], optional): è®­ç»ƒæ•°æ®é›†ã€‚
            eval_dataset (Optional[Union[Dataset, Dict[str, Dataset]]], optional): è¯„ä¼°æ•°æ®é›†ã€‚
            tokenizer (Optional[PreTrainedTokenizerBase], optional): ç”¨äºå¤„ç†è¾“å…¥æ•°æ®çš„åˆ†è¯å™¨ã€‚
            model_init (Optional[Callable[[], PreTrainedModel]], optional): åˆå§‹åŒ–æ¨¡å‹çš„å‡½æ•°ã€‚
            compute_metrics (Optional[Callable[[EvalPrediction], Dict]], optional): ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°ã€‚
            callbacks (Optional[List[TrainerCallback]], optional): è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„å›è°ƒå‡½æ•°åˆ—è¡¨ã€‚
            optimizers (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional): ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å…ƒç»„ã€‚
            preprocess_logits_for_metrics (Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]], optional): å¯¹é¢„æµ‹ç»“æœè¿›è¡Œé¢„å¤„ç†çš„å‡½æ•°ã€‚
        """

    def _activate_neftune(self, model):
        r"""
        æ¿€æ´» NEFTune æ–¹æ³•ï¼Œå‚è€ƒä»£ç å’Œè®ºæ–‡ï¼š
        https://github.com/neelsjain/NEFTune
        https://arxiv.org/abs/2310.05914
        """
        unwrapped_model = unwrap_model(model)

        if _is_peft_model(unwrapped_model):
            embeddings = unwrapped_model.base_model.model.get_input_embeddings()
        else:
            embeddings = unwrapped_model.get_input_embeddings()

        del unwrapped_model

        embeddings.neftune_noise_alpha = self.neftune_noise_alpha
        hook_handle = embeddings.register_forward_hook(neftune_post_forward_hook)
        self.neftune_hook_handle = hook_handle
        return model

    def _deactivate_neftune(self, model):
        """
        åœç”¨ NEFTune æ–¹æ³•ã€‚ç¡®ä¿å…ˆè°ƒç”¨ `_activate_neftune`ã€‚
        """
        if not hasattr(self, "neftune_hook_handle"):
            raise ValueError("Neftune is not activated make sure to call `trainer._activate_neftune()` first")

        unwrapped_model = unwrap_model(model)

        if _is_peft_model(unwrapped_model):
            embeddings = unwrapped_model.base_model.model.get_input_embeddings()
        else:
            embeddings = unwrapped_model.get_input_embeddings()

        self.neftune_hook_handle.remove()
        del embeddings.neftune_noise_alpha, unwrapped_model

    def add_callback(self, callback):
        """
        å‘å½“å‰çš„ [`~transformers.TrainerCallback`] åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªå›è°ƒå‡½æ•°ã€‚

        Args:
           callback (type or [`~transformers.TrainerCallback`]):
               [`~transformers.TrainerCallback`] ç±»æˆ–å…¶å®ä¾‹ã€‚å¦‚æœæ˜¯ç±»ï¼Œåˆ™å®ä¾‹åŒ–ä¸€ä¸ªè¯¥ç±»çš„æˆå‘˜ã€‚
        """
    def pop_callback(self, callback):
        """
        Remove a callback from the current list of [`~transformers.TrainerCallback`] and returns it.

        If the callback is not found, returns `None` (and no error is raised).

        Args:
           callback (`type` or [`~transformers.TrainerCallback`]):
               A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the
               first case, will pop the first member of that class found in the list of callbacks.

        Returns:
            [`~transformers.TrainerCallback`]: The callback removed, if found.
        """
        # è°ƒç”¨å›è°ƒå¤„ç†å™¨çš„æ–¹æ³•ï¼Œä»å½“å‰å›è°ƒåˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›æŒ‡å®šçš„å›è°ƒå¯¹è±¡
        return self.callback_handler.pop_callback(callback)

    def remove_callback(self, callback):
        """
        Remove a callback from the current list of [`~transformers.TrainerCallback`].

        Args:
           callback (`type` or [`~transformers.TrainerCallback`]):
               A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the
               first case, will remove the first member of that class found in the list of callbacks.
        """
        # è°ƒç”¨å›è°ƒå¤„ç†å™¨çš„æ–¹æ³•ï¼Œä»å½“å‰å›è°ƒåˆ—è¡¨ä¸­ç§»é™¤æŒ‡å®šçš„å›è°ƒå¯¹è±¡
        self.callback_handler.remove_callback(callback)

    def _move_model_to_device(self, model, device):
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š
        model = model.to(device)
        # å¦‚æœå½“å‰ä½¿ç”¨çš„æ˜¯TPUå¹¶ä¸”æ¨¡å‹å…·æœ‰"tie_weights"æ–¹æ³•ï¼Œåˆ™éœ€è¦é‡æ–°è¿æ¥æƒé‡
        if self.args.parallel_mode == ParallelMode.TPU and hasattr(model, "tie_weights"):
            model.tie_weights()

    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            # æ£€æŸ¥æ¨¡å‹çš„å‰å‘æ–¹æ³•ç­¾åï¼Œä»…ä¿ç•™æ¨¡å‹æ¥å—çš„å‚æ•°åˆ—
            model_to_inspect = self.model
            # å¦‚æœæ¨¡å‹æ˜¯PEFTæ¨¡å‹ï¼Œåˆ™è°ƒæ•´è¦æ£€æŸ¥çš„åŸºç¡€æ¨¡å‹
            if _is_peft_model(self.model):
                if hasattr(self.model, "get_base_model"):
                    model_to_inspect = self.model.get_base_model()
                else:
                    # å¯¹äºPeftMixedModelï¼Œæ²¡æœ‰æä¾›"get_base_model"æ–¹æ³•ï¼Œå› æ­¤éœ€è¦ç›´æ¥è®¿é—®base_model.model
                    model_to_inspect = self.model.base_model.model
            # è·å–æ¨¡å‹å‰å‘æ–¹æ³•çš„å‚æ•°ç­¾åï¼Œå¹¶å°†å‚æ•°åç§°æ·»åŠ åˆ°ç­¾ååˆ—åˆ—è¡¨ä¸­
            signature = inspect.signature(model_to_inspect.forward)
            self._signature_columns = list(signature.parameters.keys())
            # æ ‡ç­¾å¯èƒ½å‘½åä¸ºlabelæˆ–label_idsï¼Œä½¿ç”¨é»˜è®¤æ•°æ®æ‹¼æ¥å™¨æ¥å¤„ç†è¿™äº›æƒ…å†µ
            self._signature_columns += list(set(["label", "label_ids"] + self.label_names))
    def _remove_unused_columns(self, dataset: "datasets.Dataset", description: Optional[str] = None):
        # å¦‚æœä¸éœ€è¦ç§»é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œåˆ™ç›´æ¥è¿”å›åŸå§‹æ•°æ®é›†
        if not self.args.remove_unused_columns:
            return dataset
        # æ ¹æ®éœ€è¦è®¾ç½®ç­¾ååˆ—
        self._set_signature_columns_if_needed()
        # è·å–ç­¾ååˆ—
        signature_columns = self._signature_columns

        # æ‰¾å‡ºæ•°æ®é›†ä¸­è¢«å¿½ç•¥çš„åˆ—ï¼ˆå³ä¸åœ¨ç­¾ååˆ—ä¸­çš„åˆ—ï¼‰
        ignored_columns = list(set(dataset.column_names) - set(signature_columns))
        # å¦‚æœæœ‰è¢«å¿½ç•¥çš„åˆ—ï¼Œåˆ™ç”Ÿæˆæè¿°ä¿¡æ¯
        dset_description = "" if description is None else f"in the {description} set"
        # è®°å½•æ—¥å¿—ï¼ŒæŒ‡å‡ºè¢«å¿½ç•¥çš„åˆ—
        logger.info(
            f"The following columns {dset_description} don't have a corresponding argument in "
            f"`{self.model.__class__.__name__}.forward` and have been ignored: {', '.join(ignored_columns)}."
            f" If {', '.join(ignored_columns)} are not expected by `{self.model.__class__.__name__}.forward`, "
            " you can safely ignore this message."
        )

        # ç­›é€‰å‡ºåœ¨ç­¾ååˆ—ä¸­ä¸”å­˜åœ¨äºæ•°æ®é›†åˆ—åä¸­çš„åˆ—
        columns = [k for k in signature_columns if k in dataset.column_names]

        # æ ¹æ® datasets åº“çš„ç‰ˆæœ¬è¿›è¡Œä¸åŒå¤„ç†
        if version.parse(datasets.__version__) < version.parse("1.4.0"):
            # è®¾ç½®æ•°æ®é›†çš„æ ¼å¼ï¼Œä¿ç•™æŒ‡å®šçš„åˆ—
            dataset.set_format(
                type=dataset.format["type"], columns=columns, format_kwargs=dataset.format["format_kwargs"]
            )
            return dataset
        else:
            # ç§»é™¤æ•°æ®é›†ä¸­çš„è¢«å¿½ç•¥åˆ—
            return dataset.remove_columns(ignored_columns)

    def _get_collator_with_removed_columns(
        self, data_collator: Callable, description: Optional[str] = None
    ) -> Callable:
        """Wrap the data collator in a callable removing unused columns."""
        # å¦‚æœä¸éœ€è¦ç§»é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œåˆ™ç›´æ¥è¿”å›åŸå§‹çš„æ•°æ® collator
        if not self.args.remove_unused_columns:
            return data_collator
        # æ ¹æ®éœ€è¦è®¾ç½®ç­¾ååˆ—
        self._set_signature_columns_if_needed()
        # è·å–ç­¾ååˆ—
        signature_columns = self._signature_columns

        # åˆ›å»ºä¸€ä¸ªç§»é™¤æœªä½¿ç”¨åˆ—çš„æ•°æ® collator å¯¹è±¡
        remove_columns_collator = RemoveColumnsCollator(
            data_collator=data_collator,
            signature_columns=signature_columns,
            logger=logger,
            description=description,
            model_name=self.model.__class__.__name__,
        )
        return remove_columns_collator
    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:
        if self.train_dataset is None or not has_length(self.train_dataset):
            return None

        # æ„å»ºæ•°æ®é‡‡æ ·å™¨ã€‚
        if self.args.group_by_length:
            # å¦‚æœæ•°æ®é›†æ”¯æŒ datasets åº“å¹¶ä¸”æ˜¯ datasets.Dataset ç±»å‹ï¼Œåˆ™è·å–é•¿åº¦ä¿¡æ¯
            if is_datasets_available() and isinstance(self.train_dataset, datasets.Dataset):
                lengths = (
                    self.train_dataset[self.args.length_column_name]
                    if self.args.length_column_name in self.train_dataset.column_names
                    else None
                )
            else:
                lengths = None
            # è·å–æ¨¡å‹è¾“å…¥çš„åç§°ï¼Œé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæ¨¡å‹è¾“å…¥çš„åç§°
            model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None
            # è¿”å›ä¸€ä¸ª LengthGroupedSampler å¯¹è±¡ï¼Œç”¨äºæŒ‰é•¿åº¦åˆ†ç»„çš„æ•°æ®é‡‡æ ·
            return LengthGroupedSampler(
                self.args.train_batch_size * self.args.gradient_accumulation_steps,
                dataset=self.train_dataset,
                lengths=lengths,
                model_input_name=model_input_name,
            )

        else:
            # å¦‚æœä¸æŒ‰é•¿åº¦åˆ†ç»„ï¼Œåˆ™è¿”å›ä¸€ä¸ªéšæœºé‡‡æ ·å™¨ RandomSampler
            return RandomSampler(self.train_dataset)

    def get_train_dataloader(self) -> DataLoader:
        """
        è¿”å›è®­ç»ƒæ•°æ®åŠ è½½å™¨ [`~torch.utils.data.DataLoader`]ã€‚

        å¦‚æœ `train_dataset` ä¸å®ç° `__len__`ï¼Œåˆ™ä¸ä½¿ç”¨é‡‡æ ·å™¨ï¼›å¦åˆ™ä½¿ç”¨éšæœºé‡‡æ ·å™¨ï¼ˆé€‚åº”åˆ†å¸ƒå¼è®­ç»ƒï¼‰ã€‚

        å¦‚æœéœ€è¦æ³¨å…¥è‡ªå®šä¹‰è¡Œä¸ºï¼Œè¯·å­ç±»åŒ–å¹¶é‡å†™æ­¤æ–¹æ³•ã€‚
        """
        if self.train_dataset is None:
            raise ValueError("Trainer: training requires a train_dataset.")

        train_dataset = self.train_dataset
        data_collator = self.data_collator
        # å¦‚æœæ”¯æŒ datasets åº“å¹¶ä¸” train_dataset æ˜¯ datasets.Dataset ç±»å‹ï¼Œåˆ™ç§»é™¤æœªä½¿ç”¨çš„åˆ—
        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):
            train_dataset = self._remove_unused_columns(train_dataset, description="training")
        else:
            # å¦åˆ™ï¼Œä½¿ç”¨ç§»é™¤äº†æœªä½¿ç”¨åˆ—çš„æ•°æ®é›†æ”¶é›†å™¨
            data_collator = self._get_collator_with_removed_columns(data_collator, description="training")

        # è®¾ç½®æ•°æ®åŠ è½½å™¨çš„å‚æ•°
        dataloader_params = {
            "batch_size": self._train_batch_size,
            "collate_fn": data_collator,
            "num_workers": self.args.dataloader_num_workers,
            "pin_memory": self.args.dataloader_pin_memory,
            "persistent_workers": self.args.dataloader_persistent_workers,
        }

        # å¦‚æœ train_dataset ä¸æ˜¯ IterableDataset ç±»å‹ï¼Œåˆ™è®¾ç½®é‡‡æ ·å™¨ç­‰å‚æ•°
        if not isinstance(train_dataset, torch.utils.data.IterableDataset):
            dataloader_params["sampler"] = self._get_train_sampler()
            dataloader_params["drop_last"] = self.args.dataloader_drop_last
            dataloader_params["worker_init_fn"] = seed_worker
            dataloader_params["prefetch_factor"] = self.args.dataloader_prefetch_factor

        # ä½¿ç”¨åŠ é€Ÿå™¨å‡†å¤‡æ•°æ®åŠ è½½å™¨å¹¶è¿”å›
        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))
    # è¿”å›ç”¨äºè¯„ä¼°æ•°æ®é›†çš„é‡‡æ ·å™¨ï¼Œæ ¹æ®ä¸åŒæ¡ä»¶è¿”å›ä¸åŒçš„é‡‡æ ·å™¨æˆ–è€… None
    def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.Sampler]:
        # å¦‚æœä½¿ç”¨äº†é—ç•™çš„é¢„æµ‹å¾ªç¯æ–¹å¼
        if self.args.use_legacy_prediction_loop:
            # å¦‚æœå½“å‰ç¯å¢ƒæ”¯æŒ Torch XLA
            if is_torch_xla_available():
                # è¿”å›ä¸€ä¸ªæŒ‰é¡ºåºåˆ†å¸ƒçš„åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼Œç”¨äº Torch XLA ç¯å¢ƒ
                return SequentialDistributedSampler(
                    eval_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()
                )
            # å¦‚æœå½“å‰ç¯å¢ƒæ”¯æŒ SageMaker çš„å¤šè¿›ç¨‹
            elif is_sagemaker_mp_enabled():
                # è¿”å›ä¸€ä¸ªæŒ‰é¡ºåºåˆ†å¸ƒçš„åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼Œç”¨äº SageMaker å¤šè¿›ç¨‹ç¯å¢ƒ
                return SequentialDistributedSampler(
                    eval_dataset,
                    num_replicas=smp.dp_size(),
                    rank=smp.dp_rank(),
                    batch_size=self.args.per_device_eval_batch_size,
                )
            else:
                # è¿”å›ä¸€ä¸ªæŒ‰é¡ºåºçš„é‡‡æ ·å™¨
                return SequentialSampler(eval_dataset)

        # å¦‚æœè®¾å¤‡çš„æ•°é‡å°äºç­‰äº 1ï¼Œè¿”å›ä¸€ä¸ªæŒ‰é¡ºåºçš„é‡‡æ ·å™¨
        if self.args.world_size <= 1:
            return SequentialSampler(eval_dataset)
        else:
            # å¦åˆ™è¿”å› Noneï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ä»»ä½•ç‰¹å®šçš„é‡‡æ ·å™¨
            return None
    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:
        """
        Returns the evaluation `torch.utils.data.DataLoader`.

        Subclass and override this method if you want to inject some custom behavior.

        Args:
            eval_dataset (`torch.utils.data.Dataset`, *optional*):
                If provided, will override `self.eval_dataset`. If it is a `datasets.Dataset`, columns not accepted
                by the `model.forward()` method are automatically removed. It must implement `__len__`.
        """
        # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰æä¾› eval_dataset ä¸” self.eval_dataset ä¹Ÿæ²¡æœ‰è®¾ç½®ï¼Œå¦‚æœæ˜¯åˆ™æŠ›å‡ºæ•°å€¼é”™è¯¯
        if eval_dataset is None and self.eval_dataset is None:
            raise ValueError("Trainer: evaluation requires an eval_dataset.")

        # å¦‚æœå·²ç»å­˜åœ¨ _eval_dataloaderï¼Œå¹¶ä¸”è®¾ç½®äº† dataloader_persistent_workersï¼Œåˆ™é€šè¿‡åŠ é€Ÿå™¨å‡†å¤‡ _eval_dataloader å¹¶è¿”å›
        if hasattr(self, "_eval_dataloader") and self.args.dataloader_persistent_workers:
            return self.accelerator.prepare(self._eval_dataloader)

        # å¦‚æœ eval_dataset æœªæä¾›ï¼Œåˆ™ä½¿ç”¨ self.eval_dataset
        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        data_collator = self.data_collator

        # å¦‚æœä½¿ç”¨äº† datasets åº“ï¼Œå¹¶ä¸” eval_dataset æ˜¯ datasets.Dataset ç±»å‹ï¼Œåˆ™è°ƒç”¨ _remove_unused_columns æ–¹æ³•åˆ é™¤ä¸è¢« model.forward() æ–¹æ³•æ¥å—çš„åˆ—
        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):
            eval_dataset = self._remove_unused_columns(eval_dataset, description="evaluation")
        else:
            # å¦åˆ™è°ƒç”¨ _get_collator_with_removed_columns æ–¹æ³•æ›´æ–°æ•°æ®é›†çš„æ•°æ®æ”¶é›†å™¨ï¼Œä»¥ç§»é™¤ä¸éœ€è¦çš„åˆ—
            data_collator = self._get_collator_with_removed_columns(data_collator, description="evaluation")

        # è®¾ç½®æ•°æ®åŠ è½½å™¨å‚æ•°
        dataloader_params = {
            "batch_size": self.args.eval_batch_size,
            "collate_fn": data_collator,
            "num_workers": self.args.dataloader_num_workers,
            "pin_memory": self.args.dataloader_pin_memory,
            "persistent_workers": self.args.dataloader_persistent_workers,
        }

        # å¦‚æœ eval_dataset ä¸æ˜¯ IterableDataset ç±»å‹ï¼Œåˆ™è®¾ç½® samplerã€drop_last å’Œ prefetch_factor å‚æ•°
        if not isinstance(eval_dataset, torch.utils.data.IterableDataset):
            dataloader_params["sampler"] = self._get_eval_sampler(eval_dataset)
            dataloader_params["drop_last"] = self.args.dataloader_drop_last
            dataloader_params["prefetch_factor"] = self.args.dataloader_prefetch_factor

        # åˆ›å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨ DataLoader å¯¹è±¡
        eval_dataloader = DataLoader(eval_dataset, **dataloader_params)

        # å¦‚æœè®¾ç½®äº† dataloader_persistent_workersï¼Œåˆ™å°† eval_dataloader èµ‹å€¼ç»™ self._eval_dataloader
        if self.args.dataloader_persistent_workers:
            self._eval_dataloader = eval_dataloader

        # è¿”å›é€šè¿‡åŠ é€Ÿå™¨å‡†å¤‡åçš„ eval_dataloader
        return self.accelerator.prepare(eval_dataloader)
    def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:
        """
        Returns the test [`~torch.utils.data.DataLoader`].

        Subclass and override this method if you want to inject some custom behavior.

        Args:
            test_dataset (`torch.utils.data.Dataset`, *optional*):
                The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the
                `model.forward()` method are automatically removed. It must implement `__len__`.
        """
        # è·å–æ•°æ®æ”¶é›†å™¨
        data_collator = self.data_collator

        # å¦‚æœdatasetsåº“å¯ç”¨ä¸”test_datasetæ˜¯datasets.Datasetç±»å‹ï¼Œåˆ™ç§»é™¤æ¨¡å‹ä¸æ¥å—çš„åˆ—
        if is_datasets_available() and isinstance(test_dataset, datasets.Dataset):
            test_dataset = self._remove_unused_columns(test_dataset, description="test")
        else:
            # å¦åˆ™ï¼Œä½¿ç”¨ç§»é™¤ç‰¹å®šåˆ—çš„æ•°æ®æ”¶é›†å™¨
            data_collator = self._get_collator_with_removed_columns(data_collator, description="test")

        # è®¾ç½®DataLoaderçš„å‚æ•°
        dataloader_params = {
            "batch_size": self.args.eval_batch_size,
            "collate_fn": data_collator,
            "num_workers": self.args.dataloader_num_workers,
            "pin_memory": self.args.dataloader_pin_memory,
            "persistent_workers": self.args.dataloader_persistent_workers,
        }

        # å¦‚æœtest_datasetä¸æ˜¯IterableDatasetç±»å‹ï¼Œåˆ™é…ç½®é‡‡æ ·å™¨å’Œå…¶ä»–å‚æ•°
        if not isinstance(test_dataset, torch.utils.data.IterableDataset):
            dataloader_params["sampler"] = self._get_eval_sampler(test_dataset)
            dataloader_params["drop_last"] = self.args.dataloader_drop_last
            dataloader_params["prefetch_factor"] = self.args.dataloader_prefetch_factor

        # ä½¿ç”¨åŠ é€Ÿå™¨å‡†å¤‡DataLoaderå¹¶è¿”å›
        return self.accelerator.prepare(DataLoader(test_dataset, **dataloader_params))

    def create_optimizer_and_scheduler(self, num_training_steps: int):
        """
        Setup the optimizer and the learning rate scheduler.

        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
        Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or
        `create_scheduler`) in a subclass.
        """
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.create_optimizer()
        
        # æ ¹æ®æ¡ä»¶é€‰æ‹©ä¼˜åŒ–å™¨
        if IS_SAGEMAKER_MP_POST_1_10 and smp.state.cfg.fp16:
            # å¦‚æœä½¿ç”¨çš„SageMakerç‰ˆæœ¬ >= 1.10 å¹¶ä¸”å¯ç”¨äº†fp16ï¼Œè§£åŒ…ä¼˜åŒ–å™¨
            optimizer = self.optimizer.optimizer
        else:
            optimizer = self.optimizer
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.create_scheduler(num_training_steps=num_training_steps, optimizer=optimizer)

    def get_decay_parameter_names(self, model) -> List[str]:
        """
        Get all parameter names that weight decay will be applied to

        Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still
        apply to those modules since this function only filter out instance of nn.LayerNorm
        """
        # è·å–æ‰€æœ‰éœ€è¦åº”ç”¨æƒé‡è¡°å‡çš„å‚æ•°åç§°
        decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)
        
        # è¿‡æ»¤æ‰åŒ…å«"bias"çš„å‚æ•°å
        decay_parameters = [name for name in decay_parameters if "bias" not in name]
        
        return decay_parameters
    def create_optimizer(self):
        """
        Setup the optimizer.

        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
        Trainer's init through `optimizers`, or subclass and override this method in a subclass.
        """
        # æ ¹æ®æ¡ä»¶é€‰æ‹©è¦ä¼˜åŒ–çš„æ¨¡å‹
        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model

        # å¦‚æœæ²¡æœ‰æŒ‡å®šä¼˜åŒ–å™¨ï¼Œåˆ™æ ¹æ®æ¨¡å‹å‚æ•°è¿›è¡Œåˆ†ç»„è®¾ç½®
        if self.optimizer is None:
            # è·å–éœ€è¦è¿›è¡Œæƒé‡è¡°å‡çš„å‚æ•°ååˆ—è¡¨
            decay_parameters = self.get_decay_parameter_names(opt_model)
            # è®¾ç½®ä¼˜åŒ–å™¨çš„åˆ†ç»„å‚æ•°
            optimizer_grouped_parameters = [
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)
                    ],
                    "weight_decay": self.args.weight_decay,
                },
                {
                    "params": [
                        p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)
                    ],
                    "weight_decay": 0.0,
                },
            ]

            # è·å–ä¼˜åŒ–å™¨ç±»å’Œå‚æ•°
            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args, opt_model)

            # å¦‚æœ optimizer_kwargs ä¸­åŒ…å« 'params' é”®ï¼Œåˆ™ä½¿ç”¨å®ƒè¦†ç›–ä¹‹å‰è®¾ç½®çš„ optimizer_grouped_parameters
            # ä¾‹å¦‚ï¼Œé€‚ç”¨äº GaLore ä¼˜åŒ–å™¨
            if "params" in optimizer_kwargs:
                optimizer_grouped_parameters = optimizer_kwargs.pop("params")

            # å¦‚æœ optimizer_kwargs ä¸­åŒ…å« 'optimizer_dict' é”®ï¼Œåˆ™ä½¿ç”¨å®ƒè¦†ç›– optimizer_grouped_parameters
            # é¿å…å‚æ•°å†²çªé—®é¢˜ï¼Œé€‚ç”¨äºé€å±‚çš„è™šæ‹Ÿä¼˜åŒ–å™¨
            if "optimizer_dict" in optimizer_kwargs:
                optimizer_grouped_parameters = optimizer_kwargs.pop("optimizer_dict")

            # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)
            
            # å¦‚æœä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯ 'Adam8bit'ï¼Œåˆ™æ‰§è¡Œç‰¹å®šå¤„ç†
            if optimizer_cls.__name__ == "Adam8bit":
                import bitsandbytes

                # è·å–å…¨å±€ä¼˜åŒ–ç®¡ç†å™¨çš„å®ä¾‹
                manager = bitsandbytes.optim.GlobalOptimManager.get_instance()

                skipped = 0
                # æ³¨å†Œæ‰€æœ‰åµŒå…¥å±‚æ¨¡å—å¹¶æŒ‡å®šä¼˜åŒ–å‚æ•°çš„ç²¾åº¦ä¸º 32 ä½æµ®ç‚¹æ•°
                for module in opt_model.modules():
                    if isinstance(module, nn.Embedding):
                        skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())
                        logger.info(f"skipped {module}: {skipped/2**20}M params")
                        manager.register_module_override(module, "weight", {"optim_bits": 32})
                        logger.debug(f"bitsandbytes: will optimize {module} in fp32")
                logger.info(f"skipped: {skipped/2**20}M params")

        # å¦‚æœå¯ç”¨äº† SageMaker å¤šè¿›ç¨‹ï¼Œåˆ™ä½¿ç”¨åˆ†å¸ƒå¼ä¼˜åŒ–å™¨
        if is_sagemaker_mp_enabled():
            self.optimizer = smp.DistributedOptimizer(self.optimizer)

        # è¿”å›è®¾ç½®å¥½çš„ä¼˜åŒ–å™¨
        return self.optimizer

    @staticmethod
    def get_optimizer_cls_and_kwargs(
        args: TrainingArguments, model: Optional[PreTrainedModel] = None
    ):
        """
        Helper function to retrieve the optimizer class and its keyword arguments based on training arguments and model.
        """
        # çœç•¥ï¼Œç”¨äºè·å–ä¼˜åŒ–å™¨ç±»å’Œå‚æ•°çš„è¾…åŠ©å‡½æ•°
        pass
    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):
        """
        è®¾ç½®è°ƒåº¦å™¨ã€‚åœ¨è°ƒç”¨æ­¤æ–¹æ³•ä¹‹å‰ï¼Œè®­ç»ƒå™¨çš„ä¼˜åŒ–å™¨å¿…é¡»å·²ç»è®¾ç½®å¥½ï¼Œæˆ–è€…ä½œä¸ºå‚æ•°ä¼ é€’è¿›æ¥ã€‚

        Args:
            num_training_steps (int): è¦æ‰§è¡Œçš„è®­ç»ƒæ­¥æ•°ã€‚
        """
        # å¦‚æœå½“å‰æ²¡æœ‰è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œåˆ™æ ¹æ®å‚æ•°è®¾ç½®ä¸€ä¸ªæ–°çš„è°ƒåº¦å™¨
        if self.lr_scheduler is None:
            self.lr_scheduler = get_scheduler(
                self.args.lr_scheduler_type,
                optimizer=self.optimizer if optimizer is None else optimizer,
                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),
                num_training_steps=num_training_steps,
                scheduler_specific_kwargs=self.args.lr_scheduler_kwargs,
            )
            # æ ‡è®°å·²ç»åˆ›å»ºäº†å­¦ä¹ ç‡è°ƒåº¦å™¨
            self._created_lr_scheduler = True
        # è¿”å›å½“å‰çš„å­¦ä¹ ç‡è°ƒåº¦å™¨å¯¹è±¡
        return self.lr_scheduler

    def num_examples(self, dataloader: DataLoader) -> int:
        """
        è¾…åŠ©å‡½æ•°ï¼Œé€šè¿‡è®¿é—®å…¶æ•°æ®é›†æ¥è·å– [`~torch.utils.data.DataLoader`] ä¸­çš„æ ·æœ¬æ•°é‡ã€‚
        å½“ dataloader.dataset ä¸å­˜åœ¨æˆ–é•¿åº¦ä¸ºé›¶æ—¶ï¼Œå°½å¯èƒ½è¿›è¡Œä¼°è®¡ã€‚
        """
        try:
            dataset = dataloader.dataset
            # å¯¹äº IterableDatasetShardï¼Œéœ€è¦è¿›ä¸€æ­¥æ·±å…¥è·å–å…¶æ•°æ®é›†çš„é•¿åº¦
            if isinstance(dataset, IterableDatasetShard):
                return len(dataloader.dataset.dataset)
            # è¿”å› DataLoader ä¸­æ•°æ®é›†çš„é•¿åº¦
            return len(dataloader.dataset)
        except (NameError, AttributeError, TypeError):  # å¦‚æœæ²¡æœ‰æ•°æ®é›†æˆ–é•¿åº¦ä¿¡æ¯ï¼Œé€šè¿‡ DataLoader çš„é•¿åº¦ä¼°ç®—
            # é€šè¿‡ DataLoader çš„é•¿åº¦ä¼°ç®—æ ·æœ¬æ•°ï¼Œä¹˜ä»¥æ¯è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å°
            return len(dataloader) * self.args.per_device_train_batch_size

    def num_tokens(self, train_dl: DataLoader, max_steps: Optional[int] = None) -> int:
        """
        è¾…åŠ©å‡½æ•°ï¼Œé€šè¿‡æšä¸¾æ•°æ®åŠ è½½å™¨æ¥è·å– [`~torch.utils.data.DataLoader`] ä¸­çš„ä»¤ç‰Œæ•°é‡ã€‚
        """
        train_tokens = 0
        try:
            # æšä¸¾è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ­¥éª¤å’Œæ‰¹æ¬¡
            for step, batch in enumerate(train_dl):
                tokens = batch["input_ids"].numel()  # è·å–å½“å‰æ‰¹æ¬¡ä¸­ "input_ids" çš„ä»¤ç‰Œæ•°é‡
                if max_steps is not None:
                    return tokens * max_steps  # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ­¥æ•°ï¼Œåˆ™è¿”å›æŒ‰æœ€å¤§æ­¥æ•°ä¼°ç®—çš„ä»¤ç‰Œæ€»æ•°
                train_tokens += tokens  # ç´¯åŠ å½“å‰æ‰¹æ¬¡çš„ä»¤ç‰Œæ•°é‡åˆ°è®­ç»ƒä»¤ç‰Œæ€»æ•°
            return train_tokens  # è¿”å›è®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸­çš„æ€»ä»¤ç‰Œæ•°é‡
        except KeyError:
            logger.warning("Cannot get num_tokens from dataloader")  # æ—¥å¿—è­¦å‘Šï¼Œæ— æ³•ä»æ•°æ®åŠ è½½å™¨è·å–ä»¤ç‰Œæ•°é‡ä¿¡æ¯
            return train_tokens  # è¿”å›å½“å‰å·²ç»ç´¯ç§¯çš„è®­ç»ƒä»¤ç‰Œæ€»æ•°
    # è®¾ç½®è¶…å‚æ•°æœç´¢çš„åˆå§‹åŒ–ä»£ç 
    def _hp_search_setup(self, trial: Union["optuna.Trial", Dict[str, Any]]):
        """HP search setup code"""
        # å°†è¯•éªŒå¯¹è±¡ä¿å­˜åˆ°å®ä¾‹å±æ€§ä¸­
        self._trial = trial

        # å¦‚æœè¶…å‚æ•°æœç´¢åç«¯ä¸ºç©ºæˆ–è¯•éªŒå¯¹è±¡ä¸ºç©ºï¼Œåˆ™ç›´æ¥è¿”å›
        if self.hp_search_backend is None or trial is None:
            return

        # æ ¹æ®é€‰æ‹©çš„è¶…å‚æ•°æœç´¢åç«¯è¿›è¡Œå‚æ•°åˆå§‹åŒ–
        if self.hp_search_backend == HPSearchBackend.OPTUNA:
            # ä½¿ç”¨ Optuna æä¾›çš„è¶…å‚æ•°ç©ºé—´ç”Ÿæˆå‚æ•°å­—å…¸
            params = self.hp_space(trial)
        elif self.hp_search_backend == HPSearchBackend.RAY:
            # å¦‚æœä½¿ç”¨ Ray æä¾›çš„è¯•éªŒå‚æ•°ï¼Œæ’é™¤å…¶ä¸­çš„ WandB ç›¸å…³é¡¹
            params = trial
            params.pop("wandb", None)
        elif self.hp_search_backend == HPSearchBackend.SIGOPT:
            # å¦‚æœä½¿ç”¨ SigOpt æä¾›çš„è¯•éªŒåˆ†é…ï¼Œå°†å­—ç¬¦ä¸²å½¢å¼çš„å€¼è½¬æ¢ä¸ºæ•´æ•°
            params = {k: int(v) if isinstance(v, str) else v for k, v in trial.assignments.items()}
        elif self.hp_search_backend == HPSearchBackend.WANDB:
            # å¦‚æœä½¿ç”¨ WandB æä¾›çš„è¯•éªŒå‚æ•°
            params = trial

        # æ ¹æ®å‚æ•°å­—å…¸æ›´æ–°å®ä¾‹å±æ€§ä¸­çš„å‚æ•°
        for key, value in params.items():
            # å¦‚æœå‚æ•°åœ¨ self.args ä¸­ä¸å­˜åœ¨ï¼Œåˆ™å‘å‡ºè­¦å‘Š
            if not hasattr(self.args, key):
                logger.warning(
                    f"Trying to set {key} in the hyperparameter search but there is no corresponding field in"
                    " `TrainingArguments`."
                )
                continue
            old_attr = getattr(self.args, key, None)
            # å°†å€¼è½¬æ¢ä¸ºä¸æ—§å±æ€§ç›¸åŒç±»å‹çš„å€¼
            if old_attr is not None:
                value = type(old_attr)(value)

            # æ›´æ–° self.args ä¸­çš„å‚æ•°å€¼
            setattr(self.args, key, value)

        # æ ¹æ®ä¸åŒçš„è¶…å‚æ•°æœç´¢åç«¯è®°å½•æ—¥å¿—ä¿¡æ¯
        if self.hp_search_backend == HPSearchBackend.OPTUNA:
            logger.info(f"Trial: {trial.params}")
        if self.hp_search_backend == HPSearchBackend.SIGOPT:
            logger.info(f"SigOpt Assignments: {trial.assignments}")
        if self.hp_search_backend == HPSearchBackend.WANDB:
            logger.info(f"W&B Sweep parameters: {trial}")

        # å¦‚æœå¯ç”¨äº† DeepSpeed åŠ é€Ÿï¼Œå¹¶ä¸”æœªè®¾ç½® args.deepspeedï¼Œåˆ™å¼•å‘å¼‚å¸¸
        if self.is_deepspeed_enabled:
            if self.args.deepspeed is None:
                raise ValueError("For sweeps with deepspeed, `args.deepspeed` must be set")

            # é‡æ–°æ„å»º DeepSpeed é…ç½®ï¼Œä»¥åæ˜ æ›´æ–°åçš„è®­ç»ƒå‚æ•°
            from accelerate.utils import DeepSpeedPlugin
            from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig

            # åˆ›å»º HfTrainerDeepSpeedConfig å®ä¾‹
            self.args.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.args.deepspeed)
            # å¤„ç† trainer é…ç½®è¿‡ç¨‹
            self.args.hf_deepspeed_config.trainer_config_process(self.args)
            # åˆ›å»º DeepSpeed æ’ä»¶
            self.args.deepspeed_plugin = DeepSpeedPlugin(hf_ds_config=self.args.hf_deepspeed_config)

        # åˆ›å»ºåŠ é€Ÿå™¨å¹¶è¿›è¡Œåå¤„ç†
        self.create_accelerator_and_postprocess()
    # å°†è¯•éªŒç»“æœæŠ¥å‘Šç»™è¶…å‚æ•°æœç´¢åç«¯ï¼ˆä¾‹å¦‚Optunaæˆ–Rayï¼‰
    def _report_to_hp_search(self, trial: Union["optuna.Trial", Dict[str, Any]], step: int, metrics: Dict[str, float]):
        # å¦‚æœè¶…å‚æ•°æœç´¢åç«¯æœªè®¾ç½®æˆ–è¯•éªŒå¯¹è±¡ä¸ºç©ºï¼Œåˆ™ç›´æ¥è¿”å›
        if self.hp_search_backend is None or trial is None:
            return
        
        # å¤åˆ¶metricsï¼Œä»¥å…ä¿®æ”¹åŸå§‹æ•°æ®
        metrics = metrics.copy()
        
        # è®¡ç®—å½“å‰ç›®æ ‡å€¼
        self.objective = self.compute_objective(metrics)
        
        # å¦‚æœä½¿ç”¨Optunaä½œä¸ºè¶…å‚æ•°æœç´¢åç«¯
        if self.hp_search_backend == HPSearchBackend.OPTUNA:
            import optuna
            
            # å¦‚æœè¯•éªŒä¸æ˜¯å¤šç›®æ ‡ä¼˜åŒ–
            if not trial.study._is_multi_objective():
                # å‘Optunaè¯•éªŒæŠ¥å‘Šå½“å‰ç›®æ ‡å€¼å’Œæ­¥æ•°
                trial.report(self.objective, step)
                
                # å¦‚æœè¯•éªŒåº”è¯¥è¢«å‰ªæï¼Œåˆ™ç»“æŸè®­ç»ƒå¹¶æŠ›å‡ºTrialPrunedå¼‚å¸¸
                if trial.should_prune():
                    self.callback_handler.on_train_end(self.args, self.state, self.control)
                    raise optuna.TrialPruned()
        
        # å¦‚æœä½¿ç”¨Rayä½œä¸ºè¶…å‚æ•°æœç´¢åç«¯
        elif self.hp_search_backend == HPSearchBackend.RAY:
            import ray.train
            
            # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¿å­˜æ£€æŸ¥ç‚¹
            with tempfile.TemporaryDirectory() as temp_checkpoint_dir:
                checkpoint = None
                # å¦‚æœéœ€è¦ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
                if self.control.should_save:
                    # ä¿å­˜å½“å‰æ¨¡å‹æ£€æŸ¥ç‚¹åˆ°ä¸´æ—¶ç›®å½•
                    self._tune_save_checkpoint(checkpoint_dir=temp_checkpoint_dir)
                    checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)
                
                # å°†ç›®æ ‡å€¼æ·»åŠ åˆ°metricsä¸­
                metrics["objective"] = self.objective
                
                # å‘RayæŠ¥å‘Šmetricså’Œæ£€æŸ¥ç‚¹
                ray.train.report(metrics, checkpoint=checkpoint)

    # ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€çš„æ£€æŸ¥ç‚¹
    def _tune_save_checkpoint(self, checkpoint_dir: str):
        # è®¾ç½®è¾“å‡ºç›®å½•ä¸ºä¸´æ—¶æ£€æŸ¥ç‚¹ç›®å½•ä¸‹çš„æŒ‡å®šå…¨å±€æ­¥æ•°çš„ç›®å½•
        output_dir = os.path.join(checkpoint_dir, f"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}")
        
        # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šçš„è¾“å‡ºç›®å½•
        self.save_model(output_dir, _internal_call=True)
        
        # å¦‚æœéœ€è¦ä¿å­˜å‚æ•°
        if self.args.should_save:
            # å°†å½“å‰è®­ç»ƒçŠ¶æ€ä¿å­˜ä¸ºJSONæ–‡ä»¶
            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))
            
            # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸
            torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
            
            # ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å­—å…¸
            torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))

    # è°ƒç”¨æ¨¡å‹åˆå§‹åŒ–å‡½æ•°å¹¶è¿”å›åˆå§‹åŒ–åçš„æ¨¡å‹
    def call_model_init(self, trial=None):
        # è·å–model_initå‡½æ•°çš„å‚æ•°ä¸ªæ•°
        model_init_argcount = number_of_arguments(self.model_init)
        
        # æ ¹æ®model_initå‡½æ•°çš„å‚æ•°ä¸ªæ•°è°ƒç”¨ç›¸åº”çš„åˆå§‹åŒ–æ–¹å¼
        if model_init_argcount == 0:
            model = self.model_init()
        elif model_init_argcount == 1:
            model = self.model_init(trial)
        else:
            raise RuntimeError("model_init should have 0 or 1 argument.")
        
        # å¦‚æœæ¨¡å‹åˆå§‹åŒ–ç»“æœä¸ºNoneï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        if model is None:
            raise RuntimeError("model_init should not return None.")
        
        # è¿”å›åˆå§‹åŒ–åçš„æ¨¡å‹
        return model
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºåœ¨ PyTorch ä¸­è¯„ä¼° JIT æ¨¡å¼ä¸‹çš„æ¨¡å‹
    def torch_jit_model_eval(self, model, dataloader, training=False):
        # å¦‚æœä¸æ˜¯è®­ç»ƒæ¨¡å¼
        if not training:
            # å¦‚æœæ•°æ®åŠ è½½å™¨ä¸º Noneï¼Œåˆ™è®°å½•è­¦å‘Šå¹¶è¿”å›åŸå§‹æ¨¡å‹
            if dataloader is None:
                logger.warning("failed to use PyTorch jit mode due to current dataloader is none.")
                return model
            # è·å–ä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡æ•°æ®
            example_batch = next(iter(dataloader))
            # å‡†å¤‡è¾“å…¥æ•°æ®
            example_batch = self._prepare_inputs(example_batch)
            try:
                # å¤åˆ¶æ¨¡å‹ç”¨äº JIT ç¼–è¯‘
                jit_model = copy.copy(model)
                # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                jit_model.eval()
                # æå–æ¨¡å‹ä¸­çš„åŸå§‹ forward æ–¹æ³•
                original_forward = jit_model.__dict__.pop("_original_forward", None)
                # å¦‚æœå­˜åœ¨åŸå§‹ forward æ–¹æ³•ï¼Œåˆ™æ¢å¤
                if original_forward:
                    jit_model.forward = original_forward
                # ä½¿ç”¨åŠ é€Ÿå™¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œå¹¶å…³é—­ç¼“å­˜ï¼ŒåŒæ—¶ä¸è®¡ç®—æ¢¯åº¦
                with self.accelerator.autocast(cache_enabled=False), torch.no_grad():
                    # æ ¹æ® PyTorch ç‰ˆæœ¬é€‰æ‹©ä¸åŒçš„ JIT ç¼–è¯‘æ–¹å¼
                    if version.parse(version.parse(torch.__version__).base_version) >= version.parse("2.0.0"):
                        # å¦‚æœç¤ºä¾‹æ‰¹æ¬¡æ˜¯å­—å…¸ï¼Œåˆ™ä½¿ç”¨å…³é”®å­—å‚æ•°è¾“å…¥ JIT ç¼–è¯‘
                        if isinstance(example_batch, dict):
                            jit_model = torch.jit.trace(jit_model, example_kwarg_inputs=example_batch, strict=False)
                        # å¦åˆ™ï¼Œæ„å»ºé€‚åˆçš„å…³é”®å­—å‚æ•°è¾“å…¥
                        else:
                            jit_model = torch.jit.trace(
                                jit_model,
                                example_kwarg_inputs={key: example_batch[key] for key in example_batch},
                                strict=False,
                            )
                    else:
                        # æ„å»ºé€‚åˆçš„ä½ç½®å‚æ•°è¾“å…¥
                        jit_inputs = []
                        for key in example_batch:
                            example_tensor = torch.ones_like(example_batch[key])
                            jit_inputs.append(example_tensor)
                        jit_inputs = tuple(jit_inputs)
                        jit_model = torch.jit.trace(jit_model, jit_inputs, strict=False)
                # å†»ç»“ JIT ç¼–è¯‘åçš„æ¨¡å‹ï¼Œä»¥æé«˜æ€§èƒ½
                jit_model = torch.jit.freeze(jit_model)
                # ä½¿ç”¨ JIT æ¨¡å‹æ‰§è¡Œä¸¤æ¬¡ç¤ºä¾‹æ‰¹æ¬¡æ•°æ®
                with torch.no_grad():
                    jit_model(**example_batch)
                    jit_model(**example_batch)
                # æ›´æ–°æ¨¡å‹ä¸º JIT ç¼–è¯‘åçš„æ¨¡å‹ï¼Œå¹¶å…³é—­ CPU è‡ªåŠ¨æ··åˆç²¾åº¦ä¼˜åŒ–
                model = jit_model
                self.use_cpu_amp = False
            # æ•è·å¯èƒ½çš„å¼‚å¸¸å¹¶è®°å½•è­¦å‘Š
            except (RuntimeError, TypeError, ValueError, NameError, IndexError) as e:
                logger.warning(f"failed to use PyTorch jit mode due to: {e}.")

        # è¿”å›æœ€ç»ˆçš„æ¨¡å‹
        return model
    # ä½¿ç”¨ IPEX ä¼˜åŒ–æ¨¡å‹ï¼Œå¦‚æœ IPEX ä¸å¯ç”¨åˆ™æŠ›å‡º ImportError å¼‚å¸¸
    def ipex_optimize_model(self, model, training=False, dtype=torch.float32):
        if not is_ipex_available():
            raise ImportError(
                "Using IPEX but IPEX is not installed or IPEX's version does not match current PyTorch, please refer"
                " to https://github.com/intel/intel-extension-for-pytorch."
            )

        import intel_extension_for_pytorch as ipex

        if not training:
            # å¦‚æœä¸æ˜¯è®­ç»ƒæ¨¡å¼ï¼Œè®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            model.eval()
            # æ ¹æ®æ¡ä»¶è®¾ç½® dtypeï¼Œå¦‚æœä¸åœ¨è®­ç»ƒä¸­ä¸”å¯ç”¨äº† bf16 å…¨é‡è¯„ä¼°ï¼Œåˆ™ä½¿ç”¨ torch.bfloat16
            dtype = torch.bfloat16 if not self.is_in_train and self.args.bf16_full_eval else dtype
            # å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œè®¾ç½®æ•°æ®ç±»å‹å’Œä¼˜åŒ–çº§åˆ« O1ï¼ŒåŒæ—¶ç¦ç”¨ conv_bn_folding æ¥é¿å…ç¬¦å·è·Ÿè¸ªä¸­çš„é—®é¢˜
            model = ipex.optimize(model, dtype=dtype, level="O1", conv_bn_folding=False, inplace=not self.is_in_train)
        else:
            if not model.training:
                # å¦‚æœæ¨¡å‹ä¸å¤„äºè®­ç»ƒçŠ¶æ€ï¼Œåˆ™è®¾ç½®ä¸ºè®­ç»ƒçŠ¶æ€
                model.train()
            # å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œè®¾ç½®æ•°æ®ç±»å‹ã€ä¼˜åŒ–å™¨å’Œä¼˜åŒ–çº§åˆ« O1ï¼ŒåŒæ—¶è¿›è¡ŒåŸåœ°æ“ä½œ
            model, self.optimizer = ipex.optimize(
                model, dtype=dtype, optimizer=self.optimizer, inplace=True, level="O1"
            )

        return model

    # è®­ç»ƒæ–¹æ³•ï¼Œæ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤å’Œä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°æœç´¢
    def train(
        self,
        resume_from_checkpoint: Optional[Union[str, bool]] = None,
        trial: Union["optuna.Trial", Dict[str, Any]] = None,
        ignore_keys_for_eval: Optional[List[str]] = None,
        **kwargs,
    ):
    
    # å†…éƒ¨è®­ç»ƒå¾ªç¯æ–¹æ³•ï¼Œæ¥å—æ‰¹å¤„ç†å¤§å°ã€å‚æ•°ã€ä»æ£€æŸ¥ç‚¹æ¢å¤çš„æ ‡å¿—å’Œè¶…å‚æ•°æœç´¢å®ä¾‹ä½œä¸ºè¾“å…¥
    def _inner_training_loop(
        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None
    ):
    
    # æ ¹æ®è¶…å‚æ•°æœç´¢åç«¯å’Œè¯•éªŒå¯¹è±¡ç¡®å®šè¾“å‡ºç›®å½•
    def _get_output_dir(self, trial):
        if self.hp_search_backend is not None and trial is not None:
            if self.hp_search_backend == HPSearchBackend.OPTUNA:
                run_id = trial.number
            elif self.hp_search_backend == HPSearchBackend.RAY:
                import ray.train
                run_id = ray.train.get_context().get_trial_id()
            elif self.hp_search_backend == HPSearchBackend.SIGOPT:
                run_id = trial.id
            elif self.hp_search_backend == HPSearchBackend.WANDB:
                import wandb
                run_id = wandb.run.id
            # å¦‚æœè®¾ç½®äº†è¶…å‚æ•°åç§°ç”Ÿæˆå‡½æ•°ï¼Œåˆ™ä½¿ç”¨è¯¥å‡½æ•°ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤åç§°æ ¼å¼
            run_name = self.hp_name(trial) if self.hp_name is not None else f"run-{run_id}"
            # ç»„åˆè¿è¡Œç›®å½•ï¼ŒåŸºäºè®¾å®šçš„è¾“å‡ºç›®å½•å’Œç”Ÿæˆçš„è¿è¡Œåç§°
            run_dir = os.path.join(self.args.output_dir, run_name)
        else:
            # å¦‚æœæœªè®¾ç½®è¶…å‚æ•°æœç´¢åç«¯æˆ–æ²¡æœ‰è¯•éªŒå¯¹è±¡ï¼Œåˆ™ç›´æ¥ä½¿ç”¨é»˜è®¤çš„è¾“å‡ºç›®å½•
            run_dir = self.args.output_dir
        return run_dir

    # åœ¨åŠ è½½æ¨¡å‹åå‘å‡ºè­¦å‘Šï¼Œæ ¹æ®åŠ è½½ç»“æœä¸­çš„ä¸¢å¤±å’Œæ„å¤–é”®å‘å‡ºç›¸åº”çš„è­¦å‘Šä¿¡æ¯
    def _issue_warnings_after_load(self, load_result):
        if len(load_result.missing_keys) != 0:
            if self.model._keys_to_ignore_on_save is not None and set(load_result.missing_keys) == set(
                self.model._keys_to_ignore_on_save
            ):
                # å¦‚æœåŠ è½½ç»“æœä¸­çš„ä¸¢å¤±é”®ä¸ä¿å­˜æ—¶å¿½ç•¥çš„é”®åŒ¹é…ï¼Œåˆ™è¿›è¡Œæƒé‡ç»‘å®š
                self.model.tie_weights()
            else:
                # å¦åˆ™å‘å‡ºä¸¢å¤±é”®çš„è­¦å‘Š
                logger.warning(f"There were missing keys in the checkpoint model loaded: {load_result.missing_keys}.")
        if len(load_result.unexpected_keys) != 0:
            # å¦‚æœåŠ è½½ç»“æœä¸­å­˜åœ¨æ„å¤–é”®ï¼Œåˆ™å‘å‡ºç›¸åº”çš„è­¦å‘Š
            logger.warning(
                f"There were unexpected keys in the checkpoint model loaded: {load_result.unexpected_keys}."
            )
        # æ£€æŸ¥æ˜¯å¦åº”è®°å½•æ—¥å¿—ï¼Œå¹¶ä¸”å…¨å±€æ­¥æ•°å¤§äºä¸Šæ¬¡è®°å½•çš„æ­¥æ•°
        if self.control.should_log and self.state.global_step > self._globalstep_last_logged:
            # å¦‚æœä½¿ç”¨ Torch XLAï¼Œæ ‡è®°å½“å‰æ­¥éª¤
            if is_torch_xla_available():
                xm.mark_step()

            # åˆå§‹åŒ–æ—¥å¿—å­—å…¸
            logs: Dict[str, float] = {}

            # ä½¿ç”¨ all_gather + mean() è®¡ç®—æ‰€æœ‰è¿›ç¨‹çš„å¹³å‡æŸå¤±
            tr_loss_scalar = self._nested_gather(tr_loss).mean().item()

            # å°†è®­ç»ƒæŸå¤±é‡ç½®ä¸ºé›¶
            tr_loss -= tr_loss

            # è®¡ç®—å¹¶è®°å½•å¹³å‡æŸå¤±
            logs["loss"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
            
            # å¦‚æœå­˜åœ¨æ¢¯åº¦èŒƒæ•°ï¼Œè®°å½•æ¢¯åº¦èŒƒæ•°å€¼
            if grad_norm is not None:
                logs["grad_norm"] = grad_norm.detach().item() if isinstance(grad_norm, torch.Tensor) else grad_norm
            
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            logs["learning_rate"] = self._get_learning_rate()

            # ç´¯åŠ æ€»æŸå¤±
            self._total_loss_scalar += tr_loss_scalar

            # æ›´æ–°ä¸Šæ¬¡è®°å½•çš„å…¨å±€æ­¥æ•°
            self._globalstep_last_logged = self.state.global_step

            # å­˜å‚¨ FLOPsï¼ˆæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼‰
            self.store_flos()

            # è®°å½•æ—¥å¿—
            self.log(logs)

        metrics = None
        # å¦‚æœéœ€è¦è¿›è¡Œè¯„ä¼°
        if self.control.should_evaluate:
            # æ‰§è¡Œè¯„ä¼°å¹¶è·å–æŒ‡æ ‡
            metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
            # æŠ¥å‘Šç»™è¶…å‚æ•°æœç´¢å™¨å½“å‰çš„å…¨å±€æ­¥æ•°å’ŒæŒ‡æ ‡
            self._report_to_hp_search(trial, self.state.global_step, metrics)

            # å¦‚æœä½¿ç”¨ ReduceLROnPlateau ç±»å‹çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
            if isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                # è·å–ç”¨äºæœ€ä½³æ¨¡å‹é€‰æ‹©çš„æŒ‡æ ‡åç§°
                metric_to_check = self.args.metric_for_best_model
                if not metric_to_check.startswith("eval_"):
                    metric_to_check = f"eval_{metric_to_check}"
                # è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨çš„æ­¥è¿›æ–¹æ³•ï¼Œæ ¹æ®æœ€æ–°çš„æŒ‡æ ‡å€¼æ›´æ–°å­¦ä¹ ç‡
                self.lr_scheduler.step(metrics[metric_to_check])

        # å¦‚æœéœ€è¦ä¿å­˜æ¨¡å‹
        if self.control.should_save:
            # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
            self._save_checkpoint(model, trial, metrics=metrics)
            # è°ƒç”¨ä¿å­˜æ¨¡å‹æ—¶çš„å›è°ƒå¤„ç†
            self.control = self.callback_handler.on_save(self.args, self.state, self.control)
    # ä»æ£€æŸ¥ç‚¹ `checkpoint` ä¸­åŠ è½½ RNG çŠ¶æ€
    def _load_rng_state(self, checkpoint):
        # å¦‚æœæ£€æŸ¥ç‚¹ä¸º Noneï¼Œåˆ™ç›´æ¥è¿”å›
        if checkpoint is None:
            return
        
        # å¦‚æœä½¿ç”¨å¤šè¿›ç¨‹ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰ï¼Œæ ¹æ®è¿›ç¨‹ç´¢å¼•è¯»å–ç›¸åº”çš„ RNG çŠ¶æ€æ–‡ä»¶
        if self.args.world_size > 1:
            process_index = self.args.process_index
            rng_file = os.path.join(checkpoint, f"rng_state_{process_index}.pth")
            # å¦‚æœå¯¹åº”çš„ RNG æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è®°å½•è­¦å‘Šä¿¡æ¯å¹¶è¿”å›
            if not os.path.isfile(rng_file):
                logger.info(
                    f"Didn't find an RNG file for process {process_index}, if you are resuming a training that "
                    "wasn't launched in a distributed fashion, reproducibility is not guaranteed."
                )
                return
        else:
            # å¦‚æœæ˜¯å•è¿›ç¨‹ï¼Œåˆ™è¯»å–é»˜è®¤çš„ RNG çŠ¶æ€æ–‡ä»¶
            rng_file = os.path.join(checkpoint, "rng_state.pth")
            # å¦‚æœé»˜è®¤çš„ RNG æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è®°å½•è­¦å‘Šä¿¡æ¯å¹¶è¿”å›
            if not os.path.isfile(rng_file):
                logger.info(
                    "Didn't find an RNG file, if you are resuming a training that was launched in a distributed "
                    "fashion, reproducibility is not guaranteed."
                )
                return

        # åŠ è½½æ£€æŸ¥ç‚¹ä¸­çš„ RNG çŠ¶æ€
        checkpoint_rng_state = torch.load(rng_file)
        # æ¢å¤ Python å†…ç½®çš„éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
        random.setstate(checkpoint_rng_state["python"])
        # æ¢å¤ NumPy éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
        np.random.set_state(checkpoint_rng_state["numpy"])
        # æ¢å¤ PyTorch CPU éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
        torch.random.set_rng_state(checkpoint_rng_state["cpu"])
        
        # å¦‚æœ CUDA å¯ç”¨
        if torch.cuda.is_available():
            # å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¸”ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
            if self.args.parallel_mode == ParallelMode.DISTRIBUTED:
                # æ¢å¤æ‰€æœ‰ GPU çš„ CUDA éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
                torch.cuda.random.set_rng_state_all(checkpoint_rng_state["cuda"])
            else:
                try:
                    # æ¢å¤å½“å‰ GPU çš„ CUDA éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
                    torch.cuda.random.set_rng_state(checkpoint_rng_state["cuda"])
                except Exception as e:
                    logger.info(
                        f"Didn't manage to set back the RNG states of the GPU because of the following error:\n {e}"
                        "\nThis won't yield the same results as if the training had not been interrupted."
                    )
        
        # å¦‚æœä½¿ç”¨äº† Torch XLA
        if is_torch_xla_available():
            # æ¢å¤ Torch XLA çš„éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
            xm.set_rng_state(checkpoint_rng_state["xla"])
        
        # å¦‚æœä½¿ç”¨äº† Torch NPU
        if is_torch_npu_available():
            # å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¸”ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
            if self.args.parallel_mode == ParallelMode.DISTRIBUTED:
                # æ¢å¤æ‰€æœ‰ NPU çš„éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
                torch.npu.random.set_rng_state_all(checkpoint_rng_state["npu"])
            else:
                try:
                    # æ¢å¤å½“å‰ NPU çš„éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
                    torch.npu.random.set_rng_state(checkpoint_rng_state["npu"])
                except Exception as e:
                    logger.info(
                        f"Didn't manage to set back the RNG states of the NPU because of the following error:\n {e}"
                        "\nThis won't yield the same results as if the training had not been interrupted."
                    )
    # å®šä¹‰ä¸€ä¸ªä¿å­˜æ£€æŸ¥ç‚¹çš„æ–¹æ³•ï¼Œç”¨äºä¿å­˜æ¨¡å‹åŠå…¶ç›¸å…³çŠ¶æ€å’Œå‚æ•°
    def _save_checkpoint(self, model, trial, metrics=None):
        # åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨ ddp/dp/deepspeedï¼Œself.model æ€»æ˜¯æŒ‡å‘æˆ‘ä»¬æƒ³è¦ä¿å­˜çš„æ¨¡å‹çš„å¼•ç”¨ï¼Œé™¤äº† FullyShardedDDPã€‚
        # assert unwrap_model(model) is self.model, "internal model should be a reference to self.model"

        # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹çš„æ–‡ä»¶å¤¹åç§°ï¼ŒåŒ…å«å…¨å±€æ­¥æ•°ä¿¡æ¯
        checkpoint_folder = f"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}"

        # å¦‚æœæœªè¿›è¡Œè¶…å‚æ•°æœç´¢å¹¶ä¸”æ²¡æœ‰è¯•éªŒä¿¡æ¯ï¼Œåˆ™å­˜å‚¨ FLOPS
        if self.hp_search_backend is None and trial is None:
            self.store_flos()

        # è·å–è¿è¡Œè¾“å‡ºç›®å½•ï¼Œæ ¹æ®è¯•éªŒä¿¡æ¯åˆ›å»ºæ£€æŸ¥ç‚¹è¾“å‡ºç›®å½•
        run_dir = self._get_output_dir(trial=trial)
        output_dir = os.path.join(run_dir, checkpoint_folder)
        # è°ƒç”¨ä¿å­˜æ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶æŒ‡æ˜è¿™æ˜¯å†…éƒ¨è°ƒç”¨
        self.save_model(output_dir, _internal_call=True)

        # å¦‚æœä¸ä»…ä»…ä¿å­˜æ¨¡å‹ï¼Œåˆ™ç»§ç»­ä¿å­˜ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        if not self.args.save_only_model:
            # ä¿å­˜ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€
            self._save_optimizer_and_scheduler(output_dir)
            # ä¿å­˜éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
            self._save_rng_state(output_dir)

        # ç¡®å®šæ–°çš„æœ€ä½³æŒ‡æ ‡å’Œæœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
        if metrics is not None and self.args.metric_for_best_model is not None:
            metric_to_check = self.args.metric_for_best_model
            if not metric_to_check.startswith("eval_"):
                metric_to_check = f"eval_{metric_to_check}"
            metric_value = metrics[metric_to_check]

            operator = np.greater if self.args.greater_is_better else np.less
            # å¦‚æœå½“å‰çš„æŒ‡æ ‡å€¼æ›´å¥½ï¼Œåˆ™æ›´æ–°æœ€ä½³æŒ‡æ ‡å’Œæœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            if (
                self.state.best_metric is None
                or self.state.best_model_checkpoint is None
                or operator(metric_value, self.state.best_metric)
            ):
                self.state.best_metric = metric_value
                self.state.best_model_checkpoint = output_dir

        # å¦‚æœéœ€è¦ä¿å­˜ Trainer çš„çŠ¶æ€ä¿¡æ¯ï¼Œåˆ™å°†å…¶ä¿å­˜ä¸º JSON æ–‡ä»¶
        if self.args.should_save:
            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))

        # å¦‚æœæŒ‡å®šäº†æ¨é€åˆ° Hubï¼Œåˆ™ä»å½“å‰æ£€æŸ¥ç‚¹è·¯å¾„è¿›è¡Œæ¨é€
        if self.args.push_to_hub:
            self._push_from_checkpoint(output_dir)

        # å¯èƒ½ä¼šåˆ é™¤ä¸€äº›æ—§çš„æ£€æŸ¥ç‚¹
        if self.args.should_save:
            # ä»…ä¾èµ–äºæ•°å­—åŒ–çš„æ£€æŸ¥ç‚¹ id è¿›è¡Œæ—‹è½¬ç®¡ç†
            # åœ¨æŸäº›äº‘ç¯å¢ƒä¸­ï¼Œç‰¹åˆ«æ˜¯ä¸€äº› fuse æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œmtime å¹¶ä¸å¯é 
            self._rotate_checkpoints(use_mtime=False, output_dir=run_dir)
    # åœ¨éåˆ†å¸ƒå¼è®­ç»ƒä¸­ä¿å­˜éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
    rng_states = {
        "python": random.getstate(),  # è·å– Python å†…ç½®éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€
        "numpy": np.random.get_state(),  # è·å– NumPy éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€
        "cpu": torch.random.get_rng_state(),  # è·å– PyTorch CPU éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€
    }
    # å¦‚æœ CUDA å¯ç”¨
    if torch.cuda.is_available():
        # å¦‚æœåœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹
        if self.args.parallel_mode == ParallelMode.DISTRIBUTED:
            # åœ¨éåˆ†å¸ƒå¼æƒ…å†µä¸‹ï¼Œä¿å­˜å…¨å±€ CUDA éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€ï¼ˆä¼šè€ƒè™‘åˆ° DataParallelï¼‰
            rng_states["cuda"] = torch.cuda.random.get_rng_state_all()
        else:
            # è·å–å½“å‰ CUDA è®¾å¤‡ä¸Šçš„éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€
            rng_states["cuda"] = torch.cuda.random.get_rng_state()

    # å¦‚æœ PyTorch XLA å¯ç”¨
    if is_torch_xla_available():
        # è·å–å½“å‰ XLA è®¾å¤‡ä¸Šçš„éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€
        rng_states["xla"] = xm.get_rng_state()

    # å¦‚æœ PyTorch NPU å¯ç”¨
    if is_torch_npu_available():
        # å¦‚æœåœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹
        if self.args.parallel_mode == ParallelMode.DISTRIBUTED:
            # è·å–æ‰€æœ‰ NPU è®¾å¤‡ä¸Šçš„éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€
            rng_states["npu"] = torch.npu.random.get_rng_state_all()
        else:
            # è·å–å½“å‰ NPU è®¾å¤‡ä¸Šçš„éšæœºæ•°ç”Ÿæˆå™¨çš„çŠ¶æ€
            rng_states["npu"] = torch.npu.random.get_rng_state()

    # åœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰ï¼Œç¡®ä¿è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    os.makedirs(output_dir, exist_ok=True)

    # æ ¹æ®å½“å‰è¿›ç¨‹æ•°å†³å®šä¿å­˜çš„æ–‡ä»¶å
    if self.args.world_size <= 1:
        # å¦‚æœåªæœ‰ä¸€ä¸ªè¿›ç¨‹ï¼Œåˆ™ä¿å­˜ä¸º rng_state.pth
        torch.save(rng_states, os.path.join(output_dir, "rng_state.pth"))
    else:
        # å¦‚æœæœ‰å¤šä¸ªè¿›ç¨‹ï¼Œåˆ™ä¿å­˜ä¸º rng_state_{è¿›ç¨‹ç¼–å·}.pth
        torch.save(rng_states, os.path.join(output_dir, f"rng_state_{self.args.process_index}.pth"))
    # ä¿å­˜ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€åˆ°æŒ‡å®šçš„è¾“å‡ºç›®å½•
    def _save_optimizer_and_scheduler(self, output_dir):
        # å¦‚æœæ”¯æŒ Torch XLA åŠ é€Ÿ
        if is_torch_xla_available():
            # ä½¿ç”¨ XM è¿›è¡ŒåŒæ­¥ï¼Œä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šè·¯å¾„
            xm.rendezvous("saving_optimizer_states")
            xm.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
            # ä½¿ç”¨ XM ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šè·¯å¾„ï¼Œå¹¶è®°å½•è­¦å‘Šä¿¡æ¯
            with warnings.catch_warnings(record=True) as caught_warnings:
                xm.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))
                reissue_pt_warnings(caught_warnings)
        # å¦‚æœå¯ç”¨äº† SageMaker åˆ†å¸ƒå¼è®­ç»ƒ
        elif is_sagemaker_mp_enabled():
            # è·å–æœ¬åœ°ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸ï¼Œå¹¶è¿›è¡Œå±éšœåŒæ­¥
            opt_state_dict = self.optimizer.local_state_dict(gather_if_shard=False)
            smp.barrier()
            # å¦‚æœå½“å‰è¿›ç¨‹æ˜¯ç¬¬ä¸€ä¸ªæˆ–è€…é…ç½®è¦æ±‚åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€åˆ°æŒ‡å®šè·¯å¾„
            if smp.rdp_rank() == 0 or smp.state.cfg.shard_optimizer_state:
                smp.save(
                    opt_state_dict,
                    os.path.join(output_dir, OPTIMIZER_NAME),
                    partial=True,
                    v3=smp.state.cfg.shard_optimizer_state,
                )
        # å¦‚æœå¯ç”¨äº† DeepSpeed
        elif self.is_deepspeed_enabled:
            # æ ¹æ®æ¡ä»¶åˆ¤æ–­æ˜¯å¦æ’é™¤å†»ç»“å‚æ•°å¹¶ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹åˆ°æŒ‡å®šè·¯å¾„
            accept_exclude_frozen_parameters = "exclude_frozen_parameters" in set(
                inspect.signature(self.model_wrapped.save_checkpoint).parameters.keys()
            )
            if accept_exclude_frozen_parameters and _is_peft_model(self.model):
                self.model_wrapped.save_checkpoint(output_dir, exclude_frozen_parameters=True)
            else:
                self.model_wrapped.save_checkpoint(output_dir)
        # å¦‚æœå¯ç”¨äº† FSDPï¼ˆFully Sharded Data Parallelismï¼‰
        elif self.is_fsdp_enabled:
            # ä¿å­˜ FSDP ç‰¹å®šçš„æ¨¡å‹æ£€æŸ¥ç‚¹å’Œä¼˜åŒ–å™¨çŠ¶æ€åˆ°æŒ‡å®šè·¯å¾„
            save_fsdp_model(
                self.accelerator.state.fsdp_plugin, self.accelerator, self.model, output_dir, **_get_fsdp_ckpt_kwargs()
            )
            save_fsdp_optimizer(
                self.accelerator.state.fsdp_plugin, self.accelerator, self.optimizer, self.model, output_dir
            )
        # å¦‚æœéœ€è¦ä¿å­˜æ¨¡å‹
        elif self.args.should_save:
            # ä»…ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šè·¯å¾„
            torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))

        # ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å­—å…¸åˆ°æŒ‡å®šè·¯å¾„ï¼Œå¦‚æœæ»¡è¶³ä¿å­˜æ¡ä»¶
        is_deepspeed_custom_scheduler = self.is_deepspeed_enabled and not isinstance(
            self.lr_scheduler, DeepSpeedSchedulerWrapper
        )
        if (
            self.args.should_save
            and (not self.is_deepspeed_enabled or is_deepspeed_custom_scheduler)
            and not is_torch_xla_available()
        ):
            with warnings.catch_warnings(record=True) as caught_warnings:
                torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))
            # å¤„ç†ä¿å­˜è¿‡ç¨‹ä¸­çš„è­¦å‘Šä¿¡æ¯
            reissue_pt_warnings(caught_warnings)
    def hyperparameter_search(
        self,
        hp_space: Optional[Callable[["optuna.Trial"], Dict[str, float]]] = None,
        compute_objective: Optional[Callable[[Dict[str, float]], float]] = None,
        n_trials: int = 20,
        direction: Union[str, List[str]] = "minimize",
        backend: Optional[Union["str", HPSearchBackend]] = None,
        hp_name: Optional[Callable[["optuna.Trial"], str]] = None,
        **kwargs,
    ):
        """
        Perform hyperparameter search using Optuna.

        Args:
            hp_space (Optional[Callable[["optuna.Trial"], Dict[str, float]]]):
                Function defining the hyperparameter search space.
            compute_objective (Optional[Callable[[Dict[str, float]], float]]):
                Function to compute the objective given a set of hyperparameters.
            n_trials (int):
                Number of trials (hyperparameter combinations) to evaluate.
            direction (Union[str, List[str]]):
                Direction to optimize the objective, either 'minimize' or 'maximize'.
            backend (Optional[Union[str, HPSearchBackend]]):
                Backend for hyperparameter search.
            hp_name (Optional[Callable[["optuna.Trial"], str]]):
                Function to generate a name for the hyperparameter set.
            **kwargs:
                Additional keyword arguments passed to the hyperparameter search.

        Returns:
            None
        """
        pass

    def log(self, logs: Dict[str, float]) -> None:
        """
        Log `logs` on the various objects watching training.

        Subclass and override this method to inject custom behavior.

        Args:
            logs (`Dict[str, float]`):
                The values to log.
        """
        if self.state.epoch is not None:
            logs["epoch"] = round(self.state.epoch, 2)
        if self.args.include_num_input_tokens_seen:
            logs["num_input_tokens_seen"] = self.state.num_input_tokens_seen

        output = {**logs, **{"step": self.state.global_step}}
        self.state.log_history.append(output)
        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)

    def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:
        """
        Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.

        Args:
            data (Union[torch.Tensor, Any]):
                The input data to prepare.

        Returns:
            Union[torch.Tensor, Any]:
                Prepared data ready to be fed into the model.
        """
        if isinstance(data, Mapping):
            return type(data)({k: self._prepare_input(v) for k, v in data.items()})
        elif isinstance(data, (tuple, list)):
            return type(data)(self._prepare_input(v) for v in data)
        elif isinstance(data, torch.Tensor):
            kwargs = {"device": self.args.device}
            if self.is_deepspeed_enabled and (torch.is_floating_point(data) or torch.is_complex(data)):
                # Adjust dtype for deepspeed enabled models
                kwargs.update({"dtype": self.accelerator.state.deepspeed_plugin.hf_ds_config.dtype()})
            return data.to(**kwargs)
        return data
    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:
        """
        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and
        handling potential state.
        """
        # è°ƒç”¨å†…éƒ¨æ–¹æ³•ï¼Œå‡†å¤‡è¾“å…¥æ•°æ®ï¼Œç¡®ä¿è½¬æ¢ä¸ºå¼ é‡ï¼ˆå¦‚æœå°šæœªï¼‰ï¼ŒåŒæ—¶å¤„ç†æ½œåœ¨çš„çŠ¶æ€
        inputs = self._prepare_input(inputs)
        
        # å¦‚æœè¾“å…¥æ•°æ®ä¸ºç©ºï¼ŒæŠ›å‡ºæ•°å€¼é”™è¯¯ï¼Œé˜²æ­¢æ¨¡å‹æ— æ³•è®­ç»ƒ
        if len(inputs) == 0:
            raise ValueError(
                "The batch received was empty, your model won't be able to train on it. Double-check that your "
                f"training dataset contains keys expected by the model: {','.join(self._signature_columns)}."
            )
        
        # å¦‚æœè®¾ç½®äº†å†å²ç´¢å¼•ä¸”å­˜åœ¨å†å²æ•°æ®ï¼Œåˆ™å°†å†å²æ•°æ®æ·»åŠ åˆ°è¾“å…¥ä¸­
        if self.args.past_index >= 0 and self._past is not None:
            inputs["mems"] = self._past
        
        # è¿”å›å‡†å¤‡å¥½çš„è¾“å…¥æ•°æ®å­—å…¸
        return inputs

    def compute_loss_context_manager(self):
        """
        A helper wrapper to group together context managers.
        """
        # è°ƒç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„å¸®åŠ©å™¨åŒ…è£…å™¨
        return self.autocast_smart_context_manager()

    def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):
        """
        A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired
        arguments, depending on the situation.
        """
        # æ ¹æ®æ˜¯å¦å¯ç”¨ CPU è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œåˆ›å»ºç›¸åº”çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        if self.use_cpu_amp:
            ctx_manager = torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)
        else:
            ctx_manager = contextlib.nullcontext()

        return ctx_manager

    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
        """
        Perform a training step on a batch of inputs.

        Subclass and override to inject custom behavior.

        Args:
            model (`nn.Module`):
                The model to train.
            inputs (`Dict[str, Union[torch.Tensor, Any]]`):
                The inputs and targets of the model.

                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
                argument `labels`. Check your model's documentation for all accepted arguments.

        Return:
            `torch.Tensor`: The tensor with training loss on this batch.
        """
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        inputs = self._prepare_inputs(inputs)

        # å¦‚æœå¯ç”¨ SageMaker å¤šè¿›ç¨‹è®­ç»ƒï¼Œåˆ™è°ƒç”¨ç›¸åº”çš„å‰å‘-åå‘ä¼ æ’­å‡½æ•°
        if is_sagemaker_mp_enabled():
            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)
            return loss_mb.reduce_mean().detach().to(self.args.device)

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è®¡ç®—æŸå¤±
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)

        # å¦‚æœæœ‰å¤šä¸ª GPUï¼Œåˆ™å¯¹æŸå¤±è¿›è¡Œå¹³å‡ï¼Œç”¨äºå¤š GPU å¹¶è¡Œè®­ç»ƒ
        if self.args.n_gpu > 1:
            loss = loss.mean()  # mean() to average on multi-gpu parallel training

        # å¦‚æœä½¿ç”¨ Apex æ··åˆç²¾åº¦è®­ç»ƒï¼Œåˆ™ä½¿ç”¨ Amp åº”ç”¨æ¢¯åº¦ç¼©æ”¾
        if self.use_apex:
            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            self.accelerator.backward(loss)

        # è¿”å›åˆ†ç¦»åçš„æŸå¤±å€¼ï¼Œé™¤ä»¥æ¢¯åº¦ç´¯ç§¯æ­¥éª¤æ•°
        return loss.detach() / self.args.gradient_accumulation_steps
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œè®¡ç®—æ¨¡å‹çš„æŸå¤±å€¼ã€‚
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        How the loss is computed by Trainer. By default, all models return the loss in the first element.

        Subclass and override for custom behavior.
        """
        # å¦‚æœå­˜åœ¨æ ‡ç­¾å¹³æ»‘å™¨ä¸”è¾“å…¥ä¸­åŒ…å«æ ‡ç­¾ï¼Œåˆ™å°†æ ‡ç­¾ä»è¾“å…¥ä¸­å¼¹å‡º
        if self.label_smoother is not None and "labels" in inputs:
            labels = inputs.pop("labels")
        else:
            labels = None
        # ä½¿ç”¨æ¨¡å‹å¤„ç†è¾“å…¥æ•°æ®ï¼Œè·å–æ¨¡å‹è¾“å‡º
        outputs = model(**inputs)
        # å¦‚æœå®šä¹‰äº†ä¿å­˜è¿‡å»çŠ¶æ€çš„ç´¢å¼•
        if self.args.past_index >= 0:
            # å°†è¾“å‡ºä¸­å¯¹åº”ç´¢å¼•çš„å†…å®¹ä¿å­˜åˆ° _past å±æ€§ä¸­
            self._past = outputs[self.args.past_index]

        # å¦‚æœå­˜åœ¨æ ‡ç­¾ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹é€»è¾‘
        if labels is not None:
            # è·å–æœªå°è£…çš„æ¨¡å‹ï¼ˆå»é™¤æ‰€æœ‰åŒ…è£…å™¨ï¼‰
            unwrapped_model = unwrap_model(model)
            # åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¸º PEFT æ¨¡å‹
            if _is_peft_model(unwrapped_model):
                # è·å–æ¨¡å‹çš„åç§°
                model_name = unwrapped_model.base_model.model._get_name()
            else:
                model_name = unwrapped_model._get_name()
            # å¦‚æœæ¨¡å‹åç§°å­˜åœ¨äº causal LM æ˜ å°„åç§°ä¸­
            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():
                # ä½¿ç”¨æ ‡ç­¾å¹³æ»‘å™¨è®¡ç®—æŸå¤±ï¼ˆå¸¦æ ‡ç­¾åç§»ï¼‰
                loss = self.label_smoother(outputs, labels, shift_labels=True)
            else:
                # ä½¿ç”¨æ ‡ç­¾å¹³æ»‘å™¨è®¡ç®—æŸå¤±
                loss = self.label_smoother(outputs, labels)
        else:
            # å¦‚æœæ¨¡å‹è¾“å‡ºæ˜¯å­—å…¸ä¸”æ²¡æœ‰åŒ…å«æŸå¤±é”®
            if isinstance(outputs, dict) and "loss" not in outputs:
                # æŠ›å‡ºæ•°å€¼é”™è¯¯ï¼Œè¯´æ˜æ¨¡å‹æœªä»è¾“å…¥ä¸­è¿”å›æŸå¤±å€¼
                raise ValueError(
                    "The model did not return a loss from the inputs, only the following keys: "
                    f"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}."
                )
            # ä» outputs ä¸­è·å–æŸå¤±å€¼ï¼ˆå¯èƒ½æ˜¯å…ƒç»„å½¢å¼çš„ ModelOutputï¼‰
            # è¿™é‡Œä¸ä½¿ç”¨ .loss æ˜¯å› ä¸ºæ¨¡å‹å¯èƒ½è¿”å›å…ƒç»„è€Œä¸æ˜¯ ModelOutput
            loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]

        # å¦‚æœéœ€è¦è¿”å›è¾“å‡ºå†…å®¹ï¼Œåˆ™åŒæ—¶è¿”å›æŸå¤±å’Œæ¨¡å‹è¾“å‡º
        return (loss, outputs) if return_outputs else loss

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯æœ¬åœ°çš„ä¸»è¿›ç¨‹ï¼ˆåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œä¸€å°æœºå™¨ä¸Šçš„ä¸»è¿›ç¨‹ï¼‰
    def is_local_process_zero(self) -> bool:
        """
        Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several
        machines) main process.
        """
        return self.args.local_process_index == 0

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºåˆ¤æ–­å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯å…¨å±€çš„ä¸»è¿›ç¨‹ï¼ˆåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰ä¸€ä¸ªè¿›ç¨‹ä¼šè¿”å› Trueï¼‰
    def is_world_process_zero(self) -> bool:
        """
        Whether or not this process is the global main process (when training in a distributed fashion on several
        machines, this is only going to be `True` for one process).
        """
        # å¯¹äº SageMaker ModelParallel çš„ç‰¹æ®Šæƒ…å†µï¼Œè¿›ç¨‹ç´¢å¼•ä¸º dp_process_index è€Œä¸æ˜¯å…¨å±€è¿›ç¨‹ç´¢å¼•
        if is_sagemaker_mp_enabled():
            return smp.rank() == 0
        else:
            # åˆ¤æ–­å½“å‰è¿›ç¨‹ç´¢å¼•æ˜¯å¦ä¸º 0
            return self.args.process_index == 0
    # å®šä¹‰ä¿å­˜æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥æŒ‡å®šè¾“å‡ºç›®å½•å’Œæ˜¯å¦å†…éƒ¨è°ƒç”¨æ ‡å¿—
    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
        """
        Will save the model, so you can reload it using `from_pretrained()`.

        Will only save from the main process.
        """

        # å¦‚æœæœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„è¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = self.args.output_dir

        # å¦‚æœå½“å‰ç¯å¢ƒæ”¯æŒ Torch XLAï¼Œä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
        if is_torch_xla_available():
            self._save_tpu(output_dir)
        
        # å¦‚æœæ˜¯åœ¨ SageMaker å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œåˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¹¶ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
        elif is_sagemaker_mp_enabled():
            os.makedirs(output_dir, exist_ok=True)
            state_dict = self.model_wrapped.state_dict()
            # å¦‚æœåº”å½“ä¿å­˜æ¨¡å‹ï¼Œåˆ™æ‰§è¡Œä¿å­˜æ“ä½œ
            if self.args.should_save:
                self._save(output_dir, state_dict=state_dict)
            # å¦‚æœæ˜¯ SageMaker MP ç‰ˆæœ¬å¤§äºç­‰äº 1.10ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ ‡å¿—æ–‡ä»¶æŒ‡ç¤ºæ¨¡å‹çŠ¶æ€å­—å…¸å·²ä¿å­˜
            if IS_SAGEMAKER_MP_POST_1_10:
                Path(os.path.join(output_dir, "user_content.pt")).touch()
        
        # å¦‚æœå¯ç”¨äº† FSDPï¼ˆFully Sharded Data Parallelismï¼‰ï¼Œä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
        elif self.is_fsdp_enabled:
            # æ£€æŸ¥æ˜¯å¦å®Œæ•´çŠ¶æ€å­—å…¸ï¼Œå¹¶ä¸”åŠ é€Ÿåº“ç‰ˆæœ¬å¤§äº 0.24.1
            if ("FULL_STATE_DICT" in str(self.accelerator.state.fsdp_plugin.state_dict_type)) and (
                version.parse(accelerate_version) > version.parse("0.24.1")
            ):
                state_dict = self.accelerator.get_state_dict(self.model)
                # å¦‚æœåº”å½“ä¿å­˜æ¨¡å‹ï¼Œåˆ™æ‰§è¡Œä¿å­˜æ“ä½œ
                if self.args.should_save:
                    self._save(output_dir, state_dict=state_dict)
        
        # å¦‚æœå¯ç”¨äº† DeepSpeedï¼Œå°è¯•è·å– DeepSpeed çš„çŠ¶æ€å­—å…¸å¹¶ä¿å­˜
        elif self.is_deepspeed_enabled:
            try:
                state_dict = self.accelerator.get_state_dict(self.deepspeed)
                # å¦‚æœåº”å½“ä¿å­˜æ¨¡å‹ï¼Œåˆ™æ‰§è¡Œä¿å­˜æ“ä½œ
                if self.args.should_save:
                    self._save(output_dir, state_dict=state_dict)
            except ValueError:
                # å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œåˆ™è­¦å‘Šå¹¶ä¿å­˜ç©ºçš„çŠ¶æ€å­—å…¸
                logger.warning(
                    " stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use"
                    " zero_to_fp32.py to recover weights"
                )
                if self.args.should_save:
                    self._save(output_dir, state_dict={})
                # ç§»é™¤è™šæ‹Ÿçš„çŠ¶æ€å­—å…¸
                remove_dummy_checkpoint(self.args.should_save, output_dir, [WEIGHTS_NAME, SAFE_WEIGHTS_NAME])
                # ä½¿ç”¨åŒ…è£…åçš„æ¨¡å‹ä¿å­˜å®Œæ•´çš„æ£€æŸ¥ç‚¹
                self.model_wrapped.save_checkpoint(output_dir)
        
        # å¦‚æœåº”å½“ä¿å­˜æ¨¡å‹ï¼Œåˆ™æ‰§è¡Œä¿å­˜æ“ä½œ
        elif self.args.should_save:
            self._save(output_dir)

        # å½“ç”¨æˆ·è°ƒç”¨ `save_model` ä¸” `push_to_hub` ä¸ºçœŸæ—¶ï¼Œæ¨é€æ¨¡å‹åˆ° Hub
        if self.args.push_to_hub and not _internal_call:
            self.push_to_hub(commit_message="Model save")
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šç›®å½•ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•
    def _save_tpu(self, output_dir: Optional[str] = None):
        # å¦‚æœæœªæä¾›è¾“å‡ºç›®å½•ï¼Œåˆ™ä½¿ç”¨ self.args.output_dir
        output_dir = output_dir if output_dir is not None else self.args.output_dir

        # æ‰“å°æ—¥å¿—ï¼ŒæŒ‡ç¤ºæ­£åœ¨ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹åˆ°æŒ‡å®šçš„è¾“å‡ºç›®å½•
        logger.info(f"Saving model checkpoint to {output_dir}")

        # è·å–æ¨¡å‹å¯¹è±¡
        model = self.model

        # åœ¨TPUä¸Šæ ‡è®°å½“å‰æ­¥éª¤
        xm.mark_step()

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°CPUä¸Šä¿å­˜
        model.to("cpu")

        # å¦‚æœæ˜¯ä¸»èŠ‚ç‚¹ï¼ˆmaster ordinalï¼‰ï¼Œåˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œå¹¶ä¿å­˜è®­ç»ƒå‚æ•°
        if xm.is_master_ordinal():
            os.makedirs(output_dir, exist_ok=True)
            torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))

        # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé…ç½®ï¼Œä½¿ç”¨ `save_pretrained()` æ–¹æ³•
        # å¯ä»¥é€šè¿‡ `from_pretrained()` æ–¹æ³•é‡æ–°åŠ è½½
        supported_classes = (PushToHubMixin,)
        xm.rendezvous("saving_checkpoint")

        # å¦‚æœæ¨¡å‹ä¸æ˜¯æ”¯æŒçš„ç±»åˆ«ï¼Œåˆ™å°è¯•è§£å¼€æ¨¡å‹å†ä¿å­˜å…¶çŠ¶æ€å­—å…¸
        if not isinstance(model, supported_classes):
            if isinstance(unwrap_model(model), supported_classes):
                unwrap_model(model).save_pretrained(
                    output_dir,
                    is_main_process=self.args.should_save,
                    state_dict=model.state_dict(),
                    save_function=xm.save,
                    safe_serialization=self.args.save_safetensors,
                )
            else:
                logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
                state_dict = model.state_dict()
                xm.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))
        else:
            # å¦‚æœæ¨¡å‹å±äºæ”¯æŒçš„ç±»åˆ«ï¼Œåˆ™ç›´æ¥è°ƒç”¨å…¶ save_pretrained æ–¹æ³•ä¿å­˜
            model.save_pretrained(
                output_dir,
                is_main_process=self.args.should_save,
                save_function=xm.save,
                safe_serialization=self.args.save_safetensors,
            )

        # å¦‚æœå­˜åœ¨ tokenizer å¹¶ä¸”åº”è¯¥ä¿å­˜ï¼Œåˆ™ä¿å­˜ tokenizer åˆ°è¾“å‡ºç›®å½•
        if self.tokenizer is not None and self.args.should_save:
            self.tokenizer.save_pretrained(output_dir)

        # å°†æ¨¡å‹ä» CPU ç§»å›åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆé€šå¸¸æ˜¯ GPU æˆ– TPUï¼‰ï¼Œç¡®ä¿åç»­è®¡ç®—å¯ä»¥ç»§ç»­æ­£å¸¸è¿è¡Œ
        model.to(self.args.device)
    def _save(self, output_dir: Optional[str] = None, state_dict=None):
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºç›®å½•ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„è¾“å‡ºç›®å½• self.args.output_dir
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
        os.makedirs(output_dir, exist_ok=True)
        # è®°å½•æ—¥å¿—ï¼ŒæŒ‡ç¤ºæ­£åœ¨å°†æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜åˆ°å“ªä¸ªç›®å½•
        logger.info(f"Saving model checkpoint to {output_dir}")

        # å®šä¹‰æ”¯æŒçš„æ¨¡å‹ç±»åˆ«ï¼Œå¦‚æœæ²¡æœ‰ PEFT å¯ç”¨ï¼Œåˆ™åªæ”¯æŒ PreTrainedModelï¼›å¦åˆ™è¿˜æ”¯æŒ PeftModel
        supported_classes = (PreTrainedModel,) if not is_peft_available() else (PreTrainedModel, PeftModel)
        # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé…ç½®ï¼Œä½¿ç”¨ `save_pretrained()` æ–¹æ³•ä¿å­˜
        # å¯ä»¥ä½¿ç”¨ `from_pretrained()` æ–¹æ³•é‡æ–°åŠ è½½
        if not isinstance(self.model, supported_classes):
            # å¦‚æœæ²¡æœ‰æä¾›çŠ¶æ€å­—å…¸ï¼Œåˆ™è·å–å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸
            if state_dict is None:
                state_dict = self.model.state_dict()

            # å¦‚æœæ¨¡å‹å±äºæ”¯æŒçš„ç±»åˆ«ï¼Œåˆ™è°ƒç”¨å…¶ save_pretrained æ–¹æ³•ä¿å­˜æ¨¡å‹å’ŒçŠ¶æ€å­—å…¸
            if isinstance(unwrap_model(self.model), supported_classes):
                unwrap_model(self.model).save_pretrained(
                    output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors
                )
            else:
                # å¦‚æœæ¨¡å‹ä¸æ˜¯ `PreTrainedModel` ç±»å‹ï¼Œåˆ™åªä¿å­˜å…¶çŠ¶æ€å­—å…¸
                logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
                # å¦‚æœè®¾ç½®äº†ä¿å­˜å®‰å…¨å¼ é‡ï¼Œåˆ™ä½¿ç”¨ safetensors åº“ä¿å­˜æ–‡ä»¶
                if self.args.save_safetensors:
                    safetensors.torch.save_file(
                        state_dict, os.path.join(output_dir, SAFE_WEIGHTS_NAME), metadata={"format": "pt"}
                    )
                else:
                    # å¦åˆ™ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ torch.save æ–¹æ³•ä¿å­˜çŠ¶æ€å­—å…¸
                    torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))
        else:
            # å¦‚æœæ¨¡å‹å±äºæ”¯æŒçš„ç±»åˆ«ï¼Œåˆ™ç›´æ¥è°ƒç”¨å…¶ save_pretrained æ–¹æ³•ä¿å­˜æ¨¡å‹å’ŒçŠ¶æ€å­—å…¸
            self.model.save_pretrained(
                output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors
            )

        # å¦‚æœå­˜åœ¨ tokenizer å¯¹è±¡ï¼Œåˆ™ä¿å­˜å…¶é¢„è®­ç»ƒæ¨¡å‹
        if self.tokenizer is not None:
            self.tokenizer.save_pretrained(output_dir)

        # æœ€ä½³å®è·µï¼šå°†è®­ç»ƒå‚æ•°ä¸è®­ç»ƒå¥½çš„æ¨¡å‹ä¸€èµ·ä¿å­˜
        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))

    def store_flos(self):
        # å­˜å‚¨æ¨¡å‹ä¸­æ‰€ç”¨çš„æµ®ç‚¹è¿ç®—æ€»æ•°
        if self.args.parallel_mode == ParallelMode.DISTRIBUTED:
            # å¦‚æœå¹¶è¡Œæ¨¡å¼ä¸º DISTRIBUTEDï¼Œåˆ™å°†å½“å‰è®¡ç®—çš„ FLOPS åˆ†å¸ƒå¼å¹¿æ’­åˆ°æ‰€æœ‰è®¾å¤‡å¹¶æ±‚å’Œ
            self.state.total_flos += (
                distributed_broadcast_scalars([self.current_flos], device=self.args.device).sum().item()
            )
            # æ¸…é›¶å½“å‰è®¡ç®—çš„ FLOPS
            self.current_flos = 0
        else:
            # å¦åˆ™ç›´æ¥å°†å½“å‰è®¡ç®—çš„ FLOPS åŠ åˆ°æ€»æ•°ä¸­ï¼Œå¹¶æ¸…é›¶å½“å‰è®¡ç®—çš„ FLOPS
            self.state.total_flos += self.current_flos
            self.current_flos = 0

    def _sorted_checkpoints(
        self, output_dir=None, checkpoint_prefix=PREFIX_CHECKPOINT_DIR, use_mtime=False
    ):
        # è¯¥å‡½æ•°æš‚æœªæä¾›å…·ä½“å®ç°ï¼Œåœ¨è¿™é‡Œåªåšå‡½æ•°å£°æ˜
    # è¿”å›å·²æ’åºçš„æ£€æŸ¥ç‚¹è·¯å¾„åˆ—è¡¨ï¼ŒæŒ‰ç…§æ–‡ä»¶ä¿®æ”¹æ—¶é—´æˆ–è€…æ–‡ä»¶åä¸­çš„æ•°å­—æ’åº
    def _sorted_checkpoints(self, use_mtime=False, output_dir=None) -> List[str]:
        ordering_and_checkpoint_path = []

        # è·å–åŒ¹é…æŒ‡å®šå‰ç¼€çš„æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹è·¯å¾„
        glob_checkpoints = [str(x) for x in Path(output_dir).glob(f"{checkpoint_prefix}-*") if os.path.isdir(x)]

        # éå†æ¯ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹è·¯å¾„
        for path in glob_checkpoints:
            if use_mtime:
                # å¦‚æœä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºæ’åºä¾æ®ï¼Œå°†æ—¶é—´æˆ³å’Œè·¯å¾„æ·»åŠ åˆ°æ’åºåˆ—è¡¨ä¸­
                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))
            else:
                # å¦åˆ™ï¼Œå°è¯•ä»æ–‡ä»¶åä¸­æå–æ•°å­—ï¼Œå°†æ•°å­—å’Œè·¯å¾„æ·»åŠ åˆ°æ’åºåˆ—è¡¨ä¸­
                regex_match = re.match(f".*{checkpoint_prefix}-([0-9]+)", path)
                if regex_match is not None and regex_match.groups() is not None:
                    ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))

        # æŒ‰ç…§æ—¶é—´æˆ³æˆ–è€…æ–‡ä»¶åä¸­çš„æ•°å­—æ’åºæ£€æŸ¥ç‚¹è·¯å¾„
        checkpoints_sorted = sorted(ordering_and_checkpoint_path)
        # ä»…ä¿ç•™æ’åºåçš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œä¸åŒ…å«æ’åºä¾æ®
        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]

        # ç¡®ä¿æœ€ä½³æ¨¡å‹ä¸ä¼šè¢«åˆ é™¤ï¼Œå°†å…¶ç§»åŠ¨åˆ°æ£€æŸ¥ç‚¹åˆ—è¡¨çš„å‰éƒ¨
        if (
            self.state.best_model_checkpoint is not None
            and str(Path(self.state.best_model_checkpoint)) in checkpoints_sorted
        ):
            best_model_index = checkpoints_sorted.index(str(Path(self.state.best_model_checkpoint)))
            for i in range(best_model_index, len(checkpoints_sorted) - 2):
                checkpoints_sorted[i], checkpoints_sorted[i + 1] = checkpoints_sorted[i + 1], checkpoints_sorted[i]

        # è¿”å›æ’åºåçš„æ£€æŸ¥ç‚¹è·¯å¾„åˆ—è¡¨
        return checkpoints_sorted

    # æ ¹æ®ä¿å­˜çš„æ€»æ•°é™åˆ¶æ—‹è½¬æ£€æŸ¥ç‚¹ï¼Œåˆ é™¤å¤šä½™çš„æ£€æŸ¥ç‚¹
    def _rotate_checkpoints(self, use_mtime=False, output_dir=None) -> None:
        if self.args.save_total_limit is None or self.args.save_total_limit <= 0:
            return

        # è·å–æ’åºåçš„æ£€æŸ¥ç‚¹è·¯å¾„åˆ—è¡¨
        checkpoints_sorted = self._sorted_checkpoints(use_mtime=use_mtime, output_dir=output_dir)
        # å¦‚æœæ£€æŸ¥ç‚¹æ•°é‡å°äºæˆ–ç­‰äºä¿å­˜æ€»æ•°é™åˆ¶ï¼Œåˆ™ä¸éœ€è¦åˆ é™¤
        if len(checkpoints_sorted) <= self.args.save_total_limit:
            return

        # æ ¹æ®ç‰¹å®šæ¡ä»¶è°ƒæ•´ä¿å­˜æ€»æ•°é™åˆ¶
        save_total_limit = self.args.save_total_limit
        if (
            self.state.best_model_checkpoint is not None
            and self.args.save_total_limit == 1
            and checkpoints_sorted[-1] != self.state.best_model_checkpoint
        ):
            save_total_limit = 2

        # è®¡ç®—éœ€è¦åˆ é™¤çš„æ£€æŸ¥ç‚¹æ•°é‡ï¼Œå¹¶è·å–å¾…åˆ é™¤çš„æ£€æŸ¥ç‚¹è·¯å¾„åˆ—è¡¨
        number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - save_total_limit)
        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]

        # åˆ é™¤å¾…åˆ é™¤çš„æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹
        for checkpoint in checkpoints_to_be_deleted:
            logger.info(f"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit")
            shutil.rmtree(checkpoint, ignore_errors=True)
        """
        # è¿è¡Œé¢„æµ‹å¹¶è¿”å›é¢„æµ‹ç»“æœå’Œå¯èƒ½çš„æŒ‡æ ‡ã€‚

        # æ ¹æ®æ•°æ®é›†å’Œä½¿ç”¨æƒ…å†µï¼Œæµ‹è¯•æ•°æ®é›†å¯èƒ½åŒ…å«æ ‡ç­¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•è¿˜ä¼šè¿”å›æŒ‡æ ‡ï¼Œä¾‹å¦‚åœ¨ `evaluate()` ä¸­ä¸€æ ·ã€‚

        Args:
            test_dataset (`Dataset`):
                è¦è¿è¡Œé¢„æµ‹çš„æ•°æ®é›†ã€‚å¦‚æœæ˜¯ `datasets.Dataset`ï¼Œåˆ™ä¼šè‡ªåŠ¨åˆ é™¤æ¨¡å‹ `forward()` æ–¹æ³•ä¸æ¥å—çš„åˆ—ã€‚å¿…é¡»å®ç° `__len__` æ–¹æ³•ã€‚
            ignore_keys (`List[str]`, *å¯é€‰*):
                åœ¨æ¨¡å‹è¾“å‡ºä¸­åº”å¿½ç•¥çš„é”®åˆ—è¡¨ï¼ˆå¦‚æœæ˜¯å­—å…¸ï¼‰ã€‚
            metric_key_prefix (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"test"`):
                ç”¨ä½œæŒ‡æ ‡é”®å‰ç¼€çš„å¯é€‰å‰ç¼€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‰ç¼€æ˜¯ "test"ï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™æŒ‡æ ‡ "bleu" å°†å‘½åä¸º "test_bleu"ã€‚

        <Tip>

        å¦‚æœæ‚¨çš„é¢„æµ‹æˆ–æ ‡ç­¾å…·æœ‰ä¸åŒçš„åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œå› ä¸ºæ‚¨åœ¨æ ‡è®°åˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡ŒåŠ¨æ€å¡«å……ï¼‰ï¼Œåˆ™ä¼šå¯¹é¢„æµ‹è¿›è¡Œå¡«å……ï¼ˆåœ¨å³ä¾§ï¼‰ï¼Œä»¥å…è®¸ä¸²è”åˆ°ä¸€ä¸ªæ•°ç»„ä¸­ã€‚å¡«å……ç´¢å¼•ä¸º -100ã€‚

        </Tip>

        Returns: *NamedTuple* å…·æœ‰ä»¥ä¸‹é”®çš„å‘½åå…ƒç»„:

            - predictions (`np.ndarray`): å¯¹ `test_dataset` çš„é¢„æµ‹ã€‚
            - label_ids (`np.ndarray`, *å¯é€‰*): æ ‡ç­¾ï¼ˆå¦‚æœæ•°æ®é›†åŒ…å«ï¼‰ã€‚
            - metrics (`Dict[str, float]`, *å¯é€‰*): å¯èƒ½åŒ…å«æ ‡ç­¾çš„å­—å…¸ã€‚

        """
        # å†…å­˜æŒ‡æ ‡ - å¿…é¡»å°½æ—©è®¾ç½®
        self._memory_tracker.start()

        # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
        test_dataloader = self.get_test_dataloader(test_dataset)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # é€‰æ‹©ä½¿ç”¨æ—§ç‰ˆé¢„æµ‹å¾ªç¯è¿˜æ˜¯è¯„ä¼°å¾ªç¯
        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
        
        # æ‰§è¡Œé¢„æµ‹/è¯„ä¼°å¾ªç¯
        output = eval_loop(
            test_dataloader, description="Prediction", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix
        )
        
        # è®¡ç®—æ€»æ‰¹é‡å¤§å°
        total_batch_size = self.args.eval_batch_size * self.args.world_size
        
        # å¦‚æœå­˜åœ¨ç¼–è¯‘æ—¶é—´æŒ‡æ ‡ï¼Œåˆ™è°ƒæ•´å¼€å§‹æ—¶é—´
        if f"{metric_key_prefix}_jit_compilation_time" in output.metrics:
            start_time += output.metrics[f"{metric_key_prefix}_jit_compilation_time"]
        
        # æ›´æ–°é€Ÿåº¦æŒ‡æ ‡
        output.metrics.update(
            speed_metrics(
                metric_key_prefix,
                start_time,
                num_samples=output.num_samples,
                num_steps=math.ceil(output.num_samples / total_batch_size),
            )
        )

        # è°ƒç”¨é¢„æµ‹çš„å›è°ƒå¤„ç†å™¨
        self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)
        
        # åœæ­¢å†…å­˜è·Ÿè¸ªå™¨å¹¶æ›´æ–°æŒ‡æ ‡
        self._memory_tracker.stop_and_update_metrics(output.metrics)

        # è¿”å›é¢„æµ‹è¾“å‡º
        return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=output.metrics)
    def evaluation_loop(
        self,
        dataloader: DataLoader,
        description: str,
        prediction_loss_only: Optional[bool] = None,
        ignore_keys: Optional[List[str]] = None,
        metric_key_prefix: str = "eval",
    ):
        """
        Runs evaluation loop over `dataloader`.

        Args:
            dataloader (DataLoader): The data loader for evaluation.
            description (str): Description of the evaluation loop.
            prediction_loss_only (Optional[bool], optional): Whether to compute only prediction loss. Defaults to None.
            ignore_keys (Optional[List[str]], optional): List of keys to ignore during evaluation. Defaults to None.
            metric_key_prefix (str, optional): Prefix for metric keys. Defaults to "eval".
        """
        # Implementation of evaluation loop code...
        

    def _nested_gather(self, tensors, name=None):
        """
        Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before
        concatenating them to `gathered`.
        """
        if tensors is None:
            return
        # Check if running on TPU (Tensor Processing Unit)
        if is_torch_xla_available():
            if name is None:
                name = "nested_gather"
            tensors = nested_xla_mesh_reduce(tensors, name)
        # Check if running on SageMaker's multi-processing
        elif is_sagemaker_mp_enabled():
            tensors = smp_gather(tensors)
        # Check if running in distributed setting
        elif (self.args.distributed_state is not None and self.args.distributed_state.distributed_type != "NO") or (
            self.args.distributed_state is None and self.args.local_rank != -1
        ):
            tensors = distributed_concat(tensors)
        return tensors

    def prediction_step(
        self,
        model: nn.Module,
        inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool,
        ignore_keys: Optional[List[str]] = None,
    ):
        """
        Perform a prediction step using `model` on `inputs`.

        Args:
            model (nn.Module): The model for prediction.
            inputs (Dict[str, Union[torch.Tensor, Any]]): Dictionary of inputs for the model.
            prediction_loss_only (bool): Whether to compute only prediction loss.
            ignore_keys (Optional[List[str]], optional): List of keys to ignore during prediction.

        Returns:
            Depends on the model's prediction step implementation.
        """
        # Implementation of prediction step code...
        

    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):
        """
        Computes the number of floating point operations for the model.

        Args:
            inputs (Dict[str, Union[torch.Tensor, Any]]): The inputs and targets of the model.

        Returns:
            int: The number of floating-point operations.
        """
        if hasattr(self.model, "floating_point_ops"):
            return self.model.floating_point_ops(inputs)
        else:
            return 0

    def init_hf_repo(self):
        """
        Initializes a git repository in `self.args.hub_model_id`.
        """
        # Only proceed if current process is the zeroth process
        if not self.is_world_process_zero():
            return

        # Determine repository name
        if self.args.hub_model_id is None:
            repo_name = Path(self.args.output_dir).absolute().name
        else:
            repo_name = self.args.hub_model_id

        # Create or access the repository
        repo_url = create_repo(repo_name, token=self.args.hub_token, private=self.args.hub_private_repo, exist_ok=True)
        self.hub_model_id = repo_url.repo_id
        self.push_in_progress = None
    `
        # å®šä¹‰ä¸€ä¸ªæ–¹æ³• `create_model_card`ï¼Œç”¨äºåˆ›å»ºæ¨¡å‹å¡ç‰‡æ–‡æ¡£
        def create_model_card(
            self,
            # å¯é€‰å‚æ•°ï¼šæŒ‡å®šè¯­è¨€ï¼Œç”¨äºæè¿°æ¨¡å‹å¡ç‰‡çš„è¯­è¨€ç¯å¢ƒ
            language: Optional[str] = None,
            # å¯é€‰å‚æ•°ï¼šæŒ‡å®šè®¸å¯è¯ä¿¡æ¯ï¼Œç”¨äºæè¿°æ¨¡å‹çš„è®¸å¯æ¡ä»¶
            license: Optional[str] = None,
            # å¯é€‰å‚æ•°ï¼šæ¨¡å‹æ ‡ç­¾ï¼Œå¯ä»¥æ˜¯å•ä¸ªæ ‡ç­¾å­—ç¬¦ä¸²æˆ–æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºæ ‡è®°æ¨¡å‹ç‰¹æ€§
            tags: Union[str, List[str], None] = None,
            # å¯é€‰å‚æ•°ï¼šæ¨¡å‹åç§°ï¼Œç”¨äºæ ‡è¯†æ¨¡å‹çš„åç§°
            model_name: Optional[str] = None,
            # å¯é€‰å‚æ•°ï¼šæ¨¡å‹å¾®è°ƒè‡ªå“ªä¸ªæ¨¡å‹ï¼Œç”¨äºè®°å½•æ¨¡å‹çš„å¾®è°ƒæ¥æº
            finetuned_from: Optional[str] = None,
            # å¯é€‰å‚æ•°ï¼šä»»åŠ¡ç±»å‹ï¼Œå¯ä»¥æ˜¯å•ä¸ªä»»åŠ¡å­—ç¬¦ä¸²æˆ–ä»»åŠ¡åˆ—è¡¨ï¼Œæè¿°æ¨¡å‹é€‚ç”¨çš„ä»»åŠ¡ç±»å‹
            tasks: Union[str, List[str], None] = None,
            # å¯é€‰å‚æ•°ï¼šæ•°æ®é›†æ ‡ç­¾ï¼Œå¯ä»¥æ˜¯å•ä¸ªæ ‡ç­¾å­—ç¬¦ä¸²æˆ–æ ‡ç­¾åˆ—è¡¨ï¼Œæè¿°æ¨¡å‹æ‰€ç”¨çš„æ•°æ®é›†æ ‡ç­¾
            dataset_tags: Union[str, List[str], None] = None,
            # å¯é€‰å‚æ•°ï¼šæ•°æ®é›†åç§°æˆ–æ ‡è¯†ï¼Œå¯ä»¥æ˜¯å•ä¸ªåç§°å­—ç¬¦ä¸²æˆ–æ ‡è¯†å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
    ):
        """
        Creates a draft of a model card using the information available to the `Trainer`.

        Args:
            language (`str`, *optional*):
                The language of the model (if applicable)
            license (`str`, *optional*):
                The license of the model. Will default to the license of the pretrained model used, if the original
                model given to the `Trainer` comes from a repo on the Hub.
            tags (`str` or `List[str]`, *optional*):
                Some tags to be included in the metadata of the model card.
            model_name (`str`, *optional*):
                The name of the model.
            finetuned_from (`str`, *optional*):
                The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo
                of the original model given to the `Trainer` (if it comes from the Hub).
            tasks (`str` or `List[str]`, *optional*):
                One or several task identifiers, to be included in the metadata of the model card.
            dataset_tags (`str` or `List[str]`, *optional*):
                One or several dataset tags, to be included in the metadata of the model card.
            dataset (`str` or `List[str]`, *optional*):
                One or several dataset identifiers, to be included in the metadata of the model card.
            dataset_args (`str` or `List[str]`, *optional*):
                One or several dataset arguments, to be included in the metadata of the model card.
        """
        # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œåç»­æ“ä½œ
        if not self.is_world_process_zero():
            return

        # æ¨¡å‹å¡ç‰‡æ–‡ä»¶çš„ä¿å­˜è·¯å¾„
        model_card_filepath = os.path.join(self.args.output_dir, "README.md")

        # åˆ¤æ–­æ¨¡å‹å¡ç‰‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        is_peft_library = False
        if os.path.exists(model_card_filepath):
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½æ¨¡å‹å¡ç‰‡æ•°æ®å¹¶è·å–å…¶ä¸­çš„åº“åç§°
            library_name = ModelCard.load(model_card_filepath).data.get("library_name")
            # åˆ¤æ–­åŠ è½½çš„æ¨¡å‹å¡ç‰‡æ˜¯å¦æ¥è‡ªäº PEFT åº“
            is_peft_library = library_name == "peft"

            # å¦‚æœæœ‰æŒ‡å®š tagsï¼Œå¹¶ä¸”å·²å­˜åœ¨çš„ tags ä¸ä¸ºç©ºï¼Œåˆ™å°†æ–°çš„ tags æ·»åŠ åˆ°ç°æœ‰ tags ä¸­
            existing_tags = ModelCard.load(model_card_filepath).data.tags
            if tags is not None and existing_tags is not None:
                if isinstance(tags, str):
                    tags = [tags]
                for tag in existing_tags:
                    if tag not in tags:
                        tags.append(tag)

        # æ ¹æ® Trainer ä¸­çš„ä¿¡æ¯ç”Ÿæˆè®­ç»ƒæ‘˜è¦
        training_summary = TrainingSummary.from_trainer(
            self,
            language=language,
            license=license,
            tags=tags,
            model_name=model_name,
            finetuned_from=finetuned_from,
            tasks=tasks,
            dataset_tags=dataset_tags,
            dataset=dataset,
            dataset_args=dataset_args,
        )
        # å°†è®­ç»ƒæ‘˜è¦è½¬æ¢ä¸ºæ¨¡å‹å¡ç‰‡æ ¼å¼
        model_card = training_summary.to_model_card()
        # å°†æ¨¡å‹å¡ç‰‡å†™å…¥åˆ°æŒ‡å®šè·¯å¾„çš„ README.md æ–‡ä»¶ä¸­
        with open(model_card_filepath, "w") as f:
            f.write(model_card)

        # å¦‚æœæ˜¯ PEFT åº“ï¼Œåˆ™è°ƒç”¨ç‰¹å®šå‡½æ•°æ›´æ–°æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡
        if is_peft_library:
            unwrap_model(self.model).create_or_update_model_card(self.args.output_dir)
    # ä»…åœ¨å½“å‰èŠ‚ç‚¹æ˜¯ä¸»èŠ‚ç‚¹æ—¶æ‰§è¡Œæ¨é€æ“ä½œ
    if not self.is_world_process_zero() or self.args.hub_strategy == HubStrategy.END:
        return
    # å¦‚æœä¸Šæ¬¡æ¨é€æœªå®Œæˆä¸”æœªè®¾ç½® args.hub_always_push=Trueï¼Œåˆ™ä¸æ‰§è¡Œå½“å‰æ¨é€æ“ä½œ
    if not self.args.hub_always_push and self.push_in_progress is not None and not self.push_in_progress.is_done():
        return

    output_dir = self.args.output_dir
    # ä¸ºé¿å…é‡æ–°åŒæ­¥æ‰€æœ‰æ¨¡å‹æƒé‡ï¼Œä»æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ä¸­å¤åˆ¶æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    modeling_files = [CONFIG_NAME, WEIGHTS_NAME, SAFE_WEIGHTS_NAME]
    # å¦‚æœå¯ç”¨ï¼Œæ·»åŠ é€‚é…å™¨ç›¸å…³æ–‡ä»¶
    if is_peft_available():
        modeling_files.extend([ADAPTER_CONFIG_NAME, ADAPTER_WEIGHTS_NAME, ADAPTER_SAFE_WEIGHTS_NAME])
    # éå†éœ€è¦å¤åˆ¶çš„æ¨¡å‹æ–‡ä»¶åˆ—è¡¨
    for modeling_file in modeling_files:
        # å¦‚æœæ–‡ä»¶å­˜åœ¨äºæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ä¸­ï¼Œåˆ™å¤åˆ¶åˆ°è¾“å‡ºç›®å½•
        if os.path.isfile(os.path.join(checkpoint_folder, modeling_file)):
            shutil.copy(os.path.join(checkpoint_folder, modeling_file), os.path.join(output_dir, modeling_file))
    # å¦‚æœå­˜åœ¨ tokenizer å¯¹è±¡ï¼Œåˆ™ä¿å­˜å…¶å½“å‰çŠ¶æ€åˆ°è¾“å‡ºç›®å½•
    if self.tokenizer is not None:
        self.tokenizer.save_pretrained(output_dir)
    # åŒæ ·åœ°ï¼Œä¿å­˜è®­ç»ƒå‚æ•°å¯¹è±¡åˆ°è¾“å‡ºç›®å½•
    torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))

    # æ ¹æ®ä¿å­˜ç­–ç•¥ç”Ÿæˆæäº¤æ¶ˆæ¯
    if self.args.save_strategy == IntervalStrategy.STEPS:
        commit_message = f"Training in progress, step {self.state.global_step}"
    else:
        commit_message = f"Training in progress, epoch {int(self.state.epoch)}"

    # ä¸Šä¼ æ•´ä¸ªè¾“å‡ºç›®å½•åˆ°æŒ‡å®šçš„æ¨¡å‹åº“ä»“åº“ï¼Œä½œä¸ºä¸€ä¸ªæ–°ç‰ˆæœ¬çš„æäº¤
    model_push_job = upload_folder(
        repo_id=self.hub_model_id,
        folder_path=output_dir,
        commit_message=commit_message,
        token=self.args.hub_token,
        run_as_future=True,
        ignore_patterns=["_*", f"{PREFIX_CHECKPOINT_DIR}-*"],
    )

    push_jobs = [model_push_job]

    # å¦‚æœæŒ‡å®šäº†ä¿å­˜ç­–ç•¥ä¸º CHECKPOINT æˆ– ALL_CHECKPOINTSï¼Œåˆ™å°†æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ä¹Ÿä¸Šä¼ åˆ°æ¨¡å‹åº“ä»“åº“
    if self.args.hub_strategy in [HubStrategy.CHECKPOINT, HubStrategy.ALL_CHECKPOINTS]:
        # ç¡®å®šåœ¨ä»“åº“ä¸­çš„è·¯å¾„åç§°ï¼Œå¦‚æœç­–ç•¥æ˜¯ CHECKPOINT åˆ™ä½¿ç”¨ "last-checkpoint"ï¼Œå¦åˆ™ä½¿ç”¨æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹çš„åç§°
        path_in_repo = (
            "last-checkpoint" if self.args.hub_strategy == HubStrategy.CHECKPOINT else Path(checkpoint_folder).name
        )
        # åˆ›å»ºä¸€ä¸ªä¸Šä¼ ä»»åŠ¡ï¼Œå°†æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹ä½œä¸ºä¸€ä¸ªæ£€æŸ¥ç‚¹ç‰ˆæœ¬æäº¤
        checkpoint_push = upload_folder(
            repo_id=self.hub_model_id,
            folder_path=checkpoint_folder,
            path_in_repo=path_in_repo,
            commit_message=commit_message + ", checkpoint",
            token=self.args.hub_token,
            run_as_future=True,
        )
        push_jobs.append(checkpoint_push)

    # å¦‚æœå½“å‰æ²¡æœ‰è¿›è¡Œä¸­çš„æ¨é€ä»»åŠ¡æˆ–å·²å®Œæˆçš„ä»»åŠ¡ï¼Œåˆ›å»ºæ–°çš„æ¨é€ä»»åŠ¡
    if self.push_in_progress is None or self.push_in_progress.is_done():
        self.push_in_progress = PushInProgress(push_jobs)
    else:
        # å¦åˆ™ï¼Œå°†å½“å‰ç”Ÿæˆçš„æ¨é€ä»»åŠ¡æ·»åŠ åˆ°å·²æœ‰çš„æ¨é€ä»»åŠ¡åˆ—è¡¨ä¸­
        self.push_in_progress.jobs.extend(push_jobs)
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å±æ€§ "push_in_progress"ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç›´æ¥è¿”å›ï¼Œä¸è¿›è¡Œåç»­æ“ä½œ
    if not hasattr(self, "push_in_progress"):
        return
    
    # æ£€æŸ¥å½“å‰æ¨é€æ“ä½œæ˜¯å¦æ­£åœ¨è¿›è¡Œï¼Œå¹¶ä¸”è¿˜æœªå®Œæˆ
    if self.push_in_progress is not None and not self.push_in_progress.is_done():
        # è®°å½•æ—¥å¿—ï¼Œæç¤ºå½“å‰æ­£åœ¨ç­‰å¾…æ£€æŸ¥ç‚¹æ¨é€æ“ä½œå®Œæˆï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´
        logger.info("Waiting for the current checkpoint push to be finished, this might take a couple of minutes.")
        
        # ç­‰å¾…æ¨é€æ“ä½œå®Œæˆï¼Œç›´åˆ°å®Œæˆä¸ºæ­¢
        self.push_in_progress.wait_until_done()
    def push_to_hub(self, commit_message: Optional[str] = "End of training", blocking: bool = True, **kwargs) -> str:
        """
        Upload `self.model` and `self.tokenizer` to the ğŸ¤— model hub on the repo `self.args.hub_model_id`.

        Parameters:
            commit_message (`str`, *optional*, defaults to `"End of training"`):
                Message to commit while pushing.
            blocking (`bool`, *optional*, defaults to `True`):
                Whether the function should return only when the `git push` has finished.
            kwargs (`Dict[str, Any]`, *optional*):
                Additional keyword arguments passed along to [`~Trainer.create_model_card`].

        Returns:
            The URL of the repository where the model was pushed if `blocking=False`, or a `Future` object tracking the
            progress of the commit if `blocking=True`.
        """
        model_name = kwargs.pop("model_name", None)
        # å¦‚æœæœªæŒ‡å®šæ¨¡å‹åç§°ä¸”åº”è¯¥ä¿å­˜æ¨¡å‹
        if model_name is None and self.args.should_save:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹åœ¨æ¨¡å‹è¾“å‡ºç›®å½•ä¸­ä½¿ç”¨ç›®å½•åç§°ä½œä¸ºæ¨¡å‹åç§°
            if self.args.hub_model_id is None:
                model_name = Path(self.args.output_dir).name
            else:
                # å¦åˆ™ä½¿ç”¨ hub_model_id ä¸­çš„æœ€åä¸€ä¸ªéƒ¨åˆ†ä½œä¸ºæ¨¡å‹åç§°
                model_name = self.args.hub_model_id.split("/")[-1]

        # å¦‚æœæœªåˆå§‹åŒ– hub_model_idï¼Œåˆ™åˆå§‹åŒ–
        if self.hub_model_id is None:
            self.init_hf_repo()

        # éœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šæ‰§è¡Œä»¥æ”¯æŒ TPU è®­ç»ƒï¼Œä½†ä»…åœ¨ self.args.should_save ç¡®å®šçš„è¿›ç¨‹ä¸Šä¿å­˜
        self.save_model(_internal_call=True)

        # åªåœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œæ¨é€æ“ä½œ
        if not self.is_world_process_zero():
            return

        # å¦‚æœæ¨¡å‹å·²æœ‰æ ‡ç­¾å¹¶ä¸”ç”¨æˆ·ä¼ é€’äº† "tags" å‚æ•°ï¼Œåˆ™æ·»åŠ é¢å¤–çš„æ ‡ç­¾ä»¥å¤„ç†å†…éƒ¨æ ‡ç­¾
        if getattr(self.model, "model_tags", None) is not None:
            if "tags" not in kwargs:
                kwargs["tags"] = []

            # å¦‚æœ tags æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(kwargs["tags"], str):
                kwargs["tags"] = [kwargs["tags"]]

            # å°†æ¨¡å‹çš„æ¯ä¸ªæ ‡ç­¾æ·»åŠ åˆ° kwargs["tags"] ä¸­
            for model_tag in self.model.model_tags:
                if model_tag not in kwargs["tags"]:
                    kwargs["tags"].append(model_tag)

        # åˆ›å»ºæ¨¡å‹å¡ç‰‡
        self.create_model_card(model_name=model_name, **kwargs)

        # ç­‰å¾…å½“å‰ä¸Šä¼ å®Œæˆ
        self._finish_current_push()
        # è¿”å›ä¸Šä¼ æ–‡ä»¶å¤¹çš„ç»“æœ
        return upload_folder(
            repo_id=self.hub_model_id,
            folder_path=self.args.output_dir,
            commit_message=commit_message,
            token=self.args.hub_token,
            run_as_future=not blocking,
            ignore_patterns=["_*", f"{PREFIX_CHECKPOINT_DIR}-*"],
        )

    #
    # Deprecated code
    #
    def prediction_loop(
        self,
        dataloader: DataLoader,
        description: str,
        prediction_loss_only: Optional[bool] = None,
        ignore_keys: Optional[List[str]] = None,
        metric_key_prefix: str = "eval",
    ):
        """
        Perform a prediction loop over the given data loader.

        Args:
            dataloader (DataLoader): The data loader containing the data to predict on.
            description (str): Description of the prediction loop.
            prediction_loss_only (Optional[bool], optional): Whether to calculate only prediction loss.
            ignore_keys (Optional[List[str]], optional): Keys to ignore during prediction.
            metric_key_prefix (str, optional): Prefix for metric keys.

        Returns:
            None
        """

    def _gather_and_numpify(self, tensors, name):
        """
        Gather values of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy arrays.

        Args:
            tensors: Tensor or list/tuple of nested tensors to gather and convert.
            name: Name associated with the gathering operation.

        Returns:
            numpy.ndarray or list/tuple/nested structure of numpy arrays corresponding to `tensors`.
        """
        if tensors is None:
            return

        # If using Torch XLA, perform mesh reduction
        if is_torch_xla_available():
            tensors = nested_xla_mesh_reduce(tensors, name)
        # If using SageMaker multi-processing, gather tensors
        elif is_sagemaker_mp_enabled():
            tensors = smp_gather(tensors)
        # If in distributed training mode, concatenate tensors
        elif self.args.parallel_mode == ParallelMode.DISTRIBUTED:
            tensors = distributed_concat(tensors)

        # Convert gathered tensors to numpy arrays
        return nested_numpify(tensors)

    def _add_sm_patterns_to_gitignore(self) -> None:
        """
        Add SageMaker Checkpointing patterns to .gitignore file if running on the main process.

        Returns:
            None
        """
        # Ensure only the main process performs this operation
        if not self.is_world_process_zero():
            return

        # Patterns to be added to .gitignore
        patterns = ["*.sagemaker-uploading", "*.sagemaker-uploaded"]

        # Read current .gitignore content if it exists
        if os.path.exists(os.path.join(self.repo.local_dir, ".gitignore")):
            with open(os.path.join(self.repo.local_dir, ".gitignore"), "r") as f:
                current_content = f.read()
        else:
            current_content = ""

        # Prepare content to write, appending patterns if not already present
        content = current_content
        for pattern in patterns:
            if pattern not in content:
                if content.endswith("\n"):
                    content += pattern
                else:
                    content += f"\n{pattern}"

        # Write to .gitignore if there were changes
        if content != current_content:
            with open(os.path.join(self.repo.local_dir, ".gitignore"), "w") as f:
                logger.debug(f"Writing .gitignore file. Content: {content}")
                f.write(content)

        # Add .gitignore to the git index
        self.repo.git_add(".gitignore")

        # Ensure there's no race condition with git status
        time.sleep(0.5)

        # Commit changes if repository is not clean
        if not self.repo.is_repo_clean():
            self.repo.git_commit("Add *.sagemaker patterns to .gitignore.")
            self.repo.git_push()
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºåˆ›å»ºåŠ é€Ÿå™¨å¯¹è±¡å¹¶è¿›è¡Œåå¤„ç†
    def create_accelerator_and_postprocess(self):
        # è®¾ç½®æ¢¯åº¦ç´¯ç§¯æ’ä»¶çš„å‚æ•°å­—å…¸
        grad_acc_kwargs = {"num_steps": self.args.gradient_accumulation_steps}
        # è®¾ç½®ä¸ä¸æ•°æ®åŠ è½½å™¨åŒæ­¥
        grad_acc_kwargs["sync_with_dataloader"] = False
        # åˆ›å»ºæ¢¯åº¦ç´¯ç§¯æ’ä»¶å¯¹è±¡
        gradient_accumulation_plugin = GradientAccumulationPlugin(**grad_acc_kwargs)

        # åˆ›å»ºåŠ é€Ÿå™¨å¯¹è±¡
        self.accelerator = Accelerator(
            deepspeed_plugin=self.args.deepspeed_plugin,  # æŒ‡å®šæ·±åº¦åŠ é€Ÿæ’ä»¶
            gradient_accumulation_plugin=gradient_accumulation_plugin,  # æŒ‡å®šæ¢¯åº¦ç´¯ç§¯æ’ä»¶
            **self.args.accelerator_config.to_dict(),  # ä½¿ç”¨åŠ é€Ÿå™¨é…ç½®çš„å‚æ•°
        )
        # æŸäº› Trainer ç±»éœ€è¦ä½¿ç”¨ `gather` è€Œä¸æ˜¯ `gather_for_metrics`ï¼Œå› æ­¤å­˜å‚¨ä¸€ä¸ªæ ‡å¿—
        self.gather_function = self.accelerator.gather_for_metrics

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† DeepSpeed æ’ä»¶
        self.is_deepspeed_enabled = getattr(self.accelerator.state, "deepspeed_plugin", None) is not None
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† FSDP æ’ä»¶
        self.is_fsdp_enabled = getattr(self.accelerator.state, "fsdp_plugin", None) is not None

        # åŠ é€Ÿå™¨åˆ›å»ºåçš„è®¾ç½®
        if self.is_fsdp_enabled:
            fsdp_plugin = self.accelerator.state.fsdp_plugin
            # è®¾ç½® FSDP æ’ä»¶çš„æ‰€æœ‰ gather é™åˆ¶
            fsdp_plugin.limit_all_gathers = self.args.fsdp_config.get(
                "limit_all_gathers", fsdp_plugin.limit_all_gathers
            )
            # å¦‚æœåŠ é€Ÿå™¨ç‰ˆæœ¬å…¼å®¹ï¼Œåˆ™è®¾ç½®æ¿€æ´»æ£€æŸ¥ç‚¹åŠŸèƒ½
            if is_accelerate_available("0.23.0"):
                fsdp_plugin.activation_checkpointing = self.args.fsdp_config.get(
                    "activation_checkpointing", fsdp_plugin.activation_checkpointing
                )
                # å¦‚æœåŒæ—¶è®¾ç½®äº†æ¿€æ´»æ£€æŸ¥ç‚¹å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™æŠ›å‡ºé”™è¯¯
                if fsdp_plugin.activation_checkpointing and self.args.gradient_checkpointing:
                    raise ValueError(
                        "The activation_checkpointing in FSDP config and the gradient_checkpointing in training arg "
                        "can't be set to True simultaneously. Please use FSDP's activation_checkpointing logic "
                        "when using FSDP."
                    )

        # å¦‚æœå¯ç”¨äº† DeepSpeedï¼Œå¹¶ä¸”æœªæä¾› hf_deepspeed_config å‚æ•°ï¼Œåˆ™ä¼ æ’­å‚æ•°åˆ° DeepSpeed
        if self.is_deepspeed_enabled and getattr(self.args, "hf_deepspeed_config", None) is None:
            self.propagate_args_to_deepspeed()

        # å¦‚æœè®¾ç½®äº† `save_only_model` å¹¶ä¸”åŒæ—¶ä½¿ç”¨äº† DeepSpeed æˆ– FSDP ä»¥åŠ `load_best_model_at_end`ï¼Œåˆ™æŠ›å‡ºé”™è¯¯
        if (
            self.args.save_only_model
            and (self.is_deepspeed_enabled or self.is_fsdp_enabled)
            and self.args.load_best_model_at_end
        ):
            wrapper = "DeepSpeed" if self.is_deepspeed_enabled else "FSDP"
            raise ValueError(f"{wrapper} can't be used with `save_only_model` along with `load_best_model_at_end`.")

        # å¦‚æœä½¿ç”¨äº† DeepSpeed æˆ– FSDPï¼Œå¹¶ä¸”è®¾ç½®äº† `auto_find_batch_size`ï¼Œåˆ™æŠ›å‡ºæœªå®ç°é”™è¯¯
        if (self.is_deepspeed_enabled or self.is_fsdp_enabled) and self.args.auto_find_batch_size:
            wrapper = "DeepSpeed" if self.is_deepspeed_enabled else "FSDP"
            raise NotImplementedError(f"`{wrapper}` doesn't support `auto_find_batch_size`.")
    # å°† Trainer å‚æ•°ä¼ æ’­åˆ° DeepSpeed æ’ä»¶ä¸­
    def propagate_args_to_deepspeed(self, auto_find_batch_size=False):
        """
        Sets values in the deepspeed plugin based on the Trainer args
        æ ¹æ® Trainer å‚æ•°è®¾ç½® DeepSpeed æ’ä»¶ä¸­çš„æ•°å€¼
        """
        # å¯¼å…¥ DeepSpeed é…ç½®ç±»
        from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig

        # è·å–å½“å‰åŠ é€Ÿå™¨çŠ¶æ€ä¸­çš„ DeepSpeed æ’ä»¶
        ds_plugin = self.accelerator.state.deepspeed_plugin

        # ä½¿ç”¨ Trainer args é…ç½®ä¸€ä¸ª HfTrainerDeepSpeedConfig å¯¹è±¡
        ds_plugin.hf_ds_config = HfTrainerDeepSpeedConfig(ds_plugin.hf_ds_config.config)
        # å°† DeepSpeed æ’ä»¶çš„é…ç½®è®¾ç½®ä¸ºæ–°åˆ›å»ºçš„ HfTrainerDeepSpeedConfig å¯¹è±¡çš„é…ç½®
        ds_plugin.deepspeed_config = ds_plugin.hf_ds_config.config
        # æ ¹æ® Trainer å‚æ•°è¿›ä¸€æ­¥å¤„ç† DeepSpeed é…ç½®
        ds_plugin.hf_ds_config.trainer_config_process(self.args, auto_find_batch_size)

    # æ›´æ–° FSDP æ’ä»¶ä¸­çš„ QLoRa ç›¸å…³è®¾ç½®
    def _fsdp_qlora_plugin_updates(self):
        """
        Updates the FSDP plugin with QLoRa related settings if applicable
        å¦‚æœé€‚ç”¨ï¼Œæ›´æ–° FSDP æ’ä»¶çš„ QLoRa ç›¸å…³è®¾ç½®
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† FSDP å¹¶ä¸”æ¨¡å‹æ˜¯ PEFT æ¨¡å‹
        if self.is_fsdp_enabled and _is_peft_model(self.model):
            # å¯¼å…¥ PEFT é…ç½®å’Œ FSDP è‡ªåŠ¨åŒ…è£…ç­–ç•¥å·¥å…·
            from peft import LoraConfig
            from peft.utils.other import fsdp_auto_wrap_policy

            # å¦‚æœæ¨¡å‹çš„ active_peft_config æ˜¯ LoraConfig ç±»å‹
            if isinstance(self.model.active_peft_config, LoraConfig):
                # è·å–åŠ é€Ÿå™¨çŠ¶æ€ä¸­çš„ FSDP æ’ä»¶
                fsdp_plugin = self.accelerator.state.fsdp_plugin
                # è®¾ç½® FSDP æ’ä»¶çš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥
                fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(self.model)

            # å¦‚æœæ¨¡å‹çš„é‡åŒ–æ–¹æ³•æ˜¯ BITS_AND_BYTESï¼Œä¸”é‡åŒ–é…ç½®æ˜¯æµ®ç‚¹æ•°ï¼Œå¹¶ä¸”åŠ é€Ÿå™¨ç‰ˆæœ¬é«˜äº "0.27.0"
            if (
                getattr(self.model, "quantization_method", None) == QuantizationMethod.BITS_AND_BYTES
                and self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage.is_floating_point
                and version.parse(accelerate_version) > version.parse("0.27.0")
            ):
                # è·å–åŠ é€Ÿå™¨çŠ¶æ€ä¸­çš„ FSDP æ’ä»¶å¹¶è®¾ç½®æ··åˆç²¾åº¦
                fsdp_plugin.set_mixed_precision(
                    self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage, override=True
                )
```