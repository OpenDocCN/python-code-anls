# `.\models\gptsan_japanese\tokenization_gptsan_japanese.py`

```py
# è®¾ç½®ç¼–ç æ ¼å¼ä¸º UTF-8
# ç‰ˆæƒå£°æ˜
# æ ¹æ® Apache è®¸å¯è¯ç‰ˆæœ¬ 2.0 è®¸å¯
# ä»…åœ¨ç¬¦åˆè®¸å¯è¯çš„æƒ…å†µä¸‹ä½¿ç”¨æ­¤æ–‡ä»¶
# æ‚¨å¯ä»¥è·å–è®¸å¯è¯çš„å‰¯æœ¬
# åœ¨è¿™ä¸ªç½‘å€ï¼šhttp://www.apache.org/licenses/LICENSE-2.0
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™è½¯ä»¶æ˜¯åŸºäº"ç°çŠ¶"æä¾›çš„
# æ²¡æœ‰ä»»ä½•ç§ç±»çš„æ‹…ä¿æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºçš„è¿˜æ˜¯æš—ç¤ºçš„
# è¯·é˜…è¯»è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€ä¸Šçš„è®¸å¯è¯è¦æ±‚å’Œé™åˆ¶
"""GPTSANJapanese"çš„æ ‡è®°ç±»
å¯¼å…¥æ‰€éœ€çš„ä¾èµ–åº“å¹¶è®¾ç½®ä¸€äº›å¸¸é‡
"""
import collections  # å¯¼å…¥collectionsæ¨¡å—ï¼Œç”¨æ¥åˆ›å»ºæœ‰åºå­—å…¸ç­‰æ•°æ®ç»“æ„
import json  # å¯¼å…¥jsonæ¨¡å—ï¼Œç”¨æ¥å¤„ç†JSONæ•°æ®
import os  # å¯¼å…¥osæ¨¡å—ï¼Œç”¨æ¥è¿›è¡Œç³»ç»Ÿæ“ä½œ
import re  # å¯¼å…¥reæ¨¡å—ï¼Œç”¨æ¥è¿›è¡Œæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
from typing import List, Optional, Tuple, Union  # å¯¼å…¥ç±»å‹æç¤ºæ¨¡å—ï¼Œç”¨äºç±»å‹æ³¨è§£

import numpy as np  # å¯¼å…¥numpyåº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—

from ...tokenization_utils import PreTrainedTokenizer  # å¯¼å…¥é¢„è®­ç»ƒçš„Tokenizerç±»
from ...tokenization_utils_base import (  # å¯¼å…¥åŸºç¡€çš„Tokenizerç±»å’Œç›¸å…³ç±»å‹
    BatchEncoding,
    PreTokenizedInput,
    PreTokenizedInputPair,
    TextInput,
    TextInputPair,
    TruncationStrategy,
)
from ...utils import PaddingStrategy, logging  # å¯¼å…¥Paddingç­–ç•¥å’Œæ—¥å¿—è®°å½•åŠŸèƒ½

# è·å–loggerå¯¹è±¡ï¼Œç”¨äºè®°å½•æ—¥å¿—
logger = logging.get_logger(__name__)

# å®šä¹‰è¯æ±‡è¡¨æ–‡ä»¶çš„åç§°
VOCAB_FILES_NAMES = {"vocab_file": "vocab.txt", "emoji_file": "emoji.json"}

# å®šä¹‰é¢„è®­ç»ƒè¯æ±‡è¡¨æ–‡ä»¶æ˜ å°„å…³ç³»
PRETRAINED_VOCAB_FILES_MAP = {
    "vocab_file": {
        "Tanrei/GPTSAN-japanese": "https://huggingface.co/Tanrei/GPTSAN-japanese/blob/main/vocab.txt",
    },
    "emoji_file": {
        "Tanrei/GPTSAN-japanese": "https://huggingface.co/Tanrei/GPTSAN-japanese/blob/main/emoji.json",
    },
}

# å®šä¹‰é¢„è®­ç»ƒä½ç½®åµŒå…¥çš„å°ºå¯¸
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
    "Tanrei/GPTSAN-japanese": 1280,
}


def load_vocab_and_emoji(vocab_file, emoji_file):
    """åŠ è½½è¯æ±‡è¡¨æ–‡ä»¶å’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶åˆ°å­—å…¸ä¸­"""
    # ä»emojiæ–‡ä»¶ä¸­è¯»å–è¡¨æƒ…ç¬¦å·å¹¶è½¬ä¸ºå­—å…¸
    with open(emoji_file, "r", encoding="utf-8") as f:
        emoji = json.loads(f.read())

    # åˆå§‹åŒ–è¯æ±‡è¡¨å’ŒåŸå§‹è¯æ±‡è¡¨ç­‰å­—å…¸
    vocab = collections.OrderedDict()
    raw_vocab = collections.OrderedDict()
    ids_to_tokens = collections.OrderedDict()
    with open(vocab_file, "r", encoding="utf-8") as f:
        token = f.readlines()
    # å°†è¯æ±‡è¡¨æ–‡ä»¶æ‹†åˆ†ä¸ºå•è¯åˆ—è¡¨
    token = [
        [t.rstrip("\n")] if (t == ",\n" or "," not in t) else t.rstrip("\n").split(",") for t in token
    ]
    for idx, b in enumerate(token):
        ids_to_tokens[idx] = b
        raw_vocab[",".join(b)] = idx
        for wd in b:
            vocab[wd] = idx

    return vocab, raw_vocab, ids_to_tokens, emoji


class GPTSanJapaneseTokenizer(PreTrainedTokenizer):
    """
    è¿™ä¸ªæ ‡è®°å™¨åŸºäºGPTNeoXJapaneseTokenizerï¼Œå¹¶è¿›è¡Œäº†ä»¥ä¸‹ä¿®æ”¹
    - æ­£ç¡®è§£ç å­—èŠ‚0~å­—èŠ‚255æ ‡è®°
    - å¢åŠ bagofwordæ ‡è®°å¤„ç†
    - ä¸ºå‰ç¼€è¯­è¨€æ¨¡å‹è¿”å›token_type_ids
    bagofwordæ ‡è®°è¡¨ç¤ºé‡å¤å‰ä¸€ä¸ªæ ‡è®°ï¼Œå¹¶åœ¨è§£ç æ—¶è½¬æ¢ä¸º3ä¸ªè¿ç»­æ ‡è®°
    æ­¤å¤–ï¼ŒåŸå§‹çš„æ—¥æ–‡ç‰¹æ®Šå­è¯ç¼–ç å·²å‘å¸ƒåœ¨æ­¤å­˜å‚¨åº“ä¸­
    (https://github.com/tanreinama/Japanese-BPEEncoder_V2)ã€‚token_type_idsæ˜¯æŒ‡ç¤ºå‰ç¼€è¾“å…¥çš„æ©ç 
    # è®¾ç½® Prefix-LM æ¨¡å‹çš„å‰ç¼€ä½ç½®ã€‚è¦æŒ‡å®šå‰ç¼€ä½ç½®ï¼Œä¸º prefix_text æŒ‡å®šå‰ç¼€è¾“å…¥ï¼Œæˆ–è€…æŒ‡å®šå‰ç¼€éƒ¨åˆ†å’Œåé¢éƒ¨åˆ†çš„æ–‡æœ¬å¯¹ä½œä¸ºæ‰¹é‡è¾“å…¥
    position of the Prefix-LM model. To specify a prefix position, specify a prefix input for prefix_text, or specify a sentence of the prefix part and the part after it as a text pair.

    ç¤ºä¾‹:

    ```python
    >>> from transformers import GPTSanJapaneseTokenizer

    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    >>> # You can confirm both æ…¶å¿œ and æ…¶æ‡‰ are encoded to 17750
    >>> tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"]
    [35993, 35998, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]

    >>> # Both æ…¶å¿œ and æ…¶æ‡‰ are decoded to æ…¶å¿œ
    >>> tokenizer.decode(tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"])
    'å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶å¿œ)å¤§å­¦å‡ºèº«'
    ```py

    Prefix-LM ç¤ºä¾‹:

    ```python
    >>> from transformers import GPTSanJapaneseTokenizer

    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    >>> tokenizer("å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«", prefix_text="å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚")["input_ids"]
    [35993, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 35998, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]

    >>> # Mask for Prefix-LM inputs
    >>> tokenizer("å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«", prefix_text="å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚")["token_type_ids"]
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ```py

    æ‰¹é‡ç¼–ç  ç¤ºä¾‹:

    ```python
    >>> from transformers import GPTSanJapaneseTokenizer

    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    >>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["input_ids"]
    [[35993, 8640, 25948, 35998, 30647, 35675, 35999, 35999], [35993, 10382, 9868, 35998, 30646, 9459, 30646, 35675]]

    >>> # Mask for Prefix-LM inputs
    >>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["token_type_ids"]
    [[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0]]

    >>> # Mask for padding
    >>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["attention_mask"]
    [[1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]
    ```py
    Args:
        vocab_file (`str`):
            åŒ…å«è¯æ±‡è¡¨çš„æ–‡ä»¶ã€‚
        emoji_file (`str`):
            åŒ…å«è¡¨æƒ…ç¬¦å·çš„æ–‡ä»¶ã€‚
        unk_token (`str`, *optional*, é»˜è®¤ä¸º `"<|nottoken|>"`):
            ç”¨äºæœªçŸ¥å­—ç¬¦çš„æ ‡è®°
        pad_token (`str`, *optional*, é»˜è®¤ä¸º `"<|separator|>"`):
            ç”¨äºå¡«å……çš„æ ‡è®°
        bos_token (`str`, *optional*, é»˜è®¤ä¸º `"<|startoftext|>"`):
            åºåˆ—å¼€å§‹çš„æ ‡è®°ã€‚
        eos_token (`str`, *optional*, é»˜è®¤ä¸º `"<|endoftext|>"`):
            åºåˆ—ç»“æŸçš„æ ‡è®°ã€‚
        sep_token (`str`, *optional*, é»˜è®¤ä¸º `"<|segmenter|>"`):
            ç”¨äºåˆ†éš”å‰ç¼€éƒ¨åˆ†å’Œä¸€èˆ¬è¾“å…¥éƒ¨åˆ†çš„ç‰¹æ®Šæ ‡è®°ã€‚
        do_clean_text (`bool`, *optional*, é»˜è®¤ä¸º `False`):
            æ˜¯å¦å¯¹ URLã€é‚®ç®±ã€ç”µè¯ã€æ—¥æ–‡æ—¥æœŸå’Œæ—¥æ–‡ä»·æ ¼è¿›è¡Œæ–‡æœ¬æ¸…æ´—ã€‚

    """

    vocab_files_names = VOCAB_FILES_NAMES
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
    model_input_names = ["input_ids", "attention_mask", "token_type_ids"]

    def __init__(
        self,
        vocab_file,
        emoji_file,
        unk_token="<|nottoken|>",
        pad_token="<|separator|>",
        bos_token="<|startoftext|>",
        eos_token="<|endoftext|>",
        sep_token="<|segmenter|>",
        do_clean_text=False,
        **kwargs,
    ):
        if not os.path.isfile(vocab_file):
            raise ValueError(
                f"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained"
                " model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
            )
        if not os.path.isfile(emoji_file):
            raise ValueError(
                f"Can't find a emoji file at path '{emoji_file}'. To load the emoji information from a Google"
                " pretrained model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
            )
        self.do_clean_text = do_clean_text
        self.vocab, self.raw_vocab, self.ids_to_tokens, self.emoji = load_vocab_and_emoji(vocab_file, emoji_file)
        self.subword_tokenizer = SubWordJapaneseTokenizer(
            vocab=self.vocab, ids_to_tokens=self.ids_to_tokens, emoji=self.emoji
        )

        super().__init__(
            unk_token=unk_token,
            pad_token=pad_token,
            bos_token=bos_token,
            eos_token=eos_token,
            sep_token=sep_token,
            do_clean_text=do_clean_text,
            **kwargs,
        )

    @property
    # ä» tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer.vocab_size å¤åˆ¶è€Œæ¥
    def vocab_size(self):
        # self.vocab åŒ…å«äº†å¯¹æ—¥è¯­ç‰¹æœ‰å­—ç¬¦æ³¢åŠ¨çš„æ”¯æŒï¼Œå¹¶ä¸”å…·æœ‰å¤§é‡çš„è¯æ±‡é‡
        return len(self.raw_vocab)
    # ä» tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer.get_vocab å¤åˆ¶çš„æ–¹æ³•ï¼Œè¿”å›è¯æ±‡è¡¨ï¼ˆå­—å…¸å½¢å¼ï¼‰
    def get_vocab(self):
        return dict(self.raw_vocab, **self.added_tokens_encoder)

    # ä» tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer._tokenize å¤åˆ¶çš„æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†è¯
    def _tokenize(self, text):
        return self.subword_tokenizer.tokenize(text, clean=self.do_clean_text)

    # ä» tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer._convert_token_to_id å¤åˆ¶çš„æ–¹æ³•ï¼Œå°† token è½¬æ¢ä¸º id
    def _convert_token_to_id(self, token):
        """Converts a token (str) in an id using the vocab."""
        return self.vocab.get(token, self.vocab.get(self.unk_token))

    # ä» tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer._convert_id_to_token å¤åˆ¶çš„æ–¹æ³•ï¼Œå°† id è½¬æ¢ä¸º token
    def _convert_id_to_token(self, index):
        """Converts an index (integer) in a token (str) using the vocab."""
        return self.subword_tokenizer.convert_id_to_token(index)

    # å°† tokens åºåˆ—è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ä¸²
    def convert_tokens_to_string(self, tokens):
        """Converts a sequence of tokens (string) in a single string."""
        words = []
        byte_tokens = []
        for word in tokens:
            # å¤„ç†å­—èŠ‚æ ‡è®°
            if word[:6] == "<|byte" and word[-2:] == "|>":
                byte_tokens.append(int(word[6:-2]))
            else:
                # å°† byte_tokens è½¬æ¢ä¸ºå­—ç¬¦æ·»åŠ åˆ°ç»“æœä¸­
                if len(byte_tokens) > 0:
                    words.append(bytearray(byte_tokens).decode("utf-8", errors="replace"))
                    byte_tokens = []
                # å¤„ç†ç‰¹æ®Šçš„å•è¯æ ‡è®°å’Œæ¡ä»¶
                # ...
                else:
                    words.append(word)
        # å¤„ç†å‰©ä½™çš„å­—èŠ‚æ ‡è®°
        if len(byte_tokens) > 0:
            words.append(bytearray(byte_tokens).decode("utf-8", errors="replace"))
        text = "".join(words)
        return text

    @property
    def default_chat_template(self):
        """
        A simple chat template that adds standard BOS, SEP and EOS tokens between messages while discarding role
        information.
        """
        # å¦‚æœæ²¡æœ‰ä¸ºæ­¤åˆ†è¯å™¨å®šä¹‰èŠå¤©æ¨¡æ¿ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿ï¼Œå¹¶å‘å‡ºè­¦å‘Šæ¶ˆæ¯
        logger.warning_once(
            "\nNo chat template is defined for this tokenizer - using the default template "
            f"for the {self.__class__.__name__} class. If the default is not appropriate for "
            "your model, please set `tokenizer.chat_template` to an appropriate template. "
            "See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n"
        )
        # è¿”å›é»˜è®¤çš„èŠå¤©æ¨¡æ¿
        return (
            "{% for message in messages %}"
            "{% if not loop.first %}{{ bos_token}}{% endif %}"
            "{{ sep_token }}{{ message.content }} {{ eos_token }}"
            "{% endfor %}"
        )

    # ä» tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer.save_vocabulary å¤åˆ¶è€Œæ¥
    # ä¿å­˜è¯æ±‡è¡¨åˆ°æŒ‡å®šç›®å½•
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:
        # åˆå§‹åŒ–ç´¢å¼•
        index = 0
        # æ£€æŸ¥ä¿å­˜ç›®å½•æ˜¯å¦å­˜åœ¨
        if os.path.isdir(save_directory):
            # æ„å»ºè¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            vocab_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["vocab_file"]
            )
            # æ„å»ºè¡¨æƒ…ç¬¦å·æ–‡ä»¶è·¯å¾„
            emoji_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["emoji_file"]
            )
        else:
            # æ„å»ºè¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            vocab_file = (
                (filename_prefix + "-" if filename_prefix else "") + save_directory + VOCAB_FILES_NAMES["vocab_file"]
            )
            # æ„å»ºè¡¨æƒ…ç¬¦å·æ–‡ä»¶è·¯å¾„
            emoji_file = (
                (filename_prefix + "-" if filename_prefix else "") + save_directory + VOCAB_FILES_NAMES["emoji_file"]
            )
        # å†™å…¥è¯æ±‡è¡¨æ–‡ä»¶
        with open(vocab_file, "w", encoding="utf-8") as writer:
            for token_index, token in self.ids_to_tokens.items():
                if index != token_index:
                    # å¦‚æœè¯æ±‡è¡¨ç´¢å¼•ä¸æ˜¯è¿ç»­çš„ï¼Œåˆ™å‘å‡ºè­¦å‘Šæ¶ˆæ¯
                    logger.warning(
                        f"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive."
                        " Please check that the vocabulary is not corrupted!"
                    )
                    index = token_index
                writer.write(",".join(token) + "\n")
                index += 1
        # å†™å…¥è¡¨æƒ…ç¬¦å·æ–‡ä»¶
        with open(emoji_file, "w", encoding="utf-8") as writer:
            json.dump(self.emoji, writer)
        # è¿”å›è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„å’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶è·¯å¾„
        return vocab_file, emoji_file

    # ä»åºåˆ—åˆ›å»º token_type_ids
    # å°† token_ids_0 å’Œ token_ids_1 è½¬æ¢ä¸º token_type_ids
    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -> List[int]:
        # æ ‡è®°ç±»å‹ ID ä½œä¸ºå‰ç¼€éƒ¨åˆ†å’Œå…¶ä½™éƒ¨åˆ†ä¹‹é—´çš„åˆ†éš”ç¬¦
        # å‰ç¼€éƒ¨åˆ†çš„ token_type_ids ä¸º 1ï¼Œå…¶ä½™éƒ¨åˆ†ä¸º 0

        # ç¤ºä¾‹:
        ```python
        >>> from transformers import GPTSanJapaneseTokenizer

        >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
        >>> x_token = tokenizer("ï½±ï½²ï½³ï½´")
        >>> # input_ids:      | SOT | SEG | ï½± | ï½² | ï½³ | ï½´ |
        >>> # token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |

        >>> x_token = tokenizer("", prefix_text="ï½±ï½²ï½³ï½´")
        >>> # input_ids:      | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG |
        >>> # token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |

        >>> x_token = tokenizer("ï½³ï½´", prefix_text="ï½±ï½²")
        >>> # input_ids:      | SOT | ï½± | ï½² | SEG | ï½³ | ï½´ |
        >>> # token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |
        ```py"""
        # è®¡ç®—å‰ç¼€é•¿åº¦
        prefix_len = 0
        if self.sep_token in self.vocab:  # å¦‚æœåˆ†éš”ç¬¦åœ¨è¯æ±‡è¡¨ä¸­
            segid = self.vocab[self.sep_token]  # è·å–åˆ†éš”ç¬¦çš„ ID
            if segid in token_ids_0:  # å¦‚æœåˆ†éš”ç¬¦ ID å­˜åœ¨äº token_ids_0 ä¸­
                prefix_len = token_ids_0.index(segid)  # è®¡ç®—å‰ç¼€é•¿åº¦ä¸ºåˆ†éš”ç¬¦ä¹‹å‰çš„ token æ•°é‡
        if token_ids_1 is None:  # å¦‚æœ token_ids_1 ä¸ºç©º
            total_len = len(token_ids_0)  # è®¡ç®—æ€»é•¿åº¦ä¸º token_ids_0 çš„é•¿åº¦
        else:  # å¦‚æœ token_ids_1 ä¸ä¸ºç©º
            total_len = len(token_ids_0 + token_ids_1)  # è®¡ç®—æ€»é•¿åº¦ä¸ºä¸¤ä¸ª token_ids çš„é•¿åº¦ä¹‹å’Œ
        return prefix_len * [1] + (total_len - prefix_len) * [0]  # è¿”å›å‰ç¼€éƒ¨åˆ†çš„ token_type_ids

    def prepare_for_tokenization(self, text, prefix_text=None, add_sep_token=None, **kwargs):
        # GPTSAN åœ¨æ–‡æœ¬ç”Ÿæˆçš„å‰ç¼€è¯­è¨€æ¨¡å‹ä¸­é™¤äº† SOT ä¹‹å¤–è¿˜ä¼šæ’å…¥é¢å¤–çš„ SEP token
        # æ–‡æœ¬å¼€å¤´ä¸º SOTï¼Œå‰ç¼€éƒ¨åˆ†å’Œå…¶ä½™éƒ¨åˆ†ä¹‹é—´ä¸º SEP

        if add_sep_token is None:
            add_sep_token = self.sep_token not in text  # å¦‚æœåœ¨éå‰ç¼€ä½ç½®æ˜¾å¼æ’å…¥ SEP token
        prepared = self.bos_token if self.bos_token in self.vocab else ""  # å¦‚æœ BOS token åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™åŠ å…¥åˆ° prepared ä¸­
        prepared += prefix_text if prefix_text is not None else ""  # åŠ å…¥å‰ç¼€æ–‡æœ¬
        if add_sep_token:  # å¦‚æœéœ€è¦æ’å…¥ SEP token
            prepared += self.sep_token if self.sep_token in self.vocab else ""  # å¦‚æœ SEP token åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™åŠ å…¥åˆ° prepared ä¸­
        prepared += text  # åŠ å…¥æ–‡æœ¬
        return (prepared, kwargs)  # è¿”å›å‡†å¤‡å¥½çš„æ–‡æœ¬å’Œé¢å¤–çš„å‚æ•°
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºæ‰¹é‡ç¼–ç æ–‡æœ¬æˆ–æ–‡æœ¬å¯¹
    def _batch_encode_plus(
        self,
        # è¾“å…¥å‚æ•°å¯ä»¥æ˜¯æ–‡æœ¬åˆ—è¡¨ã€æ–‡æœ¬å¯¹åˆ—è¡¨ã€é¢„åˆ†è¯è¾“å…¥åˆ—è¡¨æˆ–é¢„åˆ†è¯è¾“å…¥å¯¹åˆ—è¡¨çš„è”åˆç±»å‹
        batch_text_or_text_pairs: Union[
            List[TextInput], List[TextInputPair], List[PreTokenizedInput], List[PreTokenizedInputPair]
        ],
        # æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œé»˜è®¤ä¸º True
        add_special_tokens: bool = True,
        # å¡«å……ç­–ç•¥ï¼Œé»˜è®¤ä¸ºä¸å¡«å……
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        # æˆªæ–­ç­–ç•¥ï¼Œé»˜è®¤ä¸ºä¸æˆªæ–­
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        # æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œé»˜è®¤ä¸º None
        max_length: Optional[int] = None,
        # æ­¥é•¿ï¼Œé»˜è®¤ä¸º 0
        stride: int = 0,
        # æ˜¯å¦å·²åˆ†è¯ï¼Œé»˜è®¤ä¸º False
        is_split_into_words: bool = False,
        # å¡«å……åˆ°æŒ‡å®šé•¿åº¦çš„å€æ•°ï¼Œé»˜è®¤ä¸º None
        pad_to_multiple_of: Optional[int] = None,
        # è¿”å›çš„å¼ é‡ç±»å‹ï¼Œé»˜è®¤ä¸º None
        return_tensors: Optional[str] = None,
        # æ˜¯å¦è¿”å›æ ‡è®°ç±»å‹çš„å¼ é‡ï¼Œé»˜è®¤ä¸º None
        return_token_type_ids: Optional[bool] = None,
        # æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œé»˜è®¤ä¸º None
        return_attention_mask: Optional[bool] = None,
        # æ˜¯å¦è¿”å›æº¢å‡ºçš„æ ‡è®°ï¼Œé»˜è®¤ä¸º False
        return_overflowing_tokens: bool = False,
        # æ˜¯å¦è¿”å›ç‰¹æ®Šæ ‡è®°çš„æ©ç ï¼Œé»˜è®¤ä¸º False
        return_special_tokens_mask: bool = False,
        # æ˜¯å¦è¿”å›åç§»æ˜ å°„ï¼Œé»˜è®¤ä¸º False
        return_offsets_mapping: bool = False,
        # æ˜¯å¦è¿”å›é•¿åº¦ï¼Œé»˜è®¤ä¸º False
        return_length: bool = False,
        # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼Œé»˜è®¤ä¸º True
        verbose: bool = True,
    ) -> BatchEncoding:
        # è¿™ä¸ªåˆ†è¯å™¨å°†è¾“å…¥æ–‡æœ¬å¯¹è½¬æ¢ä¸ºå‰ç¼€è¾“å…¥å’Œéšåè¾“å…¥
        if isinstance(batch_text_or_text_pairs[0], tuple) or isinstance(tuple(batch_text_or_text_pairs[0]), list):
            # å°†æ¯ä¸ªæ–‡æœ¬å¯¹æ·»åŠ å‰ç¼€å¹¶åˆå¹¶æˆä¸€ä¸ªæ–‡æœ¬åˆ—è¡¨
            batch_prefix_texts = []
            for pref, txt in batch_text_or_text_pairs:
                batch_prefix_texts.append(pref + self.sep_token + txt)
            batch_text_or_text_pairs = batch_prefix_texts

        # è°ƒç”¨çˆ¶ç±»çš„ _batch_encode_plus æ–¹æ³•è¿›è¡Œç¼–ç å¤„ç†å¹¶è¿”å›ç»“æœ
        return super()._batch_encode_plus(
            batch_text_or_text_pairs,
            add_special_tokens,
            padding_strategy,
            truncation_strategy,
            max_length,
            stride,
            is_split_into_words,
            pad_to_multiple_of,
            return_tensors,
            return_token_type_ids,
            return_attention_mask,
            return_overflowing_tokens,
            return_special_tokens_mask,
            return_offsets_mapping,
            return_length,
            verbose,
        )
# å®šä¹‰ä¸€ä¸ªåä¸º SubWordJapaneseTokenizer çš„ç±»
class SubWordJapaneseTokenizer(object):
    """
    This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications
    - Decoding byte0~byte255 tokens correctly
    - Added bagofword token handling

    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT Lisence according to the
    original repository.

    MIT License

    Copyright (c) 2020 tanreinama

    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
    documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
    permit persons to whom the Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all copies or substantial portions of
    the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
    THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
    TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE.
    """

    # Copied from tokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.__init__
    # åˆå§‹åŒ–å‡½æ•°ï¼Œè®¾ç½®ç±»çš„å±æ€§
    def __init__(self, vocab, ids_to_tokens, emoji):
        self.vocab = vocab  # å°†å‚æ•° vocab èµ‹å€¼ç»™å®ä¾‹å±æ€§ vocab
        self.ids_to_tokens = ids_to_tokens  # å°†å‚æ•° ids_to_tokens èµ‹å€¼ç»™å®ä¾‹å±æ€§ ids_to_tokens
        self.emoji = emoji  # å°†å‚æ•° emoji èµ‹å€¼ç»™å®ä¾‹å±æ€§ emoji
        self.maxlen = np.max([len(w) for w in self.vocab.keys()])  # è®¡ç®—å®ä¾‹å±æ€§ maxlen çš„æœ€å¤§å€¼
        self.content_repatter1 = re.compile(r"(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)")  # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.content_repatter2 = re.compile(r"[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*")  # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.content_repatter3 = re.compile(r"[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}")  # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.content_repatter4 = re.compile(
            r"([12]\d{3}[/\-å¹´])*(0?[1-9]|1[0-2])[/\-æœˆ]((0?[1-9]|[12][0-9]|3[01])æ—¥?)*(\d{1,2}|:|\d{1,2}æ™‚|\d{1,2}åˆ†|\(æ—¥\)|\(æœˆ\)|\(ç«\)|\(æ°´\)|\(æœ¨\)|\(é‡‘\)|\(åœŸ\)|ãˆ°|ãˆª|ãˆ«|ãˆ¬|ãˆ­|ãˆ®|ãˆ¯)*"
        )  # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.content_repatter5 = re.compile(
            r"(æ˜æ²»|å¤§æ­£|æ˜­å’Œ|å¹³æˆ|ä»¤å’Œ|ã¾|ã½|ã¼|ã»|\u32ff)\d{1,2}å¹´(0?[1-9]|1[0-2])æœˆ(0?[1-9]|[12][0-9]|3[01])æ—¥(\d{1,2}|:|\d{1,2}æ™‚|\d{1,2}åˆ†|\(æ—¥\)|\(æœˆ\)|\(ç«\)|\(æ°´\)|\(æœ¨\)|\(é‡‘\)|\(åœŸ\)|ãˆ°|ãˆª|ãˆ«|ãˆ¬|ãˆ­|ãˆ®|ãˆ¯)*"
        )  # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.content_repatter6 = re.compile(
            r"((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*å„„)*((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*ä¸‡)*((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*åƒ)*(0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*(åƒå††|ä¸‡å††|åƒä¸‡å††|å††|åƒãƒ‰ãƒ«|ä¸‡ãƒ‰ãƒ«|åƒä¸‡ãƒ‰ãƒ«|ãƒ‰ãƒ«|åƒãƒ¦ãƒ¼ãƒ­|ä¸‡ãƒ¦ãƒ¼ãƒ­|åƒä¸‡ãƒ¦ãƒ¼ãƒ­|ãƒ¦ãƒ¼ãƒ­)+(\(ç¨è¾¼\)|\(ç¨æŠœ\)|\+tax)*"
        )  # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        keisen = "â”€â”â”‚â”ƒâ”„â”…â”†â”‡â”ˆâ”‰â”Šâ”‹â”Œâ”â”â”â”â”‘â”’â”“â””â”•â”–â”—â”˜â”™â”šâ”›â”œâ”â”â”Ÿâ” â”¡â”¢â”£â”¤â”¥â”¦â”§â”¨â”©â”ªâ”«â”¬â”­â”®â”¯â”°â”±â”²â”³â”´â”µâ”¶â”·â”¸â”¹â”ºâ”»â”¼â”½â”¾â”¿â•€â•â•‚â•ƒâ•„â•…â•†â•‡â•ˆâ•‰â•Šâ•‹â•Œâ•â•â•â•â•‘â•’â•“â•”â••â•–â•—â•˜â•™â•šâ•›â•œâ•â•â•Ÿâ• â•¡â•¢â•£â•¤â•¥â•¦â•§â•¨â•©â•ªâ•«â•¬â•­â•®â•¯â•°â•±â•²â•³â•´â•µâ•¶â•·â•¸â•¹â•ºâ•»â•¼â•½â•¾â•¿"
        blocks = "â–€â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‰â–Šâ–‹â–Œâ–â–â–â–â–‘â–’â–“â–”â–•â––â–—â–˜â–™â–šâ–›â–œâ–â–â–Ÿ"
        self.content_trans1 = str.maketrans({k: "<BLOCK>" for k in keisen + blocks})  # ä½¿ç”¨ str.maketrans æ–¹æ³•åˆ›å»ºæ›¿æ¢å­—ç¬¦å­—å…¸

    # è¿”å› ids_to_tokens çš„é•¿åº¦
    def __len__(self):
        return len(self.ids_to_tokens)

    # æ¸…æ´—æ–‡æœ¬å†…å®¹
    def clean_text(self, content):
        content = self.content_repatter1.sub("<URL>", content)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å†…å®¹
        content = self.content_repatter2.sub("<EMAIL>", content)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å†…å®¹
        content = self.content_repatter3.sub("<TEL>", content)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å†…å®¹
        content = self.content_repatter4.sub("<DATE>", content)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å†…å®¹
        content = self.content_repatter5.sub("<DATE>", content)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å†…å®¹
        content = self.content_repatter6.sub("<PRICE>", content)  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å†…å®¹
        content = content.translate(self.content_trans1)  # ä½¿ç”¨ maketrans æ–¹æ³•æ›¿æ¢å†…å®¹
        while "<BLOCK><BLOCK>" in content:
            content = content.replace("<BLOCK><BLOCK>", "<BLOCK>")  # æ›¿æ¢å†…å®¹ä¸­çš„é‡å¤æ ‡è®°
        return content
    # ç”¨äºå°†æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œå¯é€‰æ‹©æ˜¯å¦è¿›è¡Œæ¸…æ´—
    def tokenize(self, text, clean=False):
        # æ›¿æ¢ç©ºæ ¼ä¸º<SP>
        text = text.replace(" ", "<SP>")
        # æ›¿æ¢å…¨è§’ç©ºæ ¼ä¸º<SP>
        text = text.replace("ã€€", "<SP>")
        # æ›¿æ¢æ¢è¡Œç¬¦ä¸º<BR>
        text = text.replace("\r\n", "<BR>")
        text = text.replace("\n", "<BR>")
        text = text.replace("\r", "<BR>")
        # æ›¿æ¢åˆ¶è¡¨ç¬¦ä¸º<TAB>
        text = text.replace("\t", "<TAB>")
        # æ›¿æ¢ç‰¹æ®Šç ´æŠ˜å·ä¸ºæ ‡å‡†ç ´æŠ˜å·
        text = text.replace("â€”", "ãƒ¼")
        text = text.replace("âˆ’", "ãƒ¼")
        # éå†emojiå­—å…¸ï¼Œå¦‚æœæ–‡æœ¬ä¸­å­˜åœ¨é”®å¯¹åº”çš„å†…å®¹ï¼Œåˆ™æ›¿æ¢ä¸ºå¯¹åº”çš„å€¼
        for k, v in self.emoji["emoji"].items():
            if k in text:
                text = text.replace(k, v)
        # å¦‚æœéœ€è¦æ¸…æ´—ï¼Œåˆ™è°ƒç”¨clean_textæ–¹æ³•è¿›è¡Œæ¸…æ´—
        if clean:
            text = self.clean_text(text)

        # å®šä¹‰æ£€æŸ¥ç‰¹æ®Šç¬¦å·çš„æ–¹æ³•
        def check_simbol(x):
            # å¦‚æœå­—ç¬¦é•¿åº¦ä¸º1ä¸”ç¼–ç é•¿åº¦ä¸º2ï¼Œåˆ™è¿›è¡Œç‰¹æ®Šç¬¦å·æ£€æŸ¥
            e = x.encode()
            if len(x) == 1 and len(e) == 2:
                c = (int(e[0]) << 8) + int(e[1])
                # è‹¥ç¬¦åˆç‰¹æ®Šç¬¦å·çš„ç¼–ç èŒƒå›´ï¼Œåˆ™è¿”å›True
                if (
                    (c >= 0xC2A1 and c <= 0xC2BF)
                    or (c >= 0xC780 and c <= 0xC783)
                    or (c >= 0xCAB9 and c <= 0xCBBF)
                    or (c >= 0xCC80 and c <= 0xCDA2)
                ):
                    return True
            return False

        # å®šä¹‰æ£€æŸ¥U+2000-U+2BFFç¼–ç èŒƒå›´çš„æ–¹æ³•
        def checku2e(x):
            # å¦‚æœå­—ç¬¦é•¿åº¦ä¸º1ä¸”ç¼–ç é•¿åº¦ä¸º3ï¼Œåˆ™è¿›è¡ŒU+2000-U+2BFFç¼–ç èŒƒå›´æ£€æŸ¥
            e = x.encode()
            if len(x) == 1 and len(e) == 3:
                c = (int(e[0]) << 16) + (int(e[1]) << 8) + int(e[2])
                # è‹¥ç¬¦åˆU+2000-U+2BFFç¼–ç èŒƒå›´ï¼Œåˆ™è¿”å›True
                if c >= 0xE28080 and c <= 0xE2B07F:
                    return True
            return False

        # åˆå§‹åŒ–ä½ç½®å˜é‡
        pos = 0
        result = []  # ç»“æœå­˜å‚¨åˆ—è¡¨
        # å¼€å§‹éå†æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†
        while pos < len(text):
            # è®¾å®šç»“æŸä½ç½®ï¼Œå¦‚æœå½“å‰å­—ç¬¦ä¸º"<"ï¼Œåˆ™ç»“æŸä½ç½®ä¸ºæœ€å¤§é•¿åº¦+1ï¼Œå¦åˆ™ä¸ºå½“å‰ä½ç½®åŠ 3
            end = min(len(text), pos + self.maxlen + 1) if text[pos] == "<" else pos + 3
            candidates = []  # å€™é€‰è¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º(token_id, token, pos)
            for e in range(end, pos, -1):
                wd = text[pos:e]
                # å¦‚æœè¯åœ¨vocabä¸­ï¼Œåˆ™åŠ å…¥å€™é€‰è¯åˆ—è¡¨
                if wd in self.vocab:
                    if wd[0] == "<" and len(wd) > 2:
                        candidates = [(self.vocab[wd], wd, e)]
                        break
                    else:
                        candidates.append((self.vocab[wd], wd, e))
            if len(candidates) > 0:
                # é€‰æ‹©å€™é€‰è¯ä¸­token_idæœ€å°çš„tokenï¼ŒåŠ å…¥ç»“æœåˆ—è¡¨ä¸­
                _, wd, e = sorted(candidates, key=lambda x: x[0])[0]
                result.append(wd)
                pos = e
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è¯ï¼Œåˆ™æˆªå–å½“å‰ä½ç½®åˆ°ç»“æŸä½ç½®çš„å­—ç¬¦ä½œä¸ºä¸€ä¸ªtoken
                end = pos + 1
                wd = text[pos:end]
                # æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®Šç¬¦å·ï¼Œè‹¥æ˜¯åˆ™æ·»åŠ "<KIGOU>"åˆ°ç»“æœåˆ—è¡¨ï¼Œè‹¥ä¸æ˜¯åˆ™æŒ‰utf-8ç¼–ç æ¯ä¸€ä¸ªå­—èŠ‚æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                if check_simbol(wd):
                    result.append("<KIGOU>")
                elif checku2e(wd):
                    result.append("<U2000U2BFF>")
                else:
                    for i in wd.encode("utf-8"):
                        result.append("<|byte%d|>" % i)
                pos = end
        return result  # è¿”å›åˆ†è¯åçš„ç»“æœåˆ—è¡¨

    # è½¬åŒ–token_idä¸ºtokenå¹¶è¿”å›
    def convert_id_to_token(self, index):
        return self.ids_to_tokens[index][0]
```