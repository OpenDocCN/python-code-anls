# `.\models\gptsan_japanese\tokenization_gptsan_japanese.py`

```
# ÊåáÂÆöÊñá‰ª∂ÁºñÁ†Å‰∏∫ UTF-8
# ÁâàÊùÉÂ£∞ÊòéÔºåÁâàÊùÉÂΩí HuggingFace Inc. Âõ¢ÈòüÊâÄÊúâ
#
# Ê†πÊçÆ Apache ËÆ∏ÂèØËØÅ 2.0 ÁâàÊú¨ÔºåÈô§ÈùûÁ¨¶ÂêàËÆ∏ÂèØËØÅË¶ÅÊ±ÇÔºåÂê¶Âàô‰∏çÂæó‰ΩøÁî®Ê≠§Êñá‰ª∂
# ÊÇ®ÂèØ‰ª•Âú®‰ª•‰∏ãÁΩëÂùÄËé∑ÂèñËÆ∏ÂèØËØÅÂâØÊú¨
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Èô§ÈùûÈÄÇÁî®Ê≥ïÂæãË¶ÅÊ±ÇÊàñ‰π¶Èù¢ÂêåÊÑèÔºåÂê¶ÂàôÊåâ"ÂéüÊ†∑"ÂàÜÂèëÁöÑËΩØ‰ª∂Ôºå
# Ê≤°Êúâ‰ªª‰ΩïÊòéÁ§∫ÊàñÊöóÁ§∫ÁöÑÊãÖ‰øùÊàñÊù°‰ª∂
# ËØ∑ÂèÇÈòÖËÆ∏ÂèØËØÅËé∑ÂèñÂÖ∑‰ΩìËØ≠Ë®ÄÁöÑÊùÉÈôêÊàñÈôêÂà∂
"""GPTSANJapanese ÁöÑÊ†áËÆ∞ÂåñÁ±ª"""
import collections  # ÂØºÂÖ•ÈõÜÂêàÊ®°ÂùóÔºåÁî®‰∫éÂ§ÑÁêÜÊúâÂ∫èÂ≠óÂÖ∏Á≠â
import json  # ÂØºÂÖ• JSON Ê®°ÂùóÔºåÁî®‰∫éÂ§ÑÁêÜ JSON Êï∞ÊçÆ
import os  # ÂØºÂÖ• OS Ê®°ÂùóÔºåÁî®‰∫éÂ§ÑÁêÜÊìç‰ΩúÁ≥ªÁªüÁõ∏ÂÖ≥ÂäüËÉΩ
import re  # ÂØºÂÖ•Ê≠£ÂàôË°®ËææÂºèÊ®°ÂùóÔºåÁî®‰∫éÂ≠óÁ¨¶‰∏≤ÂåπÈÖçÊìç‰Ωú
from typing import List, Optional, Tuple, Union  # ÂØºÂÖ•Á±ªÂûãÊèêÁ§∫Áõ∏ÂÖ≥Ê®°Âùó

import numpy as np  # ÂØºÂÖ• NumPy Ê®°ÂùóÔºåÁî®‰∫éÊï∞ÂÄºËÆ°ÁÆó

from ...tokenization_utils import PreTrainedTokenizer  # ÂØºÂÖ•È¢ÑËÆ≠ÁªÉÊ†áËÆ∞Âô®Á±ª
from ...tokenization_utils_base import (  # ÂØºÂÖ•Âü∫Á°ÄÊ†áËÆ∞ÂåñÁõ∏ÂÖ≥Ê®°Âùó
    BatchEncoding,
    PreTokenizedInput,
    PreTokenizedInputPair,
    TextInput,
    TextInputPair,
    TruncationStrategy,
)
from ...utils import PaddingStrategy, logging  # ÂØºÂÖ•Â°´ÂÖÖÁ≠ñÁï•ÂíåÊó•ÂøóÊ®°Âùó

logger = logging.get_logger(__name__)  # Ëé∑ÂèñÂΩìÂâçÊ®°ÂùóÁöÑÊó•ÂøóËÆ∞ÂΩïÂô®

VOCAB_FILES_NAMES = {"vocab_file": "vocab.txt", "emoji_file": "emoji.json"}  # ÂÆö‰πâËØçÊ±áÊñá‰ª∂ÂêçÂíåË°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂Âêç

PRETRAINED_VOCAB_FILES_MAP = {
    "vocab_file": {
        "Tanrei/GPTSAN-japanese": "https://huggingface.co/Tanrei/GPTSAN-japanese/blob/main/vocab.txt",
    },
    "emoji_file": {
        "Tanrei/GPTSAN-japanese": "https://huggingface.co/Tanrei/GPTSAN-japanese/blob/main/emoji.json",
    },
}  # È¢ÑËÆ≠ÁªÉËØçÊ±áÊñá‰ª∂Êò†Â∞ÑÔºåÊåáÂÆö GPTSAN-japanese Ê®°ÂûãÁöÑËØçÊ±áÂíåË°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂

PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
    "Tanrei/GPTSAN-japanese": 1280,
}  # È¢ÑËÆ≠ÁªÉ‰ΩçÁΩÆÂµåÂÖ•Â∞∫ÂØ∏Êò†Â∞ÑÔºåÊåáÂÆö GPTSAN-japanese Ê®°ÂûãÁöÑ‰ΩçÁΩÆÂµåÂÖ•Â∞∫ÂØ∏


def load_vocab_and_emoji(vocab_file, emoji_file):
    """Âä†ËΩΩËØçÊ±áÊñá‰ª∂ÂíåË°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂Âà∞Â≠óÂÖ∏‰∏≠„ÄÇ"""
    with open(emoji_file, "r", encoding="utf-8") as f:
        emoji = json.loads(f.read())  # ËØªÂèñÂπ∂Ëß£Êûê JSON Ê†ºÂºèÁöÑË°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂ÂÜÖÂÆπ

    vocab = collections.OrderedDict()  # ÂàõÂª∫ÊúâÂ∫èÂ≠óÂÖ∏Áî®‰∫éÂ≠òÂÇ®ËØçÊ±áË°®
    raw_vocab = collections.OrderedDict()  # ÂàõÂª∫ÊúâÂ∫èÂ≠óÂÖ∏Áî®‰∫éÂ≠òÂÇ®ÂéüÂßãËØçÊ±áË°®
    ids_to_tokens = collections.OrderedDict()  # ÂàõÂª∫ÊúâÂ∫èÂ≠óÂÖ∏Áî®‰∫éÂ≠òÂÇ®‰ªéÁ¥¢ÂºïÂà∞Ê†áËÆ∞ÁöÑÊò†Â∞ÑÂÖ≥Á≥ª
    with open(vocab_file, "r", encoding="utf-8") as f:
        token = f.readlines()  # ÈÄêË°åËØªÂèñËØçÊ±áÊñá‰ª∂ÂÜÖÂÆπ
    token = [[t.rstrip("\n")] if (t == ",\n" or "," not in t) else t.rstrip("\n").split(",") for t in token]  # ÂØπÊØèË°åËøõË°åÂ§ÑÁêÜÔºåÂ∞ÜÂÖ∂ÊãÜÂàÜ‰∏∫Ê†áËÆ∞ÂàóË°®
    for idx, b in enumerate(token):
        ids_to_tokens[idx] = b  # Â∞ÜÁ¥¢Âºï‰∏éÊ†áËÆ∞Êò†Â∞ÑÂÖ≥Á≥ªÂ≠òÂÖ•Â≠óÂÖ∏
        raw_vocab[",".join(b)] = idx  # Â∞ÜÊ†áËÆ∞ÂàóË°®ËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤‰Ωú‰∏∫ÈîÆÔºåÁ¥¢Âºï‰Ωú‰∏∫ÂÄºÂ≠òÂÖ•ÂéüÂßãËØçÊ±áË°®
        for wd in b:
            vocab[wd] = idx  # Â∞ÜÊ†áËÆ∞‰∏éÁ¥¢ÂºïÁöÑÊò†Â∞ÑÂÖ≥Á≥ªÂ≠òÂÖ•ËØçÊ±áË°®

    return vocab, raw_vocab, ids_to_tokens, emoji  # ËøîÂõûËØçÊ±áË°®„ÄÅÂéüÂßãËØçÊ±áË°®„ÄÅÁ¥¢ÂºïÂà∞Ê†áËÆ∞Êò†Â∞ÑÂíåË°®ÊÉÖÁ¨¶Âè∑Â≠óÂÖ∏


class GPTSanJapaneseTokenizer(PreTrainedTokenizer):
    """
    Êú¨Ê†áËÆ∞Âô®Âü∫‰∫é GPTNeoXJapaneseTokenizerÔºåÂπ∂ËøõË°å‰ª•‰∏ã‰øÆÊîπÔºö
    - Ê≠£Á°ÆËß£Á†ÅÂ≠óËäÇ0~255ÁöÑÊ†áËÆ∞
    - Ê∑ªÂä† bagofword Ê†áËÆ∞Â§ÑÁêÜ
    - ‰∏∫ Prefix-LM Ê®°ÂûãËøîÂõû token_type_ids
    bagofword Ê†áËÆ∞Ë°®Á§∫Ââç‰∏Ä‰∏™Ê†áËÆ∞ÁöÑÈáçÂ§çÔºåÂπ∂Âú®Ëß£Á†ÅÊó∂ËΩ¨Êç¢‰∏∫‰∏â‰∏™ËøûÁª≠ÁöÑÊ†áËÆ∞
    Ê≠§Â§ñÔºåÂéüÂßãÁöÑÊó•Êú¨ÁâπÊÆä Sub-Word-Encoding Â∑≤Âú®Ê≠§Â≠òÂÇ®Â∫ì‰∏≠ÂèëÂ∏É
    (https://github.com/tanreinama/Japanese-BPEEncoder_V2)„ÄÇtoken_type_ids ÊòØ‰∏Ä‰∏™ÊåáÁ§∫ÂâçÁºÄËæìÂÖ•ÁöÑÊé©Á†Å
    """
    pass  # GPTSanJapaneseTokenizer Á±ªÁõÆÂâçÊó†ÂÖ∑‰ΩìÂÆûÁé∞Ôºå‰ªÖÊúâÊñáÊ°£Â≠óÁ¨¶‰∏≤ËØ¥ÊòéÂÖ∂Âü∫Êú¨ÂäüËÉΩ
    >>> from transformers import GPTSanJapaneseTokenizer
    ÂºïÂÖ• GPTSanJapaneseTokenizer Á±ª‰ªé transformers Â∫ì
    
    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°Âûã "Tanrei/GPTSAN-japanese" ÂàùÂßãÂåñ‰∏Ä‰∏™ tokenizer ÂØπË±°
    
    >>> # You can confirm both ÊÖ∂Âøú and ÊÖ∂Êáâ are encoded to 17750
    # ‰ΩøÁî® tokenizer ÂØπÂ≠óÁ¨¶‰∏≤ËøõË°åÁºñÁ†ÅÔºåËøîÂõûËæìÂÖ•ÊñáÊú¨ÁöÑ token IDs ÂàóË°®
    >>> tokenizer("ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´")["input_ids"]
    [35993, 35998, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]
    
    >>> # Both ÊÖ∂Âøú and ÊÖ∂Êáâ are decoded to ÊÖ∂Âøú
    # ‰ΩøÁî® tokenizer ÂØπ token IDs ËøõË°åËß£Á†ÅÔºåËøîÂõûÂéüÂßãÊñáÊú¨
    >>> tokenizer.decode(tokenizer("ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´")["input_ids"])
    'ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Âøú)Â§ßÂ≠¶Âá∫Ë∫´'
    
    
    
    
    Example for Prefix-LM:
    
    >>> from transformers import GPTSanJapaneseTokenizer
    ÂºïÂÖ• GPTSanJapaneseTokenizer Á±ª‰ªé transformers Â∫ì
    
    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°Âûã "Tanrei/GPTSAN-japanese" ÂàùÂßãÂåñ‰∏Ä‰∏™ tokenizer ÂØπË±°
    
    >>> tokenizer("ÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´", prefix_text="ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇ")["input_ids"]
    # ‰ΩøÁî® tokenizer ÂØπÂ∏¶ÊúâÂâçÁºÄÊñáÊú¨ÁöÑÂ≠óÁ¨¶‰∏≤ËøõË°åÁºñÁ†ÅÔºåËøîÂõû token IDs ÂàóË°®
    [35993, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 35998, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]
    
    >>> # Mask for Prefix-LM inputs
    # ËøîÂõûÂ∏¶ÊúâÂâçÁºÄÊñáÊú¨ÁöÑËæìÂÖ•ÁöÑ token Á±ªÂûã IDs
    >>> tokenizer("ÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´", prefix_text="ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇ")["token_type_ids"]
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    
    
    
    
    Example for batch encode:
    
    >>> from transformers import GPTSanJapaneseTokenizer
    ÂºïÂÖ• GPTSanJapaneseTokenizer Á±ª‰ªé transformers Â∫ì
    
    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°Âûã "Tanrei/GPTSAN-japanese" ÂàùÂßãÂåñ‰∏Ä‰∏™ tokenizer ÂØπË±°
    
    >>> tokenizer([["Ê≠¶Áî∞‰ø°ÁéÑ", "„ÅØ„ÄÅ"], ["ÁπîÁî∞‰ø°Èï∑", "„ÅÆÈÖç‰∏ã„ÅÆ„ÄÅ"]], padding=True)["input_ids"]
    # ‰ΩøÁî® tokenizer ÂØπÊâπÈáèËæìÂÖ•ËøõË°åÁºñÁ†ÅÔºåËøîÂõûÂ°´ÂÖÖÂêéÁöÑ token IDs ÂàóË°®
    [[35993, 8640, 25948, 35998, 30647, 35675, 35999, 35999], [35993, 10382, 9868, 35998, 30646, 9459, 30646, 35675]]
    
    >>> # Mask for Prefix-LM inputs
    # ËøîÂõûÂ∏¶ÊúâÂâçÁºÄÊñáÊú¨ÁöÑÊâπÈáèËæìÂÖ•ÁöÑ token Á±ªÂûã IDs
    >>> tokenizer([["Ê≠¶Áî∞‰ø°ÁéÑ", "„ÅØ„ÄÅ"], ["ÁπîÁî∞‰ø°Èï∑", "„ÅÆÈÖç‰∏ã„ÅÆ„ÄÅ"]], padding=True)["token_type_ids"]
    [[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0]]
    
    >>> # Mask for padding
    # ËøîÂõûÂ°´ÂÖÖÂêéÁöÑÊâπÈáèËæìÂÖ•ÁöÑÊ≥®ÊÑèÂäõÊé©Á†Å
    >>> tokenizer([["Ê≠¶Áî∞‰ø°ÁéÑ", "„ÅØ„ÄÅ"], ["ÁπîÁî∞‰ø°Èï∑", "„ÅÆÈÖç‰∏ã„ÅÆ„ÄÅ"]], padding=True)["attention_mask"]
    [[1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]
    Args:
        vocab_file (`str`):
            File containing the vocabulary.
        emoji_file (`str`):
            File containing the emoji.
        unk_token (`str`, *optional*, defaults to `"<|nottoken|>"`):
            The token used for unknown characters.
        pad_token (`str`, *optional*, defaults to `"<|separator|>"`):
            The token used for padding.
        bos_token (`str`, *optional*, defaults to `"<|startoftext|>"`):
            The beginning of sequence token.
        eos_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The end of sequence token.
        sep_token (`str`, *optional*, defaults to `"<|segmenter|>"`):
            A special token to separate tokens into prefix and general input parts.
        do_clean_text (`bool`, *optional*, defaults to `False`):
            Whether or not to clean text for URLs, emails, telephone numbers, Japanese dates, and Japanese prices.
    """
    # Define constants for files related to vocabulary and model configurations
    vocab_files_names = VOCAB_FILES_NAMES
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
    model_input_names = ["input_ids", "attention_mask", "token_type_ids"]

    def __init__(
        self,
        vocab_file,
        emoji_file,
        unk_token="<|nottoken|>",
        pad_token="<|separator|>",
        bos_token="<|startoftext|>",
        eos_token="<|endoftext|>",
        sep_token="<|segmenter|>",
        do_clean_text=False,
        **kwargs,
    ):
        # Check if vocabulary file exists; raise an error if not found
        if not os.path.isfile(vocab_file):
            raise ValueError(
                f"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained"
                " model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
            )
        # Check if emoji file exists; raise an error if not found
        if not os.path.isfile(emoji_file):
            raise ValueError(
                f"Can't find an emoji file at path '{emoji_file}'. To load the emoji information from a Google"
                " pretrained model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
            )
        
        # Initialize the tokenizer with the provided parameters
        self.do_clean_text = do_clean_text
        self.vocab, self.raw_vocab, self.ids_to_tokens, self.emoji = load_vocab_and_emoji(vocab_file, emoji_file)
        self.subword_tokenizer = SubWordJapaneseTokenizer(
            vocab=self.vocab, ids_to_tokens=self.ids_to_tokens, emoji=self.emoji
        )

        # Initialize the superclass (TokenizerBase) with tokenizer specific parameters
        super().__init__(
            unk_token=unk_token,
            pad_token=pad_token,
            bos_token=bos_token,
            eos_token=eos_token,
            sep_token=sep_token,
            do_clean_text=do_clean_text,
            **kwargs,
        )

    @property
    # Property to get the size of the vocabulary
    # Copied from tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer.vocab_size
    def vocab_size(self):
        # The vocab_size property returns the length of the raw_vocab, which contains character variations unique to Japanese
        return len(self.raw_vocab)
    # ‰ªé raw_vocab Âíå added_tokens_encoder ÊûÑÂª∫Âπ∂ËøîÂõûËØçÊ±áË°®Â≠óÂÖ∏
    def get_vocab(self):
        return dict(self.raw_vocab, **self.added_tokens_encoder)

    # ‰ΩøÁî® subword_tokenizer ÂØπÊñáÊú¨ËøõË°åÂàÜËØçÂ§ÑÁêÜÂπ∂ËøîÂõûÁªìÊûú
    def _tokenize(self, text):
        return self.subword_tokenizer.tokenize(text, clean=self.do_clean_text)

    # Ê†πÊçÆ token Êü•ÊâæËØçÊ±áË°®‰∏≠ÁöÑÂØπÂ∫î idÔºåÂ¶ÇÊûúÊâæ‰∏çÂà∞ÂàôËøîÂõû unk_token ÁöÑ id
    def _convert_token_to_id(self, token):
        """Converts a token (str) in an id using the vocab."""
        return self.vocab.get(token, self.vocab.get(self.unk_token))

    # Ê†πÊçÆ id Êü•ÊâæËØçÊ±áË°®‰∏≠ÁöÑÂØπÂ∫î token
    def _convert_id_to_token(self, index):
        """Converts an index (integer) in a token (str) using the vocab."""
        return self.subword_tokenizer.convert_id_to_token(index)

    # Â∞Ü‰∏ÄÁ≥ªÂàó token ËΩ¨Êç¢‰∏∫Âçï‰∏™Â≠óÁ¨¶‰∏≤
    def convert_tokens_to_string(self, tokens):
        """Converts a sequence of tokens (string) in a single string."""
        words = []
        byte_tokens = []
        for word in tokens:
            if word[:6] == "<|byte" and word[-2:] == "|>":
                byte_tokens.append(int(word[6:-2]))
            else:
                if len(byte_tokens) > 0:
                    words.append(bytearray(byte_tokens).decode("utf-8", errors="replace"))
                    byte_tokens = []
                if word[:7] == "<|emoji" and word[-2:] == "|>":
                    words.append(self.emoji["emoji_inv"][word])
                elif word == "<SP>":
                    words.append(" ")
                elif word == "<BR>":
                    words.append("\n")
                elif word == "<TAB>":
                    words.append("\t")
                elif word == "<BLOCK>":
                    words.append("‚ñÄ")
                elif word == "<KIGOU>":
                    words.append("«Ä")
                elif word == "<U2000U2BFF>":
                    words.append("‚Äñ")
                elif word == "<|bagoftoken|>":
                    if len(words) > 0:
                        words.append(words[-1])
                        words.append(words[-1])
                        words.append(words[-1])
                elif word.startswith("<|") and word.endswith("|>"):
                    words.append("")
                else:
                    words.append(word)
        if len(byte_tokens) > 0:
            words.append(bytearray(byte_tokens).decode("utf-8", errors="replace"))
        text = "".join(words)
        return text
    # ÈªòËÆ§ÁöÑËÅäÂ§©Ê®°ÊùøÔºåÁî®‰∫éÂú®Ê∂àÊÅØ‰πãÈó¥Ê∑ªÂä†Ê†áÂáÜÁöÑBOS„ÄÅSEPÂíåEOSÊ†áËÆ∞ÔºåÂπ∂‰∏î‰∏çÂåÖÂê´ËßíËâ≤‰ø°ÊÅØ„ÄÇ
    def default_chat_template(self):
        """
        A simple chat template that adds standard BOS, SEP and EOS tokens between messages while discarding role
        information.
        """
        # Â¶ÇÊûúÊú™‰∏∫Ê≠§ÂàÜËØçÂô®ÂÆö‰πâËÅäÂ§©Ê®°ÊùøÔºåÂàôË≠¶ÂëäÂπ∂‰ΩøÁî®ÈªòËÆ§Ê®°Êùø
        logger.warning_once(
            "\nNo chat template is defined for this tokenizer - using the default template "
            f"for the {self.__class__.__name__} class. If the default is not appropriate for "
            "your model, please set `tokenizer.chat_template` to an appropriate template. "
            "See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n"
        )
        # ËøîÂõûÊ†ºÂºèÂåñÂêéÁöÑËÅäÂ§©Ê®°ÊùøÂ≠óÁ¨¶‰∏≤
        return (
            "{% for message in messages %}"
            "{% if not loop.first %}{{ bos_token}}{% endif %}"
            "{{ sep_token }}{{ message.content }} {{ eos_token }}"
            "{% endfor %}"
        )

    # ‰ªé GPTNeoXJapaneseTokenizer.save_vocabulary Â§çÂà∂ËÄåÊù•
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:
        # ÂàùÂßãÂåñÁ¥¢Âºï
        index = 0
        # Ê£ÄÊü•‰øùÂ≠òÁõÆÂΩïÊòØÂê¶Â≠òÂú®
        if os.path.isdir(save_directory):
            # ÊûÑÂª∫ËØçÊ±áË°®Êñá‰ª∂Ë∑ØÂæÑÂíåË°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂Ë∑ØÂæÑ
            vocab_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["vocab_file"]
            )
            emoji_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["emoji_file"]
            )
        else:
            # ÊûÑÂª∫ËØçÊ±áË°®Êñá‰ª∂Ë∑ØÂæÑÂíåË°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂Ë∑ØÂæÑÔºà‰∏çÊòØÁõÆÂΩïÔºâ
            vocab_file = (
                (filename_prefix + "-" if filename_prefix else "") + save_directory + VOCAB_FILES_NAMES["vocab_file"]
            )
            emoji_file = (
                (filename_prefix + "-" if filename_prefix else "") + save_directory + VOCAB_FILES_NAMES["emoji_file"]
            )
        # ÂÜôÂÖ•ËØçÊ±áË°®Êñá‰ª∂
        with open(vocab_file, "w", encoding="utf-8") as writer:
            # ÈÅçÂéÜËØçÊ±áË°®Êò†Â∞ÑÔºåÂ∞ÜÁ¥¢ÂºïÂíåÂØπÂ∫îÁöÑ token ÂÜôÂÖ•Êñá‰ª∂
            for token_index, token in self.ids_to_tokens.items():
                if index != token_index:
                    # Ëã•ËØçÊ±áË°®Á¥¢Âºï‰∏çËøûÁª≠ÔºåÂèëÂá∫Ë≠¶Âëä
                    logger.warning(
                        f"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive."
                        " Please check that the vocabulary is not corrupted!"
                    )
                    index = token_index
                # Â∞Ü token ÂÜôÂÖ•Êñá‰ª∂ÔºåÊØè‰∏™ token Áî®ÈÄóÂè∑ÂàÜÈöî
                writer.write(",".join(token) + "\n")
                index += 1
        # ÂÜôÂÖ•Ë°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂
        with open(emoji_file, "w", encoding="utf-8") as writer:
            json.dump(self.emoji, writer)
        # ËøîÂõûËØçÊ±áË°®Êñá‰ª∂ÂíåË°®ÊÉÖÁ¨¶Âè∑Êñá‰ª∂ÁöÑË∑ØÂæÑ
        return vocab_file, emoji_file

    # ÂàõÂª∫ token_type_ids ‰ªé token_ids_0 Âíå token_ids_1 ‰∏≠
    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -> List[int]:
        # docstyle-ignore
        """
        The tokenizer returns token_type_ids as separators between the Prefix part and the rest.
        token_type_ids is 1 for the Prefix part and 0 for the rest of the token.

        Example:
        ```python
        >>> from transformers import GPTSanJapaneseTokenizer

        >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
        >>> x_token = tokenizer("ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥")
        >>> # input_ids:      | SOT | SEG | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ |
        >>> # token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |

        >>> x_token = tokenizer("", prefix_text="ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥")
        >>> # input_ids:      | SOT | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ | SEG |
        >>> # token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |

        >>> x_token = tokenizer("ÔΩ≥ÔΩ¥", prefix_text="ÔΩ±ÔΩ≤")
        >>> # input_ids:      | SOT | ÔΩ± | ÔΩ≤ | SEG | ÔΩ≥ | ÔΩ¥ |
        >>> # token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |
        ```"""
        # ËÆ°ÁÆóÂâçÁºÄÈïøÂ∫¶ÁöÑÂàùÂßãÂÄº‰∏∫ 0
        prefix_len = 0
        # Ê£ÄÊü•ÂàÜÈöîÁ¨¶Âú®ËØçÊ±áË°®‰∏≠Â≠òÂú®
        if self.sep_token in self.vocab:
            # Ëé∑ÂèñÂàÜÈöîÁ¨¶Âú®ËØçÊ±áË°®‰∏≠ÁöÑÁ¥¢Âºï
            segid = self.vocab[self.sep_token]
            # Â¶ÇÊûú token_ids_0 ‰∏≠Â≠òÂú®ÂàÜÈöîÁ¨¶ÁöÑÁ¥¢Âºï
            if segid in token_ids_0:
                # ËÆ°ÁÆóÂâçÁºÄÈïøÂ∫¶‰∏∫ÂàÜÈöîÁ¨¶Á¥¢Âºï‰πãÂâçÁöÑÈïøÂ∫¶
                prefix_len = token_ids_0.index(segid)
        # Â¶ÇÊûú token_ids_1 ‰∏∫ NoneÔºåÂàôÊÄªÈïøÂ∫¶‰∏∫ token_ids_0 ÁöÑÈïøÂ∫¶
        if token_ids_1 is None:
            total_len = len(token_ids_0)
        else:
            # Âê¶ÂàôÊÄªÈïøÂ∫¶‰∏∫ token_ids_0 Âíå token_ids_1 ÁöÑÈïøÂ∫¶‰πãÂíå
            total_len = len(token_ids_0 + token_ids_1)
        # ËøîÂõûÂâçÁºÄÈïøÂ∫¶Êï∞ÈáèÁöÑ 1ÔºåÂêéÈù¢Ë°•ÂÖÖ (ÊÄªÈïøÂ∫¶ - ÂâçÁºÄÈïøÂ∫¶) ‰∏™ 0 ÁªÑÊàêÁöÑÂàóË°®
        return prefix_len * [1] + (total_len - prefix_len) * [0]

    def prepare_for_tokenization(self, text, prefix_text=None, add_sep_token=None, **kwargs):
        # GPTSAN Âú® Prefix-LM ‰∏≠Èô§‰∫ÜÂú®ÊñáÊú¨ÁîüÊàê‰∏≠ÊèíÂÖ•ÁöÑ SOTÔºåËøòÈ¢ùÂ§ñÊèíÂÖ• SEP Ê†áËÆ∞„ÄÇ
        # ÊñáÊú¨ÂºÄÂ§¥ÁöÑ SOTÔºå‰ª•ÂèäÂú®ÂâçÁºÄÈÉ®ÂàÜÂíåÂÖ∂‰ΩôÈÉ®ÂàÜ‰πãÈó¥ÁöÑ SEP Ê†áËÆ∞„ÄÇ
        if add_sep_token is None:
            # Â¶ÇÊûúÊú™ÊòéÁ°ÆÂú®ÈùûÂâçÁºÄ‰ΩçÁΩÆÊèíÂÖ• SEP Ê†áËÆ∞
            add_sep_token = self.sep_token not in text
        # ÂáÜÂ§á tokenization ÁöÑÊñáÊú¨ÔºåÂàùÂßã‰∏∫Á©∫Â≠óÁ¨¶‰∏≤ÊàñËÄÖ‰ª• BOS Ê†áËÆ∞ÂºÄÂ§¥ÁöÑÂ≠óÁ¨¶‰∏≤
        prepared = self.bos_token if self.bos_token in self.vocab else ""
        # Â¶ÇÊûúÊúâÂâçÁºÄÊñáÊú¨ÔºåÂàôÂ∞ÜÂÖ∂Ê∑ªÂä†Âà∞ÂáÜÂ§áÁöÑÊñáÊú¨‰∏≠
        prepared += prefix_text if prefix_text is not None else ""
        # Â¶ÇÊûúÈúÄË¶ÅÊ∑ªÂä† SEP Ê†áËÆ∞ÔºåÂàôÂ∞ÜÂÖ∂Ê∑ªÂä†Âà∞ÂáÜÂ§áÁöÑÊñáÊú¨‰∏≠
        if add_sep_token:
            prepared += self.sep_token if self.sep_token in self.vocab else ""
        # Â∞ÜÂéüÂßãÊñáÊú¨Ê∑ªÂä†Âà∞ÂáÜÂ§áÁöÑÊñáÊú¨‰∏≠
        prepared += text
        # ËøîÂõûÂåÖÂê´ÂáÜÂ§áÂ•ΩÁöÑÊñáÊú¨ÂíåÂÖ∂‰ªñÂÖ≥ÈîÆÂ≠óÂèÇÊï∞ÁöÑÂÖÉÁªÑ
        return (prepared, kwargs)
    # ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÊñπÊ≥ï `_batch_encode_plus`ÔºåÁî®‰∫éÊâπÈáèÁºñÁ†ÅÊñáÊú¨ÊàñÊñáÊú¨ÂØπ
    def _batch_encode_plus(
        self,
        batch_text_or_text_pairs: Union[
            List[TextInput], List[TextInputPair], List[PreTokenizedInput], List[PreTokenizedInputPair]
        ],
        add_special_tokens: bool = True,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        max_length: Optional[int] = None,
        stride: int = 0,
        is_split_into_words: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[str] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
    ) -> BatchEncoding:
        # Ê≠§Ê†áËÆ∞Âô®Â∞ÜËæìÂÖ•ÊñáÊú¨ÂØπËΩ¨Êç¢‰∏∫ÂâçÁºÄËæìÂÖ•ÂíåÂêéÁª≠ËæìÂÖ•
        if isinstance(batch_text_or_text_pairs[0], tuple) or isinstance(tuple(batch_text_or_text_pairs[0]), list):
            # Â¶ÇÊûúËæìÂÖ•ÊòØÊñáÊú¨ÂØπÊàñÊñáÊú¨ÂØπÂàóË°®ÔºåÂàôÂ§ÑÁêÜÊàêÂâçÁºÄÂä†ÂàÜÈöîÁ¨¶ÂêéÁöÑÂçï‰∏ÄÊñáÊú¨ÂàóË°®
            batch_prefix_texts = []
            for pref, txt in batch_text_or_text_pairs:
                batch_prefix_texts.append(pref + self.sep_token + txt)
            batch_text_or_text_pairs = batch_prefix_texts

        # Ë∞ÉÁî®Áà∂Á±ªÁöÑ `_batch_encode_plus` ÊñπÊ≥ïÔºå‰º†ÈÄíÊâÄÊúâÂèÇÊï∞ÔºåÂπ∂ËøîÂõûÁªìÊûú
        return super()._batch_encode_plus(
            batch_text_or_text_pairs,
            add_special_tokens,
            padding_strategy,
            truncation_strategy,
            max_length,
            stride,
            is_split_into_words,
            pad_to_multiple_of,
            return_tensors,
            return_token_type_ids,
            return_attention_mask,
            return_overflowing_tokens,
            return_special_tokens_mask,
            return_offsets_mapping,
            return_length,
            verbose,
        )
# ÂÆö‰πâ SubWordJapaneseTokenizer Á±ªÔºåÁî®‰∫éÊó•ËØ≠ÂàÜËØçÔºåÂü∫‰∫é GPTNeoXJapaneseTokenizer Âπ∂ËøõË°å‰∫Ü‰ª•‰∏ã‰øÆÊîπ
class SubWordJapaneseTokenizer(object):
    """
    This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications
    - Decoding byte0~byte255 tokens correctly
    - Added bagofword token handling

    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT Lisence according to the
    original repository.

    MIT License

    Copyright (c) 2020 tanreinama

    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
    documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
    permit persons to whom the Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all copies or substantial portions of
    the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
    THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
    TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE.
    """

    # ‰ªé tokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.__init__ Â§çÂà∂ËÄåÊù•
    def __init__(self, vocab, ids_to_tokens, emoji):
        self.vocab = vocab  # ÂàùÂßãÂåñËØçÊ±áË°®Â±ûÊÄßÔºå‰∏éÂèÇÊï∞sweÁõ∏Âêå
        self.ids_to_tokens = ids_to_tokens  # ÂàùÂßãÂåñ ID Âà∞ËØçÊ±áÊò†Â∞ÑÂ±ûÊÄßÔºå‰∏éÂèÇÊï∞bpeÁõ∏Âêå
        self.emoji = emoji  # ÂàùÂßãÂåñË°®ÊÉÖÁ¨¶Âè∑Â±ûÊÄß
        self.maxlen = np.max([len(w) for w in self.vocab.keys()])  # ËÆ°ÁÆóËØçÊ±áË°®‰∏≠ÊúÄÈïøËØçÁöÑÈïøÂ∫¶Âπ∂ËµãÂÄºÁªômaxlen
        # ÂàùÂßãÂåñÁî®‰∫éÂåπÈÖçÊñáÊú¨‰∏≠ÂêÑÁßçÊ®°ÂºèÁöÑÊ≠£ÂàôË°®ËææÂºè
        self.content_repatter1 = re.compile(r"(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)")
        self.content_repatter2 = re.compile(r"[A-Za-z0-9\._+]*@[\\-_0-9A-Za-z]+(\.[A-Za-z]+)*")
        self.content_repatter3 = re.compile(r"[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}")
        self.content_repatter4 = re.compile(
            r"([12]\d{3}[/\-Âπ¥])*(0?[1-9]|1[0-2])[/\-Êúà]((0?[1-9]|[12][0-9]|3[01])Êó•?)*(\d{1,2}|:|\d{1,2}ÊôÇ|\d{1,2}ÂàÜ|\(Êó•\)|\(Êúà\)|\(ÁÅ´\)|\(Ê∞¥\)|\(Êú®\)|\(Èáë\)|\(Âúü\)|„à∞|„à™|„à´|„à¨|„à≠|„àÆ|„àØ)*"
        )
        self.content_repatter5 = re.compile(
            r"(ÊòéÊ≤ª|Â§ßÊ≠£|Êò≠Âíå|Âπ≥Êàê|‰ª§Âíå|„çæ|„çΩ|„çº|„çª|\u32ff)\d{1,2}Âπ¥(0?[1-9]|1[0-2])Êúà(0?[1-9]|[12][0-9]|3[01])Êó•(\d{1,2}|:|\d{1,2}ÊôÇ|\d{1,2}ÂàÜ|\(Êó•\)|\(Êúà\)|\(ÁÅ´\)|\(Ê∞¥\)|\(Êú®\)|\(Èáë\)|\(Âúü\)|„à∞|„à™|„à´|„à¨|„à≠|„àÆ|„àØ)*"
        )
        self.content_repatter6 = re.compile(
            r"((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*ÂÑÑ)*((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*‰∏á)*((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*ÂçÉ)*(0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*(ÂçÉÂÜÜ|‰∏áÂÜÜ|ÂçÉ‰∏áÂÜÜ|ÂÜÜ|ÂçÉ„Éâ„É´|‰∏á„Éâ„É´|ÂçÉ‰∏á„Éâ„É´|„Éâ„É´|ÂçÉ„É¶„Éº„É≠|‰∏á„É¶„Éº„É≠|ÂçÉ‰∏á„É¶„Éº„É≠|„É¶„Éº„É≠)+(\(Á®éËæº\)|\(Á®éÊäú\)|\+tax)*"
        )
        keisen = "‚îÄ‚îÅ‚îÇ‚îÉ‚îÑ‚îÖ‚îÜ‚îá‚îà‚îâ‚îä‚îã‚îå‚îç‚îé‚îè‚îê‚îë‚îí‚îì‚îî‚îï‚îñ‚îó‚îò‚îô‚îö‚îõ‚îú‚îù‚îû‚îü‚î†‚î°‚î¢‚î£‚î§‚î•‚î¶‚îß‚î®‚î©‚î™‚î´‚î¨‚î≠‚îÆ‚îØ‚î∞‚î±‚î≤‚î≥‚î¥‚îµ‚î∂‚î∑‚î∏‚îπ‚î∫‚îª‚îº‚îΩ‚îæ‚îø‚ïÄ‚ïÅ‚ïÇ‚ïÉ‚ïÑ‚ïÖ‚ïÜ‚ïá‚ïà‚ïâ‚ïä‚ïã‚ïå‚ïç‚ïé‚ïè‚ïê‚ïë‚ïí‚ïì‚ïî‚ïï‚ïñ‚ïó‚ïò‚ïô‚ïö‚ïõ‚ïú‚ïù‚ïû‚ïü‚ï†‚ï°‚ï¢‚ï£‚ï§‚ï•‚ï¶‚ïß‚ï®‚ï©‚ï™‚ï´‚ï¨‚ï≠‚ïÆ‚ïØ‚ï∞‚ï±‚ï≤‚ï≥‚ï¥‚ïµ‚ï∂‚ï∑‚ï∏‚ïπ‚ï∫‚ïª‚ïº‚ïΩ‚ïæ‚ïø"
        blocks = "‚ñÄ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñâ‚ñä‚ñã‚ñå‚ñç‚ñé‚ñè‚ñê‚ñë‚ñí‚ñì‚ñî‚ñï‚ññ‚ñó‚ñò‚ñô‚ñö‚ñõ‚ñú‚ñù‚ñû‚ñü"
        self.content_trans1 = str.maketrans({k: "<BLOCK>" for k in keisen + blocks})  # ÂàõÂª∫Â≠óÁ¨¶ÊõøÊç¢Êò†Â∞ÑË°®

    # ‰ªétokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.__len__‰∏≠Â§çÂà∂ËÄåÊù•
    def __len__(self):
        return len(self.ids_to_tokens)  # ËøîÂõûids_to_tokensÁöÑÈïøÂ∫¶‰Ωú‰∏∫ÂØπË±°ÁöÑÈïøÂ∫¶

    # ‰ªétokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.clean_text‰∏≠Â§çÂà∂ËÄåÊù•
    def clean_text(self, content):
        content = self.content_repatter1.sub("<URL>", content)  # Â∞ÜÊñáÊú¨‰∏≠ÁöÑURLÊõøÊç¢‰∏∫"<URL>"
        content = self.content_repatter2.sub("<EMAIL>", content)  # Â∞ÜÊñáÊú¨‰∏≠ÁöÑÈÇÆÁÆ±Âú∞ÂùÄÊõøÊç¢‰∏∫"<EMAIL>"
        content = self.content_repatter3.sub("<TEL>", content)  # Â∞ÜÊñáÊú¨‰∏≠ÁöÑÁîµËØùÂè∑Á†ÅÊõøÊç¢‰∏∫"<TEL>"
        content = self.content_repatter4.sub("<DATE>", content)  # Â∞ÜÊñáÊú¨‰∏≠ÁöÑÊó•ÊúüÊõøÊç¢‰∏∫"<DATE>"
        content = self.content_repatter5.sub("<DATE>", content)  # Â∞ÜÊñáÊú¨‰∏≠ÁöÑÊó•ÊúüÊõøÊç¢‰∏∫"<DATE>"
        content = self.content_repatter6.sub("<PRICE>", content)  # Â∞ÜÊñáÊú¨‰∏≠ÁöÑ‰ª∑Ê†ºÊõøÊç¢‰∏∫"<PRICE>"
        content = content.translate(self.content_trans1)  # ‰ΩøÁî®content_trans1ËøõË°åÊñáÊú¨ÁöÑÂ≠óÁ¨¶ÊõøÊç¢
        while "<BLOCK><BLOCK>" in content:
            content = content.replace("<BLOCK><BLOCK>", "<BLOCK>")  # Â∞ÜËøûÁª≠ÁöÑ"<BLOCK><BLOCK>"ÊõøÊç¢‰∏∫Âçï‰∏™"<BLOCK>"
        return content

    # ‰ªétokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.tokenize‰∏≠Â§çÂà∂ËÄåÊù•
    # Â∞ÜÊñáÊú¨‰∏≠ÁöÑÁ©∫Ê†ºÊõøÊç¢‰∏∫"<SP>"
    text = text.replace(" ", "<SP>")
    # Â∞ÜÂÖ®ËßíÁ©∫Ê†ºÊõøÊç¢‰∏∫"<SP>"
    text = text.replace("„ÄÄ", "<SP>")
    # Â∞Ü Windows Êç¢Ë°åÁ¨¶"\r\n"ÊõøÊç¢‰∏∫"<BR>"
    text = text.replace("\r\n", "<BR>")
    # Â∞ÜÊôÆÈÄöÊç¢Ë°åÁ¨¶"\n"ÊõøÊç¢‰∏∫"<BR>"
    text = text.replace("\n", "<BR>")
    # Â∞ÜËÄÅÂºè Mac Êç¢Ë°åÁ¨¶"\r"ÊõøÊç¢‰∏∫"<BR>"
    text = text.replace("\r", "<BR>")
    # Â∞ÜÂà∂Ë°®Á¨¶"\t"ÊõøÊç¢‰∏∫"<TAB>"
    text = text.replace("\t", "<TAB>")
    # Â∞Ü"‚Äî"ÊõøÊç¢‰∏∫"„Éº"
    text = text.replace("‚Äî", "„Éº")
    # Â∞Ü"‚àí"ÊõøÊç¢‰∏∫"„Éº"
    text = text.replace("‚àí", "„Éº")
    
    # ÈÅçÂéÜË°®ÊÉÖÂ≠óÂÖ∏‰∏≠ÁöÑÊØè‰∏™ÈîÆÂÄºÂØπÔºåÂ¶ÇÊûúÊñáÊú¨‰∏≠ÂåÖÂê´Êüê‰∏™ÈîÆÔºåÂàôÁî®ÂØπÂ∫îÁöÑÂÄºÊõøÊç¢ÊñáÊú¨‰∏≠ÁöÑÈîÆ
    for k, v in self.emoji["emoji"].items():
        if k in text:
            text = text.replace(k, v)
    
    # Â¶ÇÊûú clean ÂèÇÊï∞‰∏∫ TrueÔºåÂàôÂØπÊñáÊú¨ËøõË°åÊ∏ÖÊ¥óÂ§ÑÁêÜ
    if clean:
        text = self.clean_text(text)

    # ÂÆö‰πâÊ£ÄÊü•Âçï‰∏™Â≠óÁ¨¶ÊòØÂê¶‰∏∫ÁâπÂÆöÁ¨¶Âè∑ÁöÑÂáΩÊï∞
    def check_simbol(x):
        e = x.encode()
        # Ê£ÄÊü•Â≠óÁ¨¶ÈïøÂ∫¶‰∏∫1‰∏îÁºñÁ†ÅÈïøÂ∫¶‰∏∫2ÁöÑÊÉÖÂÜµ
        if len(x) == 1 and len(e) == 2:
            c = (int(e[0]) << 8) + int(e[1])
            # Ê£ÄÊü•ÊòØÂê¶Á¨¶ÂêàÁâπÂÆöËåÉÂõ¥ÂÜÖÁöÑÂ≠óÁ¨¶ÁºñÁ†Å
            if (
                (c >= 0xC2A1 and c <= 0xC2BF)
                or (c >= 0xC780 and c <= 0xC783)
                or (c >= 0xCAB9 and c <= 0xCBBF)
                or (c >= 0xCC80 and c <= 0xCDA2)
            ):
                return True
        return False

    # ÂÆö‰πâÊ£ÄÊü•Âçï‰∏™Â≠óÁ¨¶ÊòØÂê¶‰∏∫ Unicode Ë°®ÊÑèÊñáÂ≠óÊâ©Â±ïÂå∫ÂüüÁöÑÂáΩÊï∞
    def checku2e(x):
        e = x.encode()
        # Ê£ÄÊü•Â≠óÁ¨¶ÈïøÂ∫¶‰∏∫1‰∏îÁºñÁ†ÅÈïøÂ∫¶‰∏∫3ÁöÑÊÉÖÂÜµ
        if len(x) == 1 and len(e) == 3:
            c = (int(e[0]) << 16) + (int(e[1]) << 8) + int(e[2])
            # Ê£ÄÊü•ÊòØÂê¶Á¨¶ÂêàÁâπÂÆöËåÉÂõ¥ÂÜÖÁöÑÂ≠óÁ¨¶ÁºñÁ†Å
            if c >= 0xE28080 and c <= 0xE2B07F:
                return True
        return False

    # ÂàùÂßãÂåñ‰ΩçÁΩÆÂèòÈáè‰∏∫0
    pos = 0
    # ÂàùÂßãÂåñÁªìÊûúÂàóË°®
    result = []
    # ÂΩì‰ΩçÁΩÆÂ∞è‰∫éÊñáÊú¨ÈïøÂ∫¶Êó∂Âæ™ÁéØÂ§ÑÁêÜÊñáÊú¨
    while pos < len(text):
        # Â¶ÇÊûúÂΩìÂâçÂ≠óÁ¨¶ÊòØ"<"ÔºåÂàôÁªìÊùü‰ΩçÁΩÆ‰∏∫ÂΩìÂâç‰ΩçÁΩÆÂä†‰∏äÊúÄÂ§ßÈïøÂ∫¶Âä†1ÔºõÂê¶ÂàôÁªìÊùü‰ΩçÁΩÆ‰∏∫ÂΩìÂâç‰ΩçÁΩÆÂä†3
        end = min(len(text), pos + self.maxlen + 1) if text[pos] == "<" else pos + 3
        # ÂÄôÈÄâËØçÂàóË°®ÂàùÂßãÂåñ‰∏∫Á©∫
        candidates = []  # (token_id, token, pos)
        # ‰ªéÁªìÊùü‰ΩçÁΩÆÂêëÂΩìÂâç‰ΩçÁΩÆÈÅçÂéÜ
        for e in range(end, pos, -1):
            # Ëé∑ÂèñÂΩìÂâç‰ΩçÁΩÆÂà∞ÁªìÊùü‰ΩçÁΩÆÁöÑÂ≠ê‰∏≤
            wd = text[pos:e]
            # Â¶ÇÊûúËØ•Â≠ê‰∏≤Âú®ËØçÊ±áË°®‰∏≠Â≠òÂú®
            if wd in self.vocab:
                # Â¶ÇÊûúÂ≠ê‰∏≤‰ª•"<"ÂºÄÂ§¥‰∏îÈïøÂ∫¶Â§ß‰∫é2ÔºåÂàôÂ∞ÜÂÖ∂‰Ωú‰∏∫‰∏Ä‰∏™ÂÄôÈÄâÈ°πÂä†ÂÖ•ÂàóË°®
                if wd[0] == "<" and len(wd) > 2:
                    candidates = [(self.vocab[wd], wd, e)]
                    break
                else:
                    candidates.append((self.vocab[wd], wd, e))
        # Â¶ÇÊûúÂÄôÈÄâËØçÂàóË°®‰∏ç‰∏∫Á©∫
        if len(candidates) > 0:
            # Ê†πÊçÆ token_id ÊúÄÂ∞èÁöÑÂéüÂàôÈÄâÂèñÂÄôÈÄâÈ°π‰∏≠ÁöÑ‰∏Ä‰∏™ËøõË°åÂ§ÑÁêÜ
            _, wd, e = sorted(candidates, key=lambda x: x[0])[0]
            # Â∞ÜÈÄâÂèñÁöÑËØçÊ∑ªÂä†Âà∞ÁªìÊûúÂàóË°®‰∏≠
            result.append(wd)
            # Êõ¥Êñ∞‰ΩçÁΩÆ‰∏∫ e
            pos = e
        else:
            # Â¶ÇÊûúÂÄôÈÄâËØçÂàóË°®‰∏∫Á©∫ÔºåÂàôÂ§ÑÁêÜÂΩìÂâç‰ΩçÁΩÆÂà∞ÁªìÊùü‰ΩçÁΩÆÁöÑÂ≠ê‰∏≤
            end = pos + 1
            wd = text[pos:end]
            # Â¶ÇÊûúÂ≠ê‰∏≤‰∏∫ÁâπÂÆöÁ¨¶Âè∑ÔºåÂàôÂ∞Ü"<KIGOU>"Âä†ÂÖ•ÁªìÊûúÂàóË°®
            if check_simbol(wd):
                result.append("<KIGOU>")
            # Â¶ÇÊûúÂ≠ê‰∏≤‰∏∫ Unicode Ë°®ÊÑèÊñáÂ≠óÊâ©Â±ïÂå∫ÂüüÁöÑÂ≠óÁ¨¶ÔºåÂàôÂ∞Ü"<U2000U2BFF>"Âä†ÂÖ•ÁªìÊûúÂàóË°®
            elif checku2e(wd):
                result.append("<U2000U2BFF>")
            else:
                # Âê¶ÂàôÂ∞ÜÂ≠ê‰∏≤‰∏≠ÁöÑÊØè‰∏™Â≠óËäÇÊåâÁÖßÊ†ºÂºè"<|byte%d|>"Ê∑ªÂä†Âà∞ÁªìÊûúÂàóË°®‰∏≠
                for i in wd.encode("utf-8"):
                    result.append("<|byte%d|>" % i)
            # Êõ¥Êñ∞‰ΩçÁΩÆ‰∏∫ end
            pos = end
    
    # ËøîÂõûÂ§ÑÁêÜÂêéÁöÑÁªìÊûúÂàóË°®
    return result
```