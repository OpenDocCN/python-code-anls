# `.\models\gptsan_japanese\tokenization_gptsan_japanese.py`

```
# æŒ‡å®šæ–‡ä»¶ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜ï¼Œç‰ˆæƒå½’ HuggingFace Inc. å›¢é˜Ÿæ‰€æœ‰
#
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼Œé™¤éç¬¦åˆè®¸å¯è¯è¦æ±‚ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æŒ‰"åŸæ ·"åˆ†å‘çš„è½¯ä»¶ï¼Œ
# æ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶
# è¯·å‚é˜…è®¸å¯è¯è·å–å…·ä½“è¯­è¨€çš„æƒé™æˆ–é™åˆ¶
"""GPTSANJapanese çš„æ ‡è®°åŒ–ç±»"""
import collections  # å¯¼å…¥é›†åˆæ¨¡å—ï¼Œç”¨äºå¤„ç†æœ‰åºå­—å…¸ç­‰
import json  # å¯¼å…¥ JSON æ¨¡å—ï¼Œç”¨äºå¤„ç† JSON æ•°æ®
import os  # å¯¼å…¥ OS æ¨¡å—ï¼Œç”¨äºå¤„ç†æ“ä½œç³»ç»Ÿç›¸å…³åŠŸèƒ½
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—ï¼Œç”¨äºå­—ç¬¦ä¸²åŒ¹é…æ“ä½œ
from typing import List, Optional, Tuple, Union  # å¯¼å…¥ç±»å‹æç¤ºç›¸å…³æ¨¡å—

import numpy as np  # å¯¼å…¥ NumPy æ¨¡å—ï¼Œç”¨äºæ•°å€¼è®¡ç®—

from ...tokenization_utils import PreTrainedTokenizer  # å¯¼å…¥é¢„è®­ç»ƒæ ‡è®°å™¨ç±»
from ...tokenization_utils_base import (  # å¯¼å…¥åŸºç¡€æ ‡è®°åŒ–ç›¸å…³æ¨¡å—
    BatchEncoding,
    PreTokenizedInput,
    PreTokenizedInputPair,
    TextInput,
    TextInputPair,
    TruncationStrategy,
)
from ...utils import PaddingStrategy, logging  # å¯¼å…¥å¡«å……ç­–ç•¥å’Œæ—¥å¿—æ¨¡å—

logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨

VOCAB_FILES_NAMES = {"vocab_file": "vocab.txt", "emoji_file": "emoji.json"}  # å®šä¹‰è¯æ±‡æ–‡ä»¶åå’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶å

PRETRAINED_VOCAB_FILES_MAP = {
    "vocab_file": {
        "Tanrei/GPTSAN-japanese": "https://huggingface.co/Tanrei/GPTSAN-japanese/blob/main/vocab.txt",
    },
    "emoji_file": {
        "Tanrei/GPTSAN-japanese": "https://huggingface.co/Tanrei/GPTSAN-japanese/blob/main/emoji.json",
    },
}  # é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶æ˜ å°„ï¼ŒæŒ‡å®š GPTSAN-japanese æ¨¡å‹çš„è¯æ±‡å’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶

PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
    "Tanrei/GPTSAN-japanese": 1280,
}  # é¢„è®­ç»ƒä½ç½®åµŒå…¥å°ºå¯¸æ˜ å°„ï¼ŒæŒ‡å®š GPTSAN-japanese æ¨¡å‹çš„ä½ç½®åµŒå…¥å°ºå¯¸


def load_vocab_and_emoji(vocab_file, emoji_file):
    """åŠ è½½è¯æ±‡æ–‡ä»¶å’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶åˆ°å­—å…¸ä¸­ã€‚"""
    with open(emoji_file, "r", encoding="utf-8") as f:
        emoji = json.loads(f.read())  # è¯»å–å¹¶è§£æ JSON æ ¼å¼çš„è¡¨æƒ…ç¬¦å·æ–‡ä»¶å†…å®¹

    vocab = collections.OrderedDict()  # åˆ›å»ºæœ‰åºå­—å…¸ç”¨äºå­˜å‚¨è¯æ±‡è¡¨
    raw_vocab = collections.OrderedDict()  # åˆ›å»ºæœ‰åºå­—å…¸ç”¨äºå­˜å‚¨åŸå§‹è¯æ±‡è¡¨
    ids_to_tokens = collections.OrderedDict()  # åˆ›å»ºæœ‰åºå­—å…¸ç”¨äºå­˜å‚¨ä»ç´¢å¼•åˆ°æ ‡è®°çš„æ˜ å°„å…³ç³»
    with open(vocab_file, "r", encoding="utf-8") as f:
        token = f.readlines()  # é€è¡Œè¯»å–è¯æ±‡æ–‡ä»¶å†…å®¹
    token = [[t.rstrip("\n")] if (t == ",\n" or "," not in t) else t.rstrip("\n").split(",") for t in token]  # å¯¹æ¯è¡Œè¿›è¡Œå¤„ç†ï¼Œå°†å…¶æ‹†åˆ†ä¸ºæ ‡è®°åˆ—è¡¨
    for idx, b in enumerate(token):
        ids_to_tokens[idx] = b  # å°†ç´¢å¼•ä¸æ ‡è®°æ˜ å°„å…³ç³»å­˜å…¥å­—å…¸
        raw_vocab[",".join(b)] = idx  # å°†æ ‡è®°åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä½œä¸ºé”®ï¼Œç´¢å¼•ä½œä¸ºå€¼å­˜å…¥åŸå§‹è¯æ±‡è¡¨
        for wd in b:
            vocab[wd] = idx  # å°†æ ‡è®°ä¸ç´¢å¼•çš„æ˜ å°„å…³ç³»å­˜å…¥è¯æ±‡è¡¨

    return vocab, raw_vocab, ids_to_tokens, emoji  # è¿”å›è¯æ±‡è¡¨ã€åŸå§‹è¯æ±‡è¡¨ã€ç´¢å¼•åˆ°æ ‡è®°æ˜ å°„å’Œè¡¨æƒ…ç¬¦å·å­—å…¸


class GPTSanJapaneseTokenizer(PreTrainedTokenizer):
    """
    æœ¬æ ‡è®°å™¨åŸºäº GPTNeoXJapaneseTokenizerï¼Œå¹¶è¿›è¡Œä»¥ä¸‹ä¿®æ”¹ï¼š
    - æ­£ç¡®è§£ç å­—èŠ‚0~255çš„æ ‡è®°
    - æ·»åŠ  bagofword æ ‡è®°å¤„ç†
    - ä¸º Prefix-LM æ¨¡å‹è¿”å› token_type_ids
    bagofword æ ‡è®°è¡¨ç¤ºå‰ä¸€ä¸ªæ ‡è®°çš„é‡å¤ï¼Œå¹¶åœ¨è§£ç æ—¶è½¬æ¢ä¸ºä¸‰ä¸ªè¿ç»­çš„æ ‡è®°
    æ­¤å¤–ï¼ŒåŸå§‹çš„æ—¥æœ¬ç‰¹æ®Š Sub-Word-Encoding å·²åœ¨æ­¤å­˜å‚¨åº“ä¸­å‘å¸ƒ
    (https://github.com/tanreinama/Japanese-BPEEncoder_V2)ã€‚token_type_ids æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‰ç¼€è¾“å…¥çš„æ©ç 
    """
    pass  # GPTSanJapaneseTokenizer ç±»ç›®å‰æ— å…·ä½“å®ç°ï¼Œä»…æœ‰æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜å…¶åŸºæœ¬åŠŸèƒ½
    >>> from transformers import GPTSanJapaneseTokenizer
    å¼•å…¥ GPTSanJapaneseTokenizer ç±»ä» transformers åº“
    
    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ "Tanrei/GPTSAN-japanese" åˆå§‹åŒ–ä¸€ä¸ª tokenizer å¯¹è±¡
    
    >>> # You can confirm both æ…¶å¿œ and æ…¶æ‡‰ are encoded to 17750
    # ä½¿ç”¨ tokenizer å¯¹å­—ç¬¦ä¸²è¿›è¡Œç¼–ç ï¼Œè¿”å›è¾“å…¥æ–‡æœ¬çš„ token IDs åˆ—è¡¨
    >>> tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"]
    [35993, 35998, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]
    
    >>> # Both æ…¶å¿œ and æ…¶æ‡‰ are decoded to æ…¶å¿œ
    # ä½¿ç”¨ tokenizer å¯¹ token IDs è¿›è¡Œè§£ç ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
    >>> tokenizer.decode(tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"])
    'å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶å¿œ)å¤§å­¦å‡ºèº«'
    
    
    
    
    Example for Prefix-LM:
    
    >>> from transformers import GPTSanJapaneseTokenizer
    å¼•å…¥ GPTSanJapaneseTokenizer ç±»ä» transformers åº“
    
    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ "Tanrei/GPTSAN-japanese" åˆå§‹åŒ–ä¸€ä¸ª tokenizer å¯¹è±¡
    
    >>> tokenizer("å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«", prefix_text="å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚")["input_ids"]
    # ä½¿ç”¨ tokenizer å¯¹å¸¦æœ‰å‰ç¼€æ–‡æœ¬çš„å­—ç¬¦ä¸²è¿›è¡Œç¼–ç ï¼Œè¿”å› token IDs åˆ—è¡¨
    [35993, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 35998, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]
    
    >>> # Mask for Prefix-LM inputs
    # è¿”å›å¸¦æœ‰å‰ç¼€æ–‡æœ¬çš„è¾“å…¥çš„ token ç±»å‹ IDs
    >>> tokenizer("å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«", prefix_text="å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚")["token_type_ids"]
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    
    
    
    
    Example for batch encode:
    
    >>> from transformers import GPTSanJapaneseTokenizer
    å¼•å…¥ GPTSanJapaneseTokenizer ç±»ä» transformers åº“
    
    >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
    ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ "Tanrei/GPTSAN-japanese" åˆå§‹åŒ–ä¸€ä¸ª tokenizer å¯¹è±¡
    
    >>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["input_ids"]
    # ä½¿ç”¨ tokenizer å¯¹æ‰¹é‡è¾“å…¥è¿›è¡Œç¼–ç ï¼Œè¿”å›å¡«å……åçš„ token IDs åˆ—è¡¨
    [[35993, 8640, 25948, 35998, 30647, 35675, 35999, 35999], [35993, 10382, 9868, 35998, 30646, 9459, 30646, 35675]]
    
    >>> # Mask for Prefix-LM inputs
    # è¿”å›å¸¦æœ‰å‰ç¼€æ–‡æœ¬çš„æ‰¹é‡è¾“å…¥çš„ token ç±»å‹ IDs
    >>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["token_type_ids"]
    [[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0]]
    
    >>> # Mask for padding
    # è¿”å›å¡«å……åçš„æ‰¹é‡è¾“å…¥çš„æ³¨æ„åŠ›æ©ç 
    >>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["attention_mask"]
    [[1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]
    Args:
        vocab_file (`str`):
            File containing the vocabulary.
        emoji_file (`str`):
            File containing the emoji.
        unk_token (`str`, *optional*, defaults to `"<|nottoken|>"`):
            The token used for unknown characters.
        pad_token (`str`, *optional*, defaults to `"<|separator|>"`):
            The token used for padding.
        bos_token (`str`, *optional*, defaults to `"<|startoftext|>"`):
            The beginning of sequence token.
        eos_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The end of sequence token.
        sep_token (`str`, *optional*, defaults to `"<|segmenter|>"`):
            A special token to separate tokens into prefix and general input parts.
        do_clean_text (`bool`, *optional*, defaults to `False`):
            Whether or not to clean text for URLs, emails, telephone numbers, Japanese dates, and Japanese prices.
    """
    # Define constants for files related to vocabulary and model configurations
    vocab_files_names = VOCAB_FILES_NAMES
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
    model_input_names = ["input_ids", "attention_mask", "token_type_ids"]

    def __init__(
        self,
        vocab_file,
        emoji_file,
        unk_token="<|nottoken|>",
        pad_token="<|separator|>",
        bos_token="<|startoftext|>",
        eos_token="<|endoftext|>",
        sep_token="<|segmenter|>",
        do_clean_text=False,
        **kwargs,
    ):
        # Check if vocabulary file exists; raise an error if not found
        if not os.path.isfile(vocab_file):
            raise ValueError(
                f"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained"
                " model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
            )
        # Check if emoji file exists; raise an error if not found
        if not os.path.isfile(emoji_file):
            raise ValueError(
                f"Can't find an emoji file at path '{emoji_file}'. To load the emoji information from a Google"
                " pretrained model use `tokenizer = GPTSanJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
            )
        
        # Initialize the tokenizer with the provided parameters
        self.do_clean_text = do_clean_text
        self.vocab, self.raw_vocab, self.ids_to_tokens, self.emoji = load_vocab_and_emoji(vocab_file, emoji_file)
        self.subword_tokenizer = SubWordJapaneseTokenizer(
            vocab=self.vocab, ids_to_tokens=self.ids_to_tokens, emoji=self.emoji
        )

        # Initialize the superclass (TokenizerBase) with tokenizer specific parameters
        super().__init__(
            unk_token=unk_token,
            pad_token=pad_token,
            bos_token=bos_token,
            eos_token=eos_token,
            sep_token=sep_token,
            do_clean_text=do_clean_text,
            **kwargs,
        )

    @property
    # Property to get the size of the vocabulary
    # Copied from tokenization_gpt_neox_japanese.GPTNeoXJapaneseTokenizer.vocab_size
    def vocab_size(self):
        # The vocab_size property returns the length of the raw_vocab, which contains character variations unique to Japanese
        return len(self.raw_vocab)
    # ä» raw_vocab å’Œ added_tokens_encoder æ„å»ºå¹¶è¿”å›è¯æ±‡è¡¨å­—å…¸
    def get_vocab(self):
        return dict(self.raw_vocab, **self.added_tokens_encoder)

    # ä½¿ç”¨ subword_tokenizer å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†å¹¶è¿”å›ç»“æœ
    def _tokenize(self, text):
        return self.subword_tokenizer.tokenize(text, clean=self.do_clean_text)

    # æ ¹æ® token æŸ¥æ‰¾è¯æ±‡è¡¨ä¸­çš„å¯¹åº” idï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› unk_token çš„ id
    def _convert_token_to_id(self, token):
        """Converts a token (str) in an id using the vocab."""
        return self.vocab.get(token, self.vocab.get(self.unk_token))

    # æ ¹æ® id æŸ¥æ‰¾è¯æ±‡è¡¨ä¸­çš„å¯¹åº” token
    def _convert_id_to_token(self, index):
        """Converts an index (integer) in a token (str) using the vocab."""
        return self.subword_tokenizer.convert_id_to_token(index)

    # å°†ä¸€ç³»åˆ— token è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ä¸²
    def convert_tokens_to_string(self, tokens):
        """Converts a sequence of tokens (string) in a single string."""
        words = []
        byte_tokens = []
        for word in tokens:
            if word[:6] == "<|byte" and word[-2:] == "|>":
                byte_tokens.append(int(word[6:-2]))
            else:
                if len(byte_tokens) > 0:
                    words.append(bytearray(byte_tokens).decode("utf-8", errors="replace"))
                    byte_tokens = []
                if word[:7] == "<|emoji" and word[-2:] == "|>":
                    words.append(self.emoji["emoji_inv"][word])
                elif word == "<SP>":
                    words.append(" ")
                elif word == "<BR>":
                    words.append("\n")
                elif word == "<TAB>":
                    words.append("\t")
                elif word == "<BLOCK>":
                    words.append("â–€")
                elif word == "<KIGOU>":
                    words.append("Ç€")
                elif word == "<U2000U2BFF>":
                    words.append("â€–")
                elif word == "<|bagoftoken|>":
                    if len(words) > 0:
                        words.append(words[-1])
                        words.append(words[-1])
                        words.append(words[-1])
                elif word.startswith("<|") and word.endswith("|>"):
                    words.append("")
                else:
                    words.append(word)
        if len(byte_tokens) > 0:
            words.append(bytearray(byte_tokens).decode("utf-8", errors="replace"))
        text = "".join(words)
        return text
    # é»˜è®¤çš„èŠå¤©æ¨¡æ¿ï¼Œç”¨äºåœ¨æ¶ˆæ¯ä¹‹é—´æ·»åŠ æ ‡å‡†çš„BOSã€SEPå’ŒEOSæ ‡è®°ï¼Œå¹¶ä¸”ä¸åŒ…å«è§’è‰²ä¿¡æ¯ã€‚
    def default_chat_template(self):
        """
        A simple chat template that adds standard BOS, SEP and EOS tokens between messages while discarding role
        information.
        """
        # å¦‚æœæœªä¸ºæ­¤åˆ†è¯å™¨å®šä¹‰èŠå¤©æ¨¡æ¿ï¼Œåˆ™è­¦å‘Šå¹¶ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        logger.warning_once(
            "\nNo chat template is defined for this tokenizer - using the default template "
            f"for the {self.__class__.__name__} class. If the default is not appropriate for "
            "your model, please set `tokenizer.chat_template` to an appropriate template. "
            "See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n"
        )
        # è¿”å›æ ¼å¼åŒ–åçš„èŠå¤©æ¨¡æ¿å­—ç¬¦ä¸²
        return (
            "{% for message in messages %}"
            "{% if not loop.first %}{{ bos_token}}{% endif %}"
            "{{ sep_token }}{{ message.content }} {{ eos_token }}"
            "{% endfor %}"
        )

    # ä» GPTNeoXJapaneseTokenizer.save_vocabulary å¤åˆ¶è€Œæ¥
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:
        # åˆå§‹åŒ–ç´¢å¼•
        index = 0
        # æ£€æŸ¥ä¿å­˜ç›®å½•æ˜¯å¦å­˜åœ¨
        if os.path.isdir(save_directory):
            # æ„å»ºè¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„å’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶è·¯å¾„
            vocab_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["vocab_file"]
            )
            emoji_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["emoji_file"]
            )
        else:
            # æ„å»ºè¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„å’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶è·¯å¾„ï¼ˆä¸æ˜¯ç›®å½•ï¼‰
            vocab_file = (
                (filename_prefix + "-" if filename_prefix else "") + save_directory + VOCAB_FILES_NAMES["vocab_file"]
            )
            emoji_file = (
                (filename_prefix + "-" if filename_prefix else "") + save_directory + VOCAB_FILES_NAMES["emoji_file"]
            )
        # å†™å…¥è¯æ±‡è¡¨æ–‡ä»¶
        with open(vocab_file, "w", encoding="utf-8") as writer:
            # éå†è¯æ±‡è¡¨æ˜ å°„ï¼Œå°†ç´¢å¼•å’Œå¯¹åº”çš„ token å†™å…¥æ–‡ä»¶
            for token_index, token in self.ids_to_tokens.items():
                if index != token_index:
                    # è‹¥è¯æ±‡è¡¨ç´¢å¼•ä¸è¿ç»­ï¼Œå‘å‡ºè­¦å‘Š
                    logger.warning(
                        f"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive."
                        " Please check that the vocabulary is not corrupted!"
                    )
                    index = token_index
                # å°† token å†™å…¥æ–‡ä»¶ï¼Œæ¯ä¸ª token ç”¨é€—å·åˆ†éš”
                writer.write(",".join(token) + "\n")
                index += 1
        # å†™å…¥è¡¨æƒ…ç¬¦å·æ–‡ä»¶
        with open(emoji_file, "w", encoding="utf-8") as writer:
            json.dump(self.emoji, writer)
        # è¿”å›è¯æ±‡è¡¨æ–‡ä»¶å’Œè¡¨æƒ…ç¬¦å·æ–‡ä»¶çš„è·¯å¾„
        return vocab_file, emoji_file

    # åˆ›å»º token_type_ids ä» token_ids_0 å’Œ token_ids_1 ä¸­
    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -> List[int]:
        # docstyle-ignore
        """
        The tokenizer returns token_type_ids as separators between the Prefix part and the rest.
        token_type_ids is 1 for the Prefix part and 0 for the rest of the token.

        Example:
        ```python
        >>> from transformers import GPTSanJapaneseTokenizer

        >>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
        >>> x_token = tokenizer("ï½±ï½²ï½³ï½´")
        >>> # input_ids:      | SOT | SEG | ï½± | ï½² | ï½³ | ï½´ |
        >>> # token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |

        >>> x_token = tokenizer("", prefix_text="ï½±ï½²ï½³ï½´")
        >>> # input_ids:      | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG |
        >>> # token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |

        >>> x_token = tokenizer("ï½³ï½´", prefix_text="ï½±ï½²")
        >>> # input_ids:      | SOT | ï½± | ï½² | SEG | ï½³ | ï½´ |
        >>> # token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |
        ```"""
        # è®¡ç®—å‰ç¼€é•¿åº¦çš„åˆå§‹å€¼ä¸º 0
        prefix_len = 0
        # æ£€æŸ¥åˆ†éš”ç¬¦åœ¨è¯æ±‡è¡¨ä¸­å­˜åœ¨
        if self.sep_token in self.vocab:
            # è·å–åˆ†éš”ç¬¦åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•
            segid = self.vocab[self.sep_token]
            # å¦‚æœ token_ids_0 ä¸­å­˜åœ¨åˆ†éš”ç¬¦çš„ç´¢å¼•
            if segid in token_ids_0:
                # è®¡ç®—å‰ç¼€é•¿åº¦ä¸ºåˆ†éš”ç¬¦ç´¢å¼•ä¹‹å‰çš„é•¿åº¦
                prefix_len = token_ids_0.index(segid)
        # å¦‚æœ token_ids_1 ä¸º Noneï¼Œåˆ™æ€»é•¿åº¦ä¸º token_ids_0 çš„é•¿åº¦
        if token_ids_1 is None:
            total_len = len(token_ids_0)
        else:
            # å¦åˆ™æ€»é•¿åº¦ä¸º token_ids_0 å’Œ token_ids_1 çš„é•¿åº¦ä¹‹å’Œ
            total_len = len(token_ids_0 + token_ids_1)
        # è¿”å›å‰ç¼€é•¿åº¦æ•°é‡çš„ 1ï¼Œåé¢è¡¥å…… (æ€»é•¿åº¦ - å‰ç¼€é•¿åº¦) ä¸ª 0 ç»„æˆçš„åˆ—è¡¨
        return prefix_len * [1] + (total_len - prefix_len) * [0]

    def prepare_for_tokenization(self, text, prefix_text=None, add_sep_token=None, **kwargs):
        # GPTSAN åœ¨ Prefix-LM ä¸­é™¤äº†åœ¨æ–‡æœ¬ç”Ÿæˆä¸­æ’å…¥çš„ SOTï¼Œè¿˜é¢å¤–æ’å…¥ SEP æ ‡è®°ã€‚
        # æ–‡æœ¬å¼€å¤´çš„ SOTï¼Œä»¥åŠåœ¨å‰ç¼€éƒ¨åˆ†å’Œå…¶ä½™éƒ¨åˆ†ä¹‹é—´çš„ SEP æ ‡è®°ã€‚
        if add_sep_token is None:
            # å¦‚æœæœªæ˜ç¡®åœ¨éå‰ç¼€ä½ç½®æ’å…¥ SEP æ ‡è®°
            add_sep_token = self.sep_token not in text
        # å‡†å¤‡ tokenization çš„æ–‡æœ¬ï¼Œåˆå§‹ä¸ºç©ºå­—ç¬¦ä¸²æˆ–è€…ä»¥ BOS æ ‡è®°å¼€å¤´çš„å­—ç¬¦ä¸²
        prepared = self.bos_token if self.bos_token in self.vocab else ""
        # å¦‚æœæœ‰å‰ç¼€æ–‡æœ¬ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°å‡†å¤‡çš„æ–‡æœ¬ä¸­
        prepared += prefix_text if prefix_text is not None else ""
        # å¦‚æœéœ€è¦æ·»åŠ  SEP æ ‡è®°ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°å‡†å¤‡çš„æ–‡æœ¬ä¸­
        if add_sep_token:
            prepared += self.sep_token if self.sep_token in self.vocab else ""
        # å°†åŸå§‹æ–‡æœ¬æ·»åŠ åˆ°å‡†å¤‡çš„æ–‡æœ¬ä¸­
        prepared += text
        # è¿”å›åŒ…å«å‡†å¤‡å¥½çš„æ–‡æœ¬å’Œå…¶ä»–å…³é”®å­—å‚æ•°çš„å…ƒç»„
        return (prepared, kwargs)
    # å®šä¹‰äº†ä¸€ä¸ªæ–¹æ³• `_batch_encode_plus`ï¼Œç”¨äºæ‰¹é‡ç¼–ç æ–‡æœ¬æˆ–æ–‡æœ¬å¯¹
    def _batch_encode_plus(
        self,
        batch_text_or_text_pairs: Union[
            List[TextInput], List[TextInputPair], List[PreTokenizedInput], List[PreTokenizedInputPair]
        ],
        add_special_tokens: bool = True,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        max_length: Optional[int] = None,
        stride: int = 0,
        is_split_into_words: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[str] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
    ) -> BatchEncoding:
        # æ­¤æ ‡è®°å™¨å°†è¾“å…¥æ–‡æœ¬å¯¹è½¬æ¢ä¸ºå‰ç¼€è¾“å…¥å’Œåç»­è¾“å…¥
        if isinstance(batch_text_or_text_pairs[0], tuple) or isinstance(tuple(batch_text_or_text_pairs[0]), list):
            # å¦‚æœè¾“å…¥æ˜¯æ–‡æœ¬å¯¹æˆ–æ–‡æœ¬å¯¹åˆ—è¡¨ï¼Œåˆ™å¤„ç†æˆå‰ç¼€åŠ åˆ†éš”ç¬¦åçš„å•ä¸€æ–‡æœ¬åˆ—è¡¨
            batch_prefix_texts = []
            for pref, txt in batch_text_or_text_pairs:
                batch_prefix_texts.append(pref + self.sep_token + txt)
            batch_text_or_text_pairs = batch_prefix_texts

        # è°ƒç”¨çˆ¶ç±»çš„ `_batch_encode_plus` æ–¹æ³•ï¼Œä¼ é€’æ‰€æœ‰å‚æ•°ï¼Œå¹¶è¿”å›ç»“æœ
        return super()._batch_encode_plus(
            batch_text_or_text_pairs,
            add_special_tokens,
            padding_strategy,
            truncation_strategy,
            max_length,
            stride,
            is_split_into_words,
            pad_to_multiple_of,
            return_tensors,
            return_token_type_ids,
            return_attention_mask,
            return_overflowing_tokens,
            return_special_tokens_mask,
            return_offsets_mapping,
            return_length,
            verbose,
        )
# å®šä¹‰ SubWordJapaneseTokenizer ç±»ï¼Œç”¨äºæ—¥è¯­åˆ†è¯ï¼ŒåŸºäº GPTNeoXJapaneseTokenizer å¹¶è¿›è¡Œäº†ä»¥ä¸‹ä¿®æ”¹
class SubWordJapaneseTokenizer(object):
    """
    This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications
    - Decoding byte0~byte255 tokens correctly
    - Added bagofword token handling

    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT Lisence according to the
    original repository.

    MIT License

    Copyright (c) 2020 tanreinama

    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
    documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
    permit persons to whom the Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all copies or substantial portions of
    the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
    THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
    TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE.
    """

    # ä» tokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.__init__ å¤åˆ¶è€Œæ¥
    def __init__(self, vocab, ids_to_tokens, emoji):
        self.vocab = vocab  # åˆå§‹åŒ–è¯æ±‡è¡¨å±æ€§ï¼Œä¸å‚æ•°sweç›¸åŒ
        self.ids_to_tokens = ids_to_tokens  # åˆå§‹åŒ– ID åˆ°è¯æ±‡æ˜ å°„å±æ€§ï¼Œä¸å‚æ•°bpeç›¸åŒ
        self.emoji = emoji  # åˆå§‹åŒ–è¡¨æƒ…ç¬¦å·å±æ€§
        self.maxlen = np.max([len(w) for w in self.vocab.keys()])  # è®¡ç®—è¯æ±‡è¡¨ä¸­æœ€é•¿è¯çš„é•¿åº¦å¹¶èµ‹å€¼ç»™maxlen
        # åˆå§‹åŒ–ç”¨äºåŒ¹é…æ–‡æœ¬ä¸­å„ç§æ¨¡å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        self.content_repatter1 = re.compile(r"(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)")
        self.content_repatter2 = re.compile(r"[A-Za-z0-9\._+]*@[\\-_0-9A-Za-z]+(\.[A-Za-z]+)*")
        self.content_repatter3 = re.compile(r"[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}")
        self.content_repatter4 = re.compile(
            r"([12]\d{3}[/\-å¹´])*(0?[1-9]|1[0-2])[/\-æœˆ]((0?[1-9]|[12][0-9]|3[01])æ—¥?)*(\d{1,2}|:|\d{1,2}æ™‚|\d{1,2}åˆ†|\(æ—¥\)|\(æœˆ\)|\(ç«\)|\(æ°´\)|\(æœ¨\)|\(é‡‘\)|\(åœŸ\)|ãˆ°|ãˆª|ãˆ«|ãˆ¬|ãˆ­|ãˆ®|ãˆ¯)*"
        )
        self.content_repatter5 = re.compile(
            r"(æ˜æ²»|å¤§æ­£|æ˜­å’Œ|å¹³æˆ|ä»¤å’Œ|ã¾|ã½|ã¼|ã»|\u32ff)\d{1,2}å¹´(0?[1-9]|1[0-2])æœˆ(0?[1-9]|[12][0-9]|3[01])æ—¥(\d{1,2}|:|\d{1,2}æ™‚|\d{1,2}åˆ†|\(æ—¥\)|\(æœˆ\)|\(ç«\)|\(æ°´\)|\(æœ¨\)|\(é‡‘\)|\(åœŸ\)|ãˆ°|ãˆª|ãˆ«|ãˆ¬|ãˆ­|ãˆ®|ãˆ¯)*"
        )
        self.content_repatter6 = re.compile(
            r"((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*å„„)*((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*ä¸‡)*((0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*åƒ)*(0|[1-9]\d*|[1-9]\d{0,2}(,\d{3})+)*(åƒå††|ä¸‡å††|åƒä¸‡å††|å††|åƒãƒ‰ãƒ«|ä¸‡ãƒ‰ãƒ«|åƒä¸‡ãƒ‰ãƒ«|ãƒ‰ãƒ«|åƒãƒ¦ãƒ¼ãƒ­|ä¸‡ãƒ¦ãƒ¼ãƒ­|åƒä¸‡ãƒ¦ãƒ¼ãƒ­|ãƒ¦ãƒ¼ãƒ­)+(\(ç¨è¾¼\)|\(ç¨æŠœ\)|\+tax)*"
        )
        keisen = "â”€â”â”‚â”ƒâ”„â”…â”†â”‡â”ˆâ”‰â”Šâ”‹â”Œâ”â”â”â”â”‘â”’â”“â””â”•â”–â”—â”˜â”™â”šâ”›â”œâ”â”â”Ÿâ” â”¡â”¢â”£â”¤â”¥â”¦â”§â”¨â”©â”ªâ”«â”¬â”­â”®â”¯â”°â”±â”²â”³â”´â”µâ”¶â”·â”¸â”¹â”ºâ”»â”¼â”½â”¾â”¿â•€â•â•‚â•ƒâ•„â•…â•†â•‡â•ˆâ•‰â•Šâ•‹â•Œâ•â•â•â•â•‘â•’â•“â•”â••â•–â•—â•˜â•™â•šâ•›â•œâ•â•â•Ÿâ• â•¡â•¢â•£â•¤â•¥â•¦â•§â•¨â•©â•ªâ•«â•¬â•­â•®â•¯â•°â•±â•²â•³â•´â•µâ•¶â•·â•¸â•¹â•ºâ•»â•¼â•½â•¾â•¿"
        blocks = "â–€â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆâ–‰â–Šâ–‹â–Œâ–â–â–â–â–‘â–’â–“â–”â–•â––â–—â–˜â–™â–šâ–›â–œâ–â–â–Ÿ"
        self.content_trans1 = str.maketrans({k: "<BLOCK>" for k in keisen + blocks})  # åˆ›å»ºå­—ç¬¦æ›¿æ¢æ˜ å°„è¡¨

    # ä»tokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.__len__ä¸­å¤åˆ¶è€Œæ¥
    def __len__(self):
        return len(self.ids_to_tokens)  # è¿”å›ids_to_tokensçš„é•¿åº¦ä½œä¸ºå¯¹è±¡çš„é•¿åº¦

    # ä»tokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.clean_textä¸­å¤åˆ¶è€Œæ¥
    def clean_text(self, content):
        content = self.content_repatter1.sub("<URL>", content)  # å°†æ–‡æœ¬ä¸­çš„URLæ›¿æ¢ä¸º"<URL>"
        content = self.content_repatter2.sub("<EMAIL>", content)  # å°†æ–‡æœ¬ä¸­çš„é‚®ç®±åœ°å€æ›¿æ¢ä¸º"<EMAIL>"
        content = self.content_repatter3.sub("<TEL>", content)  # å°†æ–‡æœ¬ä¸­çš„ç”µè¯å·ç æ›¿æ¢ä¸º"<TEL>"
        content = self.content_repatter4.sub("<DATE>", content)  # å°†æ–‡æœ¬ä¸­çš„æ—¥æœŸæ›¿æ¢ä¸º"<DATE>"
        content = self.content_repatter5.sub("<DATE>", content)  # å°†æ–‡æœ¬ä¸­çš„æ—¥æœŸæ›¿æ¢ä¸º"<DATE>"
        content = self.content_repatter6.sub("<PRICE>", content)  # å°†æ–‡æœ¬ä¸­çš„ä»·æ ¼æ›¿æ¢ä¸º"<PRICE>"
        content = content.translate(self.content_trans1)  # ä½¿ç”¨content_trans1è¿›è¡Œæ–‡æœ¬çš„å­—ç¬¦æ›¿æ¢
        while "<BLOCK><BLOCK>" in content:
            content = content.replace("<BLOCK><BLOCK>", "<BLOCK>")  # å°†è¿ç»­çš„"<BLOCK><BLOCK>"æ›¿æ¢ä¸ºå•ä¸ª"<BLOCK>"
        return content

    # ä»tokenization_gpt_neox_japanese.SubWordJapaneseTokenizer.tokenizeä¸­å¤åˆ¶è€Œæ¥
    # å°†æ–‡æœ¬ä¸­çš„ç©ºæ ¼æ›¿æ¢ä¸º"<SP>"
    text = text.replace(" ", "<SP>")
    # å°†å…¨è§’ç©ºæ ¼æ›¿æ¢ä¸º"<SP>"
    text = text.replace("ã€€", "<SP>")
    # å°† Windows æ¢è¡Œç¬¦"\r\n"æ›¿æ¢ä¸º"<BR>"
    text = text.replace("\r\n", "<BR>")
    # å°†æ™®é€šæ¢è¡Œç¬¦"\n"æ›¿æ¢ä¸º"<BR>"
    text = text.replace("\n", "<BR>")
    # å°†è€å¼ Mac æ¢è¡Œç¬¦"\r"æ›¿æ¢ä¸º"<BR>"
    text = text.replace("\r", "<BR>")
    # å°†åˆ¶è¡¨ç¬¦"\t"æ›¿æ¢ä¸º"<TAB>"
    text = text.replace("\t", "<TAB>")
    # å°†"â€”"æ›¿æ¢ä¸º"ãƒ¼"
    text = text.replace("â€”", "ãƒ¼")
    # å°†"âˆ’"æ›¿æ¢ä¸º"ãƒ¼"
    text = text.replace("âˆ’", "ãƒ¼")
    
    # éå†è¡¨æƒ…å­—å…¸ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹ï¼Œå¦‚æœæ–‡æœ¬ä¸­åŒ…å«æŸä¸ªé”®ï¼Œåˆ™ç”¨å¯¹åº”çš„å€¼æ›¿æ¢æ–‡æœ¬ä¸­çš„é”®
    for k, v in self.emoji["emoji"].items():
        if k in text:
            text = text.replace(k, v)
    
    # å¦‚æœ clean å‚æ•°ä¸º Trueï¼Œåˆ™å¯¹æ–‡æœ¬è¿›è¡Œæ¸…æ´—å¤„ç†
    if clean:
        text = self.clean_text(text)

    # å®šä¹‰æ£€æŸ¥å•ä¸ªå­—ç¬¦æ˜¯å¦ä¸ºç‰¹å®šç¬¦å·çš„å‡½æ•°
    def check_simbol(x):
        e = x.encode()
        # æ£€æŸ¥å­—ç¬¦é•¿åº¦ä¸º1ä¸”ç¼–ç é•¿åº¦ä¸º2çš„æƒ…å†µ
        if len(x) == 1 and len(e) == 2:
            c = (int(e[0]) << 8) + int(e[1])
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆç‰¹å®šèŒƒå›´å†…çš„å­—ç¬¦ç¼–ç 
            if (
                (c >= 0xC2A1 and c <= 0xC2BF)
                or (c >= 0xC780 and c <= 0xC783)
                or (c >= 0xCAB9 and c <= 0xCBBF)
                or (c >= 0xCC80 and c <= 0xCDA2)
            ):
                return True
        return False

    # å®šä¹‰æ£€æŸ¥å•ä¸ªå­—ç¬¦æ˜¯å¦ä¸º Unicode è¡¨æ„æ–‡å­—æ‰©å±•åŒºåŸŸçš„å‡½æ•°
    def checku2e(x):
        e = x.encode()
        # æ£€æŸ¥å­—ç¬¦é•¿åº¦ä¸º1ä¸”ç¼–ç é•¿åº¦ä¸º3çš„æƒ…å†µ
        if len(x) == 1 and len(e) == 3:
            c = (int(e[0]) << 16) + (int(e[1]) << 8) + int(e[2])
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆç‰¹å®šèŒƒå›´å†…çš„å­—ç¬¦ç¼–ç 
            if c >= 0xE28080 and c <= 0xE2B07F:
                return True
        return False

    # åˆå§‹åŒ–ä½ç½®å˜é‡ä¸º0
    pos = 0
    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    result = []
    # å½“ä½ç½®å°äºæ–‡æœ¬é•¿åº¦æ—¶å¾ªç¯å¤„ç†æ–‡æœ¬
    while pos < len(text):
        # å¦‚æœå½“å‰å­—ç¬¦æ˜¯"<"ï¼Œåˆ™ç»“æŸä½ç½®ä¸ºå½“å‰ä½ç½®åŠ ä¸Šæœ€å¤§é•¿åº¦åŠ 1ï¼›å¦åˆ™ç»“æŸä½ç½®ä¸ºå½“å‰ä½ç½®åŠ 3
        end = min(len(text), pos + self.maxlen + 1) if text[pos] == "<" else pos + 3
        # å€™é€‰è¯åˆ—è¡¨åˆå§‹åŒ–ä¸ºç©º
        candidates = []  # (token_id, token, pos)
        # ä»ç»“æŸä½ç½®å‘å½“å‰ä½ç½®éå†
        for e in range(end, pos, -1):
            # è·å–å½“å‰ä½ç½®åˆ°ç»“æŸä½ç½®çš„å­ä¸²
            wd = text[pos:e]
            # å¦‚æœè¯¥å­ä¸²åœ¨è¯æ±‡è¡¨ä¸­å­˜åœ¨
            if wd in self.vocab:
                # å¦‚æœå­ä¸²ä»¥"<"å¼€å¤´ä¸”é•¿åº¦å¤§äº2ï¼Œåˆ™å°†å…¶ä½œä¸ºä¸€ä¸ªå€™é€‰é¡¹åŠ å…¥åˆ—è¡¨
                if wd[0] == "<" and len(wd) > 2:
                    candidates = [(self.vocab[wd], wd, e)]
                    break
                else:
                    candidates.append((self.vocab[wd], wd, e))
        # å¦‚æœå€™é€‰è¯åˆ—è¡¨ä¸ä¸ºç©º
        if len(candidates) > 0:
            # æ ¹æ® token_id æœ€å°çš„åŸåˆ™é€‰å–å€™é€‰é¡¹ä¸­çš„ä¸€ä¸ªè¿›è¡Œå¤„ç†
            _, wd, e = sorted(candidates, key=lambda x: x[0])[0]
            # å°†é€‰å–çš„è¯æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
            result.append(wd)
            # æ›´æ–°ä½ç½®ä¸º e
            pos = e
        else:
            # å¦‚æœå€™é€‰è¯åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™å¤„ç†å½“å‰ä½ç½®åˆ°ç»“æŸä½ç½®çš„å­ä¸²
            end = pos + 1
            wd = text[pos:end]
            # å¦‚æœå­ä¸²ä¸ºç‰¹å®šç¬¦å·ï¼Œåˆ™å°†"<KIGOU>"åŠ å…¥ç»“æœåˆ—è¡¨
            if check_simbol(wd):
                result.append("<KIGOU>")
            # å¦‚æœå­ä¸²ä¸º Unicode è¡¨æ„æ–‡å­—æ‰©å±•åŒºåŸŸçš„å­—ç¬¦ï¼Œåˆ™å°†"<U2000U2BFF>"åŠ å…¥ç»“æœåˆ—è¡¨
            elif checku2e(wd):
                result.append("<U2000U2BFF>")
            else:
                # å¦åˆ™å°†å­ä¸²ä¸­çš„æ¯ä¸ªå­—èŠ‚æŒ‰ç…§æ ¼å¼"<|byte%d|>"æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
                for i in wd.encode("utf-8"):
                    result.append("<|byte%d|>" % i)
            # æ›´æ–°ä½ç½®ä¸º end
            pos = end
    
    # è¿”å›å¤„ç†åçš„ç»“æœåˆ—è¡¨
    return result
```