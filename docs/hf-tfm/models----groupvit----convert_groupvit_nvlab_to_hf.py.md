# `.\models\groupvit\convert_groupvit_nvlab_to_hf.py`

```
# è®¾ç½®æ–‡ä»¶ç¼–ç ä¸º UTF-8
# ç‰ˆæƒå£°æ˜
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼Œç¦æ­¢æœªç»è®¸å¯ä½¿ç”¨æ­¤æ–‡ä»¶
# å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥è·å–è®¸å¯è¯å‰¯æœ¬
#     http://www.apache.org/licenses/LICENSE-2.0
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäºâ€œæŒ‰åŸæ ·â€åˆ†å‘çš„ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶
# è¯·æŸ¥çœ‹è®¸å¯è¯ä»¥è·å–æœ‰å…³ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶

"""
ä»åŸå§‹å­˜å‚¨åº“è½¬æ¢ GroupViT æ£€æŸ¥ç‚¹ã€‚

URL: https://github.com/NVlabs/GroupViT
"""

import argparse

import requests
import torch
from PIL import Image

from transformers import CLIPProcessor, GroupViTConfig, GroupViTModel

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºé‡å‘½åé”®å
def rename_key(name):
    # è§†è§‰ç¼–ç å™¨
    if "img_encoder.pos_embed" in name:
        name = name.replace("img_encoder.pos_embed", "vision_model.embeddings.position_embeddings")
    if "img_encoder.patch_embed.proj" in name:
        name = name.replace("img_encoder.patch_embed.proj", "vision_model.embeddings.patch_embeddings.projection")
    if "img_encoder.patch_embed.norm" in name:
        name = name.replace("img_encoder.patch_embed.norm", "vision_model.embeddings.layernorm")
    if "img_encoder.layers" in name:
        name = name.replace("img_encoder.layers", "vision_model.encoder.stages")
    if "blocks" in name and "res" not in name:
        name = name.replace("blocks", "layers")
    if "attn" in name and "pre_assign" not in name:
        name = name.replace("attn", "self_attn")
    if "proj" in name and "self_attn" in name and "text" not in name:
        name = name.replace("proj", "out_proj")
    if "pre_assign_attn.attn.proj" in name:
        name = name.replace("pre_assign_attn.attn.proj", "pre_assign_attn.attn.out_proj")
    if "norm1" in name:
        name = name.replace("norm1", "layer_norm1")
    if "norm2" in name and "pre_assign" not in name:
        name = name.replace("norm2", "layer_norm2")
    if "img_encoder.norm" in name:
        name = name.replace("img_encoder.norm", "vision_model.layernorm")
    # æ–‡æœ¬ç¼–ç å™¨
    if "text_encoder.token_embedding" in name:
        name = name.replace("text_encoder.token_embedding", "text_model.embeddings.token_embedding")
    if "text_encoder.positional_embedding" in name:
        name = name.replace("text_encoder.positional_embedding", "text_model.embeddings.position_embedding.weight")
    if "text_encoder.transformer.resblocks." in name:
        name = name.replace("text_encoder.transformer.resblocks.", "text_model.encoder.layers.")
    if "ln_1" in name:
        name = name.replace("ln_1", "layer_norm1")
    if "ln_2" in name:
        name = name.replace("ln_2", "layer_norm2")
    if "c_fc" in name:
        name = name.replace("c_fc", "fc1")
    if "c_proj" in name:
        name = name.replace("c_proj", "fc2")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"text_encoder"ï¼Œåˆ™æ›¿æ¢ä¸º"text_model"
    if "text_encoder" in name:
        name = name.replace("text_encoder", "text_model")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"ln_final"ï¼Œåˆ™æ›¿æ¢ä¸º"final_layer_norm"
    if "ln_final" in name:
        name = name.replace("ln_final", "final_layer_norm")
    # å¤„ç†æŠ•å½±å±‚
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"img_projector.linear_hidden."ï¼Œåˆ™æ›¿æ¢ä¸º"visual_projection."
    if "img_projector.linear_hidden." in name:
        name = name.replace("img_projector.linear_hidden.", "visual_projection.")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"img_projector.linear_out."ï¼Œåˆ™æ›¿æ¢ä¸º"visual_projection.3."
    if "img_projector.linear_out." in name:
        name = name.replace("img_projector.linear_out.", "visual_projection.3.")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"text_projector.linear_hidden"ï¼Œåˆ™æ›¿æ¢ä¸º"text_projection"
    if "text_projector.linear_hidden" in name:
        name = name.replace("text_projector.linear_hidden", "text_projection")
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å«"text_projector.linear_out"ï¼Œåˆ™æ›¿æ¢ä¸º"text_projection.3"
    if "text_projector.linear_out" in name:
        name = name.replace("text_projector.linear_out", "text_projection.3")

    # è¿”å›å¤„ç†åçš„æ–‡ä»¶å
    return name
# å°†åŸå§‹çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºæ–°çš„çŠ¶æ€å­—å…¸
def convert_state_dict(orig_state_dict, config):
    return orig_state_dict


# å‡†å¤‡ä¸€å¼ å¯çˆ±çŒ«å’ªçš„å›¾ç‰‡ç”¨äºéªŒè¯ç»“æœ
def prepare_img():
    # å›¾ç‰‡é“¾æ¥
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # é€šè¿‡é“¾æ¥è·å–å›¾ç‰‡
    im = Image.open(requests.get(url, stream=True).raw)
    return im


# è½¬æ¢ GroupViT æ£€æŸ¥ç‚¹åˆ° Transformers è®¾è®¡
@torch.no_grad()
def convert_groupvit_checkpoint(
    checkpoint_path, pytorch_dump_folder_path, model_name="groupvit-gcc-yfcc", push_to_hub=False
):
    """
    å¤åˆ¶/ç²˜è´´/è°ƒæ•´æ¨¡å‹çš„æƒé‡ä»¥é€‚åº” Transformers è®¾è®¡ã€‚
    """
    # åˆ›å»º GroupViT é…ç½®
    config = GroupViTConfig()
    # åˆ›å»º GroupViT æ¨¡å‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = GroupViTModel(config).eval()

    # åŠ è½½æ£€æŸ¥ç‚¹çš„çŠ¶æ€å­—å…¸
    state_dict = torch.load(checkpoint_path, map_location="cpu")["model"]
    # è½¬æ¢çŠ¶æ€å­—å…¸
    new_state_dict = convert_state_dict(state_dict, config)
    # åŠ è½½æ–°çš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­
    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
    assert missing_keys == ["text_model.embeddings.position_ids"]
    assert (unexpected_keys == ["multi_label_logit_scale"]) or (len(unexpected_keys) == 0)

    # éªŒè¯ç»“æœ
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    image = prepare_img()
    inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, padding=True, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**inputs)

    if model_name == "groupvit-gcc-yfcc":
        expected_logits = torch.tensor([[13.3523, 6.3629]])
    elif model_name == "groupvit-gcc-redcaps":
        expected_logits = torch.tensor([[16.1873, 8.6230]])
    else:
        raise ValueError(f"Model name {model_name} not supported.")
    assert torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3)

    # ä¿å­˜å¤„ç†å™¨å’Œæ¨¡å‹
    processor.save_pretrained(pytorch_dump_folder_path)
    model.save_pretrained(pytorch_dump_folder_path)
    print("Successfully saved processor and model to", pytorch_dump_folder_path)

    if push_to_hub:
        print("Pushing to the hub...")
        processor.push_to_hub(model_name, organization="nielsr")
        model.push_to_hub(model_name, organization="nielsr")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to dump the processor and PyTorch model."
    )
    parser.add_argument("--checkpoint_path", default=None, type=str, help="Path to GroupViT checkpoint")
    parser.add_argument(
        "--model_name",
        default="groupvit-gccy-fcc",
        type=str,
        help="Name of the model. Expecting either 'groupvit-gcc-yfcc' or 'groupvit-gcc-redcaps'",
    )
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="Whether or not to push the converted model and processor to the ğŸ¤— hub using the provided `model_name`.",
    )
    args = parser.parse_args()

    # è°ƒç”¨è½¬æ¢å‡½æ•°
    convert_groupvit_checkpoint(args.checkpoint_path, args.pytorch_dump_folder_path, args.model_name, args.push_to_hub)
```