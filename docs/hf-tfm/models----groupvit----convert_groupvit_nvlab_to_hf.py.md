# `.\models\groupvit\convert_groupvit_nvlab_to_hf.py`

```py
# å®šä¹‰å‡½æ•°ç”¨äºé‡å‘½åæ¨¡å‹å‚æ•°é”®å
def rename_key(name):
    # å¦‚æœé”®åä¸­åŒ…å« "img_encoder.pos_embed"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.embeddings.position_embeddings"
    if "img_encoder.pos_embed" in name:
        name = name.replace("img_encoder.pos_embed", "vision_model.embeddings.position_embeddings")
    # å¦‚æœé”®åä¸­åŒ…å« "img_encoder.patch_embed.proj"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.embeddings.patch_embeddings.projection"
    if "img_encoder.patch_embed.proj" in name:
        name = name.replace("img_encoder.patch_embed.proj", "vision_model.embeddings.patch_embeddings.projection")
    # å¦‚æœé”®åä¸­åŒ…å« "img_encoder.patch_embed.norm"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.embeddings.layernorm"
    if "img_encoder.patch_embed.norm" in name:
        name = name.replace("img_encoder.patch_embed.norm", "vision_model.embeddings.layernorm")
    # å¦‚æœé”®åä¸­åŒ…å« "img_encoder.layers"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.encoder.stages"
    if "img_encoder.layers" in name:
        name = name.replace("img_encoder.layers", "vision_model.encoder.stages")
    # å¦‚æœé”®åä¸­åŒ…å« "blocks" ä¸”ä¸åŒ…å« "res"ï¼Œåˆ™æ›¿æ¢ä¸º "layers"
    if "blocks" in name and "res" not in name:
        name = name.replace("blocks", "layers")
    # å¦‚æœé”®åä¸­åŒ…å« "attn" ä¸”ä¸åŒ…å« "pre_assign"ï¼Œåˆ™æ›¿æ¢ä¸º "self_attn"
    if "attn" in name and "pre_assign" not in name:
        name = name.replace("attn", "self_attn")
    # å¦‚æœé”®åä¸­åŒ…å« "proj" ä¸”åŒæ—¶åŒ…å« "self_attn" ä¸”ä¸åŒ…å« "text"ï¼Œåˆ™æ›¿æ¢ä¸º "out_proj"
    if "proj" in name and "self_attn" in name and "text" not in name:
        name = name.replace("proj", "out_proj")
    # å¦‚æœé”®åä¸­åŒ…å« "pre_assign_attn.attn.proj"ï¼Œåˆ™æ›¿æ¢ä¸º "pre_assign_attn.attn.out_proj"
    if "pre_assign_attn.attn.proj" in name:
        name = name.replace("pre_assign_attn.attn.proj", "pre_assign_attn.attn.out_proj")
    # å¦‚æœé”®åä¸­åŒ…å« "norm1"ï¼Œåˆ™æ›¿æ¢ä¸º "layer_norm1"
    if "norm1" in name:
        name = name.replace("norm1", "layer_norm1")
    # å¦‚æœé”®åä¸­åŒ…å« "norm2" ä¸”ä¸åŒ…å« "pre_assign"ï¼Œåˆ™æ›¿æ¢ä¸º "layer_norm2"
    if "norm2" in name and "pre_assign" not in name:
        name = name.replace("norm2", "layer_norm2")
    # å¦‚æœé”®åä¸­åŒ…å« "img_encoder.norm"ï¼Œåˆ™æ›¿æ¢ä¸º "vision_model.layernorm"
    if "img_encoder.norm" in name:
        name = name.replace("img_encoder.norm", "vision_model.layernorm")
    # å¦‚æœé”®åä¸­åŒ…å« "text_encoder.token_embedding"ï¼Œåˆ™æ›¿æ¢ä¸º "text_model.embeddings.token_embedding"
    if "text_encoder.token_embedding" in name:
        name = name.replace("text_encoder.token_embedding", "text_model.embeddings.token_embedding")
    # å¦‚æœé”®åä¸­åŒ…å« "text_encoder.positional_embedding"ï¼Œåˆ™æ›¿æ¢ä¸º "text_model.embeddings.position_embedding.weight"
    if "text_encoder.positional_embedding" in name:
        name = name.replace("text_encoder.positional_embedding", "text_model.embeddings.position_embedding.weight")
    # å¦‚æœé”®åä¸­åŒ…å« "text_encoder.transformer.resblocks."ï¼Œåˆ™æ›¿æ¢ä¸º "text_model.encoder.layers."
    if "text_encoder.transformer.resblocks." in name:
        name = name.replace("text_encoder.transformer.resblocks.", "text_model.encoder.layers.")
    # å¦‚æœé”®åä¸­åŒ…å« "ln_1"ï¼Œåˆ™æ›¿æ¢ä¸º "layer_norm1"
    if "ln_1" in name:
        name = name.replace("ln_1", "layer_norm1")
    # å¦‚æœé”®åä¸­åŒ…å« "ln_2"ï¼Œåˆ™æ›¿æ¢ä¸º "layer_norm2"
    if "ln_2" in name:
        name = name.replace("ln_2", "layer_norm2")
    # å¦‚æœé”®åä¸­åŒ…å« "c_fc"ï¼Œåˆ™æ›¿æ¢ä¸º "fc1"
    if "c_fc" in name:
        name = name.replace("c_fc", "fc1")
    # å¦‚æœé”®åä¸­åŒ…å« "c_proj"ï¼Œåˆ™æ›¿æ¢ä¸º "fc2"
    if "c_proj" in name:
        name = name.replace("c_proj", "fc2")
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "text_encoder"
    if "text_encoder" in name:
        # å°†å…¶æ›¿æ¢ä¸º "text_model"
        name = name.replace("text_encoder", "text_model")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "ln_final"
    if "ln_final" in name:
        # å°†å…¶æ›¿æ¢ä¸º "final_layer_norm"
        name = name.replace("ln_final", "final_layer_norm")
    
    # å¤„ç†æŠ•å½±å±‚çš„å‘½åæ˜ å°„
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "img_projector.linear_hidden."
    if "img_projector.linear_hidden." in name:
        # å°†å…¶æ›¿æ¢ä¸º "visual_projection."
        name = name.replace("img_projector.linear_hidden.", "visual_projection.")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "img_projector.linear_out."
    if "img_projector.linear_out." in name:
        # å°†å…¶æ›¿æ¢ä¸º "visual_projection.3."
        name = name.replace("img_projector.linear_out.", "visual_projection.3.")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "text_projector.linear_hidden"
    if "text_projector.linear_hidden" in name:
        # å°†å…¶æ›¿æ¢ä¸º "text_projection"
        name = name.replace("text_projector.linear_hidden", "text_projection")
    
    # å¦‚æœå˜é‡ name ä¸­åŒ…å«å­—ç¬¦ä¸² "text_projector.linear_out"
    if "text_projector.linear_out" in name:
        # å°†å…¶æ›¿æ¢ä¸º "text_projection.3"
        name = name.replace("text_projector.linear_out", "text_projection.3")
    
    # è¿”å›å¤„ç†åçš„ name å˜é‡ä½œä¸ºç»“æœ
    return name
def convert_state_dict(orig_state_dict, config):
    # ç®€å•åœ°è¿”å›åŸå§‹çŠ¶æ€å­—å…¸ï¼Œæœªç»ä»»ä½•æ”¹åŠ¨
    return orig_state_dict


# æˆ‘ä»¬å°†åœ¨ä¸€å¼ å¯çˆ±çŒ«å’ªçš„å›¾åƒä¸ŠéªŒè¯æˆ‘ä»¬çš„ç»“æœ
def prepare_img():
    # å›¾åƒçš„ URL
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    # ä½¿ç”¨ requests åº“è·å–å›¾åƒçš„åŸå§‹å­—èŠ‚æµï¼Œå¹¶ç”± PIL æ‰“å¼€
    im = Image.open(requests.get(url, stream=True).raw)
    return im


@torch.no_grad()
def convert_groupvit_checkpoint(
    checkpoint_path, pytorch_dump_folder_path, model_name="groupvit-gcc-yfcc", push_to_hub=False
):
    """
    å¤åˆ¶/ç²˜è´´/è°ƒæ•´æ¨¡å‹çš„æƒé‡ä»¥ç¬¦åˆ Transformers è®¾è®¡ã€‚
    """
    # åˆ›å»º GroupViT æ¨¡å‹é…ç½®
    config = GroupViTConfig()
    # åˆå§‹åŒ– GroupViT æ¨¡å‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = GroupViTModel(config).eval()

    # ä»æŒ‡å®šè·¯å¾„åŠ è½½ GroupViT æ¨¡å‹çš„çŠ¶æ€å­—å…¸åˆ° CPU
    state_dict = torch.load(checkpoint_path, map_location="cpu")["model"]
    # å°†åŠ è½½çš„çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºæ–°çš„çŠ¶æ€å­—å…¸ï¼Œä½¿ç”¨ç»™å®šçš„é…ç½®
    new_state_dict = convert_state_dict(state_dict, config)
    # åŠ è½½æ–°çš„çŠ¶æ€å­—å…¸åˆ°æ¨¡å‹ä¸­ï¼Œå…è®¸ä¸ä¸¥æ ¼åŒ¹é…
    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
    # æ–­è¨€ç¡®å®ç¼ºå¤±çš„é”®ä¸ºæŒ‡å®šå€¼
    assert missing_keys == ["text_model.embeddings.position_ids"]
    # æ–­è¨€ç¡®å®çš„é”®ä¸ºæŒ‡å®šå€¼æˆ–é•¿åº¦ä¸º 0
    assert (unexpected_keys == ["multi_label_logit_scale"]) or (len(unexpected_keys) == 0)

    # éªŒè¯ç»“æœ
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    image = prepare_img()
    # å‡†å¤‡è¾“å…¥æ•°æ®ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒï¼Œä½¿ç”¨ CLIP å¤„ç†å™¨
    inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, padding=True, return_tensors="pt")

    with torch.no_grad():
        # åœ¨ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—çš„æƒ…å†µä¸‹ï¼Œå‘æ¨¡å‹è¾“å…¥æ•°æ®å¹¶è·å–è¾“å‡º
        outputs = model(**inputs)

    # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®æœŸæœ›çš„ logits å€¼ï¼Œå¹¶è¿›è¡Œæ•°å€¼è¿‘ä¼¼æ¯”è¾ƒ
    if model_name == "groupvit-gcc-yfcc":
        expected_logits = torch.tensor([[13.3523, 6.3629]])
    elif model_name == "groupvit-gcc-redcaps":
        expected_logits = torch.tensor([[16.1873, 8.6230]])
    else:
        raise ValueError(f"Model name {model_name} not supported.")
    assert torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3)

    # å°†å¤„ç†å™¨å’Œæ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    processor.save_pretrained(pytorch_dump_folder_path)
    model.save_pretrained(pytorch_dump_folder_path)
    print("Successfully saved processor and model to", pytorch_dump_folder_path)

    # å¦‚æœè®¾ç½®äº†æ¨é€åˆ° Hubï¼Œæ‰§è¡Œæ¨é€æ“ä½œ
    if push_to_hub:
        print("Pushing to the hub...")
        processor.push_to_hub(model_name, organization="nielsr")
        model.push_to_hub(model_name, organization="nielsr")


if __name__ == "__main__":
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--pytorch_dump_folder_path", default=None, type=str, help="Path to dump the processor and PyTorch model."
    )
    parser.add_argument("--checkpoint_path", default=None, type=str, help="Path to GroupViT checkpoint")
    parser.add_argument(
        "--model_name",
        default="groupvit-gccy-fcc",
        type=str,
        help="Name of the model. Expecting either 'groupvit-gcc-yfcc' or 'groupvit-gcc-redcaps'",
    )
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="Whether or not to push the converted model and processor to the ğŸ¤— hub using the provided `model_name`.",
    )
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨å‡½æ•°ï¼Œæ‰§è¡Œ GroupViT æ¨¡å‹æ£€æŸ¥ç‚¹çš„è½¬æ¢
    convert_groupvit_checkpoint(args.checkpoint_path, args.pytorch_dump_folder_path, args.model_name, args.push_to_hub)
```