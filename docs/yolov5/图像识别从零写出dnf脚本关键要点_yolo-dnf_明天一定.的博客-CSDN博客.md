<!--yml
category: æ¸¸æˆ
date: 2023-02-28 15:59:47
-->

# å›¾åƒè¯†åˆ«ä»é›¶å†™å‡ºdnfè„šæœ¬å…³é”®è¦ç‚¹_yolo dnf_æ˜å¤©ä¸€å®š.çš„åšå®¢-CSDNåšå®¢

> æ¥æºï¼š[https://blog.csdn.net/wai_58934/article/details/126090670](https://blog.csdn.net/wai_58934/article/details/126090670)

# æ€è·¯ï¼š

1.  çŸ¥é“æ¸¸æˆè§’è‰²åœ¨å“ªä¸ªåæ ‡
2.  çŸ¥é“æ€ªç‰©å’Œé—¨åœ¨å“ªä¸ªåæ ‡
3.  é©±åŠ¨çº§é”®é¼ æ“ä½œè®©è§’è‰²èµ°åˆ°æ€ªç‰©åæ ‡ç„¶åæ”»å‡»é‡Šæ”¾æŠ€èƒ½ã€‚

æ»¡è¶³ç¬¬ä¸€å’Œç¬¬äºŒæ¡å°±è¦æ±‚å¿…é¡»å®æ—¶è¯»å…¥å›¾åƒä»¥åŠèƒ½å¤Ÿè¯†åˆ«æ ‡å¿—æ€§å›¾åƒç„¶åç»™å‡ºåæ ‡ã€‚

# å®æ—¶è¯»å…¥å›¾åƒ

æ²¡ç²¾åŠ›ç©pythonäº†ï¼Œè¿˜æ˜¯å¥½å¥½å­¦Javaå§

åºŸè¯ä¸å¤šè¯´ç›´æ¥ä¸Šä»£ç 

```py
import cv2
from PIL import ImageGrab
import numpy as np

while True:
    im = ImageGrab.grab()
    imm = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)
    imm = imm[0:500, 0:500]
    imm = cv2.resize(imm, None, fx=0.5, fy=0.5)
    cv2.imshow("capture", imm)
    if cv2.waitKey(1) & 0xFF == ord('q'):  # qé”®æ¨å‡º
        break
cv2.destroyAllWindows()
```

# å›¾åƒè¯†åˆ«

æˆ‘ä½¿ç”¨çš„æ˜¯yolov5ã€‚

pythonç‰ˆæœ¬æ˜¯ï¼š3.10.5

å‰è¾¹ç®€å•è¯´ä¸€ä¸‹å§ï¼Œçœçš„ä»¥åæˆ‘ç”¨äº†å†å¿˜è®°äº†

## å‰ç½®å·¥ä½œ

1.  ç”¨pycharmåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
2.  åœ¨githubæŠŠé¡¹ç›®å…‹éš†ä¸‹æ¥[GitHub - ultralytics/yolov5: YOLOv5 ğŸš€ in PyTorch > ONNX > CoreML > TFLite](https://github.com/ultralytics/yolov5 "GitHub - ultralytics/yolov5: YOLOv5 ğŸš€ in PyTorch > ONNX > CoreML > TFLite")
3.  ä¸‹è½½ä¾èµ–ï¼ˆé¡¹ç›®é‡Œçš„requirements.txtï¼‰ï¼špip install -r requirements.txt
4.  åœ¨å…‹éš†ä¸‹è½½çš„ç›®å½•çº§åˆ›å»ºè¿™æ ·ä¸€ä¸ªç›®å½•ï¼Œå…¶å®åˆ›å»ºåœ¨å“ªéƒ½æ— æ‰€è°“ï¼Œå°±æ˜¯æ–¹ä¾¿ï¼ˆä½œç”¨å°±æ˜¯æ”¾å›¾ç‰‡ï¼Œæ”¾æ ‡ç­¾ï¼Œæ”¾é…ç½®ï¼‰![](img/5c9b9fb57a2ab00eb9dd9d211a663f48.png)
5.  å¼€å§‹æˆªå›¾ï¼ˆæŠŠæˆªä¸‹æ¥çš„å›¾éƒ½æ”¾åœ¨imagesé‡Œï¼‰
6.  å¼€å§‹æ ‡æ³¨ï¼ˆè¿™é‡Œæˆ‘ç”¨çš„æ˜¯labelImgï¼‰
7.  è®­ç»ƒ
8.  é¢„æµ‹

## 6.æ ‡æ³¨

pythonç‰ˆæœ¬ï¼š3.7.8ï¼ˆé«˜ç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹ï¼Œä½†æˆ‘å¿˜è®°å“ªé‡Œä¸å…¼å®¹äº†ï¼Œé™ä½å°±å¯¹äº†ï¼‰

ä»githubä¸‹è½½å…‹éš†[GitHub - heartexlabs/labelImg: ğŸ–ï¸ LabelImg is a graphical image annotation tool and label object bounding boxes in images](https://github.com/heartexlabs/labelImg "GitHub - heartexlabs/labelImg: ğŸ–ï¸ LabelImg is a graphical image annotation tool and label object bounding boxes in images")

Â å®‰è£…ï¼š**pip install PyQt5**

å®‰è£…ï¼š**pip install lxml**

è¿›å…¥å…‹éš†ç›®å½•æ‰§è¡Œï¼š**pyrcc5 -o resources.py resources.qrc**

æŠŠresource.pyæ”¾å…¥libsç›®å½•

æ‰§è¡Œï¼špythonÂ labelImg.py åå¯åŠ¨çª—å£

![](img/b7c19bbf891a82a3933614b97a923038.png)

Â æˆ‘ä»¬ç›´æ¥æ‰“å¼€ç›®å½•ï¼ˆæŒ‡çš„æ˜¯ä½ çš„å›¾ç‰‡å­˜æ”¾ç›®å½•ï¼‰ï¼Œç„¶åæŒ‡å®šæ”¹å˜å­˜æ”¾ç›®å½•ï¼ˆæŒ‡çš„æ˜¯ä½ çš„labelç›®å½•ï¼‰ï¼Œé‡è¦çš„æ˜¯ï¼šåˆ«å¿˜äº†æ”¹ä¸ºyoloã€‚æ ‡æ³¨æŠ€å·§ï¼šwæ˜¯æ ‡æ³¨ï¼Œdæ˜¯ä¸‹ä¸€å¼ 

Â ![](img/26ff27ba49e2b6a7ff2ae1cea2a89268.png)

Â ç„¶åç°åœ¨ä½ çš„![](img/57cd13cb3e1158951a8ab7a61ffc6b15.png)

Â è¿™ä¸ªç›®å½•åº”è¯¥æ˜¯æœ‰ä¸œè¥¿çš„ï¼ˆé™¤äº†yamlæ–‡ä»¶è¿˜æ²¡å†™ï¼‰

## 7.è®­ç»ƒ

å¤šä¹ˆç—›çš„é¢†æ‚Ÿï¼Œæˆ‘ç”¨æˆ‘ç”µè„‘è®­ç»ƒäº†ä¸€å¤©ä¹Ÿæ²¡è®­ç»ƒå®Œã€‚

æˆ‘é€‰æ‹©ä½¿ç”¨Â colabå»è®­ç»ƒï¼ˆéœ€è¦è°·æ­Œè´¦å·ï¼Œéœ€è¦ç¿»å¢™ï¼‰

> Colaboratory ç®€ç§°â€œColabâ€ï¼ŒGoogle Research å›¢é˜Ÿå¼€å‘ï¼Œä»»ä½•äººéƒ½å¯ä»¥é€šè¿‡æµè§ˆå™¨ç¼–å†™å’Œæ‰§è¡Œä»»æ„ Python ä»£ç ï¼Œå°¤å…¶é€‚åˆæœºå™¨å­¦ä¹ ã€æ•°æ®åˆ†æã€æ•™è‚²ç›®çš„ã€‚Colab æ˜¯ä¸€ç§æ‰˜ç®¡å¼ Jupyter ç¬”è®°æœ¬æœåŠ¡ï¼Œç”¨æˆ·æ— éœ€è®¾ç½®ï¼Œå°±å¯ç›´æ¥ä½¿ç”¨ï¼Œè¿˜èƒ½å…è´¹ä½¿ç”¨ GPU/TPUè®¡ç®—èµ„æºã€‚

Â æˆ‘è´¦å·å¼‚å¸¸äº†ï¼Œæ²¡åŠæ³•æˆªå›¾ç»†è¯´äº†ï¼š[Colaboratoryï¼ˆ ç®€ç§°"Colab"ï¼‰ä»‹ç»ä¸ä½¿ç”¨ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/302864983 "Colaboratoryï¼ˆ ç®€ç§°"Colab"ï¼‰ä»‹ç»ä¸ä½¿ç”¨ - çŸ¥ä¹")

ç®€å•æ¥è¯´å°±æ˜¯åˆ›å»ºÂ Colaboratoryï¼Œç„¶åæŒ‚è½½æ–‡ä»¶ï¼ŒæŠŠyolo_Aæ‰“åŒ…æ”¾ä¸Šå»ï¼Œç„¶ååœ¨ä¸Šè¾¹è§£å‹ç¼©

ï¼Œæ­å»ºyolov5ç¯å¢ƒï¼Œé€‰æ‹©GPUä¹‹åï¼Œå¯ä»¥åƒåœ¨æœ¬åœ°ä¸€æ ·è®­ç»ƒæ¨¡å‹ã€‚

Â è®­ç»ƒçš„æ—¶å€™éœ€è¦ç”¨åˆ°ä¸Šè¾¹çš„A.yaml.æ¡ˆä¾‹å†™æ³•

> ```py
> # train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]
> train: ../yolo_A/images/
> val: ../yolo_A/images/
> # number of classes ç±»å‹çš„æ•°é‡ï¼Œå‡ ä¸ªç±»å‹åå­—å°±å¡«å‡ 
> nc: 1
> 
> # class names ç±»å‹çš„åå­—ï¼Œå¯ä»¥æœ‰å¤šä¸ª
> names: ['be']
> ```

Â è®­ç»ƒæ¨¡å‹ä»£ç å®ä¾‹ï¼šyolov5s.ptå¯ä»¥åœ¨[Releases Â· ultralytics/yolov5 Â· GitHub](https://github.com/ultralytics/yolov5/releases "Releases Â· ultralytics/yolov5 Â· GitHub")æ‰¾åˆ°å¹¶ä¸‹è½½

> python train.py --img 640 --batch 54 --epochs 100 --data A.yaml --weights yolov5s.pt --nosave --cache

è®­ç»ƒå®Œçš„æ–‡ä»¶æˆ‘ä»¬éœ€è¦best.ptï¼Œåœ¨yolov5-master\runs\trainé‡Œè¾¹ã€‚ç›´æ¥æ‹¿åˆ°æœ¬åœ°ä½¿ç”¨ã€‚

## 8.é¢„æµ‹

å› ä¸ºæˆ‘éœ€è¦æ‹¿åˆ°é¢„æµ‹åçš„ç»“æœï¼Œä»¥åŠæˆ‘ä¼ è¿›å»çš„æ˜¯cv2.imread()åçš„numpyæ•°ç»„ï¼Œæ‰€ä»¥æˆ‘å¯¹detectæºç æ–‡ä»¶çš„runå‡½æ•°åšäº†ç²¾ç®€å’Œä¿®æ”¹ã€‚ä¿®æ”¹åçš„æ–‡ä»¶æ¯”è¾ƒå†—ä½™ï¼Œå› ä¸ºæˆ‘åªæ˜¯ç©ç©ï¼Œæ‰€ä»¥å°±æ²¡æµªè´¹æ—¶é—´æ•´ç†ä»£ç ï¼Œå¤§å®¶å‚è€ƒå³å¯ã€‚æ–°å¢çš„imMyå‚æ•°å°±æ˜¯ä¼ å…¥çš„æ•°ç»„ï¼Œsourceæ²¡å•¥ç”¨ï¼Œä½†æ˜¯ä½ å¾—ä¼ å…¥ä¸€ä¸ªæœ¬åœ°å­˜åœ¨çš„å›¾ç‰‡è·¯å¾„ï¼ˆè¿™ä¸ªå†—ä½™ï¼Œæ²¡ä¿®æ”¹ï¼‰

```
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Run inference on images, videos, directories, streams, etc.

Usage - sources:
    $ python path/to/detect.py --weights yolov5s.pt --source 0              # webcam
                                                             img.jpg        # image
                                                             vid.mp4        # video
                                                             path/          # directory
                                                             path/*.jpg     # glob
                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch
                                         yolov5s.torchscript        # TorchScript
                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                         yolov5s.xml                # OpenVINO
                                         yolov5s.engine             # TensorRT
                                         yolov5s.mlmodel            # CoreML (macOS-only)
                                         yolov5s_saved_model        # TensorFlow SavedModel
                                         yolov5s.pb                 # TensorFlow GraphDef
                                         yolov5s.tflite             # TensorFlow Lite
                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
"""

import argparse
import os
import platform
import sys
from pathlib import Path
import numpy as np
import torch
import torch.backends.cudnn as cudnn

from utils.augmentations import letterbox

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams
from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, time_sync

@torch.no_grad()
def run(
        imMy = None,
        weights=ROOT / 'yolov5s.pt',  # model.pt path(s)
        source=ROOT / 'data/images',  # file/dir/URL/glob, 0 for webcam
        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        visualize=False,  # visualize features
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
):
    source = str(source)
    # Load model
    device = select_device(device)
    print(device)
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    stride, names, pt = model.stride, model.names, model.pt
    imgsz = check_img_size(imgsz, s=stride)  # check image size

    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)
    bs = 1  # batch_size

    # Run inference
    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup

    for path, im, im0s, vid_cap, s in dataset:
        im0s = imMy
        # Padded resize
        img = letterbox(im0s, (800,608), stride=32, auto=True)[0]
        # Convert
        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
        img = np.ascontiguousarray(img)
        im = img

        im = torch.from_numpy(im).to(device)
        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
        im /= 255  # 0 - 255 to 0.0 - 1.0
        if len(im.shape) == 3:
            im = im[None]  # expand for batch dim

        # Inference
        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
        pred = model(im, augment=augment, visualize=visualize)
        # pred = model(im, augment=augment, visualize=False)

        # NMS
        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

        # Second-stage classifier (optional)
        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)
        # Process predictions
        res = []
        for i, det in enumerate(pred):  # per image
            im0 = im0s.copy()

            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()

                # Write results
                for *xyxy, conf, cls in reversed(det):
                    parm = []
                    c = int(cls)

                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')
                    p1, p2 = (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3]))
                    parm.append(label)
                    parm.append(p1)
                    parm.append(p2)
                    res.append(parm)

        return res

def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')
    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt

def main(opt):
    check_requirements(exclude=('tensorboard', 'thop'))
    run(**vars(opt))

if __name__ == "__main__":
    opt = parse_opt()
    main(opt) 
```py

ä½¿ç”¨ï¼š

```
res = detect.run(weights='./dnf/best.pt',source="yolo_A/images/2a.jpg",data="yolo_A/A.yaml",imgsz=(800, 608),
                 imMy=imm)
```py

Â è¿™æ ·ï¼Œç»“åˆå‰è¾¹å®æ—¶æ•è·æ¡Œé¢ï¼Œå°±å¯ä»¥å®ç°å¤§éƒ¨åˆ†çš„åŠŸèƒ½äº†ã€‚

## åŸºäºgpué¢„æµ‹

æœ€åæˆ‘å«Œå¼ƒé¢„æµ‹å¤ªæ…¢ï¼Œæƒ³æŒ‡å®šGPUé¢„æµ‹ï¼Œä½†æ˜¯å‘ç°ä¸€ç›´false

> ```
> print(torch.cuda.is_available())  // False
> ```py

Â ç„¶åæˆ‘å®‰è£…äº†cudaï¼Œå‚è€ƒï¼š[CUDAå®‰è£…æ•™ç¨‹ï¼ˆè¶…è¯¦ç»†ï¼‰_Billieä½¿åŠ²å­¦çš„åšå®¢-CSDNåšå®¢_cudaå®‰è£…](https://blog.csdn.net/m0_45447650/article/details/123704930 "CUDAå®‰è£…æ•™ç¨‹ï¼ˆè¶…è¯¦ç»†ï¼‰_Billieä½¿åŠ²å­¦çš„åšå®¢-CSDNåšå®¢_cudaå®‰è£…")

Â ç„¶åä»è™šæ‹Ÿç¯å¢ƒå¸è½½torchå’Œtorchversionï¼ˆpip uninstallä¸€ä¸‹å³å¯ï¼‰

ä»[Start Locally | PyTorch](https://pytorch.org/get-started/locally/ "Start Locally | PyTorch")æ‰¾åˆ°é€‚åˆä½ ç”µè„‘çš„ç‰ˆæœ¬ï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„

![](img/1067dbf6ccbb081d1edad749c513c4d5.png)

Â ç„¶åä»[https://download.pytorch.org/whl/torch_stable.html](https://download.pytorch.org/whl/torch_stable.html "https://download.pytorch.org/whl/torch_stable.html")å®‰è£…wheelæ–‡ä»¶

![](img/ff0badb07e9b9fc5a7c7416b48d0d132.png)

Â å®‰è£…torchå’Œtorchvision,å…·ä½“å®‰è£…cpå¤šå°‘ï¼ˆæˆ‘æ˜¯æ ¹æ®å®‰è£…yoloç¯å¢ƒæ—¶pip install -r requirements.txtæ—¶æ§åˆ¶å°æ‰“å°å®‰è£…çš„torchå¯¹åº”çš„cpï¼‰

ç„¶åå®‰è£…wheelæ–‡ä»¶ã€‚

è¿è¡Œå‘ç°

> print(torch.cuda.is_available()) // True

Â æˆ‘çš„ç‰ˆæœ¬deviceæŒ‡å®šä¸ºç©ºå¹¶gpuå¯ç”¨æ—¶åˆ™é€‰æ‹©gpu

å®Œç»“æ’’èŠ±ã€‚

# é©±åŠ¨çº§é”®ç›˜æ“ä½œç»•è¿‡æ¸¸æˆæ£€æµ‹

æŸ¥äº†ä¸€å †åšå®¢é©±åŠ¨çº§æ“ä½œï¼Œç”šè‡³åŠ ç¾¤é—®åˆ«äººæ€ä¹ˆåšéƒ½æ²¡é—®å‡ºæ¥ï¼ŒçœŸã€‚ã€‚ã€‚

æœ€åè¯•åˆ°å‡Œæ™¨ï¼ŒåºŸè¯ä¸å¤šè¯´ç›´æ¥ä¸Šæ­£ç¡®æ¡ˆä¾‹ï¼Œéœ€è¦ç”¨åˆ°pywin32ï¼Œè‡ªå·±pipä¸€ä¸‹ï¼ˆè¿è¡Œæ—¶éœ€è¦ç®¡ç†å‘˜è¿è¡Œï¼‰

```
import time
import win32api
import win32con
import ctypes
import win32gui

VK_CODE = {
    'backspace': 0x08,
    'tab': 0x09,
    'clear': 0x0C,
    'enter': 0x0D,
    'shift': 0x10,
    'ctrl': 0x11,
    'alt': 0x12,
    'pause': 0x13,
    'caps_lock': 0x14,
    'esc': 0x1B,
    'spacebar': 0x20,
    'page_up': 0x21,
    'page_down': 0x22,
    'end': 0x23,
    'home': 0x24,
    'left_arrow': 0x25,
    'up_arrow': 0x26,
    'right_arrow': 0x27,
    'down_arrow': 0x28,
    'select': 0x29,
    'print': 0x2A,
    'execute': 0x2B,
    'print_screen': 0x2C,
    'ins': 0x2D,
    'del': 0x2E,
    'help': 0x2F,
    '0': 0x30,
    '1': 0x31,
    '2': 0x32,
    '3': 0x33,
    '4': 0x34,
    '5': 0x35,
    '6': 0x36,
    '7': 0x37,
    '8': 0x38,
    '9': 0x39,
    'a': 0x41,
    'b': 0x42,
    'c': 0x43,
    'd': 0x44,
    'e': 0x45,
    'f': 0x46,
    'g': 0x47,
    'h': 0x48,
    'i': 0x49,
    'j': 0x4A,
    'k': 0x4B,
    'l': 0x4C,
    'm': 0x4D,
    'n': 0x4E,
    'o': 0x4F,
    'p': 0x50,
    'q': 0x51,
    'r': 0x52,
    's': 0x53,
    't': 0x54,
    'u': 0x55,
    'v': 0x56,
    'w': 0x57,
    'x': 0x58,
    'y': 0x59,
    'z': 0x5A,
    'numpad_0': 0x60,
    'numpad_1': 0x61,
    'numpad_2': 0x62,
    'numpad_3': 0x63,
    'numpad_4': 0x64,
    'numpad_5': 0x65,
    'numpad_6': 0x66,
    'numpad_7': 0x67,
    'numpad_8': 0x68,
    'numpad_9': 0x69,
    'multiply_key': 0x6A,
    'add_key': 0x6B,
    'separator_key': 0x6C,
    'subtract_key': 0x6D,
    'decimal_key': 0x6E,
    'divide_key': 0x6F,
    'F1': 0x70,
    'F2': 0x71,
    'F3': 0x72,
    'F4': 0x73,
    'F5': 0x74,
    'F6': 0x75,
    'F7': 0x76,
    'F8': 0x77,
    'F9': 0x78,
    'F10': 0x79,
    'F11': 0x7A,
    'F12': 0x7B,
    'F13': 0x7C,
    'F14': 0x7D,
    'F15': 0x7E,
    'F16': 0x7F,
    'F17': 0x80,
    'F18': 0x81,
    'F19': 0x82,
    'F20': 0x83,
    'F21': 0x84,
    'F22': 0x85,
    'F23': 0x86,
    'F24': 0x87,
    'num_lock': 0x90,
    'scroll_lock': 0x91,
    'left_shift': 0xA0,
    'right_shift ': 0xA1,
    'left_control': 0xA2,
    'right_control': 0xA3,
    'left_menu': 0xA4,
    'right_menu': 0xA5,
    'browser_back': 0xA6,
    'browser_forward': 0xA7,
    'browser_refresh': 0xA8,
    'browser_stop': 0xA9,
    'browser_search': 0xAA,
    'browser_favorites': 0xAB,
    'browser_start_and_home': 0xAC,
    'volume_mute': 0xAD,
    'volume_Down': 0xAE,
    'volume_up': 0xAF,
    'next_track': 0xB0,
    'previous_track': 0xB1,
    'stop_media': 0xB2,
    'play/pause_media': 0xB3,
    'start_mail': 0xB4,
    'select_media': 0xB5,
    'start_application_1': 0xB6,
    'start_application_2': 0xB7,
    'attn_key': 0xF6,
    'crsel_key': 0xF7,
    'exsel_key': 0xF8,
    'play_key': 0xFA,
    'zoom_key': 0xFB,
    'clear_key': 0xFE,
    '+': 0xBB,
    ',': 0xBC,
    '-': 0xBD,
    '.': 0xBE,
    '/': 0xBF,
    ';': 0xBA,
    '[': 0xDB,
    '\\': 0xDC,
    ']': 0xDD,
    "'": 0xDE,
    '`': 0xC0}

# handle = win32gui.FindWindow(None, 'åœ°ä¸‹åŸä¸å‹‡å£«')
# win32gui.SetForegroundWindow(handle)

while True:
    time.sleep(5)
    MapVirtualKey = ctypes.windll.user32.MapVirtualKeyA
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_8"], win32api.MapVirtualKey(VK_CODE["numpad_8"], 0), 0, 0)  # 0
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_8"], win32api.MapVirtualKey(VK_CODE["numpad_8"], 0), win32con.KEYEVENTF_KEYUP, 0)
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_5"], win32api.MapVirtualKey(VK_CODE["numpad_5"], 0), 0, 0)  # 0
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_5"], win32api.MapVirtualKey(VK_CODE["numpad_5"], 0), win32con.KEYEVENTF_KEYUP,
                         0)
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_4"], win32api.MapVirtualKey(VK_CODE["numpad_4"], 0), 0, 0)  # 0
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_4"], win32api.MapVirtualKey(VK_CODE["numpad_4"], 0), win32con.KEYEVENTF_KEYUP, 0)
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_6"], win32api.MapVirtualKey(VK_CODE["numpad_6"], 0), 0, 0)  # 0
    time.sleep(0.3)
    win32api.keybd_event(VK_CODE["numpad_6"], win32api.MapVirtualKey(VK_CODE["numpad_6"], 0), win32con.KEYEVENTF_KEYUP, 0)
    # win32api.keybd_event(0x0D, 0, 0, 0)  # enter
    # win32api.keybd_event(0x0D, 0, win32con.KEYEVENTF_KEYUP, 0) 
```