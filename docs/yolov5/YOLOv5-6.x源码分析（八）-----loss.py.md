<!--yml
category: æ¸¸æˆ
date: 2023-09-17 14:44:10
-->

# YOLOv5-6.xæºç åˆ†æï¼ˆå…«ï¼‰---- loss.py

> æ¥æºï¼š[https://blog.csdn.net/weixin_51322383/article/details/130426149](https://blog.csdn.net/weixin_51322383/article/details/130426149)

### æ–‡ç« ç›®å½•

*   [å‰è¨€](#_1)
*   [ğŸš€YOLOv5-6.xæºç åˆ†æï¼ˆå…«ï¼‰---- loss.py](#YOLOv56x_losspy_21)
*   *   [0\. å¯¼åŒ…](#0__22)
    *   [1\. smooth_BCE](#1_smooth_BCE_31)
    *   [2\. BCEBlurWithLogitsLoss](#2_BCEBlurWithLogitsLoss_45)
    *   [3\. FocalLoss](#3_FocalLoss_68)
    *   [4\. QFocalLoss](#4_QFocalLoss_112)
    *   [5\. ComputeLoss](#5_ComputeLoss_141)
    *   *   [5.1 __init__å‡½æ•°](#51___init___142)
        *   [5.2 build_targets](#52_build_targets_184)
        *   [5.3 __call__å‡½æ•°](#53___call___291)
*   [è¡¥å……](#_375)
*   *   [åˆ†ç±»æŸå¤±ï¼ˆClassificationï¼‰](#Classification_377)
    *   [ç½®ä¿¡åº¦æŸå¤±ï¼ˆObjectnessï¼‰](#Objectness_385)
    *   [è¾¹æ¡†æŸå¤±ï¼ˆRegressionï¼‰](#Regression_392)
    *   [æ€»ç»“](#_434)

# å‰è¨€

**ä»Šå¤©æ˜¯23-04-28ï¼Œå‘¨äº”ï¼Œå› ä¸ºè¦æ”¾äº”ä¸€èŠ‚ï¼Œå°±å›å®¶äº†ã€‚å›å®¶è¿˜æ˜¯å¾ˆchillçš„ï¼Œå°±æ˜¯æ•ˆç‡æ²¡æœ‰åœ¨å­¦æ ¡é‡Œé¢é«˜ã€‚é¢„è®¡è¿™ä¸ªäº”ä¸€èŠ‚å°±æŠŠè¿™ä¸ªä¸“æ å®Œæˆå¾—å·®ä¸å¤šäº†å§ï¼Œåç»­æ‰“ç®—å†å¼€ä¸ªä¸“æ ï¼Œå»å†™WebServerï¼Œæ­£å¥½æˆ‘çš„è¯¾è®¾ä¹Ÿå‡†å¤‡äº¤è¿™ä¸ªä¸Šå»ã€‚**

**ä»Šå¤©åˆšå›åˆ°å®¶ï¼Œå°±çœ‹åˆ°å§å®¤æ¡Œä¸Šä¸€ä¸ªå¾ˆç†Ÿæ‚‰çš„æœ¬å­ï¼Œå“ˆå“ˆå“ˆå“ˆè¿™ä¸æ˜¯æˆ‘é«˜ä¸­è®°å•è¯çš„æœ¬å­å—ï¼Œæ€ä¹ˆè·‘å‡ºæ¥äº†ï¼Œä¼°è®¡æ˜¯è¢«æˆ‘å¦ˆæ•´ç†æˆ¿é—´çš„æ—¶å€™ç»™ç¿»å‡ºæ¥äº†å§ã€‚é‚£æ—¶å€™çš„å­—è¿˜å¾ˆé’æ¶©ï¼ˆè™½ç„¶ç°åœ¨ä¹Ÿå·®ä¸å¤šï¼‰å¥½å¤šå›å¿†ç¬é—´å°±æ¶Œä¸Šå¿ƒå¤´äº†ã€‚æ³ªç›®~**
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/ceaf139aa2f06b5d7d8dcb4a84722a90.png)![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/a7ba079c95961b6a416e5589f9b8c577.png)

* * *

**OKï¼Œè¨€å½’æ­£ä¼ ï¼Œä»Šå¤©å‡†å¤‡è°ˆä¸€ä¸‹YOLOv5çš„æŸå¤±å‡½æ•°`loss.py`ã€‚è¿™ä¸ªæ–‡ä»¶ä»£ç é‡ä¸å¤šï¼Œä½†æˆ‘è§‰å¾—å¯¹äºç†è§£æ•´ä¸ªYOLOç½‘ç»œæ˜¯å¦‚ä½•è¿ä½œçš„å°¤ä¸ºé‡è¦ï¼Œè€Œä¸”éš¾åº¦ä¹Ÿä¸å°ï¼Œè€Œä¸”ä¹Ÿéå¸¸é‡è¦ã€‚**

**åœ¨å‡†å¤‡å†™è¿™ç¯‡åšå®¢ä¹‹å‰ï¼Œæˆ‘åˆå»è¡¥äº†ä¸€ä¸‹çŸ¥è¯†ç‚¹ï¼Œå¯ä»¥çœ‹ä¸‹åšä¸»çš„è¿™ä¸¤ç¯‡ï¼š[ã€PyTorch ç†è®ºã€‘äº¤å‰ç†µæŸå¤±å‡½æ•°çš„ç†è§£](https://blog.csdn.net/qq_38253797/article/details/116225218)å’Œ[ã€PyTorchã€‘ä¸¤ç§å¸¸ç”¨çš„äº¤å‰ç†µæŸå¤±å‡½æ•°BCELosså’ŒBCEWithLogitsLoss](https://blog.csdn.net/qq_38253797/article/details/116193381)ã€‚**

**æŸå¤±å‡½æ•°æ€»ç»“ï¼š**

*   **äº¤å‰ç†µæŸå¤±å‡½æ•°**ï¼š L = âˆ’ [ y log â¡ y ^ + ( 1 âˆ’ y ) log â¡ ( 1 âˆ’ y ^ ) ] \mathrm{L}=-[\mathrm{y} \log \hat{\mathrm{y}}+(1-\mathrm{y}) \log (1-\hat{\mathrm{y}})] L=âˆ’[ylogy^â€‹+(1âˆ’y)log(1âˆ’y^â€‹)]
    é¢„æµ‹è¾“å‡ºè¶Šæ¥è¿‘çœŸæ˜¯æ ·æœ¬æ ‡ç­¾ï¼ŒæŸå¤±å‡½æ•°Lè¶Šå°ã€‚

    BCELosså’ŒBCEWithLogitsLossæ˜¯ä¸€ç»„å¸¸ç”¨çš„äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¸¸ç”¨äºäºŒåˆ†ç±»é—®é¢˜ã€‚åŒºåˆ«åœ¨äºBCELossçš„è¾“å…¥éœ€è¦å…ˆè¿›è¡ŒSigmoidå¤„ç†ï¼Œ**è€ŒBCEWithLogitsLossåˆ™æ˜¯å°†Sigmoidå’ŒBCELossåˆæˆä¸€æ­¥**ï¼Œä¹Ÿå°±æ˜¯è¯´BCEWithLogitsLosså‡½æ•°å†…éƒ¨è‡ªåŠ¨å…ˆå¯¹outputè¿›è¡ŒSigmoidå¤„ç†ï¼Œå†å¯¹outputå’Œtargetè¿›è¡ŒBCELossè®¡ç®—ã€‚

**å¯¼èˆª**ï¼š[YOLOv5-6.xæºç åˆ†æ å…¨æµç¨‹è®°å½•](https://blog.csdn.net/weixin_51322383/article/details/130353834?spm=1001.2014.3001.5502)

* * *

# ğŸš€YOLOv5-6.xæºç åˆ†æï¼ˆå…«ï¼‰---- loss.py

## 0\. å¯¼åŒ…

```py
import torch
import torch.nn as nn

from utils.metrics import bbox_iou
from utils.torch_utils import is_parallel 
```

## 1\. smooth_BCE

```py
def smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441
    # return positive, negative label smoothing BCE targets
    return 1.0 - 0.5 * eps, 0.5 * eps 
```

**è¿™æ®µä»£ç æ˜¯æ ‡ç­¾å¹³æ»‘çš„ç­–ç•¥ï¼ˆtrickï¼‰**ï¼Œç›®çš„æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

è¯¥å‡½æ•°å°†åŸæœ¬çš„æ­£è´Ÿæ ·æœ¬1å’Œ0ä¿®æ”¹ä¸º`1.0 - 0.5 * eps`,å’Œ`0.5 * eps`

åœ¨ComputeLossä¸­å®šä¹‰ï¼Œå¹¶åœ¨`__call__`ä¸­è°ƒç”¨
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/2a9383010282a2bff6c29cc7e65d1231.png)

## 2\. BCEBlurWithLogitsLoss

```py
class BCEBlurWithLogitsLoss(nn.Module):
    # BCEwithLogitLoss() with reduced missing label effects.
    def __init__(self, alpha=0.05):
        super().__init__()
        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()
        self.alpha = alpha

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)
        pred = torch.sigmoid(pred)  # prob from logits
        # dx = [-1, 1]  å½“pred=1 true=0æ—¶(ç½‘ç»œé¢„æµ‹è¯´è¿™é‡Œæœ‰ä¸ªobjä½†æ˜¯gtè¯´è¿™é‡Œæ²¡æœ‰), dx=1 => alpha_factor=0 => loss=0
        # è¿™ç§å°±æ˜¯æ£€æµ‹æˆæ­£æ ·æœ¬äº†ä½†æ˜¯æ£€æµ‹é”™äº†ï¼ˆfalse positiveï¼‰æˆ–è€…missing labelçš„æƒ…å†µ è¿™ç§æƒ…å†µä¸åº”è¯¥è¿‡å¤šçš„æƒ©ç½š->loss=0
        dx = pred - true  # reduce only missing label effects
        # å¦‚æœé‡‡æ ·ç»å¯¹å€¼çš„è¯ ä¼šå‡è½»predå’Œgtå·®å¼‚è¿‡å¤§è€Œé€ æˆçš„å½±å“
        # dx = (pred - true).abs()  # reduce missing label and false label effects
        alpha_factor = 1 - torch.exp((dx - 1) / (self.alpha + 1e-4))
        loss *= alpha_factor
        return loss.mean() 
```

è¿™æ®µä»£ç æ˜¯BCEå‡½æ•°çš„ä¸€ä¸ªæ›¿ä»£ï¼Œå¯ä»¥ç›´æ¥åœ¨ComputeLossç±»ä¸­çš„`__init__`ä¸­ä»£æ›¿ä¼ ç»Ÿçš„BCEå‡½æ•°

## 3\. FocalLoss

```py
class FocalLoss(nn.Module):
    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma  # å‚æ•°gamma  ç”¨äºå‰Šå¼±ç®€å•æ ·æœ¬å¯¹lossçš„è´¡çŒ®ç¨‹åº¦
        self.alpha = alpha  # å‚æ•°alpha  ç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬ä¸ªæ•°ä¸å‡è¡¡çš„é—®é¢˜
        self.reduction = loss_fcn.reduction
        # focallossä¸­çš„BCEå‡½æ•°çš„reduction='None'  BCEä¸ä½¿ç”¨Sumæˆ–è€…Mean
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)    # æ­£å¸¸BCEçš„loss:   loss = -log(p_t)
        # p_t = torch.exp(-loss)
        # loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability

        # TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py
        pred_prob = torch.sigmoid(pred)  # prob from logits
        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = (1.0 - p_t) ** self.gamma
        # å…¬å¼å†…å®¹
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss 
```

è¿™ä¸ªæŸå¤±å‡½æ•°çš„ä¸»è¦æ€è·¯æ˜¯ï¼š**å¸Œæœ›é‚£äº›hard exampleså¯¹æŸå¤±çš„è´¡çŒ®å˜å¤§ï¼Œä½¿ç½‘ç»œæ›´å€¾å‘äºä»è¿™äº›æ ·æœ¬ä¸Šå­¦ä¹ ã€‚é˜²æ­¢ç”±äºeasy examplesè¿‡å¤šï¼Œä¸»å¯¼æ•´ä¸ªæŸå¤±å‡½æ•°ã€‚**

ä¼˜ç‚¹ï¼š

1.  è§£å†³äº†one-stage object detectionä¸­å›¾ç‰‡ä¸­æ­£è´Ÿæ ·æœ¬ï¼ˆå‰æ™¯å’ŒèƒŒæ™¯ï¼‰ä¸å‡è¡¡çš„é—®é¢˜ï¼›
2.  é™ä½ç®€å•æ ·æœ¬çš„æƒé‡ï¼Œä½¿æŸå¤±å‡½æ•°æ›´å…³æ³¨å›°éš¾æ ·æœ¬ï¼›

åŒæ ·åœ¨ComputeLossä¸­ç”¨æ¥ä»£æ›¿åŸæœ¬çš„BCEclså’ŒBCEobj
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/6fcbd9debcb4e070ab3fdb38da24375e.png)

## 4\. QFocalLoss

```py
class QFocalLoss(nn.Module):
    # Wraps Quality focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = loss_fcn.reduction
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)

        pred_prob = torch.sigmoid(pred)  # prob from logits
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = torch.abs(true - pred_prob) ** self.gamma
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss 
```

**ç”¨æ¥ä»£æ›¿FocalLossï¼Œå¯ä»¥ç›´æ¥åœ¨`__init__`ä¸­æ›¿æ¢**

## 5\. ComputeLoss

### 5.1 __init__å‡½æ•°

```py
class ComputeLoss:
    sort_obj_iou = False

    # Compute losses
    def __init__(self, model, autobalance=False):
        device = next(model.parameters()).device  # get model device
        h = model.hyp  # hyperparameters

        # Define criteria
        # å®šä¹‰åˆ†ç±»æŸå¤±å’Œç½®ä¿¡åº¦æŸå¤±
        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device))
        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device))

        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3
        # æ ‡ç­¾å¹³æ»‘å¤„ç†ï¼Œcpä»£è¡¨positiveçš„æ ‡ç­¾å€¼ï¼Œcnä»£è¡¨negativeçš„æ ‡ç­¾å€¼
        self.cp, self.cn = smooth_BCE(eps=h.get('label_smoothing', 0.0))  # positive, negative BCE targets

        # Focal loss    g=0ï¼Œä»£è¡¨ä¸ç”¨focal loss
        g = h['fl_gamma']  # focal loss gamma
        if g > 0:
            # g > 0, å°†åˆ†ç±»æŸå¤±å’Œç½®ä¿¡åº¦æŸå¤±(BCE)éƒ½æ¢æˆfocallossæŸå¤±å‡½æ•°
            BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)
            # BCEcls, BCEobj = QFocalLoss(BCEcls, g), QFocalLoss(BCEobj, g)

        # è¿”å›çš„æ˜¯æ¨¡å‹çš„æ£€æµ‹å¤´ Detector 3ä¸ª åˆ†åˆ«å¯¹åº”äº§ç”Ÿä¸‰ä¸ªè¾“å‡ºfeature map
        m = de_parallel(model).model[-1]  # Detect() module
        # balanceç”¨æ¥è®¾ç½®ä¸‰ä¸ªfeature mapå¯¹åº”è¾“å‡ºçš„ç½®ä¿¡åº¦æŸå¤±ç³»æ•°(å¹³è¡¡ä¸‰ä¸ªfeature mapçš„ç½®ä¿¡åº¦æŸå¤±)
        self.balance = {3: [4.0, 1.0, 0.4]}.get(m.nl, [4.0, 1.0, 0.25, 0.06, 0.02])  # P3-P7
        # ä¸‰ä¸ªé¢„æµ‹å¤´çš„ä¸‹é‡‡æ ·ç‡m.stride: [8, 16, 32]  .index(16): æ±‚å‡ºä¸‹é‡‡æ ·ç‡stride=16çš„ç´¢å¼•
        # è¿™ä¸ªå‚æ•°ä¼šç”¨æ¥è‡ªåŠ¨è®¡ç®—æ›´æ–°3ä¸ªfeature mapçš„ç½®ä¿¡åº¦æŸå¤±ç³»æ•°self.balance
        self.ssi = list(m.stride).index(16) if autobalance else 0  # stride 16 index
        # self.gr: è®¡ç®—çœŸå®æ¡†çš„ç½®ä¿¡åº¦æ ‡å‡†çš„iou ratio    self.autobalance: æ˜¯å¦è‡ªåŠ¨æ›´æ–°å„feature mapçš„ç½®ä¿¡åº¦æŸå¤±å¹³è¡¡ç³»æ•°  é»˜è®¤False
        self.BCEcls, self.BCEobj, self.gr, self.hyp, self.autobalance = BCEcls, BCEobj, 1.0, h, autobalance
        self.na = m.na  # number of anchors 3ä¸ª
        self.nc = m.nc  # number of classes ç±»åˆ«æ•°
        self.nl = m.nl  # number of layers
        self.anchors = m.anchors
        self.device = device 
```

**è¿™éƒ¨åˆ†å°±æ˜¯å®šä¹‰äº†ä¸€äº›åé¢è¦ä½¿ç”¨çš„å˜é‡ã€‚**

### 5.2 build_targets

```py
 def build_targets(self, p, targets):
        # p: ç½‘ç»œè¾“å‡ºï¼›targetsï¼šGTæ¡†ï¼›
        # Build targets for compute_loss(), input targets(image,class,x,y,w,h)
        na, nt = self.na, targets.shape[0]  # number of anchors, targets
        tcls, tbox, indices, anch = [], [], [], [] # åˆå§‹åŒ–
        # gainæ˜¯ä¸ºäº†åé¢å°†targets=[na,nt,7]ä¸­çš„å½’ä¸€åŒ–äº†çš„xywhæ˜ å°„åˆ°ç›¸å¯¹feature mapå°ºåº¦ä¸Š
        # 7: image_index+class+xywh+anchor_index
        gain = torch.ones(7, device=self.device)  # normalized to gridspace gain
        # anchorç´¢å¼•ï¼Œåé¢æœ‰ç”¨ï¼Œç”¨æ¥è¡¨ç¤ºå½“å‰bboxå’Œå½“å‰å±‚çš„å“ªä¸ªanchoråŒ¹é…
        ai = torch.arange(na, device=self.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)
        # å…ˆrepeat targetså’Œå½“å‰å±‚anchorä¸ªæ•°ä¸€æ ·ï¼Œç›¸å½“äºæ¯ä¸ªbboxå˜æˆäº†3ä¸ªï¼Œç„¶åå’Œ3ä¸ªanchorå•ç‹¬åŒ¹é…
        targets = torch.cat((targets.repeat(na, 1, 1), ai[..., None]), 2)  # append anchor indices

        # è¿™ä¸¤ä¸ªå˜é‡æ˜¯ç”¨æ¥æ‰©å±•æ­£æ ·æœ¬çš„ å› ä¸ºé¢„æµ‹æ¡†é¢„æµ‹åˆ°targetæœ‰å¯èƒ½ä¸æ­¢å½“å‰çš„æ ¼å­é¢„æµ‹åˆ°äº†
        # å¯èƒ½å‘¨å›´çš„æ ¼å­ä¹Ÿé¢„æµ‹åˆ°äº†é«˜è´¨é‡çš„æ ·æœ¬ æˆ‘ä»¬ä¹Ÿè¦æŠŠè¿™éƒ¨åˆ†çš„é¢„æµ‹ä¿¡æ¯åŠ å…¥æ­£æ ·æœ¬ä¸­
        # è®¾ç½®ç½‘ç»œä¸­å¿ƒåç§»é‡
        g = 0.5  # bias ç”¨æ¥è¡¡é‡targetä¸­å¿ƒç‚¹ç¦»å“ªä¸ªæ ¼å­è¿‘
        # é™„è¿‘4ä¸ªç½‘æ ¼
        off = torch.tensor(
            [
                [0, 0],
                [1, 0],
                [0, 1],
                [-1, 0],
                [0, -1],  # j,k,l,m
                # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm    æ–œæ–¹å‘
            ],
            device=self.device).float() * g  # offsets

        # å¯¹æ¯ä¸ªæ£€æµ‹å±‚è¿›è¡Œå¤„ç†
        for i in range(self.nl):    # ä¸‰ä¸ªå°ºåº¦çš„é¢„æµ‹ç‰¹å¾å›¾è¾“å‡ºåˆ†æ”¯
            # å½“å‰feature mapå¯¹åº”çš„ä¸‰ä¸ªanchorå°ºå¯¸
            anchors, shape = self.anchors[i], p[i].shape
            # [1, 1, 1, 1, 1, 1, 1] -> [1, 1, 112, 112, 112,112, 1]=image_index+class+xywh+anchor_index
            gain[2:6] = torch.tensor(shape)[[3, 2, 3, 2]]  # xyxy gain

            # Match targets to anchors
            t = targets * gain  # shape(3,n,7)

            if nt:  # å¼€å§‹åŒ¹é…
                # Matches
                r = t[..., 4:6] / anchors[:, None]  # wh ratio

                # ç­›é€‰æ¡ä»¶  GTä¸anchorçš„å®½æ¯”æˆ–é«˜æ¯”è¶…è¿‡ä¸€å®šçš„é˜ˆå€¼ å°±å½“ä½œè´Ÿæ ·æœ¬

                # ç­›é€‰å‡ºå®½æ¯”w1/w2 w2/w1 é«˜æ¯”h1/h2 h2/h1ä¸­æœ€å¤§çš„é‚£ä¸ª
                # .max(2)è¿”å›å®½æ¯” é«˜æ¯”ä¸¤è€…ä¸­è¾ƒå¤§çš„ä¸€ä¸ªå€¼å’Œå®ƒçš„ç´¢å¼•  [0]è¿”å›è¾ƒå¤§çš„ä¸€ä¸ªå€¼
                # j: [3, 63]  False: å½“å‰anchoræ˜¯å½“å‰gtçš„è´Ÿæ ·æœ¬  True: å½“å‰anchoræ˜¯å½“å‰gtçš„æ­£æ ·æœ¬
                j = torch.max(r, 1 / r).max(2)[0] < self.hyp['anchor_t']  # compare
                # j = wh_iou(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))
                t = t[j]  # filter

                # Offsets ç­›é€‰å½“å‰æ ¼å­å‘¨å›´æ ¼å­ æ‰¾åˆ°2ä¸ªç¦»targetä¸­å¿ƒæœ€è¿‘çš„ä¸¤ä¸ªæ ¼å­  å¯èƒ½å‘¨å›´çš„æ ¼å­ä¹Ÿé¢„æµ‹åˆ°äº†é«˜è´¨é‡çš„æ ·æœ¬ æˆ‘ä»¬ä¹Ÿè¦æŠŠè¿™éƒ¨åˆ†çš„é¢„æµ‹ä¿¡æ¯åŠ å…¥æ­£æ ·æœ¬ä¸­
                # é™¤äº†targetæ‰€åœ¨çš„å½“å‰æ ¼å­å¤–, è¿˜æœ‰2ä¸ªæ ¼å­å¯¹ç›®æ ‡è¿›è¡Œæ£€æµ‹(è®¡ç®—æŸå¤±) ä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªç›®æ ‡éœ€è¦3ä¸ªæ ¼å­å»é¢„æµ‹(è®¡ç®—æŸå¤±)
                # é¦–å…ˆå½“å‰æ ¼å­æ˜¯å…¶ä¸­1ä¸ª å†ä»å½“å‰æ ¼å­çš„ä¸Šä¸‹å·¦å³å››ä¸ªæ ¼å­ä¸­é€‰æ‹©2ä¸ª ç”¨è¿™ä¸‰ä¸ªæ ¼å­å»é¢„æµ‹è¿™ä¸ªç›®æ ‡(è®¡ç®—æŸå¤±)
                # feature mapä¸Šçš„åŸç‚¹åœ¨å·¦ä¸Šè§’ å‘å³ä¸ºxè½´æ­£åæ ‡ å‘ä¸‹ä¸ºyè½´æ­£åæ ‡
                gxy = t[:, 2:4]  # grid xy
                gxi = gain[[2, 3]] - gxy  # inverse
                # ç­›é€‰ä¸­å¿ƒåæ ‡ è·ç¦»å½“å‰grid_cellçš„å·¦ã€ä¸Šæ–¹åç§»å°äºg=0.5 ä¸” ä¸­å¿ƒåæ ‡å¿…é¡»å¤§äº1(åæ ‡ä¸èƒ½åœ¨è¾¹ä¸Š æ­¤æ—¶å°±æ²¡æœ‰4ä¸ªæ ¼å­äº†)
                # j: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„å·¦è¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                # k: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„ä¸Šè¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                j, k = ((gxy % 1 < g) & (gxy > 1)).T
                # ç­›é€‰ä¸­å¿ƒåæ ‡ è·ç¦»å½“å‰grid_cellçš„å³ã€ä¸‹æ–¹åç§»å°äºg=0.5 ä¸” ä¸­å¿ƒåæ ‡å¿…é¡»å¤§äº1(åæ ‡ä¸èƒ½åœ¨è¾¹ä¸Š æ­¤æ—¶å°±æ²¡æœ‰4ä¸ªæ ¼å­äº†)
                # l: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„å³è¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                # m: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„ä¸‹è¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                l, m = ((gxi % 1 < g) & (gxi > 1)).T
                # j: [5, 126]  torch.ones_like(j): å½“å‰æ ¼å­, ä¸éœ€è¦ç­›é€‰å…¨æ˜¯True  j, k, l, m: å·¦ä¸Šå³ä¸‹æ ¼å­çš„ç­›é€‰ç»“æœ
                j = torch.stack((torch.ones_like(j), j, k, l, m))
                # å¾—åˆ°ç­›é€‰åæ‰€æœ‰æ ¼å­çš„æ­£æ ·æœ¬ æ ¼å­æ•°<=3*126 éƒ½ä¸åœ¨è¾¹ä¸Šç­‰å·æˆç«‹
                # t: [126, 7] -> å¤åˆ¶5ä»½target[5, 126, 7]  åˆ†åˆ«å¯¹åº”å½“å‰æ ¼å­å’Œå·¦ä¸Šå³ä¸‹æ ¼å­5ä¸ªæ ¼å­
                # j: [5, 126] + t: [5, 126, 7] => t: [378, 7] ç†è®ºä¸Šæ˜¯å°äºç­‰äº3å€çš„126 å½“ä¸”ä»…å½“æ²¡æœ‰è¾¹ç•Œçš„æ ¼å­ç­‰å·æˆç«‹
                t = t.repeat((5, 1, 1))[j]
                # æ·»åŠ åç§»é‡
                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]
            else:
                t = targets[0]
                offsets = 0

            # Define
            bc, gxy, gwh, a = t.chunk(4, 1)  # (image, class), grid xy, grid wh, anchors
            a, (b, c) = a.long().view(-1), bc.long().T  # anchors, image, class
            gij = (gxy - offsets).long()    # é¢„æµ‹çœŸå®æ¡†çš„ç½‘æ ¼æ‰€åœ¨çš„å·¦ä¸Šè§’åæ ‡(æœ‰å·¦ä¸Šå³ä¸‹çš„ç½‘æ ¼)
            gi, gj = gij.T  # grid indices

            # Append
            # gj: ç½‘æ ¼çš„å·¦ä¸Šè§’yåæ ‡  gi: ç½‘æ ¼çš„å·¦ä¸Šè§’xåæ ‡
            indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid
            tbox.append(torch.cat((gxy - gij, gwh), 1))  # box
            anch.append(anchors[a])  # anchors
            tcls.append(c)  # class

        return tcls, tbox, indices, anch 
```

**è¿™æ®µä»£ç ä¸»è¦æ˜¯å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰å›¾ç‰‡çš„targetsï¼Œå°†é¢„æµ‹çš„æ ¼å¼è½¬åŒ–ä¸ºä¾¿äºè®¡ç®—lossçš„targetæ ¼å¼ã€‚ç­›é€‰æ¡ä»¶æ˜¯æ¯”è¾ƒGTå’Œanchorçš„å®½æ¯”å’Œé«˜æ¯”ï¼Œå¤§äºä¸€å®šçš„é˜ˆå€¼å°±æ˜¯è´Ÿæ ·æœ¬ï¼Œåä¹‹æ­£æ ·æœ¬ã€‚**

**ä½œç”¨ï¼šç”¨äºç½‘ç»œè®­ç»ƒæ—¶è®¡ç®—lossæ‰€éœ€è¦çš„ç›®æ ‡æ¡†ï¼Œå³æ­£æ ·æœ¬ã€‚**

ç­›é€‰åˆ°çš„æ­£æ ·æœ¬ä¿¡æ¯ï¼ˆimage_index, anchor_index, gridy, gridxï¼‰ï¼Œä¼ å…¥__call__å‡½æ•°ï¼Œé€šè¿‡è¿™ä¸ªä¿¡æ¯å»ç­›é€‰predæ¯ä¸ªgridé¢„æµ‹å¾—åˆ°çš„ä¿¡æ¯ï¼Œä¿ç•™å¯¹åº”grid_cellä¸Šçš„æ­£æ ·æœ¬ã€‚é€šè¿‡build_targetsç­›é€‰çš„GTä¸­çš„æ­£æ ·æœ¬å’Œpredç­›é€‰å‡ºçš„å¯¹åº”ä½ç½®çš„é¢„æµ‹æ ·æœ¬è¿›è¡Œè®¡ç®—æŸå¤±ã€‚

ä¸ºä»€ä¹ˆåŸå›¾ä¸Šå½’ä¸€åŒ–çš„æ¡†ç‰¹å¾å›¾çš„å¤§å°å°±æ˜¯ç‰¹å¾å›¾ä¸Šçš„åæ ‡äº†å‘¢ï¼Ÿ

å³è¿™è¡Œä»£ç ï¼š`t = targets*gain` ï¼Œå…·ä½“å¯çœ‹è¿™ç¯‡åšæ–‡[åšå®¢](https://blog.csdn.net/qq_21539375/article/details/118345636)
å¯¹äºä¸‹é‡‡æ ·32å€çš„ç‰¹å¾å›¾æ¥è¯´ï¼Œæ¯ä¸€ä¸ªæ ¼å­å¯¹åº”ç€åŸå›¾ä¸Š( h / 32 , w / 32 ) (h/32,w/32)(h/32,w/32)çš„å¤§å°,å…¶ä¸­h,wæ˜¯åŸå›¾çš„é«˜å’Œå®½

### 5.3 __call__å‡½æ•°

```py
 def __call__(self, p, targets):  # predictions, targets
    	'''
            :params targets: æ•°æ®å¢å¼ºåçš„çœŸå®æ¡† [num_object,  batch_index+class+xywh] æˆ‘è¿™é‡Œçš„æ•°æ®æ˜¯[35,6]
            :params loss * bs: æ•´ä¸ªbatchçš„æ€»æŸå¤±  è¿›è¡Œåå‘ä¼ æ’­
            :params torch.cat((lbox, lobj, lcls, loss)).detach(): å›å½’æŸå¤±ã€ç½®ä¿¡åº¦æŸå¤±ã€åˆ†ç±»æŸå¤±å’Œæ€»æŸå¤± è¿™ä¸ªå‚æ•°åªç”¨æ¥å¯è§†åŒ–å‚æ•°æˆ–ä¿å­˜ä¿¡æ¯
        '''
        # åˆå§‹åŒ–ä¸‰ç§æŸå¤±
        lcls = torch.zeros(1, device=self.device)  # class loss
        lbox = torch.zeros(1, device=self.device)  # box loss
        lobj = torch.zeros(1, device=self.device)  # object loss

        # tcls: è¡¨ç¤ºè¿™ä¸ªtargetæ‰€å±çš„class index
        # tbox: xywh å…¶ä¸­xyä¸ºè¿™ä¸ªtargetå¯¹å½“å‰grid_cellå·¦ä¸Šè§’çš„åç§»é‡
        # indices: b: è¡¨ç¤ºè¿™ä¸ªtargetå±äºçš„image index
        #          a: è¡¨ç¤ºè¿™ä¸ªtargetä½¿ç”¨çš„anchor index
        #          gj: ç»è¿‡ç­›é€‰åç¡®å®šæŸä¸ªtargetåœ¨æŸä¸ªç½‘æ ¼ä¸­è¿›è¡Œé¢„æµ‹(è®¡ç®—æŸå¤±)  gjè¡¨ç¤ºè¿™ä¸ªç½‘æ ¼çš„å·¦ä¸Šè§’yåæ ‡
        #          gi: è¡¨ç¤ºè¿™ä¸ªç½‘æ ¼çš„å·¦ä¸Šè§’xåæ ‡
        # anch: è¡¨ç¤ºè¿™ä¸ªtargetæ‰€ä½¿ç”¨anchorçš„å°ºåº¦ï¼ˆç›¸å¯¹äºè¿™ä¸ªfeature mapï¼‰  æ³¨æ„å¯èƒ½ä¸€ä¸ªtargetä¼šä½¿ç”¨å¤§å°ä¸åŒanchorè¿›è¡Œè®¡ç®—
        tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets

        # Losses
        for i, pi in enumerate(p):  # layer index, layer predictions
            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx
            tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj

            n = b.shape[0]  # number of targets
            if n:
                # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0
                pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions

                # Regression
                pxy = pxy.sigmoid() * 2 - 0.5
                pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i]
                pbox = torch.cat((pxy, pwh), 1)  # predicted box
                iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target)
                lbox += (1.0 - iou).mean()  # iou loss

                # Objectness    åªè®¡ç®—æ‰€æœ‰æ­£æ ·æœ¬çš„å›å½’æŸå¤±
                iou = iou.detach().clamp(0).type(tobj.dtype)
                if self.sort_obj_iou:
                    j = iou.argsort()
                    # æ’åºä¹‹å å¦‚æœåŒä¸€ä¸ªgridå‡ºç°ä¸¤ä¸ªgt é‚£ä¹ˆæˆ‘ä»¬ç»è¿‡æ’åºä¹‹åæ¯ä¸ªgridä¸­çš„score_iouéƒ½èƒ½ä¿è¯æ˜¯æœ€å¤§çš„
                    # (å°çš„ä¼šè¢«è¦†ç›– å› ä¸ºåŒä¸€ä¸ªgridåæ ‡è‚¯å®šç›¸åŒ)é‚£ä¹ˆä»æ—¶é—´é¡ºåºçš„è¯, æœ€å1ä¸ªæ€»æ˜¯å’Œæœ€å¤§çš„IOUå»è®¡ç®—LOSS, æ¢¯åº¦ä¼ æ’­
                    b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]
                if self.gr < 1:
                    iou = (1.0 - self.gr) + self.gr * iou
                # é¢„æµ‹ä¿¡æ¯æœ‰ç½®ä¿¡åº¦ ä½†æ˜¯çœŸå®æ¡†ä¿¡æ¯æ˜¯æ²¡æœ‰ç½®ä¿¡åº¦çš„ æ‰€ä»¥éœ€è¦æˆ‘ä»¬äººä¸ºçš„ç»™ä¸€ä¸ªæ ‡å‡†ç½®ä¿¡åº¦
                # self.græ˜¯iou ratio [0, 1]  self.grè¶Šå¤§ç½®ä¿¡åº¦è¶Šæ¥è¿‘iou  self.grè¶Šå°ç½®ä¿¡åº¦è¶Šæ¥è¿‘1(äººä¸ºåŠ å¤§è®­ç»ƒéš¾åº¦)
                tobj[b, a, gj, gi] = iou  # iou ratio

                # Classification    åªè®¡ç®—æ‰€æœ‰æ­£æ ·æœ¬çš„åˆ†ç±»æŸå¤±
                if self.nc > 1:  # cls loss (only if multiple classes)
                    t = torch.full_like(pcls, self.cn, device=self.device)  # targets
                    t[range(n), tcls[i]] = self.cp
                    lcls += self.BCEcls(pcls, t)  # BCE

                # Append targets to text file
                # with open('targets.txt', 'a') as file:
                #     [file.write('%11.5g ' * 4 % tuple(x) + '\n') for x in torch.cat((txy[i], twh[i]), 1)]

            obji = self.BCEobj(pi[..., 4], tobj)
            # æ¯ä¸ªfeature mapçš„ç½®ä¿¡åº¦æŸå¤±æƒé‡ä¸åŒ  è¦ä¹˜ä»¥ç›¸åº”çš„æƒé‡ç³»æ•°self.balance[i]
            # ä¸€èˆ¬æ¥è¯´ï¼Œæ£€æµ‹å°ç‰©ä½“çš„éš¾åº¦å¤§ä¸€ç‚¹ï¼Œæ‰€ä»¥ä¼šå¢åŠ å¤§ç‰¹å¾å›¾çš„æŸå¤±ç³»æ•°ï¼Œè®©æ¨¡å‹æ›´åŠ ä¾§é‡å°ç‰©ä½“çš„æ£€æµ‹
            lobj += obji * self.balance[i]  # obj loss
            if self.autobalance:
                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()

        if self.autobalance:
            self.balance = [x / self.balance[self.ssi] for x in self.balance]
        lbox *= self.hyp['box']
        lobj *= self.hyp['obj']
        lcls *= self.hyp['cls']
        bs = tobj.shape[0]  # batch size

        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach() 
```

**åˆ†åˆ«è®¡ç®—äº†ä¸‰ç±»æŸå¤±ï¼Œåœ¨train.pyä¸­è°ƒç”¨è¿”å›**

```py
loss, loss_items = compute_loss(pred, targets.to(device)) 
```

* * *

# è¡¥å……

åˆå­¦äº†ä¸€élossï¼Œè¿™æ¬¡ç®—æ˜¯çœŸçš„å­¦é€šäº†ï¼ï¼ï¼

## åˆ†ç±»æŸå¤±ï¼ˆClassificationï¼‰

åˆ†ç±»æŸå¤±é‡‡ç”¨çš„æ˜¯nn.BCEWithLogitsLossï¼Œå³äºŒåˆ†ç±»æŸå¤±ï¼Œä½ æ²¡å¬é”™ï¼Œå°±æ˜¯ç”¨çš„äºŒåˆ†ç±»æŸå¤±ï¼Œæ¯”å¦‚ç°åœ¨æœ‰4ä¸ªåˆ†ç±»ï¼šçŒ«ã€ç‹—ã€çŒªã€é¸¡ï¼Œå½“å‰æ ‡ç­¾çœŸå€¼ä¸ºçŒªï¼Œé‚£ä¹ˆè®¡ç®—æŸå¤±çš„æ—¶å€™ï¼Œtargetså°±æ˜¯[0, 0, 1, 0]ã€‚

æŒ‰ç…§640ä¹˜640åˆ†è¾¨ç‡ï¼Œ3ä¸ªè¾“å‡ºå±‚æ¥ç®—çš„è¯ï¼ŒP3æ˜¯80ä¹˜80ä¸ªæ ¼å­ï¼ŒP4æ˜¯40ä¹˜40ï¼ŒP5æ˜¯20ä¹˜20ï¼Œä¸€å…±æœ‰8400ä¸ªæ ¼å­ï¼Œå¹¶ä¸æ˜¯æ¯ä¸€ä¸ªæ ¼å­ä¸Šçš„è¾“å‡ºéƒ½è¦å»åšåˆ†ç±»æŸå¤±è®¡ç®—çš„ï¼Œ**åªæœ‰è´Ÿè´£é¢„æµ‹å¯¹åº”ç‰©ä½“çš„æ ¼å­æ‰éœ€è¦åšåˆ†ç±»æŸå¤±è®¡ç®—ï¼ˆè¾¹æ¡†æŸå¤±è®¡ç®—ä¹Ÿæ˜¯ä¸€æ ·ï¼‰**ã€‚è‡³äºå“ªäº›æ ¼å­æ‰ä¼šè´Ÿè´£å»é¢„æµ‹å¯¹åº”çš„ç‰©ä½“ï¼Œè¿™ä¸ªé€»è¾‘ä¸‹é¢å†è¯´ã€‚

## ç½®ä¿¡åº¦æŸå¤±ï¼ˆObjectnessï¼‰

ç½®ä¿¡åº¦æŸå¤±å°±æ˜¯è·ŸIOUæŒ‚é’©çš„ã€‚

ç½®ä¿¡åº¦æŸå¤±åŒæ ·ä¹Ÿæ˜¯BCEWithLogitsLossï¼Œä¸è¿‡**ç½®ä¿¡åº¦æ˜¯æ¯ä¸€ä¸ªæ ¼å­éƒ½è¦åšæŸå¤±è®¡ç®—çš„**ï¼Œå› ä¸ºæœ€ç»ˆåœ¨ä½¿ç”¨çš„æ—¶å€™æˆ‘ä»¬é¦–å…ˆå°±æ˜¯ç”±ç½®ä¿¡åº¦é˜ˆå€¼æ¥åˆ¤æ–­å¯¹åº”æ ¼å­çš„è¾“å‡ºæ˜¯ä¸æ˜¯å¯ä¿¡çš„ã€‚**ç½®ä¿¡åº¦çš„çœŸå€¼å¹¶ä¸æ˜¯å›ºå®šçš„ï¼Œå¦‚æœè¯¥æ ¼å­è´Ÿè´£é¢„æµ‹å¯¹åº”çš„ç‰©ä½“ï¼Œé‚£ä¹ˆç½®ä¿¡åº¦çœŸå€¼å°±æ˜¯é¢„æµ‹è¾¹æ¡†ä¸æ ‡ç­¾è¾¹æ¡†çš„IOUã€‚å¦‚æœä¸è´Ÿè´£é¢„æµ‹ä»»ä½•ç‰©ä½“ï¼Œé‚£çœŸå€¼å°±æ˜¯0ã€‚**
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/ab8f16c4bbf810deee936887af03473e.png)

## è¾¹æ¡†æŸå¤±ï¼ˆRegressionï¼‰

æ ·æœ¬åˆ†é…æ˜¯åœ¨ç½‘ç»œæœ€åè¾“å‡ºçš„ä¸‰ä¸ªä¸åŒä¸‹é‡‡æ ·å€æ•°çš„ç‰¹å¾å›¾ä¸Šé€å±‚è¿›è¡Œçš„ï¼š

*   é¦–å…ˆå°†å½’ä¸€åŒ–çš„gtæ˜ å°„åˆ°ç‰¹å¾å›¾å¯¹åº”çš„å¤§å°ï¼›
*   åˆ†åˆ«è®¡ç®—gtä¸è¯¥å°ºåº¦ç‰¹å¾å›¾ä¸Šé¢„è®¾çš„ä¸‰ä¸ªä¸åŒå¤§å°çš„anchorçš„å®½é«˜æ¯”å¹¶åˆ¤æ–­æ˜¯å¦æ»¡è¶³ï¼š1/thr < ratio <thrï¼Œå¦‚æœæ»¡è¶³è¯´æ˜è¿™ä¸ªgtä¸anchorå°ºå¯¸åŒ¹é…ï¼Œæ¥ä¸‹æ¥ä¼šè¿›ä¸€æ­¥ä¸ºå…¶åˆ†é…æ­£æ ·æœ¬ï¼›ä¸æ»¡è¶³åˆ™è¯´æ˜è¿™ä¸ªgtä¸è¿™ä¸ªanchorå°ºå¯¸ä¸åŒ¹é…ï¼Œä¸ä¼šä¸ºå…¶åŒ¹é…å¯¹åº”anchorçš„æ­£æ ·æœ¬ã€‚å‡è®¾æˆ‘ä»¬æœ‰mä¸ªæ ‡æ³¨çš„çœŸå®è¾¹ç•Œæ¡†gtï¼Œé‚£ä¹ˆä¸€å±‚ç‰¹å¾å›¾ä¸Šç†è®ºæœ€å¤šä¼šæœ‰3*må¯¹åŒ¹é…æˆåŠŸçš„gt-anchorï¼ˆå› ä¸ºYOLOv3&v5ä¸­æ¯ä¸ªæ ¼ç‚¹å¯¹åº”3ä¸ªanchorï¼‰ï¼›

æ¥ä¸‹æ¥å°±æ˜¯anchoråœ¨æ¨¡å‹ä¸­çš„åº”ç”¨äº†ã€‚è¿™å°±æ¶‰åŠåˆ°äº†yoloç³»åˆ—ç›®æ ‡æ¡†å›å½’çš„è¿‡ç¨‹äº†ã€‚

è¾¹æ¡†æŸå¤±ç”±é¢„æµ‹è¾¹æ¡†ä¸æ ‡ç­¾è¾¹æ¡†çš„IOUæ¥å®šï¼ŒIOUè¶Šå¤§ï¼ŒæŸå¤±è‡ªç„¶è¶Šå°ï¼ŒIOUå¦‚æœæ˜¯1ï¼ŒæŸå¤±å°±æ˜¯0ï¼ŒIOUå¦‚æœæ˜¯0ï¼ŒæŸå¤±å°±è¶Šå¤§ï¼Œä¸Šé™å®šä¸º1ï¼Œæ‰€ä»¥è¾¹æ¡†æŸå¤±å°±æ˜¯1-IOUã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/95595490335aed9fddee7d30717a4419.png)

åœ¨è®¡ç®—box IOUæŸå¤±æ—¶ï¼Œç”¨çš„æ˜¯è¿™ä¸ªå…¬å¼ï¼š![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/9eac2d6da92a6bc0d978e147c68debf2.png)

å¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬æ£€æµ‹åˆ°çš„ä¸æ˜¯æ¡†ï¼Œè€Œæ˜¯åç§»é‡ã€‚å¾—åˆ°bx,by,bw,bhå°±æ˜¯æœ€ç»ˆçš„æ£€æµ‹ç»“æœã€‚

å…¶ä¸­ï¼Œtxã€tyã€twã€thä¸ºæ¨¡å‹é¢„æµ‹è¾“å‡ºï¼Œbxã€byã€bwã€bhä¸ºæœ€ç»ˆé¢„æµ‹ç›®æ ‡è¾¹æ¡†ä¸­å¿ƒç‚¹ï¼Œå®½é«˜ã€‚

> å…³äºanchorï¼Œçœ‹è¿™é‡Œï¼š[YOLO v2](https://blog.csdn.net/u012655441/article/details/108042286#t5)ã€‚anchorä¸»è¦æ˜¯å¯ä»¥åŠ é€Ÿè®­ç»ƒï¼ˆä»ç›´æ¥é¢„æµ‹ä½ç½®å˜ä¸ºé¢„æµ‹åç§»é‡ï¼‰

**è¿‡ç¨‹æ€»ç»“**

1.  **é¦–å…ˆé€šè¿‡gtä¸å½“å‰å±‚anchoråšä¸€éè¿‡æ»¤**ã€‚å¯¹äºä»»ä½•ä¸€å±‚è®¡ç®—å½“å‰gtä¸å½“å‰å±‚anchorçš„åŒ¹é…ç¨‹åº¦ï¼Œä¸é‡‡ç”¨IoUï¼Œè€Œé‡‡ç”¨shapeæ¯”ä¾‹ã€‚å¦‚æœanchorä¸gtçš„å®½é«˜æ¯”å·®è·å¤§äº4ï¼Œåˆ™è®¤ä¸ºä¸åŒ¹é…ï¼Œä¿ç•™ä¸‹åŒ¹é…çš„anchorã€‚ï¼ˆå®é™…ä¸ŠæŠŠæŠŠæ ‡ç­¾é‡å¤3æ¬¡ï¼Œæ ‡ç­¾æ–°å¢ä¸€åˆ—ï¼‰
2.  **æœ€åæ ¹æ®ç•™ä¸‹çš„bboxï¼Œåœ¨ä¸Šä¸‹å·¦å³å››ä¸ªç½‘æ ¼å››ä¸ªæ–¹å‘æ‰©å¢é‡‡æ ·ã€‚**
3.  åªæœ‰æœ€åç•™ä¸‹æ¥çš„bboxï¼Œæ‰ä¼šå»è¿›è¡Œä¸Šæ–¹å…¬å¼çš„è®¡ç®—ï¼ˆencodeï¼‰ã€‚æˆ‘ä»¬å°†è¿™ä¸ªanchorå’Œä»–å¯¹åº”çš„gtåšåå·®ï¼Œè¿›è¡Œencodingï¼Œå¾—åˆ°targetï¼Œå†å°†predåšencodingï¼Œè¿™äºŒè€…æ”¾å…¥æŸå¤±å‡½æ•°ä¸­è¿›è¡Œlossè®¡ç®—ï¼Œä½¿predé€æ¸è¶‹äºanchorï¼Œè¿™é‡Œçš„æŸå¤±å‡½æ•°ç”¨çš„1-CIoUã€‚åœ¨åé¢çš„ç½®ä¿¡åº¦æŸå¤±ä¹Ÿç”¨åˆ°äº†è¿™ä¸ªCIoUï¼Œä¸è¿‡ä»–ç”¨çš„æŸå¤±å‡½æ•°æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

```py
iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target)
lbox += (1.0 - iou).mean()  # iou loss 
```

> [MMDetectionç§»æ¤yolov5â€”â€”(äºŒ)å‰å‘æ¨ç†](https://zhuanlan.zhihu.com/p/599643109)

é™„ä¸€å¼ æµç¨‹å›¾ï¼Œå†™çš„å¤ªå¥½äº†ï¼è¿˜å°±é‚£ä¸ªé†é†çŒé¡¶ï¼ï¼ï¼

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/d512cccbffc1a449e1e9fc32ab7e992e.png)

## æ€»ç»“

è¿™ä¸ªæ–‡ä»¶æˆ‘è§‰å¾—æ˜¯æ•´ä¸ªYOLOv5æºç ä¸­æœ€éš¾çš„ä¸€ä¸ªäº†ï¼Œå¤ªéš¾ç†è§£äº†ï¼Œå°¤å…¶æ˜¯æˆ‘çš„pytorchè¿˜ä¸æ˜¯å¾ˆç†Ÿï¼Œå„ç§èŠ±é‡Œèƒ¡å“¨çš„çŸ©é˜µæ“ä½œçœ‹çš„æˆ‘å¤ªç—›è‹¦äº†ã€‚å°½ç®¡å†™è¿™ç¯‡åšå®¢çš„æ—¶å€™çœ‹äº†å¤§é‡çš„å…¶ä»–åšå®¢ï¼Œä½†è¿˜æ˜¯éš¾ä»¥ç†è§£ï¼Œæˆ‘å¤ªéš¾äº†TnT

* * *

ä¹‹å‰ä¸€ç›´è§‰å¾—ç½®ä¿¡åº¦æŸå¤±å’ŒbboxæŸå¤±å¾ˆåƒï¼Œä¸€ç›´äº‘é‡Œé›¾é‡Œçš„ï¼Œé—®äº†ä¸‹gptï¼Œæ„Ÿè§‰è¿˜æ˜¯è®²çš„æŒºé€å½»çš„ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/0c48ec2e6e2f35fb57941cc173d51abd.png)

* * *

**References**

> CSDN è¥¿è¥¿å¼—Sisyphusï¼š [ç›®æ ‡æ£€æµ‹ YOLOv5 - Sample Assignment](https://blog.csdn.net/flyfish1986/article/details/119332396)
> CSDN æ»¡èˆ¹æ¸…æ¢¦å‹æ˜Ÿæ²³HKï¼š[ã€YOLOV5-5.x æºç è§£è¯»ã€‘loss.py](https://blog.csdn.net/qq_38253797/article/details/119444854)
> CSDN å°å“ˆè’™å¾·ï¼š[YOLO-V3-SPP è®­ç»ƒæ—¶æ­£æ ·æœ¬ç­›é€‰æºç è§£æä¹‹build_targets](https://blog.csdn.net/qq_38109282/article/details/119411005)
> CSDN guikunchen:[yolov5 ä»£ç è§£è¯» æŸå¤±å‡½æ•° loss.py](https://blog.csdn.net/guikunchen/article/details/118452790)
> CSDN gorgeous(à¹‘>Ø‚<à¹‘ï¼‰[ã€ä»£ç è§£è¯»ã€‘è¶…è¯¦ç»†ï¼ŒYOLOV5ä¹‹build_targetså‡½æ•°è§£è¯»ã€‚](https://blog.csdn.net/wxd1233/article/details/126148680)
> Bç«™ è–›å®šè°”çš„AI [yolo v5 è§£è¯»ï¼Œè®­ç»ƒï¼Œå¤ç°](https://www.bilibili.com/video/BV1JR4y1g77H?p=6&vd_source=6ddff31927787e4db97428b9835fa86f)