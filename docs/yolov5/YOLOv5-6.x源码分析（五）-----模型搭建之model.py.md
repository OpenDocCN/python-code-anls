<!--yml
category: æ¸¸æˆ
date: 2023-09-17 14:44:45
-->

# YOLOv5-6.xæºç åˆ†æï¼ˆäº”ï¼‰---- æ¨¡å‹æ­å»ºä¹‹model.py

> æ¥æºï¼š[https://blog.csdn.net/weixin_51322383/article/details/130379982](https://blog.csdn.net/weixin_51322383/article/details/130379982)

### æ–‡ç« ç›®å½•

*   [å‰è¨€](#_1)
*   [ğŸš€YOLOv5-6.xæºç åˆ†æï¼ˆäº”ï¼‰---- æ¨¡å‹æ­å»ºä¹‹model.py](#YOLOv56x_modelpy_10)
*   *   [1\. å¯¼å…¥éœ€è¦çš„åŒ…](#1__11)
    *   [2\. åŸºæœ¬ç»„ä»¶](#2__40)
    *   *   [2.1 autopad](#21_autopad_44)
        *   [2.2 Conv](#22_Conv_67)
        *   [2.3 DWConv](#23_DWConv_95)
        *   [2.4 Bottlenect](#24_Bottlenect_104)
        *   [2.5 BottleneckCSP](#25_BottleneckCSP_126)
        *   [2.6 C3](#26_C3_153)
        *   [2.7 SPP](#27_SPP_174)
        *   [2.8 SPPF](#28_SPPF_197)
        *   [2.9 Concat](#29_Concat_221)
    *   [3\. æ³¨æ„åŠ›æ¨¡å—](#3__236)
    *   *   [3.1 TransformerLayer](#31_TransformerLayer_255)
        *   [3.2 TransformerBlock](#32_TransformerBlock_281)
    *   [4\. æ¨¡å‹æ‰©å±•æ¨¡å—](#4__302)
    *   *   [4.1 C3TR(C3)](#41_C3TRC3_304)
        *   [4.2 AutoShape](#42_AutoShape_318)
        *   [4.3 Detections](#43_Detections_424)
        *   [4.4 ClassifyäºŒçº§åˆ†ç±»](#44_Classify_537)
    *   [æ€»ç»“](#_560)

# å‰è¨€

**æˆ‘è›®å–œæ¬¢è¿™ç§å¿™ç¢Œçš„æ„Ÿè§‰çš„ï¼Œç¢Œç¢Œæ— ä¸ºæ— æ‰€äº‹äº‹ä¼šç»™æˆ‘ä¸€ç§ç„¦è™‘æ„Ÿï¼Œä¸è¿‡ä¸¢æ‰ç„¦è™‘æ„Ÿçš„ä»£ä»·å°±æ˜¯æˆ‘ä¸èƒ½èŠ±æ›´å¤šçš„æ—¶é—´åœ¨éŸ³ä¹ä¸Šäº†ï¼Œæ™šä¸Šå›å»éƒ½åœ¨å’Œå®¤å‹ç©æ¸¸æˆï¼Œè¿™å­¦æœŸå¼€å§‹å°±æ²¡æœ‰å¤šå°‘æ—¶é—´åœ¨ç»ƒç´äº†ï¼Œä¸‹å­¦æœŸä¸€å®šè¦å¼¥è¡¥ä¸Šã€‚**

* * *

**è¿™ä¸€ç¯‡ä¸»è¦æ˜¯è®²ä¸€ä¸‹YOLOçš„`model`è„šæœ¬ï¼Œè¿™ä¸ªè„šæœ¬åœ¨`./models/common.py`è·¯å¾„ä¸‹ï¼ŒåŒ…æ‹¬äº†å„ç§é€šç”¨çš„ç½‘ç»œæ­å»ºç±»æ¨¡å—ï¼Œæ¯”å¦‚Convã€C3ã€SPPç­‰ç­‰ã€‚**

**å¯¼èˆªï¼š**[YOLOv5-6.xæºç åˆ†æ å…¨æµç¨‹è®°å½•](https://blog.csdn.net/weixin_51322383/article/details/130353834)

* * *

# ğŸš€YOLOv5-6.xæºç åˆ†æï¼ˆäº”ï¼‰---- æ¨¡å‹æ­å»ºä¹‹model.py

## 1\. å¯¼å…¥éœ€è¦çš„åŒ…

```py
# ç½‘ç»œæ¨¡å‹ç»„ä»¶
import json
import math                 # æ•°å­¦å‡½æ•°æ¨¡å—
import platform
import warnings
from collections import OrderedDict, namedtuple
from copy import copy       # æ•°æ®æ‹·è´æ¨¡å— åˆ†æµ…æ‹·è´å’Œæ·±æ‹·è´
from pathlib import Path    # Pathå°†strè½¬æ¢ä¸ºPathå¯¹è±¡ ä½¿å­—ç¬¦ä¸²è·¯å¾„æ˜“äºæ“ä½œçš„æ¨¡å—

import cv2
import numpy as np
import pandas as pd
import requests              # Pythonçš„HTTPå®¢æˆ·ç«¯åº“
import torch                 # pytorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn        # ä¸“é—¨ä¸ºç¥ç»ç½‘ç»œè®¾è®¡çš„æ¨¡å—åŒ–æ¥å£
import yaml
from PIL import Image
from torch.cuda import amp  # æ··åˆç²¾åº¦è®­ç»ƒæ¨¡å—

from utils.dataloaders import exif_transpose, letterbox     # åŠ è½½æ•°æ®é›†
from utils.general import (LOGGER, check_requirements, check_suffix, check_version, colorstr, increment_path,
                           make_divisible, non_max_suppression, scale_coords, xywh2xyxy, xyxy2xywh) # å¸¸ç”¨çš„å·¥å…·å‡½æ•°
from utils.plots import Annotator, colors, save_one_box # ç»˜åˆ¶çŸ©å½¢æ¡†å’Œæ ‡æ³¨ä¿¡æ¯
from utils.torch_utils import copy_attr, time_sync  # ä¸pytorchç›¸å…³çš„å·¥å…·å‡½æ•° 
```

## 2\. åŸºæœ¬ç»„ä»¶

ç”±`yolov5s.yaml`å¯ä»¥çœ‹åˆ°ï¼Œæœ‰è®¸å¤šçš„åŸºæœ¬ç»„ä»¶ç»„æˆäº†æ•´ä¸ªç½‘ç»œã€‚

### 2.1 autopad

**è¿™ä¸ªæ¨¡å—å¯ä»¥æ ¹æ®è¾“å…¥çš„å·ç§¯æ ¸è®¡ç®—å·ç§¯æ¨¡å—éœ€è¦çš„padå€¼ã€‚ç”¨äºä¸‹é¢çš„Convç±»å’ŒClassifyç±»ä¸­ã€‚ä¸»è¦æ˜¯ä¸ºäº†æŠŠtensorè¡¥æˆåŸæ¥çš„å½¢çŠ¶ã€‚**

```py
def autopad(k, p=None):  # å·ç§¯æ ¸çš„kernel_size, è‡ªåŠ¨è®¡ç®—çš„éœ€è¦padå€¼ï¼ˆ0å¡«å……ï¼‰
    '''
     v5ä¸­åªæœ‰ä¸¤ç§å·ç§¯ï¼š
       1ã€ä¸‹é‡‡æ ·å·ç§¯:conv3x3 s=2 p=k//2=1
       2ã€feature sizeä¸å˜çš„å·ç§¯:conv1x1 s=1 p=k//2=1
       kï¼šå·ç§¯æ ¸çš„kernel_size
    '''
    # Pad to 'same'
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad è‡ªåŠ¨è®¡ç®—padæ•°
    return p 
```

è¿™é‡Œé¦–å…ˆæ˜¯åˆ¤æ–­æ˜¯å¦æœ‰på€¼ï¼š

*   å¦‚æœæœ‰æ—¢å®šçš„ p ï¼Œåˆ™ç›´æ¥ return pï¼Œè‡ªåŠ¨è®¡ç®—æ‰€éœ€è¦çš„padå€¼
*   å¦‚æœæ— è®¾å®šçš„ pï¼Œåˆ™ return ä½¿å›¾åƒåœ¨å·ç§¯æ“ä½œåå°ºå¯¸ä¸å˜çš„ p

### 2.2 Conv

```py
class Conv(nn.Module):
    # Standard convolution  æ ‡å‡†å·ç§¯+BN+hardswishæ¿€æ´»ï¼ˆSiLUï¼‰
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    def forward(self, x):   # ç½‘ç»œçš„æ‰§è¡Œé¡ºåº
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        """ç”¨äºModelç±»çš„fuseå‡½æ•°
        èåˆconv+bn åŠ é€Ÿæ¨ç† ä¸€èˆ¬ç”¨äºæµ‹è¯•/éªŒè¯é˜¶æ®µ
        """
        return self.act(self.conv(x))   # æ²¡æœ‰BN 
```

**æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œæ•´ä¸ªç½‘ç»œä¸­æœ€åŸºç¡€çš„ç»„ä»¶ï¼ŒConv+BN+æ¿€æ´»å‡½æ•°SiLUï¼Œç»“æ„å¦‚ä¸‹**

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/b868bc977e89220499ea60bf5d5419c1.png)

æ³¨æ„è¿™é‡Œæœ‰ä¸ªç‰¹æ®Šçš„å‡½æ•°`forward_fuse`ï¼Œè¿™æ˜¯ä¸€ä¸ª**å‰å‘åŠ é€Ÿæ¨ç†æ¨¡å—**ï¼Œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡èåˆ**conv + bn**å±‚ï¼Œè¾¾åˆ°**åŠ é€Ÿæ¨ç†çš„ä½œç”¨**ï¼Œä¸€èˆ¬ç”¨äº**æµ‹è¯•æˆ–éªŒè¯**é˜¶æ®µã€‚

### 2.3 DWConv

```py
# æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆæ²¡ç”¨åˆ°ï¼‰
class DWConv(Conv):
    # Depth-wise convolution class
    def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)     # è¿”å›æœ€å¤§å…¬çº¦æ•° 
```

### 2.4 Bottlenect

```py
class Bottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion; shortcut: bool æ˜¯å¦æœ‰shortcutè¿æ¥ é»˜è®¤æ˜¯True
                                                            # e*c2å°±æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºchannel=ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥channel
        super().__init__()
        c_ = int(c2 * e)  # hidden channels è¾“å‡ºå‡åŠ
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):   # æ ¹æ®self.addç¡®å®šæ˜¯å¦æœ‰shortcutï¼ˆç›¸åŠ ï¼‰
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) 
```

**ä¼ å…¥çš„å‚æ•°ä¸­æœ‰ä¸€ä¸ª`shortcut`ï¼Œåˆ†ä¸ºtrueå’Œfalseï¼Œä¸»è¦ä½œç”¨æ˜¯æ˜¯å¦åŠ å…¥æ®‹å·®è¿æ¥ã€‚
æ®‹å·®è¿æ¥å¯ä»¥æœ‰æ•ˆåœ°æå–ç‰¹å¾ï¼Œå¯ä»¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ï¼Œå¹¶é™ä½è¿‡æ‹Ÿåˆï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚**

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/e1262b61a5290bd8c08aeeb91c4453ef.png)
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/7ca31b1b0eddc4b5769c437cbbf6d631.png)

### 2.5 BottleneckCSP

```py
# æ ‡å‡†çš„ç“¶é¢ˆå±‚   1x1conv+3x3conv+æ®‹å·®å—
class BottleneckCSP(nn.Module): # BCSPn
    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        self.act = nn.SiLU()
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n))) # *å¯ä»¥æŠŠlistæ‹†åˆ†æˆç‹¬ç«‹çš„å…ƒç´ 

    def forward(self, x):
        y1 = self.cv3(self.m(self.cv1(x)))
        y2 = self.cv2(x)
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), 1)))) 
```

**æ ‡å‡†çš„ç“¶é¢ˆå±‚ï¼Œç”±Bottlenectå’ŒCSPç»„æˆ**

**å¯ä»¥å°†yamlæ–‡ä»¶ä¸­çš„C3ç›´æ¥æ›´æ¢æˆè¿™ä¸ªï¼Œä½†ä¸€èˆ¬C3ç»“æ„æ•ˆæœæ›´å¥½ã€‚**
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/f581b5ecc0fc18d7024ed0e620cdbd9d.png)

### 2.6 C3

```py
# è¿™ä¸ªæ¨¡å—æ˜¯ä¸€ç§ç®€åŒ–ç‰ˆçš„BottleneckCSPï¼Œå› ä¸ºé™¤äº†Bottleneckéƒ¨åˆ†åªæœ‰3ä¸ªå·ç§¯ï¼Œå¯ä»¥å‡å°‘å‚æ•°ï¼Œæ‰€ä»¥å–åC3ã€‚
class C3(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))   # ç»´åº¦1 
```

**C3å°±æ˜¯ç®€åŒ–ç‰ˆçš„BottlenectCSPï¼Œåªæ˜¯å°‘äº†ä¸€ä¸ªConvï¼Œå¯ä»¥å‡å°‘å‚æ•°ï¼Œæ‰€ä»¥å–åC3ã€‚**

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/41abb8c297b5de8fdb5267f43ca5ff25.png)

### 2.7 SPP

**è¿™ä¸ªæ¨¡å—ä»v3å°±å¼€å§‹ç”¨äº†ï¼Œä¸»è¦ç›®çš„æ˜¯å°†ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æ›´å¤šçš„ä¿¡æ¯ã€‚**

```py
# è¿™ä¸ªæ¨¡å—çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†å°†æ›´å¤šä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æ›´å¤šçš„ä¿¡æ¯ã€‚
# ç©ºé—´é‡‘å­—å¡”æ± åŒ–   åœ¨yolo.pyçš„parse_modelæ¨¡å—è°ƒç”¨
class SPP(nn.Module):
    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729
    def __init__(self, c1, c2, k=(5, 9, 13)):
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1)) # å¯¹æ¯ä¸ªmåšæœ€å¤§æ± åŒ–ï¼Œå†å åŠ æ²¡æœ‰æ± åŒ–çš„mï¼Œå†æ‹¼æ¥ 
```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/d7d25397beb9b19bdaddfaf67335f565.png)

### 2.8 SPPF

```py
class SPPF(nn.Module):
    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher
    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            y1 = self.m(x)
            y2 = self.m(y1)
            return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1)) 
```

**SPPFæ˜¯å¿«é€Ÿç‰ˆçš„ç©ºé—´é‡‘å­—å¡”ï¼Œæ± åŒ–å°ºå¯¸ç­‰ä»·äºï¼š5ã€9ã€13ï¼Œå’ŒåŸæ¥ä¸€æ ·ï¼Œä½†æ˜¯è®¡ç®—é‡å‡å°‘äº†ã€‚**

> å¯ä»¥å‚è€ƒè¿™ç¯‡åšæ–‡ï¼š[YOLOv5ä¸­çš„SPP/SPPFç»“æ„è¯¦è§£](https://blog.csdn.net/weixin_55073640/article/details/122621148)

### 2.9 Concat

```py
class Concat(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension  # æ²¿æŸä¸ªç»´åº¦æ‹¼æ¥

    def forward(self, x):
        return torch.cat(x, self.d) 
```

**è¿™ä¸ªå‡½æ•°æ˜¯å°†è‡ªèº«ï¼ˆa list of tensorsï¼‰æŒ‰ç…§æŸä¸ªç»´åº¦è¿›è¡Œconcatï¼Œå¸¸ç”¨æ¥åˆå¹¶å‰åä¸¤ä¸ªfeature mapï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢yolov5sç»“æ„å›¾ä¸­çš„Concatã€‚**

**æ³¨æ„ï¼Œè¿™é‡Œçš„xæ˜¯ä¸€ä¸ªlistï¼Œä¸€èˆ¬åŒ…æ‹¬-1å’Œå¦å¤–ä¸€å±‚ã€‚**

## 3\. æ³¨æ„åŠ›æ¨¡å—

**æ¥äº†æ¥äº†ï¼Œæ³¨æ„åŠ›æ¨¡å—ï¼Œæˆ‘æ„¿ç§°ä¹‹ä¸ºæ·±åº¦å­¦ä¹ ä¸­æœ€ä¸ºç„å­¦çš„æ¨¡å—ï¼ˆæ ¼å±€å¤§ç‚¹ï¼Œæ•´ä¸ªæ·±åº¦å­¦ä¹ éƒ½æ˜¯ç„å­¦dogeï¼‰ã€‚æˆ‘å°è¯•è¿‡åŠ å…¥å¥½å¤šç§æ³¨æ„åŠ›æ¨¡å—åˆ°ç½‘ç»œä¸­ï¼Œæœ€åç»“æœæˆ‘éƒ½æ„¿ç§°ä¹‹ä¸ºâ€œå®éªŒè¯¯å·®â€ã€‚ä¸åŒçš„æ•°æ®é›†ï¼Œä¸åŒçš„ä½ç½®ï¼Œä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶éƒ½æœ‰å½±å“ï¼Œåªæœ‰æå°‘æ•°èƒ½æ¶¨ç‚¹ã€‚**

**æ ¹æ®ç½‘ä¸Šå’Œæˆ‘è‡ªå·±å®éªŒçš„ç»éªŒæ¥çœ‹ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¸€èˆ¬æ”¾åœ¨è¿™å‡ ä¸ªåœ°æ–¹ï¼Œä»¥ä¸‹ç»“æœä¸ä¸€å®šæ­£ç¡®ï¼Œè¿˜éœ€æ ¹æ®è‡ªå·±ç›¸å…³æ•°æ®é›†æ¥çœ‹ï¼š**

1.  **ä¸Šé‡‡æ ·+concatä¹‹ååŠ å…¥æ³¨æ„åŠ›æœºåˆ¶**
2.  **backboneç»“å°¾åŠ ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶**
3.  channel-wiseæ¯”spatial-wiseæ›´å¥½ç”¨ï¼Ÿ
4.  æ¯ä¸ªblockï¼ˆå¦‚residual blockï¼‰ç»“å°¾ä½¿ç”¨æ¯”æ¯ä¸ªConvé‡Œä½¿ç”¨æ›´å¥½ï¼Ÿ

* * *

å¤§åé¼é¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰å­¦ï¼Œæ‰“ç®—åé¢å†çœ‹ï¼Œå°±å…ˆåªè´´ä»£ç å§ã€‚

å¦‚ä¸‹å›¾æ˜¯æ•´ä¸ªtransformerçš„ç»“æ„ï¼Œæˆ‘ä»¬è¿™é‡Œä»£ç éƒ¨åˆ†åªç”¨äº†å·¦è¾¹çš„Encodingéƒ¨åˆ†ï¼š

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/418113f4c2a0e93fd0ea5503e0b1741d.png)

### 3.1 TransformerLayer

```py
# transformerè‡ªæ³¨æ„åŠ›æ¨¡å—
class TransformerLayer(nn.Module):
    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)
    def __init__(self, c, num_heads):
        super().__init__()
        self.q = nn.Linear(c, c, bias=False)
        self.k = nn.Linear(c, c, bias=False)
        self.v = nn.Linear(c, c, bias=False)
        # è¾“å…¥: queryã€keyã€value
        # è¾“å‡º: 0 attn_output å³é€šè¿‡self-attentionä¹‹åï¼Œä»æ¯ä¸€ä¸ªè¯è¯­ä½ç½®è¾“å‡ºæ¥çš„attention å’Œè¾“å…¥çš„queryå®ƒä»¬å½¢çŠ¶ä¸€æ ·çš„
        #      1 attn_output_weights å³attention weights æ¯ä¸€ä¸ªå•è¯å’Œä»»æ„å¦ä¸€ä¸ªå•è¯ä¹‹é—´éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªweight
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)
        self.fc1 = nn.Linear(c, c, bias=False)
        self.fc2 = nn.Linear(c, c, bias=False)

    def forward(self, x):
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ + æ®‹å·®(è¿™é‡Œç§»é™¤äº†LayerNorm for better performance)
        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x
        # feed forward å‰é¦ˆç¥ç»ç½‘ç»œ + æ®‹å·®(è¿™é‡Œç§»é™¤äº†LayerNorm for better performance)
        x = self.fc2(self.fc1(x)) + x
        return x 
```

### 3.2 TransformerBlock

```py
class TransformerBlock(nn.Module):
    # Vision Transformer https://arxiv.org/abs/2010.11929
    def __init__(self, c1, c2, num_heads, num_layers):
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        self.c2 = c2

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2).permute(2, 0, 1)
        return self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h) 
```

## 4\. æ¨¡å‹æ‰©å±•æ¨¡å—

### 4.1 C3TR(C3)

```py
# C3TR(C3)ï¼šç»§æ‰¿è‡ª C3ï¼Œn ä¸ª Bottleneck æ›´æ¢ä¸º 1 ä¸ª TransformerBlock
class C3TR(C3):
    # C3 module with TransformerBlock()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n) 
```

**è¿™éƒ¨åˆ†ç»§æ‰¿è‡ª`C3`ï¼Œç›¸å½“äºæŠŠåŸå…ˆçš„`Bottlenect`æ¢ä½äº†`TransformerBlock`æ¨¡å—**

### 4.2 AutoShape

```py
# é¢„å¤„ç†è°ƒæ•´shape
class AutoShape(nn.Module):
    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
    conf = 0.25  # NMS confidence threshold
    iou = 0.45  # NMS IoU threshold
    agnostic = False  # NMS class-agnostic
    multi_label = False  # NMS multiple labels per box
    classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs
    max_det = 1000  # maximum number of detections per image
    amp = False  # Automatic Mixed Precision (AMP) inference

    def __init__(self, model, verbose=True):
        super().__init__()
        if verbose:
            LOGGER.info('Adding AutoShape... ')
        copy_attr(self, model, include=('yaml', 'nc', 'hyp', 'names', 'stride', 'abc'), exclude=())  # copy attributes
        self.dmb = isinstance(model, DetectMultiBackend)  # DetectMultiBackend() instance
        self.pt = not self.dmb or model.pt  # PyTorch model
        self.model = model.eval()
        if self.pt:
            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Detect()
            m.inplace = False  # Detect.inplace=False for safe multithread inference

    def _apply(self, fn):
        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers
        self = super()._apply(fn)
        if self.pt:
            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Detect()
            m.stride = fn(m.stride)
            m.grid = list(map(fn, m.grid))
            if isinstance(m.anchor_grid, list):
                m.anchor_grid = list(map(fn, m.anchor_grid))
        return self

    @torch.no_grad()
    def forward(self, imgs, size=640, augment=False, profile=False):
        # Inference from various sources. For height=640, width=1280, RGB images example inputs are:
        #   file:       imgs = 'data/images/zidane.jpg'  # str or PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images

        t = [time_sync()]
        p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type
        autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference
        if isinstance(imgs, torch.Tensor):  # torch
            with amp.autocast(autocast):
                return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference

        # Pre-process
        n, imgs = (len(imgs), list(imgs)) if isinstance(imgs, (list, tuple)) else (1, [imgs])  # number, list of images
        shape0, shape1, files = [], [], []  # image and inference shapes, filenames
        for i, im in enumerate(imgs):
            f = f'image{i}'  # filename
            if isinstance(im, (str, Path)):  # filename or uri
                im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im
                im = np.asarray(exif_transpose(im))
            elif isinstance(im, Image.Image):  # PIL Image
                im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f
            files.append(Path(f).with_suffix('.jpg').name)
            if im.shape[0] < 5:  # image in CHW
                im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)
            im = im[..., :3] if im.ndim == 3 else np.tile(im[..., None], 3)  # enforce 3ch input
            s = im.shape[:2]  # HWC
            shape0.append(s)  # image shape
            g = (size / max(s))  # gain
            shape1.append([y * g for y in s])
            imgs[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update
        shape1 = [make_divisible(x, self.stride) if self.pt else size for x in np.array(shape1).max(0)]  # inf shape
        x = [letterbox(im, shape1, auto=False)[0] for im in imgs]  # pad
        x = np.ascontiguousarray(np.array(x).transpose((0, 3, 1, 2)))  # stack and BHWC to BCHW
        x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32
        t.append(time_sync())

        with amp.autocast(autocast):
            # Inference
            y = self.model(x, augment, profile)  # forward
            t.append(time_sync())

            # Post-process
            y = non_max_suppression(y if self.dmb else y[0],
                                    self.conf,
                                    self.iou,
                                    self.classes,
                                    self.agnostic,
                                    self.multi_label,
                                    max_det=self.max_det)  # NMS
            for i in range(n):
                scale_coords(shape1, y[i][:, :4], shape0[i])

            t.append(time_sync())
            return Detections(imgs, y, files, t, self.names, x.shape) 
```

**AutoShapeæ˜¯ä¸€ä¸ªæ¨¡å‹æ‰©å±•æ¨¡å—ï¼Œç»™æ¨¡å‹å°è£…æˆåŒ…å«å‰å¤„ç†ã€æ¨ç†ã€åå¤„ç†çš„æ¨¡å—(é¢„å¤„ç† + æ¨ç† + nms)**ã€‚

æ³¨æ„Autoshapeæ¨¡å—åœ¨trainä¸­ä¸ä¼šè¢«è°ƒç”¨ï¼Œå½“æ¨¡å‹è®­ç»ƒç»“æŸåï¼Œä¼šé€šè¿‡è¿™ä¸ªæ¨¡å—å¯¹å›¾ç‰‡è¿›è¡Œé‡å¡‘ï¼Œæ¥æ–¹ä¾¿æ¨¡å‹çš„é¢„æµ‹ã€‚

å› ä¸ºè¿™ä¸ªæ¨¡å—åŸºæœ¬æ²¡å•¥ç”¨ï¼Œæ‰€ä»¥ä¸åšç»†è®²ã€‚

### 4.3 Detections

```py
class Detections:
    # YOLOv5 detections class for inference results
    def __init__(self, imgs, pred, files, times=(0, 0, 0, 0), names=None, shape=None):
        super().__init__()
        d = pred[0].device  # device
        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # normalizations
        self.imgs = imgs  # list of images as numpy arrays
        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)
        self.names = names  # class names
        self.files = files  # image filenames
        self.times = times  # profiling times
        self.xyxy = pred  # xyxy pixels
        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels
        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized
        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized
        self.n = len(self.pred)  # number of images (batch size)
        self.t = tuple((times[i + 1] - times[i]) * 1000 / self.n for i in range(3))  # timestamps (ms)
        self.s = shape  # inference BCHW shape

    def display(self, pprint=False, show=False, save=False, crop=False, render=False, labels=True, save_dir=Path('')):
        crops = []
        for i, (im, pred) in enumerate(zip(self.imgs, self.pred)):
            s = f'image {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # string
            if pred.shape[0]:
                for c in pred[:, -1].unique():
                    n = (pred[:, -1] == c).sum()  # detections per class
                    s += f"{n}  {self.names[int(c)]}{'s' * (n > 1)}, "  # add to string
                if show or save or render or crop:
                    annotator = Annotator(im, example=str(self.names))
                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class
                        label = f'{self.names[int(cls)]}  {conf:.2f}'
                        if crop:
                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None
                            crops.append({
                                'box': box,
                                'conf': conf,
                                'cls': cls,
                                'label': label,
                                'im': save_one_box(box, im, file=file, save=save)})
                        else:  # all others
                            annotator.box_label(box, label if labels else '', color=colors(cls))
                    im = annotator.im
            else:
                s += '(no detections)'

            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np
            if pprint:
                print(s.rstrip(', '))
            if show:
                im.show(self.files[i])  # show
            if save:
                f = self.files[i]
                im.save(save_dir / f)  # save
                if i == self.n - 1:
                    LOGGER.info(f"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}")
            if render:
                self.imgs[i] = np.asarray(im)
        if crop:
            if save:
                LOGGER.info(f'Saved results to {save_dir}\n')
            return crops

    def print(self):
        self.display(pprint=True)  # print results
        print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' % self.t)

    def show(self, labels=True):
        self.display(show=True, labels=labels)  # show results

    def save(self, labels=True, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True)  # increment save_dir
        self.display(save=True, labels=labels, save_dir=save_dir)  # save results

    def crop(self, save=True, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True) if save else None
        return self.display(crop=True, save=save, save_dir=save_dir)  # crop results

    def render(self, labels=True):
        self.display(render=True, labels=labels)  # render results
        return self.imgs

    def pandas(self):
        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])
        new = copy(self)  # return copy
        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns
        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns
        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):
            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update
            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])
        return new

    def tolist(self):
        # return a list of Detections objects, i.e. 'for result in results.tolist():'
        r = range(self.n)  # iterable
        x = [Detections([self.imgs[i]], [self.pred[i]], [self.files[i]], self.times, self.names, self.s) for i in r]
        # for d in x:
        #    for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:
        #        setattr(d, k, getattr(d, k)[0])  # pop out of list
        return x

    def __len__(self):
        return self.n  # override len(results)

    def __str__(self):
        self.print()  # override print(results)
        return '' 
```

**è¿™éƒ¨åˆ†æ˜¯å¯¹æ¨ç†ç»“æœè¿›è¡Œå¤„ç†ï¼Œå°±ä¸Šé¢AutoShapeç»“å°¾ç”¨äº†ä¸€ä¸‹ï¼ŒåŸºæœ¬ä¸ç”¨ï¼Œå°±ä¸ç»†çœ‹äº†ã€‚åªéœ€è¦é‡ç‚¹æŒæ¡yolo.pyä¸­çš„Detection**

### 4.4 ClassifyäºŒçº§åˆ†ç±»

```py
# ç”¨äºç¬¬äºŒçº§åˆ†ç±»ï¼ˆè½¦ç‰Œè¯†åˆ«ï¼‰
class Classify(nn.Module):
    # Classification head, i.e. x(b,c1,20,20) to x(b,c2)
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        self.aap = nn.AdaptiveAvgPool2d(1)  # to x(b,c1,1,1) è‡ªé€‚åº”å¹³å‡æ± åŒ–æ“ä½œ
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g)  # to x(b,c2,1,1)
        self.flat = nn.Flatten()

    def forward(self, x):
        z = torch.cat([self.aap(y) for y in (x if isinstance(x, list) else [x])], 1)  # cat if list
        return self.flat(self.conv(z))  # flatten to x(b,c2) 
```

**ä»€ä¹ˆæ˜¯äºŒçº§åˆ†ç±»ï¼Ÿ**

æ¯”å¦‚åšè½¦ç‰Œè¯†åˆ«ï¼Œå…ˆè¯†åˆ«å‡ºè½¦ç‰Œï¼Œå¦‚æœæƒ³å¯¹è½¦ç‰Œä¸Šçš„å­—è¿›è¡Œè¯†åˆ«ï¼Œå°±éœ€è¦äºŒçº§åˆ†ç±»è¿›ä¸€æ­¥æ£€æµ‹ã€‚å¦‚æœå¯¹æ¨¡å‹è¾“å‡ºçš„åˆ†ç±»å†è¿›è¡Œåˆ†ç±»ï¼Œå°±å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å—ã€‚

å†æ¯”å¦‚è¦åšè¯†åˆ«äººè„¸é¢éƒ¨è¡¨æƒ…ï¼Œå…ˆè¦è¯†åˆ«å‡ºäººè„¸ï¼Œå¦‚æœæƒ³è¯†åˆ«å‡ºäººçš„é¢éƒ¨è¡¨æƒ…ï¼Œå°±éœ€è¦äºŒçº§åˆ†ç±»è¿›ä¸€æ­¥æ£€æµ‹ã€‚

## æ€»ç»“

**è¿™éƒ¨åˆ†æˆ‘å°†æœ€åŸºæœ¬çš„ä¸€äº›æ¨¡å—ç»™è´´äº†å‡ºæ¥ï¼Œä¸€äº›åœ¨YOLOv5ä¸­æ¯”è¾ƒå†·é—¨çš„ã€åçš„æˆ‘å°±æ²¡å†™äº†ã€‚å¤§å®¶é‡ç‚¹éœ€è¦æŒæ¡çš„æ˜¯åŸºç¡€ç»„ä»¶éƒ¨åˆ†ï¼Œå…¶ä»–çš„åƒæ³¨æ„åŠ›æœºåˆ¶è¿™äº›å¯ä»¥åšä¸€ä¸‹å°è¯•ï¼Œçœ‹èƒ½ä¸èƒ½æ¶¨ç‚¹ã€‚åé¢ç¬¬4æ¨¡å—éƒ½å¯ä»¥ä¸ç”¨çœ‹ã€‚**

**2023-04-26 13ï¼š39**

**References**

> CSDN æ»¡èˆ¹æ¸…æ¢¦å‹æ˜Ÿæ²³HK [YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ7ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ2ï¼‰common.py](https://blog.csdn.net/weixin_43334693/article/details/129854764)
> CSDN è·¯äººè´¾â€™Ï‰â€™ [ã€YOLOV5-5.x æºç è§£è¯»ã€‘common.py](https://blog.csdn.net/qq_38253797/article/details/119684388)