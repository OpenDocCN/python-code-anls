<!--yml
category: æ¸¸æˆ
date: 2023-09-17 14:44:33
-->

# YOLOv5-6.xæºç åˆ†æï¼ˆå…­ï¼‰---- æ•°æ®é›†åˆ›å»ºä¹‹dataloaders.py

> æ¥æºï¼š[https://blog.csdn.net/weixin_51322383/article/details/130387945](https://blog.csdn.net/weixin_51322383/article/details/130387945)

### æ–‡ç« ç›®å½•

*   [å‰è¨€](#_1)
*   [ğŸš€YOLOv5-6.xæºç åˆ†æï¼ˆå…­ï¼‰---- æ•°æ®é›†åˆ›å»ºä¹‹dataloaders.py](#YOLOv56x_dataloaderspy_6)
*   *   [1\. å¯¼åŒ…](#1__7)
    *   [2\. ç›¸æœºè®¾ç½®](#2__50)
    *   [3\. create_dataloader](#3_create_dataloader_79)
    *   [4\. è‡ªå®šä¹‰DataLoader](#4_DataLoader_141)
    *   [5\. LoadImagesAndLabels](#5_LoadImagesAndLabels_178)
    *   *   [5.1 __init__](#51___init___184)
        *   [5.2 cache_labels](#52_cache_labels_360)
        *   [5.3 len](#53_len_408)
        *   [5.4 getitem](#54_getitem_417)
        *   [5.5 collate_fn](#55_collate_fn_532)
    *   [6\. img2label_paths](#6_img2label_paths_572)
    *   [7\. verify_image_label](#7_verify_image_label_596)
    *   [8\. load_image](#8_load_image_663)
    *   [9\. load_mosaic](#9_load_mosaic_699)
    *   [æ€»ç»“](#_793)

# å‰è¨€

è¿™ä¸ªæ–‡ä»¶ä¸»è¦æ˜¯åˆ›å»ºæ•°æ®é›†+å„ç§æ•°æ®å¢å¼ºæ“ä½œã€‚

å¯¼èˆªï¼š[YOLOv5-6.xæºç åˆ†æ å…¨æµç¨‹è®°å½•](https://blog.csdn.net/weixin_51322383/article/details/130353834?spm=1001.2014.3001.5502)

* * *

# ğŸš€YOLOv5-6.xæºç åˆ†æï¼ˆå…­ï¼‰---- æ•°æ®é›†åˆ›å»ºä¹‹dataloaders.py

## 1\. å¯¼åŒ…

```py
import cv2
import contextlib
import glob # æ–‡ä»¶æ“ä½œç›¸å…³æ¨¡å—
import hashlib  # å“ˆå¸Œæ¨¡å—ï¼Œç«¥å·¥äº†å¤šç§å®‰å…¨æ–¹ä¾¿çš„hashæ–¹æ³•
import json
import math
import os
import random
import shutil   # æ–‡ä»¶å¤¹ã€å‹ç¼©åŒ…å¤„ç†æ¨¡å—
import time
from itertools import repeat    # å¤åˆ¶æ¨¡å—
from multiprocessing.pool import Pool, ThreadPool   # å¤šçº¿ç¨‹æ¨¡å—ã€‚çº¿ç¨‹æ± 
from pathlib import Path
from threading import Thread
from urllib.parse import urlparse
from zipfile import ZipFile

import numpy as np
import torch
import torch.nn.functional as F # å°è£…äº†å¾ˆå¤šå·ç§¯ã€æ± åŒ–å‡½æ•°
import yaml     # yamlæ–‡ä»¶æ“ä½œæ¨¡å—
from PIL import ExifTags, Image, ImageOps       # å›¾ç‰‡ã€ç›¸æœºæ“ä½œæ¨¡å—
from torch.utils.data import DataLoader, Dataset, dataloader, distributed   # è‡ªå®šä¹‰æ•°æ®é›†æ¨¡å—
from tqdm import tqdm

from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective   # æ•°æ®å¢å¼º
from utils.general import (DATASETS_DIR, LOGGER, NUM_THREADS, check_dataset, check_requirements, check_yaml, clean_str,
                           cv2, is_colab, is_kaggle, segments2boxes, xyn2xy, xywh2xyxy, xywhn2xyxy, xyxy2xywhn) # å¸¸ç”¨çš„ä¸€äº›å·¥å…·å‡½æ•°
from utils.torch_utils import torch_distributed_zero_first  # åˆ†å¸ƒå¼è®­ç»ƒç›¸å…³

# Parameters
HELP_URL = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'
IMG_FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'  # include image suffixes    å›¾ç‰‡æ ¼å¼
VID_FORMATS = 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv'  # include video suffixes è§†é¢‘æ ¼å¼
BAR_FORMAT = '{l_bar}{bar:10}{r_bar}{bar:-10b}'  # tqdm bar format 
LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html åœ¨æ•´ä¸ªåˆ†å¸ƒå¼ä¸­çš„åºå·ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰ä¸€ä¸ªrankå’Œä¸€ä¸ªlocal_rank 
```

å…³äºlocal_rankçš„ç†è§£ï¼š[local_rankï¼Œrankï¼Œnodeç­‰ç†è§£](https://blog.csdn.net/shenjianhua005/article/details/127318594) 

* * *

## 2\. ç›¸æœºè®¾ç½®

```py
# è¿™éƒ¨åˆ†æ˜¯ç›¸æœºç›¸å…³è®¾ç½®ï¼Œå½“ä½¿ç”¨ç›¸æœºé‡‡æ ·æ—¶æ‰ä¼šä½¿ç”¨ã€‚
# Get orientation exif tag
# å¯äº¤æ¢å›¾åƒæ–‡ä»¶æ ¼å¼ æ˜¯ä¸“é—¨ä¸ºæ•°ç ç›¸æœºçš„ç…§ç‰‡è®¾å®šçš„ï¼Œå¯ä»¥è®°å½•æ•°ç ç…§ç‰‡çš„å±æ€§ä¿¡æ¯å’Œæ‹æ‘„æ•°æ®
for orientation in ExifTags.TAGS.keys():
    if ExifTags.TAGS[orientation] == 'Orientation':
        break

# è¿”å›æ–‡ä»¶åˆ—è¡¨çš„hashå€¼
def get_hash(paths):
    # Returns a single hash value of a list of paths (files or dirs)
    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes
    h = hashlib.md5(str(size).encode())  # hash sizes
    h.update(''.join(paths).encode())  # hash paths
    return h.hexdigest()  # return hash

# è·å–å›¾ç‰‡çš„å®½é«˜ä¿¡æ¯
def exif_size(img):
    # Returns exif-corrected PIL size
    # è·å–æ•°ç ç›¸æœºçš„å›¾ç‰‡å®½é«˜ä¿¡æ¯  å¹¶ä¸”åˆ¤æ–­æ˜¯å¦éœ€è¦æ—‹è½¬ï¼ˆæ•°ç ç›¸æœºå¯ä»¥å¤šè§’åº¦æ‹æ‘„ï¼‰
    s = img.size  # (width, height)
    with contextlib.suppress(Exception):
        rotation = dict(img._getexif().items())[orientation]    # è°ƒæ•´æ•°ç ç›¸æœºç…§ç‰‡æ–¹å‘
        if rotation in [6, 8]:  # rotation 270 or 90
            s = (s[1], s[0])
    return s 
```

## 3\. create_dataloader

```py
def create_dataloader(path,         # å›¾ç‰‡æ•°æ®åŠ è½½è·¯å¾„ train/test
                      imgsz,        # train/testå›¾ç‰‡å°ºå¯¸ï¼ˆæ•°æ®å¢å¼ºåå¤§å°ï¼‰ 640
                      batch_size,   # batch size å¤§å° 8/16/32
                      stride,       # æ¨¡å‹æœ€å¤§stride=32   [32 16 8]
                      single_cls=False,     # æ•°æ®é›†æ˜¯å¦æ˜¯å•ç±»åˆ« é»˜è®¤False
                      hyp=None,             # è¶…å‚åˆ—è¡¨dict ç½‘ç»œè®­ç»ƒæ—¶çš„ä¸€äº›è¶…å‚æ•°ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ç­‰ï¼Œè¿™é‡Œä¸»è¦ç”¨åˆ°é‡Œé¢ä¸€äº›å…³äºæ•°æ®å¢å¼º(æ—‹è½¬ã€å¹³ç§»ç­‰)çš„ç³»æ•°
                      augment=False,        # æ˜¯å¦è¦è¿›è¡Œæ•°æ®å¢å¼º  True
                      cache=False,          # æ˜¯å¦cache_images False
                      pad=0.0,      # è®¾ç½®çŸ©å½¢è®­ç»ƒçš„shapeæ—¶è¿›è¡Œçš„å¡«å…… é»˜è®¤0.0
                      rect=False,   # æ˜¯å¦å¼€å¯çŸ©å½¢train/test  é»˜è®¤è®­ç»ƒé›†å…³é—­ éªŒè¯é›†å¼€å¯
                      rank=-1,      # å¤šå¡è®­ç»ƒæ—¶çš„è¿›ç¨‹ç¼–å· rankä¸ºè¿›ç¨‹ç¼–å·  -1ä¸”gpu=1æ—¶ä¸è¿›è¡Œåˆ†å¸ƒå¼  -1ä¸”å¤šå—gpuä½¿ç”¨DataParallelæ¨¡å¼  é»˜è®¤-1
                      workers=8,
                      image_weights=False,  # è®­ç»ƒæ—¶æ˜¯å¦æ ¹æ®å›¾ç‰‡æ ·æœ¬çœŸå®æ¡†åˆ†å¸ƒæƒé‡æ¥é€‰æ‹©å›¾ç‰‡  é»˜è®¤False
                      quad=False,
                      prefix='',            # æ˜¾ç¤ºä¿¡æ¯  ä¸€ä¸ªæ ‡å¿— å¤šä¸ºtrain/valï¼Œå¤„ç†æ ‡ç­¾æ—¶ä¿å­˜cacheæ–‡ä»¶ä¼šç”¨åˆ°
                      shuffle=False):
    # æ˜¯å¦ä½¿ç”¨çŸ©å½¢è®­ç»ƒæ¨¡å¼
    if rect and shuffle:    # åšä¸€ä¸ªä¿æŠ¤ï¼Œrectæ—¶ä¸èƒ½æ‰“ä¹±shuffleï¼ˆå› ä¸ºåºåˆ—æ˜¯å›ºå®šçš„ï¼‰
        LOGGER.warning('WARNING: --rect is incompatible with DataLoader shuffle, setting shuffle=False')
        shuffle = False
    # ä¸»è¿›ç¨‹å®ç°æ•°æ®çš„é¢„è¯»å–å¹¶ç¼“å­˜ï¼Œç„¶åå…¶å®ƒå­è¿›ç¨‹åˆ™ä»ç¼“å­˜ä¸­è¯»å–æ•°æ®å¹¶è¿›è¡Œä¸€ç³»åˆ—è¿ç®—ã€‚
    # ä¸ºäº†å®Œæˆæ•°æ®çš„æ­£å¸¸åŒæ­¥, yolov5åŸºäºtorch.distributed.barrier()å‡½æ•°å®ç°äº†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    with torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP   åˆ†å¸ƒå¼
        # è½½å…¥æ–‡ä»¶æ•°æ®(å¢å¼ºæ•°æ®é›†)
        dataset = LoadImagesAndLabels(
            path,
            imgsz,
            batch_size,
            augment=augment,  # augmentation
            hyp=hyp,  # hyperparameters
            rect=rect,  # rectangular batches
            cache_images=cache,
            single_cls=single_cls,
            stride=int(stride),
            pad=pad,
            image_weights=image_weights,
            prefix=prefix)

    batch_size = min(batch_size, len(dataset))
    nd = torch.cuda.device_count()  # number of CUDA devices
    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨DistributedSampler
    sampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)
    # ä½¿ç”¨InfiniteDataLoaderå’Œ_RepeatSampleræ¥å¯¹DataLoaderè¿›è¡Œå°è£…, ä»£æ›¿åŸDå…ˆçš„DataLoader, èƒ½å¤Ÿæ°¸ä¹…æŒç»­çš„é‡‡æ ·æ•°æ®
    loader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates
    generator = torch.Generator()
    generator.manual_seed(0)
    return loader(dataset,
                  batch_size=batch_size,
                  shuffle=shuffle and sampler is None,
                  num_workers=0,
                  sampler=sampler,
                  pin_memory=True,
                  collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn,
                  worker_init_fn=seed_worker,
                  generator=generator), dataset 
```

è¿™ä¸ªå‡½æ•°ä¼šåœ¨`train.py`ä¸­è¢«è°ƒç”¨ï¼Œç”¨äºç”Ÿæˆ`dataloader`,`dataset`

## 4\. è‡ªå®šä¹‰DataLoader

```py
# å½“image_weights=Falseæ—¶ï¼ˆä¸æ ¹æ®å›¾ç‰‡æ ·æœ¬çœŸå®æ¡†åˆ†å¸ƒæƒé‡æ¥é€‰æ‹©å›¾ç‰‡ï¼‰å°±ä¼šè°ƒç”¨è¿™ä¸¤ä¸ªå‡½æ•° è¿›è¡Œè‡ªå®šä¹‰DataLoaderï¼Œè¿›è¡ŒæŒç»­æ€§é‡‡æ ·ã€‚åœ¨ä¸Šé¢çš„create_dataloaderæ¨¡å—ä¸­è¢«è°ƒç”¨ã€‚
class InfiniteDataLoader(dataloader.DataLoader):
    """ Dataloader that reuses workers

    Uses same syntax as vanilla DataLoader
    """
    # ä½¿ç”¨InfiniteDataLoaderå’Œ_RepeatSampleræ¥å¯¹DataLoaderè¿›è¡Œå°è£…, ä»£æ›¿åŸå…ˆçš„DataLoader, èƒ½å¤Ÿæ°¸ä¹…æŒç»­çš„é‡‡æ ·æ•°æ®
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))
        self.iterator = super().__iter__()

    def __len__(self):
        return len(self.batch_sampler.sampler)

    def __iter__(self):
        for _ in range(len(self)):
            yield next(self.iterator)

class _RepeatSampler:
    """ Sampler that repeats forever
    è¿™éƒ¨åˆ†æ˜¯è¿›è¡ŒæŒç»­é‡‡æ ·
    Args:
        sampler (Sampler)
    """

    def __init__(self, sampler):
        self.sampler = sampler

    def __iter__(self):
        while True:
            yield from iter(self.sampler) 
```

## 5\. LoadImagesAndLabels

**è¿™éƒ¨åˆ†åœ¨create_dataloaderé‡Œé¢ç”¨åˆ°ã€‚**

**èµ·ä½œç”¨ä¸»è¦æ˜¯æ•°æ®åŠ è½½ï¼Œä¹Ÿæ˜¯æ•°æ®å¢å¼ºéƒ¨åˆ†ï¼Œå³è‡ªå®šä¹‰æ•°æ®é›†éƒ¨åˆ†ï¼Œç»§æ‰¿è‡ªDatasetï¼Œä¸»è¦æ˜¯é‡å†™äº†`__getitem()__`æ–¹æ³•ã€‚è¿™ä¸ªå‡½æ•°éå¸¸å…³é”®ï¼Œæ˜¯ç†è§£æ•°æ®å¢å¼ºçš„å…³é”®ã€‚**

### 5.1 **init**

```py
def __init__(self,
             path,  # æ•°æ®path
             img_size=640,
             batch_size=16,
             augment=False, # æ•°æ®å¢å¼º
             hyp=None,  # è¶…æƒ¨
             rect=False,
             image_weights=False,   # å›¾ç‰‡æƒé‡
             cache_images=False,
             single_cls=False,
             stride=32,
             pad=0.0,
             prefix=''):

    # 1ã€èµ‹å€¼ä¸€äº›åŸºç¡€çš„selfå˜é‡ ç”¨äºåé¢åœ¨__getitem__ä¸­è°ƒç”¨
    self.img_size = img_size    # ç»è¿‡æ•°æ®å¢å¼ºåçš„æ•°æ®å›¾ç‰‡çš„å¤§å°
    self.augment = augment      # æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º
    self.hyp = hyp              # è¶…å‚æ•°
    self.image_weights = image_weights  # å›¾ç‰‡é‡‡æ ·æƒé‡
    self.rect = False if image_weights else rect    # çŸ©é˜µè®­ç»ƒ   # æ˜¯å¦å¯åŠ¨çŸ©å½¢è®­ç»ƒ ä¸€èˆ¬è®­ç»ƒæ—¶å…³é—­ éªŒè¯æ—¶æ‰“å¼€ å¯ä»¥åŠ é€Ÿ
    # mosaicæ•°æ®å¢å¼º
    self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training) å››å¼ å›¾ç‰‡æ‹¼æˆä¸€å¼ å›¾
    self.mosaic_border = [-img_size // 2, -img_size // 2]
    self.stride = stride    # æ¨¡å‹ä¸‹é‡‡æ ·çš„æ­¥é•¿
    self.path = path
    self.albumentations = Albumentations() if augment else None

    # 2ã€å¾—åˆ°pathè·¯å¾„ä¸‹çš„æ‰€æœ‰å›¾ç‰‡çš„è·¯å¾„self.img_files
    try:
        f = []  # image files
        for p in path if isinstance(path, list) else [path]:
            # è·å–æ•°æ®é›†è·¯å¾„pathï¼ŒåŒ…å«å›¾ç‰‡è·¯å¾„çš„txtæ–‡ä»¶æˆ–åŒ…å«å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„
            # ä½¿ç”¨pathlib.Pathç”Ÿæˆä¸æ“ä½œç³»ç»Ÿæ— å…³çš„è·¯å¾„ï¼Œå› ä¸ºä¸åŒæ“ä½œç³»ç»Ÿè·¯å¾„çš„â€˜/â€™ä¼šæœ‰æ‰€ä¸åŒ
            p = Path(p)  # os-agnostic
            if p.is_dir():  # dir
                # glob.glab: è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨  é€’å½’è·å–pè·¯å¾„ä¸‹æ‰€æœ‰æ–‡ä»¶
                f += glob.glob(str(p / '**' / '*.*'), recursive=True)
                # f = list(p.rglob('*.*'))  # pathlib
            elif p.is_file():  # file
                with open(p) as t:
                    t = t.read().strip().splitlines()   # strip:åˆ é™¤å‰å¯¼å’Œå°¾éšç©ºæ ¼  splitlines()æ–¹æ³•ï¼ŒæŒ‰è¡Œå°†å­—ç¬¦ä¸²åˆ†ä¸ºå­—ç¬¦ä¸²list
                    parent = str(p.parent) + os.sep # è·å–æ•°æ®é›†è·¯å¾„çš„ä¸Šçº§çˆ¶ç›®å½•ï¼›os.sepä¸ºåˆ†éš”ç¬¦ï¼ˆä¸åŒæ“ä½œç³»ç»Ÿçš„åˆ†éš”ç¬¦ä¸ä¸€æ ·ï¼‰
                    f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path
                    # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)
            else:
                raise FileNotFoundError(f'{prefix}{p} does not exist')
        self.im_files = sorted(x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS)
        # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib
        assert self.im_files, f'{prefix}No images found'
    except Exception as e:
        raise Exception(f'{prefix}Error loading data from {path}: {e}\nSee {HELP_URL}')

    # Check cache
    # 3ã€æ ¹æ®imgsè·¯å¾„æ‰¾åˆ°labelsçš„è·¯å¾„self.label_files
    self.label_files = img2label_paths(self.im_files)

    # 4ã€cache label ä¸‹æ¬¡è¿è¡Œè¿™ä¸ªè„šæœ¬çš„æ—¶å€™ç›´æ¥ä»cacheä¸­å–labelè€Œä¸æ˜¯å»æ–‡ä»¶ä¸­å–label é€Ÿåº¦æ›´å¿«
    cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')
    try:
        cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # load dict
        assert cache['version'] == self.cache_version  # matches current version
        assert cache['hash'] == get_hash(self.label_files + self.im_files)  # identical hash
    except Exception:
        # å¦åˆ™è°ƒç”¨cache_labelsç¼“å­˜æ ‡ç­¾åŠæ ‡ç­¾ç›¸å…³ä¿¡æ¯
        cache, exists = self.cache_labels(cache_path, prefix), False  # run cache ops

    # Display cache
    # æ‰“å°cacheçš„ç»“æœ nf nm ne nc n = æ‰¾åˆ°çš„æ ‡ç­¾æ•°é‡ï¼Œæ¼æ‰çš„æ ‡ç­¾æ•°é‡ï¼Œç©ºçš„æ ‡ç­¾æ•°é‡ï¼ŒæŸåçš„æ ‡ç­¾æ•°é‡ï¼Œæ€»çš„æ ‡ç­¾æ•°é‡
    nf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupt, total
    if exists and LOCAL_RANK in {-1, 0}:
        d = f"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupt"
        tqdm(None, desc=prefix + d, total=n, initial=n, bar_format=BAR_FORMAT)  # display cache results
        if cache['msgs']:
            LOGGER.info('\n'.join(cache['msgs']))  # display warnings
    # æ•°æ®é›†æ²¡æœ‰æ ‡ç­¾ä¿¡æ¯ å°±å‘å‡ºè­¦å‘Šå¹¶æ˜¾ç¤ºæ ‡ç­¾labelä¸‹è½½åœ°å€help_url
    assert nf > 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {HELP_URL}'

    # Read cache
    # 5ã€Read cache  ä»cacheä¸­è¯»å‡ºæœ€æ–°å˜é‡èµ‹ç»™self  æ–¹ä¾¿ç»™forwardä¸­ä½¿ç”¨
    # cacheä¸­çš„é”®å€¼å¯¹æœ€åˆæœ‰: cache[img_file]=[l, shape, segments] cache[hash] cache[results] cache[msg] cache[version]
    # å…ˆä»cacheä¸­å»é™¤cacheæ–‡ä»¶ä¸­å…¶ä»–æ— å…³é”®å€¼å¦‚:'hash', 'version', 'msgs'ç­‰éƒ½åˆ é™¤
    [cache.pop(k) for k in ('hash', 'version', 'msgs')]  # remove items
    # popæ‰resultsã€hashã€versionã€msgsååªå‰©ä¸‹cache[img_file]=[l, shape, segments]
    # cache.values(): å–cacheä¸­æ‰€æœ‰å€¼ å¯¹åº”æ‰€æœ‰l, shape, segments
    # labels: å¦‚æœæ•°æ®é›†æ‰€æœ‰å›¾ç‰‡ä¸­æ²¡æœ‰ä¸€ä¸ªå¤šè¾¹å½¢label  labelså­˜å‚¨çš„labelå°±éƒ½æ˜¯åŸå§‹label(éƒ½æ˜¯æ­£å¸¸çš„çŸ©å½¢label)
    #         å¦åˆ™å°†æ‰€æœ‰å›¾ç‰‡æ­£å¸¸gtçš„labelå­˜å…¥labels ä¸æ­£å¸¸gt(å­˜åœ¨ä¸€ä¸ªå¤šè¾¹å½¢)ç»è¿‡segments2boxesè½¬æ¢ä¸ºæ­£å¸¸çš„çŸ©å½¢label
    # shapes: æ‰€æœ‰å›¾ç‰‡çš„shape
    # self.segments: å¦‚æœæ•°æ®é›†æ‰€æœ‰å›¾ç‰‡ä¸­æ²¡æœ‰ä¸€ä¸ªå¤šè¾¹å½¢label  self.segments=None
    #                å¦åˆ™å­˜å‚¨æ•°æ®é›†ä¸­æ‰€æœ‰å­˜åœ¨å¤šè¾¹å½¢gtçš„å›¾ç‰‡çš„æ‰€æœ‰åŸå§‹label(è‚¯å®šæœ‰å¤šè¾¹å½¢label ä¹Ÿå¯èƒ½æœ‰çŸ©å½¢æ­£å¸¸label æœªçŸ¥æ•°)
    # zip æ˜¯å› ä¸ºcacheä¸­æ‰€æœ‰labelsã€shapesã€segmentsä¿¡æ¯éƒ½æ˜¯æŒ‰æ¯å¼ imgåˆ†å¼€å­˜å‚¨çš„, zipæ˜¯å°†æ‰€æœ‰å›¾ç‰‡å¯¹åº”çš„ä¿¡æ¯å åœ¨ä¸€èµ·
    labels, shapes, self.segments = zip(*cache.values())
    self.labels = list(labels)      # labels æ‰€æœ‰å›¾ç‰‡çš„æ‰€æœ‰gtæ¡†çš„ä¿¡æ¯
    self.shapes = np.array(shapes, dtype=np.float64)
    self.im_files = list(cache.keys())  # update
    self.label_files = img2label_paths(cache.keys())  # update
    n = len(shapes)  # number of images
    bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index
    nb = bi[-1] + 1  # number of batches
    self.batch = bi  # batch index of image
    self.n = n
    self.indices = range(n)

    # Update labels
    include_class = []  # filter labels to include only these classes (optional)
    include_class_array = np.array(include_class).reshape(1, -1)
    for i, (label, segment) in enumerate(zip(self.labels, self.segments)):
        if include_class:
            j = (label[:, 0:1] == include_class_array).any(1)
            self.labels[i] = label[j]
            if segment:
                self.segments[i] = segment[j]
        if single_cls:  # single-class training, merge all classes into 0
            self.labels[i][:, 0] = 0
            if segment:
                self.segments[i][:, 0] = 0

    # Rectangular Training
    # 6ã€ä¸ºRectangular Trainingä½œå‡†å¤‡
    # è¿™é‡Œä¸»è¦æ˜¯æ³¨æ„shapesçš„ç”Ÿæˆ è¿™ä¸€æ­¥å¾ˆé‡è¦ å› ä¸ºå¦‚æœé‡‡æ ·çŸ©å½¢è®­ç»ƒé‚£ä¹ˆæ•´ä¸ªbatchçš„å½¢çŠ¶è¦ä¸€æ · å°±è¦è®¡ç®—è¿™ä¸ªç¬¦åˆæ•´ä¸ªbatchçš„shape
    # è€Œä¸”è¿˜è¦å¯¹æ•°æ®é›†æŒ‰ç…§é«˜å®½æ¯”è¿›è¡Œæ’åº è¿™æ ·æ‰èƒ½ä¿è¯åŒä¸€ä¸ªbatchçš„å›¾ç‰‡çš„å½¢çŠ¶å·®ä¸å¤šç›¸åŒ å†é€‰åˆ™ä¸€ä¸ªå…±åŒçš„shapeä»£ä»·ä¹Ÿæ¯”è¾ƒå°
    if self.rect:
        # Sort by aspect ratio
        s = self.shapes  # wh
        ar = s[:, 1] / s[:, 0]  # aspect ratio
        irect = ar.argsort()
        self.im_files = [self.im_files[i] for i in irect]
        self.label_files = [self.label_files[i] for i in irect]
        self.labels = [self.labels[i] for i in irect]
        self.shapes = s[irect]  # wh
        ar = ar[irect]

        # Set training image shapes
        shapes = [[1, 1]] * nb  # åˆå§‹åŒ–shapesï¼Œnbä¸ºä¸€è½®æ‰¹æ¬¡batchçš„æ•°é‡
        for i in range(nb):
            ari = ar[bi == i]
            mini, maxi = ari.min(), ari.max()
            if maxi < 1:
                shapes[i] = [maxi, 1]
            elif mini > 1:
                shapes[i] = [1, 1 / mini]

        self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride

    # Cache images into RAM/disk for faster training (WARNING: large datasets may exceed system resources)
    self.ims = [None] * n
    self.npy_files = [Path(f).with_suffix('.npy') for f in self.im_files]
    if cache_images:
        gb = 0  # Gigabytes of cached images
        self.im_hw0, self.im_hw = [None] * n, [None] * n
        fcn = self.cache_images_to_disk if cache_images == 'disk' else self.load_image
        results = ThreadPool(NUM_THREADS).imap(fcn, range(n))
        pbar = tqdm(enumerate(results), total=n, bar_format=BAR_FORMAT, disable=LOCAL_RANK > 0)
        for i, x in pbar:
            if cache_images == 'disk':
                gb += self.npy_files[i].stat().st_size
            else:  # 'ram'
                self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)
                gb += self.ims[i].nbytes
            pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB {cache_images})'
        pbar.close() 
```

è¿™æ®µä»£ç å‡ ä¸ªæ­¥éª¤ï¼š

1.  èµ‹å€¼ä¸€äº›åŸºç¡€å˜é‡ï¼Œä¸ºåé¢çš„å‡½æ•°åšå‡†å¤‡
2.  è·å–pathè·¯å¾„ä¸‹æ‰€æœ‰å›¾ç‰‡çš„è·¯å¾„`self.img_files`
3.  æ ¹æ®imgsè·¯å¾„æ‰¾åˆ°labelsçš„è·¯å¾„`self.label_files`ï¼Œè¿™é‡Œç”¨åˆ°äº†`img2label_paths`å‡½æ•°
4.  å°†labelå­˜æ”¾åˆ°äº†cacheä¸­ï¼Œè¿™æ ·ä¸‹æ¬¡è¿è¡Œè¿™ä¸ªè„šæœ¬çš„æ—¶å€™å°±å¯ä»¥ç›´æ¥ä»cacheä¸­å–å‡ºlabelï¼Œé€Ÿåº¦æ›´å¿«ï¼Œç›¸å½“äºé«˜é€Ÿç¼“å­˜
5.  æ‰“å°cacheä¸­çš„ç»“æœï¼Œæ¯”å¦‚æ‰¾åˆ°çš„æ ‡ç­¾æ•°é‡ã€æ¼æ‰çš„æ ‡ç­¾æ•°é‡ç¯ç­‰
6.  ä»cacheä¸­è¯»å–æœ€æ–°å˜é‡ç»™selfï¼Œæ–¹ä¾¿ç»™forwardä¸­ä½¿ç”¨ï¼Œå¹¶å°†cacheä¸­å…¶ä»–æ— å…³çš„hashå€¼åˆ é™¤
7.  ä¸ºRetangular Trainingåšå‡†å¤‡ï¼šç”Ÿæˆself.batch_shapes
8.  æ˜¯å¦éœ€è¦cache imageï¼ˆå¤ªå¤§äº†ï¼Œä¸€èˆ¬falseï¼‰

### 5.2 cache_labels

```py
# è¿™ä¸ªå‡½æ•°ç”¨äºåŠ è½½æ–‡ä»¶è·¯å¾„ä¸­çš„labelä¿¡æ¯ç”Ÿæˆcacheæ–‡ä»¶ã€‚cacheæ–‡ä»¶ä¸­åŒ…æ‹¬çš„ä¿¡æ¯æœ‰ï¼šim_file, l, shape, segments, hash, results, msgs, versionç­‰
def cache_labels(self, path=Path('./labels.cache'), prefix=''): # æ—¥å¿—å¤´éƒ¨ä¿¡æ¯(å½©æ‰“é«˜äº®éƒ¨åˆ†)
    # Cache dataset labels, check images and read shapes
    x = {}  # dict  åˆå§‹åŒ–æœ€ç»ˆcacheä¸­ä¿å­˜çš„å­—å…¸dict
    nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages
    desc = f"{prefix}Scanning '{path.parent / path.stem}' images and labels..."
    with Pool(NUM_THREADS) as pool:
        # å®šä¹‰pbarè¿›åº¦æ¡
        # pool.imap_unordered: å¯¹å¤§é‡æ•°æ®éå†å¤šè¿›ç¨‹è®¡ç®— è¿”å›ä¸€ä¸ªè¿­ä»£å™¨
        # æŠŠself.img_files, self.label_files, repeat(prefix) listä¸­çš„å€¼ä½œä¸ºå‚æ•°ä¾æ¬¡é€å…¥(ä¸€æ¬¡é€ä¸€ä¸ª)verify_image_labelå‡½æ•°
        pbar = tqdm(pool.imap(verify_image_label, zip(self.im_files, self.label_files, repeat(prefix))),
                    desc=desc,
                    total=len(self.im_files),
                    bar_format=BAR_FORMAT)
        for im_file, lb, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:
            nm += nm_f
            nf += nf_f
            ne += ne_f
            nc += nc_f
            if im_file:
                x[im_file] = [lb, shape, segments]
            if msg:
                msgs.append(msg)
            pbar.desc = f"{desc}{nf} found, {nm} missing, {ne} empty, {nc} corrupt"

    pbar.close()    # å…³é—­è¿›åº¦æ¡
    if msgs:
        LOGGER.info('\n'.join(msgs))
    if nf == 0:
        LOGGER.warning(f'{prefix}WARNING: No labels found in {path}. See {HELP_URL}')
    x['hash'] = get_hash(self.label_files + self.im_files)
    x['results'] = nf, nm, ne, nc, len(self.im_files)
    x['msgs'] = msgs  # warnings
    x['version'] = self.cache_version  # cache version
    try:
        np.save(path, x)  # save cache for next time
        path.with_suffix('.cache.npy').rename(path)  # remove .npy suffix
        LOGGER.info(f'{prefix}New cache created: {path}')
    except Exception as e:
        LOGGER.warning(f'{prefix}WARNING: Cache directory {path.parent} is not writeable: {e}')  # not writeable
    return x 
```

**è¿™ä¸ªå‡½æ•°ç”¨äºåŠ è½½æ–‡ä»¶è·¯å¾„ä¸­çš„labelä¿¡æ¯ç”Ÿæˆcacheæ–‡ä»¶ã€‚cacheæ–‡ä»¶ä¸­åŒ…æ‹¬çš„ä¿¡æ¯æœ‰ï¼šim_file, l, shape, segments, hash, results, msgs, versionç­‰**

### 5.3 len

```py
def __len__(self):
    return len(self.im_files) 
```

**è·å–æ•°æ®é›†å›¾ç‰‡çš„æ•°é‡**

### 5.4 getitem

```py
# è¿™éƒ¨åˆ†æ˜¯æ•°æ®å¢å¼ºå‡½æ•°ï¼Œä¸€èˆ¬ä¸€æ¬¡æ€§æ‰§è¡Œbatch_sizeæ¬¡ã€‚
def __getitem__(self, index):
    """
           è¿™éƒ¨åˆ†æ˜¯æ•°æ®å¢å¼ºå‡½æ•°ï¼Œä¸€èˆ¬ä¸€æ¬¡æ€§æ‰§è¡Œbatch_sizeæ¬¡ã€‚
           è®­ç»ƒ æ•°æ®å¢å¼º: mosaic(random_perspective) + hsv + ä¸Šä¸‹å·¦å³ç¿»è½¬
           æµ‹è¯• æ•°æ®å¢å¼º: letterbox
           :return torch.from_numpy(img): è¿™ä¸ªindexçš„å›¾ç‰‡æ•°æ®(å¢å¼ºå) [3, 640, 640]
           :return labels_out: è¿™ä¸ªindexå›¾ç‰‡çš„gt label [6, 6] = [gt_num, 0+class+xywh(normalized)]
           :return self.img_files[index]: è¿™ä¸ªindexå›¾ç‰‡çš„è·¯å¾„åœ°å€
           :return shapes: è¿™ä¸ªbatchçš„å›¾ç‰‡çš„shapes æµ‹è¯•æ—¶(çŸ©å½¢è®­ç»ƒ)æ‰æœ‰  éªŒè¯æ—¶ä¸ºNone   for COCO mAP rescaling
    """
    index = self.indices[index]  # linear, shuffled, or image_weights  å¦‚æœå­˜åœ¨image_weightsï¼Œåˆ™è·å–æ–°çš„ä¸‹æ ‡
    hyp = self.hyp
    mosaic = self.mosaic and random.random() < hyp['mosaic']
    # mosaicå¢å¼º å¯¹å›¾åƒè¿›è¡Œ4å¼ å›¾æ‹¼æ¥è®­ç»ƒ  ä¸€èˆ¬è®­ç»ƒæ—¶è¿è¡Œ
    # mosaic + MixUp
    if mosaic:
        # Load mosaic
        img, labels = self.load_mosaic(index)
        shapes = None

        # MixUp augmentation mixupæ•°æ®å¢å¼º
        if random.random() < hyp['mixup']:
            img, labels = mixup(img, labels, *self.load_mosaic(random.randint(0, self.n - 1)))

    # å¦åˆ™ï¼šè½½å…¥å›¾ç‰‡ + letterboxï¼ˆvalï¼‰
    else:
        # Load image
        # è½½å…¥å›¾ç‰‡  è½½å…¥å›¾ç‰‡åè¿˜ä¼šè¿›è¡Œä¸€æ¬¡resize  å°†å½“å‰å›¾ç‰‡çš„æœ€é•¿è¾¹ç¼©æ”¾åˆ°æŒ‡å®šçš„å¤§å°(512), è¾ƒå°è¾¹åŒæ¯”ä¾‹ç¼©æ”¾
        # load image img=(343, 512, 3)=(h, w, c)  (h0, w0)=(335, 500)  numpy  index=4
        # img: resizeåçš„å›¾ç‰‡   (h0, w0): åŸå§‹å›¾ç‰‡çš„hw  (h, w): resizeåçš„å›¾ç‰‡çš„hw
        # è¿™ä¸€æ­¥æ˜¯å°†(335, 500, 3) resize-> (343, 512, 3)
        img, (h0, w0), (h, w) = self.load_image(index)

        # Letterbox
        # letterboxä¹‹å‰ç¡®å®šè¿™å¼ å½“å‰å›¾ç‰‡letterboxä¹‹åçš„shape  å¦‚æœä¸ç”¨self.rectçŸ©å½¢è®­ç»ƒshapeå°±æ˜¯self.img_size
        # å¦‚æœä½¿ç”¨self.rectçŸ©å½¢è®­ç»ƒshapeå°±æ˜¯å½“å‰batchçš„shape å› ä¸ºçŸ©å½¢è®­ç»ƒçš„è¯æˆ‘ä»¬æ•´ä¸ªbatchçš„shapeå¿…é¡»ç»Ÿä¸€(åœ¨__init__å‡½æ•°ç¬¬6èŠ‚å†…å®¹)
        shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape
        # letterbox è¿™ä¸€æ­¥å°†ç¬¬ä¸€æ­¥ç¼©æ”¾å¾—åˆ°çš„å›¾ç‰‡å†ç¼©æ”¾åˆ°å½“å‰batchæ‰€éœ€è¦çš„å°ºåº¦ (343, 512, 3) pad-> (384, 512, 3)
        # (çŸ©å½¢æ¨ç†éœ€è¦ä¸€ä¸ªbatchçš„æ‰€æœ‰å›¾ç‰‡çš„shapeå¿…é¡»ç›¸åŒï¼Œè€Œè¿™ä¸ªshapeåœ¨initå‡½æ•°ä¸­ä¿æŒåœ¨self.batch_shapesä¸­)
        # è¿™é‡Œæ²¡æœ‰ç¼©æ”¾æ“ä½œï¼Œæ‰€ä»¥è¿™é‡Œçš„ratioæ°¸è¿œéƒ½æ˜¯(1.0, 1.0)  pad=(0.0, 20.5)
        img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)
        shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling

        # å›¾ç‰‡è¿›è¡Œletterboxålabelçš„åæ ‡ä¹Ÿè¦ç›¸åº”å˜åŒ–ï¼Œæ ¹æ®padè°ƒæ•´labelåæ ‡ å¹¶å°†å½’ä¸€åŒ–çš„xywh -> æœªå½’ä¸€åŒ–çš„xyxy
        labels = self.labels[index].copy()
        if labels.size:  # normalized xywh to pixel xyxy format æ ¹æ®padè°ƒæ•´æ¡†çš„æ ‡ç­¾åæ ‡ï¼Œå¹¶ä»å½’ä¸€åŒ–xywh->æœªå½’ä¸€åŒ–çš„xyxy
            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])

        # æµ‹è¯•ä»£ç  æµ‹è¯•letterboxæ•ˆæœ
        # cv2.imshow("letterbox", img)
        # cv2.waitKey(0)
        # cv2.destroyAllWindows()
        # print(img.shape)   # (640, 640, 3)

        if self.augment:
            # ä¸åšmosaicçš„è¯å°±è¦åšrandom_perspectiveå¢å¼º å› ä¸ºmosaicå‡½æ•°å†…éƒ¨æ‰§è¡Œäº†random_perspectiveå¢å¼º
            # random_perspectiveå¢å¼º: éšæœºå¯¹å›¾ç‰‡è¿›è¡Œæ—‹è½¬ï¼Œå¹³ç§»ï¼Œç¼©æ”¾ï¼Œè£å‰ªï¼Œé€è§†å˜æ¢
            img, labels = random_perspective(img,
                                             labels,
                                             degrees=hyp['degrees'],
                                             translate=hyp['translate'],
                                             scale=hyp['scale'],
                                             shear=hyp['shear'],
                                             perspective=hyp['perspective'])

    nl = len(labels)  # number of labels
    if nl:
        labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=True, eps=1E-3)

    if self.augment:
        # Albumentations
        img, labels = self.albumentations(img, labels)
        nl = len(labels)  # update after albumentations

        # HSV color-space éšæœºæ”¹å˜å›¾ç‰‡çš„è‰²è°ƒHã€é¥±å’Œåº¦Sã€äº®åº¦V
        augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])

        # Flip up-down
        if random.random() < hyp['flipud']:
            img = np.flipud(img)
            if nl:
                labels[:, 2] = 1 - labels[:, 2]

        # Flip left-right
        if random.random() < hyp['fliplr']:
            img = np.fliplr(img)
            if nl:
                labels[:, 1] = 1 - labels[:, 1]

        # Cutouts
        # labels = cutout(img, labels, p=0.5)
        # nl = len(labels)  # update after cutout

    labels_out = torch.zeros((nl, 6))
    if nl:
        labels_out[:, 1:] = torch.from_numpy(labels)

    # Convert
    # img[:,:,::-1]çš„ä½œç”¨æ˜¯å®ç°BGRåˆ°RGBé€šé“çš„è½¬æ¢ï¼Œå¯¹äºåˆ—è¡¨imgè¿›è¡Œ[:,:,::-1]çš„ä½œç”¨æ˜¯åˆ—è¡¨æ•°ç»„å·¦å³ç¿»è½¬
    # channelè½´æ¢åˆ°å‰é¢
    # torch.Tensor é«˜ç»´çŸ©é˜µçš„è¡¨ç¤ºï¼š (nSample)*C*H*W
    # num.ndarry é«˜ç»´çŸ©é˜µçš„è¡¨ç¤ºï¼š H*W*C
    img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
    img = np.ascontiguousarray(img)

    return torch.from_numpy(img), labels_out, self.im_files[index], shapes 
```

**ç›¸å½“äºé‡å†™`[]`ï¼Œè·Ÿæ•°æ®å¢å¼ºç›¸å…³ï¼Œä¸€èˆ¬ä¸€æ¬¡æ€§æ‰§è¡Œ`batch_size`æ¬¡ã€‚**

### 5.5 collate_fn

```py
@staticmethod
def collate_fn(batch):  # æ•´ç†å‡½æ•°ï¼šå¦‚ä½•å–æ ·æœ¬çš„ï¼Œå¯ä»¥å®šä¹‰è‡ªå·±çš„å‡½æ•°å®ç°æƒ³è¦çš„åŠŸèƒ½
    """è¿™ä¸ªå‡½æ•°ä¼šåœ¨create_dataloaderä¸­ç”Ÿæˆdataloaderæ—¶è°ƒç”¨ï¼š
            æ•´ç†å‡½æ•°  å°†imageå’Œlabelæ•´åˆåˆ°ä¸€èµ·
            :return torch.stack(img, 0): å¦‚[16, 3, 640, 640] æ•´ä¸ªbatchçš„å›¾ç‰‡
            :return torch.cat(label, 0): å¦‚[15, 6] [num_target, img_index+class_index+xywh(normalized)] æ•´ä¸ªbatchçš„label
            :return path: æ•´ä¸ªbatchæ‰€æœ‰å›¾ç‰‡çš„è·¯å¾„
            :return shapes: (h0, w0), ((h / h0, w / w0), pad)    for COCO mAP rescaling
            pytorchçš„DataLoaderæ‰“åŒ…ä¸€ä¸ªbatchçš„æ•°æ®é›†æ—¶è¦ç»è¿‡æ­¤å‡½æ•°è¿›è¡Œæ‰“åŒ… é€šè¿‡é‡å†™æ­¤å‡½æ•°å®ç°æ ‡ç­¾ä¸å›¾ç‰‡å¯¹åº”çš„åˆ’åˆ†ï¼Œä¸€ä¸ªbatchä¸­å“ªäº›æ ‡ç­¾å±äºå“ªä¸€å¼ å›¾ç‰‡,å½¢å¦‚
                [[0, 6, 0.5, 0.5, 0.26, 0.35],
                 [0, 6, 0.5, 0.5, 0.26, 0.35],
                 [1, 6, 0.5, 0.5, 0.26, 0.35],
                 [2, 6, 0.5, 0.5, 0.26, 0.35],]
               å‰ä¸¤è¡Œæ ‡ç­¾å±äºç¬¬ä¸€å¼ å›¾ç‰‡, ç¬¬ä¸‰è¡Œå±äºç¬¬äºŒå¼ ã€‚ã€‚ã€‚
    """
    # img: ä¸€ä¸ªtuple ç”±batch_sizeä¸ªtensorç»„æˆ æ•´ä¸ªbatchä¸­æ¯ä¸ªtensorè¡¨ç¤ºä¸€å¼ å›¾ç‰‡
    # label: ä¸€ä¸ªtuple ç”±batch_sizeä¸ªtensorç»„æˆ æ¯ä¸ªtensorå­˜æ”¾ä¸€å¼ å›¾ç‰‡çš„æ‰€æœ‰çš„targetä¿¡æ¯
    #        label[6, object_num] 6ä¸­çš„ç¬¬ä¸€ä¸ªæ•°ä»£è¡¨ä¸€ä¸ªbatchä¸­çš„ç¬¬å‡ å¼ å›¾
    # path: ä¸€ä¸ªtuple ç”±4ä¸ªstrç»„æˆ, æ¯ä¸ªstrå¯¹åº”ä¸€å¼ å›¾ç‰‡çš„åœ°å€ä¿¡æ¯
    im, label, path, shapes = zip(*batch)  # transposed
    for i, lb in enumerate(label):
        lb[:, 0] = i  # add target image index for build_targets()
    # è¿”å›çš„img=[batch_size, 3, 736, 736]
    #      torch.stack(img, 0): å°†batch_sizeä¸ª[3, 736, 736]çš„çŸ©é˜µæ‹¼æˆä¸€ä¸ª[batch_size, 3, 736, 736]
    # label=[target_sums, 6]  6ï¼šè¡¨ç¤ºå½“å‰targetå±äºå“ªä¸€å¼ å›¾+class+x+y+w+h
    #      torch.cat(label, 0): å°†[n1,6]ã€[n2,6]ã€[n3,6]...æ‹¼æ¥æˆ[n1+n2+n3+..., 6]
    # è¿™é‡Œä¹‹æ‰€ä»¥æ‹¼æ¥çš„æ–¹å¼ä¸åŒæ˜¯å› ä¸ºimgæ‹¼æ¥çš„æ—¶å€™å®ƒçš„æ¯ä¸ªéƒ¨åˆ†çš„å½¢çŠ¶æ˜¯ç›¸åŒçš„ï¼Œéƒ½æ˜¯[3, 736, 736]
    # è€Œæˆ‘labelçš„æ¯ä¸ªéƒ¨åˆ†çš„å½¢çŠ¶æ˜¯ä¸ä¸€å®šç›¸åŒçš„ï¼Œæ¯å¼ å›¾çš„ç›®æ ‡ä¸ªæ•°æ˜¯ä¸ä¸€å®šç›¸åŒçš„ï¼ˆlabelè‚¯å®šä¹Ÿå¸Œæœ›ç”¨stack,æ›´æ–¹ä¾¿,ä½†æ˜¯ä¸èƒ½é‚£æ ·æ‹¼ï¼‰
    # å¦‚æœæ¯å¼ å›¾çš„ç›®æ ‡ä¸ªæ•°æ˜¯ç›¸åŒçš„ï¼Œé‚£æˆ‘ä»¬å°±å¯èƒ½ä¸éœ€è¦é‡å†™collate_fnå‡½æ•°äº†
    return torch.stack(im, 0), torch.cat(label, 0), path, shapes 
```

**å¾ˆå¤šäººä»¥ä¸ºå†™å®Œ init å’Œ getitem å‡½æ•°æ•°æ®å¢å¼ºå°±åšå®Œäº†ï¼Œæˆ‘ä»¬åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„ç¡®å†™å®Œè¿™ä¸¤ä¸ªå‡½æ•°å°±å¯ä»¥äº†ï¼Œå› ä¸ºç³»ç»Ÿä¸­æ˜¯ç»™æˆ‘ä»¬å†™å¥½äº†ä¸€ä¸ªcollate_fnå‡½æ•°çš„ï¼Œä½†æ˜¯åœ¨ç›®æ ‡æ£€æµ‹ä¸­æˆ‘ä»¬å´éœ€è¦é‡å†™collate_fnå‡½æ•°**

> æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°ä¸€èˆ¬æ˜¯å½“è°ƒç”¨äº†batch_sizeæ¬¡ getitem å‡½æ•°åæ‰ä¼šè°ƒç”¨ä¸€æ¬¡è¿™ä¸ªå‡½æ•°ï¼Œå¯¹batch_sizeå¼ å›¾ç‰‡å’Œå¯¹åº”çš„labelè¿›è¡Œæ‰“åŒ…ã€‚
> **References**
> CSDN qq_27172615ï¼š [ç²¾è¯»yolo LoadImagesAndLabelsç±»](https://blog.csdn.net/qq_27172615/article/details/128729070)

## 6\. img2label_paths

```py
def img2label_paths(img_paths):
    '''
        ç”¨åœ¨LoadImagesAndLabelsçš„initå‡½æ•°ä¸­
        æ ¹æ®imgså›¾ç‰‡çš„è·¯å¾„æ‰¾åˆ°å¯¹åº”labelsçš„è·¯å¾„
    Define label paths as a function of image paths
    :params img_paths: {list: 50}  æ•´ä¸ªæ•°æ®é›†çš„å›¾ç‰‡ç›¸å¯¹è·¯å¾„  ä¾‹å¦‚: '..\\datasets\\VOC\\images\\train2007\\000012.jpg'
                                                        =>   '..\\datasets\\VOC\\labels\\train2007\\000012.txt'

    '''
    # Define label paths as a function of image paths
    # os.sep å¯ä»¥æ ¹æ®æ‰€å¤„çš„å¹³å°ä¸åŒï¼Œè‡ªé€‚åº”é‡‡ç”¨åˆ†éš”ç¬¦
    # sa: '\\images\\'    sb: '\\labels\\'
    sa, sb = f'{os.sep}images{os.sep}', f'{os.sep}labels{os.sep}'  # /images/, /labels/ substrings
    # æŠŠimg_pathsä¸­æ‰€æœ‰å›¾ç‰‡è·¯å¾„ä¸­çš„imagesæ›¿æ¢ä¸ºlabels
    return [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths] 
```

æ ¹æ®æºç å¯çŸ¥ï¼Œæˆ‘ä»¬åœ¨åˆ¶ä½œæ•°æ®é›†æ—¶ï¼Œå›¾ç‰‡çš„æ–‡ä»¶å¤¹å¿…é¡»è®¾ç½®ä¸º`images`ï¼Œæ ‡ç­¾çš„åå­—å¿…é¡»è®¾ä¸º`labels`ã€‚

å¹¶ä¸”æ”¾åœ¨ç›¸åŒçš„è·¯å¾„ä¸‹ã€‚åœ¨æ–‡ä»¶å¤¹é‡Œé¢ï¼Œå†åˆ†`train`ã€`val`ã€`test`ç­‰æ–‡ä»¶å¤¹

## 7\. verify_image_label

```py
# è¿™ä¸ªå‡½æ•°ç”¨äºæ£€æŸ¥æ¯ä¸€å¼ å›¾ç‰‡å’Œæ¯ä¸€å¼ labelæ–‡ä»¶æ˜¯å¦å®Œå¥½ã€‚
def verify_image_label(args):
    # Verify one image-label pair
    im_file, lb_file, prefix = args
    # segments: å­˜æ”¾è¿™å¼ å›¾æ‰€æœ‰gtæ¡†çš„ä¿¡æ¯(åŒ…å«segmentså¤šè¾¹å½¢: labelæŸä¸€åˆ—æ•°å¤§äº8)
    nm, nf, ne, nc, msg, segments = 0, 0, 0, 0, '', []  # number (missing, found, empty, corrupt), message, segments
    try:
        # verify images
        im = Image.open(im_file)    # æ‰“å¼€å›¾ç‰‡
        im.verify()  # PIL verify   æ£€æŸ¥å›¾ç‰‡å†…å®¹å’Œæ ¼å¼æ˜¯å¦æ­£å¸¸
        shape = exif_size(im)  # image size
        assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'
        assert im.format.lower() in IMG_FORMATS, f'invalid image format {im.format}'
        if im.format.lower() in ('jpg', 'jpeg'):
            with open(im_file, 'rb') as f:
                f.seek(-2, 2)
                if f.read() != b'\xff\xd9':  # corrupt JPEG
                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, 'JPEG', subsampling=0, quality=100)
                    msg = f'{prefix}WARNING: {im_file}: corrupt JPEG restored and saved'

        # verify labels
        if os.path.isfile(lb_file):
            nf = 1  # label found
            with open(lb_file) as f:
                lb = [x.split() for x in f.read().strip().splitlines() if len(x)]
                if any(len(x) > 6 for x in lb):  # is segment
                    classes = np.array([x[0] for x in lb], dtype=np.float32)
                    # segments(å¤šè¾¹å½¢) -> bbox(æ­£æ–¹å½¢), å¾—åˆ°æ–°æ ‡ç­¾  [gt_num, cls+xywh(normalized)]
                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in lb]  # (cls, xy1...)
                    lb = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)
                lb = np.array(lb, dtype=np.float32)
            nl = len(lb)
            if nl:
                # åˆ¤æ–­æ ‡ç­¾æ˜¯å¦æœ‰5åˆ—
                assert lb.shape[1] == 5, f'labels require 5 columns, {lb.shape[1]} columns detected'
                # æ˜¯å¦å…¨éƒ¨å¤§äº0
                assert (lb >= 0).all(), f'negative label values {lb[lb < 0]}'
                # åˆ¤æ–­æ ‡ç­¾åæ ‡x y w hæ˜¯å¦å½’ä¸€åŒ–
                assert (lb[:, 1:] <= 1).all(), f'non-normalized or out of bounds coordinates {lb[:, 1:][lb[:, 1:] > 1]}'
                # åˆ¤æ–­æ ‡ç­¾ä¸­æ˜¯å¦æœ‰é‡å¤çš„åæ ‡
                _, i = np.unique(lb, axis=0, return_index=True)
                if len(i) < nl:  # duplicate row check
                    lb = lb[i]  # remove duplicates
                    if segments:
                        segments = segments[i]
                    msg = f'{prefix}WARNING: {im_file}: {nl - len(i)} duplicate labels removed'
            else:
                ne = 1  # label empty
                lb = np.zeros((0, 5), dtype=np.float32)
        else:
            nm = 1  # label missing
            lb = np.zeros((0, 5), dtype=np.float32)
        return im_file, lb, shape, segments, nm, nf, ne, nc, msg
    except Exception as e:
        nc = 1
        msg = f'{prefix}WARNING: {im_file}: ignoring corrupt image/label: {e}'
        return [None, None, None, None, nm, nf, ne, nc, msg] 
```

è¿™ä¸€éƒ¨åˆ†æ˜¯æ£€æŸ¥æ¯ä¸€å¼ å›¾ç‰‡å’Œæ¯ä¸€å¼ labelæ˜¯å¦å®Œå¥½ã€‚

*   **å›¾ç‰‡ï¼š**ä¸»è¦æ˜¯çœ‹æ ¼å¼æ˜¯å¦æŸå
*   **labelsï¼š**çœ‹æ ‡ç­¾æ˜¯å¦æœ‰5åˆ—ã€å½’ä¸€åŒ–ã€é‡å¤ç­‰

## 8\. load_image

```py
 def load_image(self, i):
        """ç”¨åœ¨LoadImagesAndLabelsæ¨¡å—çš„__getitem__å‡½æ•°å’Œload_mosaicæ¨¡å—ä¸­
            ä»selfæˆ–è€…ä»å¯¹åº”å›¾ç‰‡è·¯å¾„ä¸­è½½å…¥å¯¹åº”indexçš„å›¾ç‰‡ å¹¶å°†åŸå›¾ä¸­hwä¸­è¾ƒå¤§è€…æ‰©å±•åˆ°self.img_size, è¾ƒå°è€…åŒæ¯”ä¾‹æ‰©å±•
            loads 1 image from dataset, returns img, original hw, resized hw
            :params self: ä¸€èˆ¬æ˜¯å¯¼å…¥LoadImagesAndLabelsä¸­çš„self
            :param index: å½“å‰å›¾ç‰‡çš„index
            :return: img: resizeåçš„å›¾ç‰‡
                    (h0, w0): hw_original  åŸå›¾çš„hw
                    img.shape[:2]: hw_resized resizeåçš„å›¾ç‰‡hw(hwä¸­è¾ƒå¤§è€…æ‰©å±•åˆ°self.img_size, è¾ƒå°è€…åŒæ¯”ä¾‹æ‰©å±•)
        """
        # Loads 1 image from dataset index 'i', returns (im, original hw, resized hw)
        im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i],
        # å›¾ç‰‡æ˜¯ç©ºçš„ï¼Œåˆ™ä»å¯¹åº”è·¯å¾„è¯»å–
        if im is None:  # not cached in RAM
            if fn.exists():  # load npy
                im = np.load(fn)
            else:  # read image
                im = cv2.imread(f)  # BGR
                assert im is not None, f'Image Not Found {f}'
            h0, w0 = im.shape[:2]  # orig hw
            r = self.img_size / max(h0, w0)  # ratio
            if r != 1:  # if sizes are not equal
                # ä¸åŒæ–¹å¼çš„ç¼©æ”¾
                interp = cv2.INTER_LINEAR if (self.augment or r > 1) else cv2.INTER_AREA
                im = cv2.resize(im, (int(w0 * r), int(h0 * r)), interpolation=interp)
            return im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized
        return self.ims[i], self.im_hw0[i], self.im_hw[i]  # im, hw_original, hw_resized 
```

**è¿™ä¸€éƒ¨åˆ†æ˜¯åŠ è¼‰å›¾ç‰‡å¹¶æ ¹æ®è®¾å®šçš„è¾“å…¥å¤§å°ä¸å›¾ç‰‡åŸå¤§å°çš„æ¯”ä¾‹ratioè¿›è¡Œresize**

**ç”¨åœ¨LoadImagesAndLabelsæ¨¡å—çš„__getitem__å‡½æ•°å’Œload_mosaicæ¨¡å—ä¸­**

## 9\. load_mosaic

è¿™ä¸ªæ¿å—å°±æ˜¯å¤§åé¼é¼çš„mosaicï¼Œè®­ç»ƒçš„æ—¶å€™éƒ½ä¼šç”¨åˆ°å®ƒï¼Œå¯ä»¥å¤§å¹…åº¦æå‡å°ç›®æ ‡çš„mAPã€‚éå¸¸é‡è¦ï¼Œéœ€è¦ç†Ÿç»ƒæŒæ¡ã€‚

```py
# ç”Ÿæˆä¸€ä¸ªmosaicå¢å¼ºçš„å›¾ç‰‡
def load_mosaic(self, index):
    """ç”¨åœ¨LoadImagesAndLabelsæ¨¡å—çš„__getitem__å‡½æ•° è¿›è¡Œmosaicæ•°æ®å¢å¼º
        å°†å››å¼ å›¾ç‰‡æ‹¼æ¥åœ¨ä¸€å¼ é©¬èµ›å…‹å›¾åƒä¸­  loads images in a 4-mosaic
        :param index: éœ€è¦è·å–çš„å›¾åƒç´¢å¼•
        :return: img4: mosaicå’Œéšæœºé€è§†å˜æ¢åçš„ä¸€å¼ å›¾ç‰‡  numpy(640, 640, 3)
                 labels4: img4å¯¹åº”çš„target  [M, cls+x1y1x2y2]
    """
    # YOLOv5 4-mosaic loader. Loads 1 image + 3 random images into a 4-image mosaic
    # labels4: ç”¨äºå­˜æ”¾æ‹¼æ¥å›¾åƒï¼ˆ4å¼ å›¾æ‹¼æˆä¸€å¼ ï¼‰çš„labelä¿¡æ¯(ä¸åŒ…å«segmentså¤šè¾¹å½¢)
    # segments4: ç”¨äºå­˜æ”¾æ‹¼æ¥å›¾åƒï¼ˆ4å¼ å›¾æ‹¼æˆä¸€å¼ ï¼‰çš„labelä¿¡æ¯(åŒ…å«segmentså¤šè¾¹å½¢)
    labels4, segments4 = [], []
    s = self.img_size
    # éšæœºåˆå§‹åŒ–æ‹¼æ¥å›¾åƒçš„ä¸­å¿ƒç‚¹åæ ‡  [0, s*2]ä¹‹é—´éšæœºå–2ä¸ªæ•°ä½œä¸ºæ‹¼æ¥å›¾åƒçš„ä¸­å¿ƒåæ ‡
    yc, xc = (int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border)  # mosaic center x, y å–ä¸­å¿ƒç‚¹
    # ä»datasetä¸­éšæœºå¯»æ‰¾é¢å¤–çš„ä¸‰å¼ å›¾åƒè¿›è¡Œæ‹¼æ¥ [14, 26, 2, 16] å†éšæœºé€‰ä¸‰å¼ å›¾ç‰‡çš„index
    indices = [index] + random.choices(self.indices, k=3)  # 3 additional image indices
    random.shuffle(indices) # å°†åˆ—è¡¨ä¸­å…ƒç´ æ‰“ä¹±
    for i, index in enumerate(indices):
        # Load image
        # æ¯æ¬¡æ‹¿ä¸€å¼ å›¾ç‰‡ å¹¶å°†è¿™å¼ å›¾ç‰‡resizeåˆ°self.size(h,w)
        # åŠ è½½å›¾ç‰‡å¹¶æ ¹æ®è®¾å®šçš„è¾“å…¥å¤§å°ä¸å›¾ç‰‡åŸå¤§å°çš„æ¯”ä¾‹ratioè¿›è¡Œresize
        img, _, (h, w) = self.load_image(index)

        # place img in img4
        if i == 0:  # top left
            # åˆå§‹åŒ–å¤§å›¾
            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles
            # è®¾ç½®å¤§å›¾ä¸Šçš„ä½ç½®ï¼ˆå·¦ä¸Šè§’ï¼‰
            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)
            # é€‰å–å°å›¾ä¸Šçš„ä½ç½®
            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)
        elif i == 1:  # top right
            # è®¾ç½®å¤§å›¾ä¸Šçš„ä½ç½®ï¼ˆå³ä¸Šè§’ï¼‰
            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc
            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h
        elif i == 2:  # bottom left å·¦ä¸‹è§’
            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)
        elif i == 3:  # bottom right å³ä¸‹è§’
            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)

        # å°†å°å›¾ä¸Šæˆªå–çš„éƒ¨åˆ†è´´åˆ°å¤§å›¾ä¸Š
        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]
        # è®¡ç®—å°å›¾åˆ°å¤§å›¾æ—¶äº§ç”Ÿçš„åç§»ï¼Œç”¨æ¥è®¡ç®—mosaicå¢å¼ºåçš„æ ‡ç­¾æ¡†çš„ä½ç½®
        padw = x1a - x1b
        padh = y1a - y1b

        # Labels
        labels, segments = self.labels[index].copy(), self.segments[index].copy()
        # è·å–æ ‡ç­¾
        if labels.size:
            # å°†xywhï¼ˆç™¾åˆ†æ¯”é‚£äº›å€¼ï¼‰æ ‡å‡†åŒ–ä¸ºåƒç´ xyæ ¼å¼
            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # normalized xywh to pixel xyxy format
            #è½¬ä¸ºåƒç´ æ®µ
            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]
        labels4.append(labels)
        # å¡«è¿›åˆ—è¡¨
        segments4.extend(segments)

    # Concat/clip labels
    # è°ƒæ•´æ ‡ç­¾æ¡†åœ¨å›¾ç‰‡å†…éƒ¨
    labels4 = np.concatenate(labels4, 0)    # å¯¹arrayè¿›è¡Œæ‹¼æ¥çš„å‡½æ•°ï¼Œä»¥ç¬¬ä¸€ç»´åº¦è¿›è¡Œæ‹¼æ¥
    for x in (labels4[:, 1:], *segments4):
        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()
    # img4, labels4 = replicate(img4, labels4)  # replicate

    # Augment
    # è¿›è¡Œmosaicçš„æ—¶å€™å°†å››å¼ å›¾ç‰‡æ•´åˆåˆ°ä¸€èµ·ä¹‹åshapeä¸º[2*img_size,2*img_size]
    # å¯¹mosaicæ•´åˆçš„å›¾ç‰‡è¿›è¡Œéšæœºæ—‹è½¬ã€å¹³ç§»ã€ç¼©æ”¾ã€è£å‰ªï¼Œå¹¶resizeä¸ºè¾“å…¥å¤§å°img_size
    img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp['copy_paste'])
    img4, labels4 = random_perspective(img4,
                                       labels4,
                                       segments4,
                                       degrees=self.hyp['degrees'],
                                       translate=self.hyp['translate'],
                                       scale=self.hyp['scale'],
                                       shear=self.hyp['shear'],
                                       perspective=self.hyp['perspective'],
                                       border=self.mosaic_border)  # border to remove

    return img4, labels4 
```

åŒç†ï¼Œè¿˜æœ‰ä¸ªload_mosaic9å‡½æ•°ï¼Œåšæ³•ç›¸åŒï¼Œç”¨çš„å¥½åƒå¹¶ä¸æ˜¯å¾ˆå¤šï¼Œæ•ˆæœæ²¡mosaic4å¥½ã€‚

å¤§è‡´æ­¥éª¤ï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](img/ac51e4a38c97b3cd118bdb70033259c1.png)
å­—æœ‰ç‚¹ä¸‘å“ˆå“ˆå“ˆï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œlabels4ä¹Ÿæ˜¯éœ€è¦è¿›è¡Œç›¸å¯¹ä½ç½®å˜æ¢çš„ã€‚

## æ€»ç»“

dataloaderéƒ¨åˆ†å·®ä¸å¤šå°±æ˜¯è¿™äº›äº†ï¼Œè¿™éƒ¨åˆ†çš„å†…å®¹ä¸»è¦æ˜¯åˆ›å»ºæ•°æ®é›†ï¼Œæ–¹ä¾¿åé¢è®­ç»ƒçš„æ—¶å€™è°ƒç”¨ï¼Œå¹¶ä¸”åœ¨è¿›è¡ŒæŸå¤±å‡½æ•°è®¡ç®—çš„æ—¶å€™ï¼Œä¹Ÿéœ€è¦ä¼ å…¥labelsã€‚ä¸ç®—å¾ˆéš¾ï¼Œæœ€éš¾çš„åº”è¯¥ç®—æ˜¯æ¥ä¸‹æ¥çš„`loss.py`äº†ã€‚