# `.\pytorch\aten\src\ATen\nnapi\codegen.py`

```
#!/usr/bin/env python3
"""
Code generator for NNAPI wrapper.  We can't link directly against
libneuralnetworks.so because we want PyTorch to work on Android
devices that don't have it available.  Instead, we generate a wrapper
that opens libneuralnetworks.so with dlopen and finds the functions
we need with dlsym.  We also generate a "check" wrapper that checks
return values and throws C++ exceptions on errors.
"""

import re
import sys
import textwrap
from pathlib import Path

# 定义一个多行字符串，包含版权信息和许可协议
PREFIX = """\
/**
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// This file is generated by nnapi/codegen.py
"""

# 定义 NNAPI 函数列表，每个元素是一个三元组，包括函数返回类型、函数名和参数列表
NNAPI_FUNCTIONS = [
    ("int", "ANeuralNetworks_getDeviceCount", "uint32_t* numDevices"),  # noqa: B950
    (
        "int",
        "ANeuralNetworks_getDevice",
        "uint32_t devIndex, ANeuralNetworksDevice** device",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksDevice_getName",
        "const ANeuralNetworksDevice* device, const char** name",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksDevice_getVersion",
        "const ANeuralNetworksDevice* device, const char** version",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksDevice_getFeatureLevel",
        "const ANeuralNetworksDevice* device, int64_t* featureLevel",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksModel_getSupportedOperationsForDevices",
        " const ANeuralNetworksModel* model, const ANeuralNetworksDevice* const* devices, uint32_t numDevices, bool* supportedOps",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksCompilation_createForDevices",
        "ANeuralNetworksModel* model, const ANeuralNetworksDevice* const* devices, uint32_t numDevices, ANeuralNetworksCompilation** compilation",  # noqa: B950
    ),
    (
        "int",
        "ANeuralNetworksExecution_compute",
        "ANeuralNetworksExecution* execution",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksMemory_createFromFd",
        "size_t size, int protect, int fd, size_t offset, ANeuralNetworksMemory** memory",
    ),  # noqa: B950
    (
        "void",
        "ANeuralNetworksMemory_free",
        "ANeuralNetworksMemory* memory",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksModel_create",
        "ANeuralNetworksModel** model",
    ),  # noqa: B950
    # 声明一个元组，每个元素包含函数返回类型、函数名和参数列表，使用元组中的每个注释都是自动化提示的一部分
    ("void", "ANeuralNetworksModel_free", "ANeuralNetworksModel* model"),  # noqa: B950
    # 声明一个元组，每个元素包含函数返回类型、函数名和参数列表，使用元组中的每个注释都是自动化提示的一部分
    ("int", "ANeuralNetworksModel_finish", "ANeuralNetworksModel* model"),  # noqa: B950
    # 声明一个元组，每个元素包含函数返回类型、函数名和参数列表，使用元组中的每个注释都是自动化提示的一部分
    (
        "int",
        "ANeuralNetworksModel_addOperand",
        "ANeuralNetworksModel* model, const ANeuralNetworksOperandType* type",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksModel_setOperandValue",
        "ANeuralNetworksModel* model, int32_t index, const void* buffer, size_t length",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksModel_setOperandValueFromMemory",
        "ANeuralNetworksModel* model, int32_t index, const ANeuralNetworksMemory* memory, size_t offset, size_t length",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksModel_addOperation",
        "ANeuralNetworksModel* model, ANeuralNetworksOperationType type, uint32_t inputCount, const uint32_t* inputs, uint32_t outputCount, const uint32_t* outputs",  # noqa: B950
    ),
    (
        "int",
        "ANeuralNetworksModel_identifyInputsAndOutputs",
        "ANeuralNetworksModel* model, uint32_t inputCount, const uint32_t* inputs, uint32_t outputCount, const uint32_t* outputs",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksModel_relaxComputationFloat32toFloat16",
        "ANeuralNetworksModel* model, bool allow",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksCompilation_create",
        "ANeuralNetworksModel* model, ANeuralNetworksCompilation** compilation",
    ),  # noqa: B950
    (
        "void",
        "ANeuralNetworksCompilation_free",
        "ANeuralNetworksCompilation* compilation",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksCompilation_setPreference",
        "ANeuralNetworksCompilation* compilation, int32_t preference",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksCompilation_finish",
        "ANeuralNetworksCompilation* compilation",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_create",
        "ANeuralNetworksCompilation* compilation, ANeuralNetworksExecution** execution",
    ),  # noqa: B950
    (
        "void",
        "ANeuralNetworksExecution_free",
        "ANeuralNetworksExecution* execution",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_setInput",
        "ANeuralNetworksExecution* execution, int32_t index, const ANeuralNetworksOperandType* type, const void* buffer, size_t length",  # noqa: B950
    ),
    (
        "int",
        "ANeuralNetworksExecution_setInputFromMemory",
        "ANeuralNetworksExecution* execution, int32_t index, const ANeuralNetworksOperandType* type, const ANeuralNetworksMemory* memory, size_t offset, size_t length",  # noqa: B950
    ),
    (
        "int",
        "ANeuralNetworksExecution_setOutput",
        "ANeuralNetworksExecution* execution, int32_t index, const ANeuralNetworksOperandType* type, void* buffer, size_t length",
    ),
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_setOutputFromMemory",
        "ANeuralNetworksExecution* execution, int32_t index, const ANeuralNetworksOperandType* type, const ANeuralNetworksMemory* memory, size_t offset, size_t length",  # noqa: B950
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_startCompute",
        "ANeuralNetworksExecution* execution, ANeuralNetworksEvent** event",
    ),  # noqa: B950
    ("int", "ANeuralNetworksEvent_wait", "ANeuralNetworksEvent* event"),  # noqa: B950
    ("void", "ANeuralNetworksEvent_free", "ANeuralNetworksEvent* event"),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_getOutputOperandRank",
        "ANeuralNetworksExecution* execution, int32_t index, uint32_t* rank",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_getOutputOperandDimensions",
        "ANeuralNetworksExecution* execution, int32_t index, uint32_t* dimensions",
    ),  # noqa: B950



    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_setOutputFromMemory",
        "ANeuralNetworksExecution* execution, int32_t index, const ANeuralNetworksOperandType* type, const ANeuralNetworksMemory* memory, size_t offset, size_t length",  # noqa: B950
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_startCompute",
        "ANeuralNetworksExecution* execution, ANeuralNetworksEvent** event",
    ),  # noqa: B950
    ("int", "ANeuralNetworksEvent_wait", "ANeuralNetworksEvent* event"),  # noqa: B950
    ("void", "ANeuralNetworksEvent_free", "ANeuralNetworksEvent* event"),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_getOutputOperandRank",
        "ANeuralNetworksExecution* execution, int32_t index, uint32_t* rank",
    ),  # noqa: B950
    (
        "int",
        "ANeuralNetworksExecution_getOutputOperandDimensions",
        "ANeuralNetworksExecution* execution, int32_t index, uint32_t* dimensions",
    ),  # noqa: B950
# 主函数，接收命令行参数 argv
def main(argv):
    # 定义空列表来存储结构成员、加载函数和定义检查函数
    struct_members = []
    load_functions = []
    define_checks = []

    # 遍历 NNAPI_FUNCTIONS 中的每个元素，元素包含三个部分：返回类型、名称、参数列表
    for ret, name, args in NNAPI_FUNCTIONS:
        # 从名称中去掉开头的 "ANeuralNetworks"，得到短名称
        short_name = name.replace("ANeuralNetworks", "", 1)

        # 将成员函数的声明添加到 struct_members 列表中
        struct_members.append(f"  {ret}(*{short_name})({args});")

        # 将加载函数的代码添加到 load_functions 列表中，使用 dlsym 动态加载符号
        load_functions.append(
            f'    *(void**)&nnapi_.{short_name} = dlsym(handle, "{name}");'
        )
        # 将检查函数的声明添加到 load_functions 列表中
        load_functions.append(f"    check_nnapi_.{short_name} = check_{short_name};")

        # 从参数列表 args 中提取调用函数时需要的参数
        call_args = "".join(re.findall(r"\w+(?:,|$)", args))
        # 根据返回类型 ret，定义不同类型的检查函数
        if ret == "void":
            define_checks.append(
                textwrap.dedent(
                    f"""\
                {ret} check_{short_name}({args}) {{
                  // 断言 nnapi_.{short_name} 不为空指针
                  CAFFE_ENFORCE(nnapi_.{short_name});
                  // 调用 nnapi_.{short_name} 函数
                  nnapi_.{short_name}({call_args});
                }}"""
                )
            )
        elif ret == "int":
            define_checks.append(
                textwrap.dedent(
                    f"""\
                {ret} check_{short_name}({args}) {{
                  // 断言 nnapi_.{short_name} 不为空指针
                  CAFFE_ENFORCE(nnapi_.{short_name});
                  // 调用 nnapi_.{short_name} 函数，并获取返回值
                  int ret = nnapi_.{short_name}({call_args});
                  // TODO: 可能在这里添加更好的日志记录
                  // 断言调用返回值为 ANEURALNETWORKS_NO_ERROR
                  CAFFE_ENFORCE(
                    ret == ANEURALNETWORKS_NO_ERROR,
                    "{short_name}", "failed with error ", ret
                  );
                  return ret;
                }}"""
                )
            )

    # 获取当前文件所在目录的路径，并赋值给 out_dir
    out_dir = Path(__file__).parent

    # 将生成的头文件内容写入 "nnapi_wrapper.h" 文件
    (out_dir / "nnapi_wrapper.h").write_text(
        PREFIX
        + textwrap.dedent(
            """\
            #ifndef NNAPI_WRAPPER_H_
            #define NNAPI_WRAPPER_H_
            #include <stddef.h>
            #include <stdint.h>
            #include <ATen/nnapi/NeuralNetworks.h>
            // 定义 nnapi_wrapper 结构体，包含所有成员函数的声明
            struct nnapi_wrapper {
            __STRUCT_MEMBERS__
            };
            #ifdef __cplusplus
            // 声明 nnapi_wrapper_load 函数
            void nnapi_wrapper_load(struct nnapi_wrapper** nnapi, struct nnapi_wrapper** check_nnapi);
            #endif
            #endif
            """
        ).replace("__STRUCT_MEMBERS__", "\n".join(struct_members))
    )
    # 将生成的 C++ 包装器代码写入文件 nnapi_wrapper.cpp 中
    (out_dir / "nnapi_wrapper.cpp").write_text(
        PREFIX
        + textwrap.dedent(
            """\
            #ifndef _WIN32
            #include <dlfcn.h>
            #endif
            #include <ATen/nnapi/nnapi_wrapper.h>
            #include <c10/util/Logging.h>
            // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
            static int loaded = 0;
            // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
            static struct nnapi_wrapper nnapi_;
            // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
            static struct nnapi_wrapper check_nnapi_;
            __DEFINE_CHECK_FUNCTIONS__
            void nnapi_wrapper_load(struct nnapi_wrapper** nnapi, struct nnapi_wrapper** check_nnapi) {
            #ifdef _WIN32
              // 如果运行在 Windows 上，抛出错误，因为不支持运行 NNAPI 模型
              TORCH_CHECK(false, "Running NNAPI models is not supported on Windows.");
            #else
              // 如果 loaded 标志为 0，表示尚未加载 NNAPI 函数库
              if (!loaded) {
                // 清除错误标志
                dlerror();
                // 尝试动态加载 libneuralnetworks.so 库文件
                void* handle = dlopen("libneuralnetworks.so", RTLD_LAZY | RTLD_LOCAL);
                // 断言加载成功，否则输出错误信息并终止程序执行
                CAFFE_ENFORCE(handle, "Failed to load libneuralnetworks.so ", dlerror());
            __LOAD_FUNCTIONS__
                // 标记为已加载
                loaded = 1;
              }
              // 将 nnapi 和 check_nnapi 指针指向静态变量 nnapi_ 和 check_nnapi_
              *nnapi = &nnapi_;
              *check_nnapi = &check_nnapi_;
            #endif
            }
            """
        )
        .replace("__DEFINE_CHECK_FUNCTIONS__", "\n".join(define_checks))  # 插入定义检查函数的代码段
        .replace("__LOAD_FUNCTIONS__", "\n".join(load_functions))  # 插入加载函数的代码段
    )
# 如果当前脚本被直接执行（而不是被导入为模块），则执行以下代码
if __name__ == "__main__":
    # 调用 main 函数，并传入命令行参数 sys.argv
    sys.exit(main(sys.argv))
```