# `.\pytorch\test\cpp\jit\test_graph_executor.cpp`

```py
#include <gtest/gtest.h>

#include "test/cpp/jit/test_utils.h"  // 引入测试工具函数头文件
#include "torch/csrc/jit/runtime/graph_executor.h"  // 引入图执行器头文件
#include "torch/jit.h"  // 引入 Torch JIT 头文件
#include "torch/script.h"  // 引入 Torch 脚本头文件
#include "torch/torch.h"  // 引入 Torch 头文件

namespace torch {
namespace jit {

TEST(GraphExecutorTest, Basic_CUDA) {
  constexpr int batch_size = 4;  // 定义批大小常量
  constexpr int input_size = 256;  // 定义输入大小常量

  int hidden_size = 2 * input_size;  // 计算隐藏层大小

  auto input = at::randn({batch_size, input_size}, at::kCUDA);  // 生成随机输入张量（CUDA）
  auto hx = at::randn({batch_size, hidden_size}, at::kCUDA);  // 生成随机隐藏状态张量（CUDA）
  auto cx = at::randn({batch_size, hidden_size}, at::kCUDA);  // 生成随机细胞状态张量（CUDA）
  auto w_ih = t_def(at::randn({4 * hidden_size, input_size}, at::kCUDA));  // 生成随机输入到隐藏层权重张量（CUDA）
  auto w_hh = t_def(at::randn({4 * hidden_size, hidden_size}, at::kCUDA));  // 生成随机隐藏到隐藏层权重张量（CUDA）

  auto g = build_lstm();  // 构建 LSTM 图
  GraphExecutor executor(g, "");  // 创建图执行器对象
  auto stack = createStack({input, hx, cx, w_ih, w_hh});  // 创建输入栈
  executor.run(stack);  // 执行图
  ASSERT_EQ(stack.size(), 2);  // 断言栈的大小为2
  at::Tensor r0, r1;
  std::tie(r0, r1) = lstm(input, hx, cx, w_ih, w_hh);  // 执行 LSTM 操作
  ASSERT_TRUE(almostEqual(stack[0].toTensor(), r0));  // 断言第一个输出与预期结果接近
  ASSERT_TRUE(almostEqual(stack[1].toTensor(), r1));  // 断言第二个输出与预期结果接近
}

TEST(GraphExecutorTest, runAsync_executor) {
  /*
  TODO: there are some problem with C++ parsing script program involving
  fork. Use the test module below for now.
  issue about this: github.com/pytorch/pytorch/issues/46368
  The test module file is generated by following:
    class DemoModule(torch.nn.Module):
      def forward(self):
        r1 = torch.jit.fork(torch.mm, torch.rand(100,100),torch.rand(100,100))
        r2 = torch.jit.fork(torch.mm, torch.rand(100,100),torch.rand(100,100))
        return r1.wait() + r2.wait()
  demo = DemoModule()
  torch.jit.save(torch.jit.script(demo), 'test_interpreter_async.pt')
  */
  std::string filePath(__FILE__);  // 获取当前文件路径
  auto testModelFile = filePath.substr(0, filePath.find_last_of("/\\") + 1);  // 获取测试模型文件路径
  testModelFile.append("test_interpreter_async.pt");  // 拼接测试模型文件名
  auto module = load(testModelFile);  // 加载模型
  auto graph = module.get_method("forward").graph();  // 获取模型的前向方法的图
  GraphExecutor graphExecutor(graph, "");  // 创建图执行器对象
  auto asyncCounter = 0;  // 异步计数器初始化为0
  std::mutex mtx;  // 创建互斥量

  // 一个虚拟的执行器，实际上使用 at::launch，并增加计数器
  auto launcher = [&](std::function<void()> f) {
    mtx.lock();  // 加锁
    ++asyncCounter;  // 增加异步计数器
    mtx.unlock();  // 解锁
    at::launch(std::move(f));  // 执行传入的函数对象
  };

  std::vector<IValue> stack;  // 创建输入栈
  // NOLINTNEXTLINE(modernize-use-emplace)
  stack.push_back(module._ivalue());  // 将模型的 IValue 对象推入栈中
  graphExecutor.runAsync(stack, launcher)->wait();  // 异步运行图，并等待完成
  ASSERT_TRUE(asyncCounter > 0);  // 断言异步计数器大于0
}

} // namespace jit
} // namespace torch
```