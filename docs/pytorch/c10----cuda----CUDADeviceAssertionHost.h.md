# `.\pytorch\c10\cuda\CUDADeviceAssertionHost.h`

```
#pragma once

#include <c10/cuda/CUDAMacros.h> // 引入 CUDA 宏定义头文件

#include <cstdint> // 引入标准整数类型
#include <memory> // 引入内存管理相关头文件
#include <mutex> // 引入互斥锁头文件
#include <string> // 引入字符串处理头文件
#include <utility> // 引入实用工具头文件
#include <vector> // 引入向量容器头文件

#ifdef USE_CUDA
#define TORCH_USE_CUDA_DSA // 如果定义了 USE_CUDA，则定义 TORCH_USE_CUDA_DSA
#endif

/// Number of assertion failure messages we can store. If this is too small
/// threads will fail silently.
constexpr int C10_CUDA_DSA_ASSERTION_COUNT = 10; // 设备端断言失败消息的最大存储数量
constexpr int C10_CUDA_DSA_MAX_STR_LEN = 512; // 设备端断言失败消息的最大字符串长度

namespace c10::cuda {

/// Holds information about any device-side assertions that fail.
/// Held in managed memory and access by both the CPU and the GPU.
struct DeviceAssertionData {
  /// Stringification of the assertion
  // NOLINTNEXTLINE(*-c-arrays)
  char assertion_msg[C10_CUDA_DSA_MAX_STR_LEN]{}; // 断言的字符串化消息
  /// File the assertion was in
  // NOLINTNEXTLINE(*-c-arrays)
  char filename[C10_CUDA_DSA_MAX_STR_LEN]{}; // 断言所在的文件名
  /// Name of the function the assertion was in
  // NOLINTNEXTLINE(*-c-arrays)
  char function_name[C10_CUDA_DSA_MAX_STR_LEN]{}; // 断言所在的函数名
  /// Line number the assertion was at
  int line_number{}; // 断言所在的行号
  /// Number uniquely identifying the kernel launch that triggered the assertion
  uint32_t caller{}; // 唯一标识触发断言的内核启动号
  /// block_id of the thread that failed the assertion
  // NOLINTNEXTLINE(*-c-arrays)
  int32_t block_id[3]{}; // 失败断言的线程块 ID
  /// third_id of the thread that failed the assertion
  // NOLINTNEXTLINE(*-c-arrays)
  int32_t thread_id[3]{}; // 失败断言的线程 ID
};

/// Used to hold assertions generated by the device
/// Held in managed memory and access by both the CPU and the GPU.
struct DeviceAssertionsData {
  /// Total number of assertions found; a subset of thse will be recorded
  /// in `assertions`
  int32_t assertion_count{}; // 发现的断言总数
  /// An array of assertions that will be written to in a race-free manner
  // NOLINTNEXTLINE(*-c-arrays)
  DeviceAssertionData assertions[C10_CUDA_DSA_ASSERTION_COUNT]{}; // 断言数组，以无竞争写入的方式记录
};

/// Use to hold info about kernel launches so that we can run kernels
/// asynchronously and still associate launches with device-side
/// assertion failures
struct CUDAKernelLaunchInfo {
  /// Filename of the code where the kernel was launched from
  const char* launch_filename; // 启动内核的代码文件名
  /// Function from which the kernel was launched
  const char* launch_function; // 启动内核的函数名
  /// Line number of where the code was launched from
  uint32_t launch_linenum; // 启动代码的行号
  /// Backtrace of where the kernel was launched from, only populated if
  /// CUDAKernelLaunchRegistry::gather_launch_stacktrace is True
  std::string launch_stacktrace; // 内核启动的回溯信息（仅在 gather_launch_stacktrace 为真时填充）
  /// Kernel that was launched
  const char* kernel_name; // 被启动的内核名
  /// Device the kernel was launched on
  int device; // 启动内核的设备
  /// Stream the kernel was launched on
  int32_t stream; // 启动内核的流
  /// A number that uniquely identifies the kernel launch
  uint64_t generation_number; // 唯一标识内核启动的编号
};

/// Circular buffer used to hold information about kernel launches
/// this is later used to reconstruct how a device-side kernel assertion failure
/// occurred CUDAKernelLaunchRegistry is used as a singleton
class C10_CUDA_API CUDAKernelLaunchRegistry {
 private:
  /// Assume that this is the max number of kernel launches that might ever be
  /// enqueued across all streams on a single device
  // 定义一个静态常量，表示单个设备上所有流中可能同时排队的最大内核启动次数
  static constexpr int max_kernel_launches = 1024;
  /// How many kernel launch infos we've inserted. Used to ensure that circular
  /// queue doesn't provide false information by always increasing, but also to
  /// mark where we are inserting into the queue
  // 记录已插入的内核启动信息数量。用于确保循环队列不提供虚假信息，始终递增，同时标记插入队列的位置
#ifdef TORCH_USE_CUDA_DSA
  // 如果定义了 TORCH_USE_CUDA_DSA 宏
  uint64_t generation_number = 0;
// 结束预处理指令，关闭条件编译区块
#endif
  /// 用于确保写入和访问之间的多线程安全的共享互斥量
  mutable std::mutex read_write_mutex;
  /// 用于确保GPU内存分配时防止竞态条件的互斥量
  mutable std::mutex gpu_alloc_mutex;
  /// 指向管理设备端断言信息的内存的指针。对于每个可能工作的设备，都有一个条目。未使用的条目为nullptr。
  /// 使用unique_ptr和自定义析构器，预期设备数较少，速度更快，浪费的内存很少。
  std::vector<
      std::unique_ptr<DeviceAssertionsData, void (*)(DeviceAssertionsData*)>>
      uvm_assertions;
  /// 单个循环缓冲区，保存进程在所有设备上进行的每次内核启动的信息。
  std::vector<CUDAKernelLaunchInfo> kernel_launches;
  /// 检查环境变量以确定是否启用内核启动堆栈跟踪
  bool check_env_for_enable_launch_stacktracing() const;
  /// 检查环境变量以确定是否启用设备端断言
  bool check_env_for_dsa_enabled() const;

public:
  // 构造函数，用于初始化CUDA内核启动注册表
  CUDAKernelLaunchRegistry();
  /// 注册一个新的内核启动，并返回生成号以传递给内核
  uint32_t insert(
      const char* launch_filename,
      const char* launch_function,
      const uint32_t launch_linenum,
      const char* kernel_name,
      const int32_t stream_id);
  /// 获取内核启动注册表和每个设备的断言失败缓冲区的副本，以便无需引发竞争条件即可检查它们
  std::
      pair<std::vector<DeviceAssertionsData>, std::vector<CUDAKernelLaunchInfo>>
      snapshot() const;
  /// 获取当前设备的断言失败缓冲区的指针。如果不存在此类缓冲区，则创建一个。
  /// 这意味着每个设备上的第一个内核启动会稍慢，因为需要内存分配。
  DeviceAssertionsData* get_uvm_assertions_ptr_for_current_device();
  /// 获取注册表的全局单例引用
  static CUDAKernelLaunchRegistry& get_singleton_ref();
  /// 如果不是所有设备都支持DSA，则禁用它
  const bool do_all_devices_support_managed_memory = false;
  /// 是否在启动内核时收集堆栈跟踪信息
  bool gather_launch_stacktrace = false;
  /// 是否在运行时启用主机端DSA
  /// 注意：设备端代码不能在运行时启用/禁用
  bool enabled_at_runtime = false;
  /// 设备是否发生了故障
  bool has_failed() const;
#ifdef TORCH_USE_CUDA_DSA
  // 编译时DSA是否启用
  const bool enabled_at_compile_time = true;
#else
  const bool enabled_at_compile_time = false;
#endif
};

// 检索设备端断言信息的函数声明
std::string c10_retrieve_device_side_assertion_info();

} // namespace c10::cuda

// 使用TORCH_DSA_KERNEL_LAUNCH启动的每个内核都需要相同的输入参数。
// 为了标准化这些参数，引入以下宏定义。
# 定义 TORCH_DSA_KERNEL_ARGS 宏，用于声明 CUDA 设备断言相关的参数
#define TORCH_DSA_KERNEL_ARGS                                              \
  [[maybe_unused]] c10::cuda::DeviceAssertionsData *const assertions_data, \
      [[maybe_unused]] uint32_t assertion_caller_id

# 定义 TORCH_DSA_KERNEL_ARGS_PASS 宏，用于将设备断言相关的参数传递给另一个函数
#define TORCH_DSA_KERNEL_ARGS_PASS assertions_data, assertion_caller_id
```