# `.\pytorch\torch\ao\quantization\fx\_model_report\model_report.py`

```
# mypy: allow-untyped-defs
# 引入类型提示需要的模块
from typing import Any, Dict, Set, Tuple, Callable
from collections import OrderedDict
import torch
# 导入用于模型报告的检测器和相关类
from torch.ao.quantization.fx._model_report.detector import (
    DetectorBase,
    DETECTOR_OBS_ARGS_KEY,
    DETECTOR_OBS_TO_INSERT_KEY,
    DETECTOR_IS_POST_OBS_KEY,
    DETECTOR_TARGET_NODE_KEY,
    DetectorQConfigInfo
)
# 导入模型报告可视化工具类
from torch.ao.quantization.fx._model_report.model_report_visualizer import ModelReportVisualizer
# 导入图模块类
from torch.ao.quantization.fx.graph_module import GraphModule
# 导入观察器基类
from torch.ao.quantization.observer import ObserverBase
# 导入量化配置映射相关类
from torch.ao.quantization.qconfig_mapping import QConfigMapping, QConfig
# 导入均衡化量化配置相关类
from torch.ao.quantization.fx._equalize import EqualizationQConfig

# 定义模型报告类
class ModelReport:
    r"""
    The ModelReport class aims to provide users an easy way to diagnose issues that they run into
    with their models. The class works with all traceable GraphModules to help diagnose issues,
    though the requirements on the type of model more-so depends on the specific report the user
    is trying to generate. With respect to the reports, the ModelReport class is initialized with
    a set of Detector classes, each of which generate reports on quantization configuration
    issues a use might have.

    Currently supports generating reports on:
    - Suggestions for per-channel vs. per-tensor quantization (nn.Module)
    - Suggestions for dynamic vs static quantization for linear layers (Graph Modules)
    - Suggestions for input-weight equalization for linear and conv layers (Graph Modules)
    - Suggestions for outlier detection for all layers (Graph Modules)

    The ModelReport class has the primary functionality of inserting observers (primarily the ModelReportObserver)
    where needed for each detector to gather the information it needs, and then after callibration, the ModelReport
    class compiles the report generated by each Detector class into a single report to return to the user. It also
    has the capability to remove all the observers it inserted as well.

    * :attr:`_model` The model we wish to generate the report for. Must be a traceable GraphModule

    * :attr:`_desired_report_detectors` The set of Detectors representing desired reports from the ModelReport class
        Make sure that these are all unique types of detectors [do not have more than 1 of the same class]

    * :attr:`_desired_detector_names` The set of detector names of the _desired_report_detectors.
        This set is generated by calling the get_detector_name() of each detector

    * :attr:`_detector_name_to_observer_fqns` The mapping from each detector to fqns of observers of interest
        The purpose of this is to keep track of what observers were inserted for each detector, so that they
        can be removed at the end if desired
    # `_prepared_flag`：布尔标志，用于跟踪模型是否已经准备就绪
    #   用途是确保只在 ModelReport 实例中插入观察者一次
    
    # `_removed_observers`：布尔值，用于跟踪是否已经移除了观察者
    #   目的是确保不会尝试使用相同的 ModelReport 实例两次移除观察者。
    #   这也允许多次生成报告，只要尚未移除观察者。
    
    # 注意：
    #   此类最初设计用于与 Fx 图模式工作流一起使用。
    #   然而，只要有可追踪的 GraphModule 被使用，就可以获得完整功能。
    #   一个获取可追踪 GraphModule 的方法，而不需要经过 Fx 工作流程，是使用 QuantizationTracer 类。
    
    # Fx 工作流通用流程：
    # 1.) 使用初始化的检测器对象和模型，初始化 ModelReport 对象以获取感兴趣的报告
    # 2.) 使用 prepare_fx 准备您的模型
    # 3.) 调用 model_report.prepare_detailed_calibration 添加相关的观察者
    # 4.) 使用数据校准您的模型
    # 5.) 调用 model_report.generate_report 生成报告，并可选择移除添加的观察者
    # 可选步骤：
    # 6.) 调用 model_report.generate_visualizer 获取一个 ModelReportVisualizer 实例
    # 7.) 为了帮助解析报告信息和调试，将报告信息视为：
    #   - 表格
    #   - 直方图
    #   - 折线图
    # 8.) 调用 model_report.generate_qconfigs 根据报告建议生成 qconfigs
    Example (with QuantizationTracer):
        >>> # xdoctest: +SKIP
        >>> # 获取必要的量化配置
        >>> config = PrepareCustomConfig()
        >>> # 获取跳过的模块名称和类列表
        >>> skipped_module_names, skipped_module_classes = get_skipped_module_name_and_classes(config, False)

        >>> # 初始化模型并获取 GraphModule
        >>> model = SomeModel()
        >>> # 创建量化追踪器
        >>> tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)
        >>> # 创建 GraphModule 对象，包含量化追踪信息
        >>> graph_module = GraphModule(model, tracer.trace(model))

        >>> # 获取检测器集合和 ModelReport 实例
        >>> detector_set = set([DynamicStaticDetector(tolerance=0.5), InputWeightEqualizationDetector(ratio_threshold=0.7)])
        >>> # 创建 ModelReport 实例，基于 GraphModule 和量化追踪信息的检测器集合
        >>> tracer_reporter = ModelReport(graph_module, tracer_detector_set)

        >>> # 插入观察器并校准模型
        >>> tracer_model_with_observers = tracer_reporter.prepare_detailed_calibration()
        >>> # 运行多个校准批次
        >>> for i in range(num_callibration_batches):
        >>>     # 获取校准输入示例
        >>>     example_input = get_callibration_input()
        >>>     # 对具有观察器的模型进行调用
        >>>     tracer_model_with_observers(example_input)

        >>> # 生成报告并选择性地移除插入的观察器
        >>> reports = tracer_reporter.generate_model_report(remove_inserted_observers=True)

        >>> # 可选：基于建议生成量化配置映射
        >>> qconfigs = model_report.generate_qconfig_mapping()

        >>> # 可选：基于建议生成均衡化映射
        >>> qconfigs = model_report.generate_equalization_mapping()

        >>> # 可选：获取 ModelReportVisualizer 实例以进行所需的任何可视化操作
        >>> model_report_visualizer = tracer_reporter.generate_visualizer()
    def __init__(self, model: GraphModule, desired_report_detectors: Set[DetectorBase]):
        # 检查是否至少包含一个期望的报告，否则抛出值错误异常
        if len(desired_report_detectors) == 0:
            raise ValueError("Should include at least 1 desired report")

        # 记录我们希望生成报告的模型
        self._model: GraphModule = model

        # 将期望的报告集合设为私有，防止被修改
        self._desired_report_detectors = desired_report_detectors
        # 从期望的报告检测器集合中获取每个检测器的名称，存储在集合中
        self._desired_detector_names = {detector.get_detector_name() for detector in desired_report_detectors}

        # 维护期望报告与感兴趣观察者之间的映射
        # 这是为了获取读数，以及删除它们，可能会创建一个大集合
        # 这个集合可以用来遍历图并移除添加的观察者
        self._detector_name_to_observer_fqns: Dict[str, Set[str]] = {}

        # 初始化每个报告的观察者集合为空集
        for desired_report in self._desired_detector_names:
            self._detector_name_to_observer_fqns[desired_report] = set()

        # 标志位，确保只能准备和移除观察者一次
        self._prepared_flag = False
        self._removed_observers = False

        # 存储我们为可视化目的生成的报告
        # 初始为空，因为尚未生成报告
        self._generated_reports: Dict[str, Dict] = {}

    def get_desired_reports_names(self) -> Set[str]:
        """ 返回期望报告名称的副本，用于查看 """
        return self._desired_detector_names.copy()

    def get_observers_of_interest(self) -> Dict[str, Set[str]]:
        """ 返回感兴趣观察者的副本，用于查看 """
        return self._detector_name_to_observer_fqns.copy()
    def prepare_detailed_calibration(self) -> GraphModule:
        r"""
        Takes in a graph model and inserts the following observers:
        - ModelReportObserver

        Each observer is inserted based on the desired_reports into the relevant locations

        Right now, each report in self._desired_detector_names has independent insertions
            However, if a module already has a Observer of the same type, the insertion will not occur
            This is because all of the same type of Observer collect same information, so redundant

        Returns the same GraphModule with the observers inserted
        """

        # if already prepared once, cannot prepare again
        if self._prepared_flag:
            raise ValueError("Already ran preparing detailed callibration. Run the report generation next after callibration.")

        # loop through each detector, find where placements should be, and keep track
        insert_observers_fqns: Dict[str, Any] = {}

        for detector in self._desired_report_detectors:
            # determine observer points for each detector
            obs_fqn_to_info = detector.determine_observer_insert_points(self._model)
            # map each insert point to the observer to use
            insert_observers_fqns.update(obs_fqn_to_info)
            # update the set of observers this report cares about
            self._detector_name_to_observer_fqns[detector.get_detector_name()] = set(obs_fqn_to_info.keys())

        # now insert all the observers at their desired locations
        for observer_fqn in insert_observers_fqns:
            target_node = insert_observers_fqns[observer_fqn][DETECTOR_TARGET_NODE_KEY]
            insert_obs = insert_observers_fqns[observer_fqn][DETECTOR_OBS_TO_INSERT_KEY]
            insert_post = insert_observers_fqns[observer_fqn][DETECTOR_IS_POST_OBS_KEY]
            observer_args = insert_observers_fqns[observer_fqn][DETECTOR_OBS_ARGS_KEY]
            self._insert_observer_around_module(
                observer_fqn, target_node, insert_obs, observer_args, insert_post
            )

        self._prepared_flag = True

        return self._model

    def _insert_observer_around_module(
        self,
        obs_fqn: str,
        target_node: torch.fx.node.Node,
        obs_to_insert: ObserverBase,
        observer_args: Tuple,
        insert_post: bool
    ):
        """
        Inserts an observer around a specific module in the model.

        Args:
            obs_fqn (str): Fully qualified name of the observer class.
            target_node (torch.fx.node.Node): Target node where the observer should be inserted.
            obs_to_insert (ObserverBase): Observer object to be inserted.
            observer_args (Tuple): Arguments for the observer.
            insert_post (bool): Flag indicating if the observer should be inserted post execution.

        This method handles the insertion of an observer around a specified module in the model graph.
        """
    ):
        r"""
        Helper function that inserts the observer into both the graph structure and the module of the model

        Args
            node_fqn (str): The fully qualified name of the observer we want to insert
            target_node (torch.fx.node.Node): The node in model we are inserting observers around
            obs_to_insert (ObserverBase): The observer we are inserting around target_node
            observer_args (Tuple): The arguments we want to pass into the observer
            insert_post (bool): whether this is meant to be a post observer for this node
        """
        # if we are inserting post, then our target node is the next node
        if insert_post:
            target_node = target_node.next

        # 插入观察器节点之前，使用上下文管理器进行操作
        with self._model.graph.inserting_before(target_node):
            # 将观察器模块添加到模型中
            self._model.add_submodule(obs_fqn, obs_to_insert)
            # 在图中创建一个调用模块的节点，表示调用观察器
            self._model.graph.create_node(op="call_module", target=obs_fqn, args=observer_args)

        # 插入完成后重新编译模型
        self._model.recompile()

    def _get_node_from_fqn(self, node_fqn: str) -> torch.fx.node.Node:
        r"""
        Takes in a node fqn and returns the node based on the fqn

        Args
            node_fqn (str): The fully qualified name of the node we want to find in model

        Returns the Node object of the given node_fqn otherwise returns None
        """
        node_to_return = None
        # 遍历模型图中的节点
        for node in self._model.graph.nodes:
            # 如果节点的目标与给定的节点名称匹配，找到了我们要找的节点
            if node.target == node_fqn:
                node_to_return = node
                break

        if node_to_return is None:
            # 如果找不到对应的节点，则抛出值错误异常
            raise ValueError("The node_fqn was not found within the module.")

        # 使用断言确认返回的节点是 torch.fx.node.Node 类型，供 MyPy 检查使用
        assert isinstance(node_to_return, torch.fx.node.Node)

        return node_to_return

    def generate_model_report(
        self, remove_inserted_observers: bool
    def _is_same_info_for_same_key(self, info_dict_a: Dict, info_dict_b: Dict) -> bool:
        r"""
        Takes in two dictionaries and ensures that any common keys between the two have the same
        values.

        Args:
            info_dict_a (Dict): First dictionary we wish to compare
            info_dict_b (Dict): Second dictionary we wish to compare

        Returns True if all shared keys have same values, false otherwise
        """
        # 获取第一个字典的所有键，并转换为集合
        dict_a_keys: Set = set(info_dict_a.keys())
        # 获取第二个字典的所有键，并转换为集合
        dict_b_keys: Set = set(info_dict_b.keys())

        # 获取两个字典键的交集
        intersecting_keys: Set = dict_a_keys.intersection(dict_b_keys)

        # 遍历交集中的键，逐一比较对应的值
        for key in intersecting_keys:
            dict_a_val = info_dict_a[key]
            dict_b_val = info_dict_b[key]

            # 如果值是 torch.Tensor 类型，需要特殊处理
            if type(dict_a_val) == torch.Tensor:
                # 如果 dict_b_val 不是 tensor，或者两个 tensor 值不相等，则返回 False
                if type(dict_b_val) != torch.Tensor or torch.sum(dict_a_val != dict_b_val) != 0:
                    return False
            else:
                # 对于非 tensor 类型的值，直接比较是否相等
                if dict_a_val != dict_b_val:
                    return False

        # 如果所有共同键的值都相等，则返回 True
        return True
    def _reformat_reports_for_visualizer(self) -> OrderedDict:
        r"""
        Takes the generated reports and reformats them into the format that is desired by the
        ModelReportVisualizer

        Returns an OrderedDict mapping module_fqns to their features
        """
        # 我们希望重新排序和重新格式化信息，使其按模型中的顺序排列

        # 首先创建一个新字典，所有模块作为键，并将特征放在相应模块下
        module_fqns_to_features: Dict[str, Dict] = {}

        for report_name in self._generated_reports:
            # 获取模块 -> 特征字典并进行遍历
            module_info = self._generated_reports[report_name]

            for module_fqn in module_info:
                # 检查累积字典中是否已存在
                if module_fqn in module_fqns_to_features:
                    # 合并所有特征
                    new_info: Dict = module_info[module_fqn]
                    present_info: Dict = module_fqns_to_features[module_fqn]

                    # 将它们合并到新的联合字典中
                    # 相同的特征键 -> 相同的信息，所以如果覆盖也没问题
                    if self._is_same_info_for_same_key(new_info, present_info):
                        module_fqns_to_features[module_fqn] = {**new_info, **present_info}
                    else:
                        error_str = "You have the same key with different values across detectors. "
                        error_str += "Someone incorrectly implemented a detector with conflicting keys to existing detectors."
                        raise ValueError(error_str)
                else:
                    # 直接设置它
                    module_fqns_to_features[module_fqn] = module_info[module_fqn]

        # 我们的有序字典，使模块可以按其在模型中出现的顺序排序
        features_by_module: OrderedDict[str, Dict] = OrderedDict()

        # 我们按顺序遍历图中的模块
        for fqn, module in self._model.named_modules():
            # 在fqns_to_features中查找该fqn
            if fqn in module_fqns_to_features:
                # 将其添加到我们的有序字典中
                features_by_module[fqn] = module_fqns_to_features[fqn]

        # 返回我们创建的信息的有序字典
        return features_by_module
    def generate_visualizer(self) -> ModelReportVisualizer:
        r"""
        Generates a ModelReportVisualizer instance using the reports generated
        by the generate_model_report() method.

        Returns the generated ModelReportVisualizer instance initialized

        Note:
            Throws exception if attempt to get visualizers without generating report
        """
        # 检查是否至少生成过一次报告
        if len(self._generated_reports) == 0:
            raise Exception("Unable to generate visualizers without first generating reports")  # noqa: TRY002

        # 获取模块到其完整特征/统计集的有序字典映射
        module_fqns_to_features: OrderedDict = self._reformat_reports_for_visualizer()

        # 创建并返回一个已初始化的 ModelReportVisualizer 实例
        visualizer: ModelReportVisualizer = ModelReportVisualizer(module_fqns_to_features)

        return visualizer

    def _generate_qconfig_mapping_helper(
        self,
        detector_qconfig_info_combined: Dict[str, DetectorQConfigInfo],
        generation_function: Callable
    ) -> QConfigMapping:
        r"""
        This helper takes in the compiled detector qconfig info that
        has been compiled together and merges it into a QConfigMapping
        """
        # 跟踪 QConfigMapping
        qconfig_mapping = QConfigMapping()

        # 遍历每个模块/全限定名，并尝试创建 QConfigMapping
        for fqn, module in self._model.named_modules():
            # 如果对该模块有 QConfig 信息
            if fqn in detector_qconfig_info_combined:
                qconfig_info_compiled = detector_qconfig_info_combined[fqn]

                # 生成 QConfig 并添加到映射中
                generated_qconfig = generation_function(qconfig_info_compiled, module)

                # 添加到配置中
                qconfig_mapping.set_module_name(fqn, generated_qconfig)

        # 返回编译后的映射
        return qconfig_mapping

    def _update_detector_quantizaiton_qconfig_info(self, combined_info: DetectorQConfigInfo, new_info: DetectorQConfigInfo):
        r"""
        Takes in the old and new information and updates the combined information.

        Args:
            combined_info (DetectorQConfigInfo): The DetectorQConfigInfo we are compiling all of the information in
            new_info (DetectorQConfigInfo): The DetectorQConfigInfo with the information we are trying to merge the new info
                into it
        """
        # 更新组合信息的激活动态性和权重每通道性
        combined_info.is_activation_dynamic = combined_info.is_activation_dynamic or new_info.is_activation_dynamic
        combined_info.is_weight_per_channel = combined_info.is_weight_per_channel or new_info.is_weight_per_channel
    def _update_detector_equalization_qconfig_info(self, combined_info: DetectorQConfigInfo, new_info: DetectorQConfigInfo):
        r"""
        Takes in the old and new information and updates the combined information.

        Args:
            combined_info (DetectorQConfigInfo): The DetectorQConfigInfo we are compiling all of the information in
            new_info (DetectorQConfigInfo): The DetectorQConfigInfo with the information we are trying to merge the new info
                into it
        """
        # 计算新的等化推荐值：如果任一信息建议等化，则设置为 True
        is_equalization_recommended = combined_info.is_equalization_recommended or new_info.is_equalization_recommended
        # 更新合并信息中的等化推荐值
        combined_info.is_equalization_recommended = is_equalization_recommended

    def _generate_module_fqn_to_detector_info_mapping(
        self,
        update_qconfig_info_function: Callable
    ):
        # 生成模块全限定名到检测器信息的映射
        # 使用给定的回调函数来更新配置信息
        pass
    ) -> Dict[str, DetectorQConfigInfo]:
        r"""
        Generates a QConfigMapping based on the suggestions of the
        ModelReport API. The generated mapping encompasses all the
        different types of feedback from the different detectors
        all into one place.

        These configs are based on the suggestions provided by the ModelReport API
        and can only be generated once the reports have been generated.

        Args:
            update_qconfig_info_function (Callable) takes in a function that takes in two DetectorQConfigInfo
            and updates the one that is being compiled

        Returns a Dict mapping module_fqns to DetectorQConfigInfo objects

        Note:
            Throws exception if we try to generate mapping on model we already removed observers from
            Throws exception if we try to generate mapping without preparing for callibration
        """
        # if we haven't prepped model for callibration, then we shouldn't generate mapping yet
        if not self._prepared_flag:
            raise Exception("Cannot generate report without preparing model for callibration")  # noqa: TRY002

        # if we already removed the observers, we cannot mapping
        if self._removed_observers:
            raise Exception("Cannot generate report on model you already removed observers from")  # noqa: TRY002

        # keep track of qconfig info for each module across detectors
        detector_qconfig_info_combined: Dict[str, DetectorQConfigInfo] = {}

        for detector in self._desired_report_detectors:
            # get the info from the detector
            detector_info: Dict[str, DetectorQConfigInfo] = detector.get_qconfig_info(self._model)

            # we go through the modules
            for module_fqn in detector_info:
                # see if we already have info on it
                if module_fqn in detector_qconfig_info_combined:
                    # we combine the current options with what is there
                    current_options = detector_qconfig_info_combined[module_fqn]
                    detector_options = detector_info[module_fqn]

                    update_qconfig_info_function(current_options, detector_options)
                else:
                    # we just use this for now
                    detector_qconfig_info_combined[module_fqn] = detector_info[module_fqn]

        return detector_qconfig_info_combined
    # 根据 ModelReport API 的建议生成一个 QConfigMapping，包括不同探测器提供的各种反馈类型
    # 这些配置基于 ModelReport API 提供的建议，只能在生成报告后生成

    # 如果尝试在已经从模型中移除观察器的情况下生成映射，则抛出异常
    # 如果尝试在未准备好校准的情况下生成映射，则抛出异常
    def generate_qconfig_mapping(self) -> QConfigMapping:
        r"""
        Generates a QConfigMapping based on the suggestions of the
        ModelReport API. The generated mapping encompasses all the
        different types of feedback from the different detectors
        all into one place.

        These configs are based on the suggestions provided by the ModelReport API
        and can only be generated once the reports have been generated.

        Returns a QConfigMapping for the quantization configuration

        Note:
            Throws exception if we try to generate mapping on model we already removed observers from
            Throws exception if we try to generate mapping without preparing for callibration
        """
        # 获取映射信息
        detector_qconfig_info_combined = self._generate_module_fqn_to_detector_info_mapping(
            self._update_detector_quantizaiton_qconfig_info
        )

        # 进行一些处理，并移除没有推荐输入权重的 FQN

        # 现在为每个选项生成 QConfig
        mapping: QConfigMapping = self._generate_qconfig_mapping_helper(
            detector_qconfig_info_combined,
            self._quantization_config_generator
        )

        # 返回生成的映射
        return mapping

    # 根据 DetectorQConfigInfo 对象生成量化配置
    def _quantization_config_generator(self, detector_qconfig_info: DetectorQConfigInfo, module: torch.nn.Module) -> QConfig:
        r"""
        Returns the quantization configuration generated by the DetectorQConfigInfo object
        """
        return detector_qconfig_info.generate_quantization_qconfig(module)

    # 根据 DetectorQConfigInfo 对象生成均衡化配置
    def _equalization_config_generator(
        self,
        detector_qconfig_info: DetectorQConfigInfo,
        module: torch.nn.Module
    ) -> EqualizationQConfig:
        r"""
        We ignore the module argument here, and only focus on thedetector_qconfig_info

        Returns the equalization configuration generated by the DetectorQConfigInfo object
        """
        return detector_qconfig_info.generate_equalization_qconfig()
    def generate_equalization_mapping(self) -> QConfigMapping:
        r"""
        Generates a QConfigMapping based on the suggestions of the
        ModelReport API for equalization. The generated mapping encompasses all the
        different types of feedback from the input-weight equalization detector.

        These configs are based on the suggestions provided by the ModelReport API
        and can only be generated once the reports have been generated.

        Returns a QConfigMapping for the equalization configuration
        """
        # 获取模块全限定名到检测器信息映射
        detector_qconfig_info_combined = self._generate_module_fqn_to_detector_info_mapping(
            self._update_detector_equalization_qconfig_info
        )

        # 使用生成的映射和均衡配置生成器，生成每个选项的 QConfig
        mapping: QConfigMapping = self._generate_qconfig_mapping_helper(
            detector_qconfig_info_combined,
            self._equalization_config_generator
        )

        # 返回生成的映射对象
        return mapping
```