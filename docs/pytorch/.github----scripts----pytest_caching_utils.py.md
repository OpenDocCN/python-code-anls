# `.\pytorch\.github\scripts\pytest_caching_utils.py`

```
import hashlib  # 导入 hashlib 模块，用于生成哈希值
import os  # 导入 os 模块，提供操作系统相关功能
from pathlib import Path  # 导入 Path 类，用于处理文件路径
from typing import Dict, NamedTuple  # 导入类型提示相关的类和函数

from file_io_utils import (  # 导入自定义的文件操作工具函数
    copy_file,  # 复制文件
    download_s3_objects_with_prefix,  # 下载以指定前缀开头的 S3 对象
    load_json_file,  # 加载 JSON 文件内容
    sanitize_for_s3,  # 准备用于 S3 的文件路径
    unzip_folder,  # 解压文件夹
    upload_file_to_s3,  # 上传文件到 S3
    write_json_file,  # 写入 JSON 文件
    zip_folder,  # 压缩文件夹
)

PYTEST_CACHE_KEY_PREFIX = "pytest_cache"  # 定义 pytest 缓存键前缀
PYTEST_CACHE_DIR_NAME = ".pytest_cache"  # 定义 pytest 缓存目录名
BUCKET = "gha-artifacts"  # 指定默认的存储桶名称
LASTFAILED_FILE_PATH = Path("v/cache/lastfailed")  # 设置最近失败文件的路径
TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL = "previous_failures_additional.json"  # 设置额外的先前失败信息文件名

# Temp folders
ZIP_UPLOAD = "zip-upload"  # 指定用于上传的临时文件夹名
CACHE_ZIP_DOWNLOADS = "cache-zip-downloads"  # 指定用于缓存下载的临时文件夹名
UNZIPPED_CACHES = "unzipped-caches"  # 指定用于解压的临时文件夹名


# Since the pr identifier can be based on include user defined text (like a branch name)
# we hash it to sanitize the input and avoid corner cases
class PRIdentifier(str):
    def __new__(cls, value: str) -> "PRIdentifier":
        md5 = hashlib.md5(value.encode("utf-8")).hexdigest()  # 对输入的值进行 MD5 哈希处理
        return super().__new__(cls, md5)  # 返回处理后的 PRIdentifier 对象


class GithubRepo(NamedTuple):
    owner: str  # 仓库所有者名
    name: str  # 仓库名称

    # Create a Repo from a string like "owner/repo"
    @classmethod
    def from_string(cls, repo_string: str) -> "GithubRepo":
        if "/" not in repo_string:
            raise ValueError(
                f"repo_string must be of the form 'owner/repo', not {repo_string}"
            )
        owner, name = repo_string.split("/")  # 分割字符串获取所有者和仓库名
        return cls(owner, name)

    def __str__(self) -> str:
        return f"{self.owner}/{self.name}"  # 返回格式化的所有者/仓库名字符串


def upload_pytest_cache(
    pr_identifier: PRIdentifier,
    repo: GithubRepo,
    job_identifier: str,
    sha: str,
    test_config: str,
    shard: str,
    cache_dir: Path,
    temp_dir: Path,
    bucket: str = BUCKET,
) -> None:
    """
    Uploads the pytest cache to S3, merging it with any previous caches from previous runs of the same job.
    In particular, this keeps all the failed tests across all runs of this job in the cache, so that
    future jobs that download this cache will prioritize running tests that have failed in the past.

    Args:
        pr_identifier: A unique, human readable identifier for the PR
        job: The name of the job that is uploading the cache
    """

    if not isinstance(pr_identifier, PRIdentifier):  # 检查 pr_identifier 是否为 PRIdentifier 类型
        raise ValueError(
            f"pr_identifier must be of type PRIdentifier, not {type(pr_identifier)}"
        )

    if not bucket:
        bucket = BUCKET  # 如果未指定 bucket，则使用默认的 BUCKET

    # Upload the cache
    obj_key_prefix = _get_s3_key_prefix(  # 获取 S3 对象键前缀
        pr_identifier, repo, job_identifier, sha, test_config, shard
    )
    zip_file_path = zip_folder(cache_dir, temp_dir / ZIP_UPLOAD / obj_key_prefix)  # 压缩缓存目录到临时文件夹路径
    obj_key = f"{obj_key_prefix}{os.path.splitext(zip_file_path)[1]}"  # 构建 S3 对象键，保留新的文件扩展名
    upload_file_to_s3(zip_file_path, bucket, obj_key)  # 将压缩文件上传到 S3


def download_pytest_cache(
    pr_identifier: PRIdentifier,
    repo: GithubRepo,
    job_identifier: str,
    dest_cache_dir: Path,
    temp_dir: Path,
    bucket: str = BUCKET,
) -> None:
    """
    Downloads the pytest cache from S3 and extracts it to the destination cache directory.

    Args:
        pr_identifier: A unique, human readable identifier for the PR
        job: The name of the job that is downloading the cache
        dest_cache_dir: The directory where the cache will be extracted
        temp_dir: Temporary directory used for intermediate operations
        bucket: The name of the S3 bucket where the cache is stored
    """
    # 从 S3 下载 pytest 缓存。目标是检测过去失败的测试，并优先运行它们，以便开发者能更快地获得反馈。

    # 如果未提供 bucket 参数，则使用默认的 BUCKET 值
    if not bucket:
        bucket = BUCKET

    # 确保 pr_identifier 是 PRIdentifier 类型，否则抛出 ValueError 异常
    if not isinstance(pr_identifier, PRIdentifier):
        raise ValueError(
            f"pr_identifier must be of type PRIdentifier, not {type(pr_identifier)}"
        )

    # 根据 pr_identifier、repo 和 job_identifier 获取 S3 对象键的前缀
    obj_key_prefix = _get_s3_key_prefix(pr_identifier, repo, job_identifier)

    # 设置用于下载 ZIP 文件的临时目录路径
    zip_download_dir = temp_dir / CACHE_ZIP_DOWNLOADS / obj_key_prefix

    # 下载具有指定前缀的所有缓存 ZIP 文件
    downloads = download_s3_objects_with_prefix(
        bucket, obj_key_prefix, zip_download_dir
    )

    # 遍历下载的每个 ZIP 文件
    for downloaded_zip in downloads:
        # 解压缩到随机文件夹中，然后与当前缓存合并
        cache_dir_for_shard = (
            temp_dir / UNZIPPED_CACHES / os.urandom(16).hex() / PYTEST_CACHE_DIR_NAME
        )

        # 解压缩下载的 ZIP 文件到指定的缓存目录
        unzip_folder(downloaded_zip, cache_dir_for_shard)

        # 打印提示信息，显示正在合并哪个 ZIP 文件的缓存
        print(f"Merging cache from {downloaded_zip}")

        # 合并 pytest 缓存
        _merge_pytest_caches(cache_dir_for_shard, dest_cache_dir)
# 构建 S3 对象键的前缀，用于 pytest 缓存
def _get_s3_key_prefix(
    pr_identifier: PRIdentifier,
    repo: GithubRepo,
    job_identifier: str,
    sha: str = "",
    test_config: str = "",
    shard: str = "",
) -> str:
    """
    The prefix to any S3 object key for a pytest cache. It's only a prefix though, not a full path to an object.
    For example, it won't include the file extension.
    """
    # 构建基础前缀，包括 pytest 缓存键前缀、仓库所有者、仓库名称、PR 标识、经过 S3 清理的作业标识
    prefix = f"{PYTEST_CACHE_KEY_PREFIX}/{repo.owner}/{repo.name}/{pr_identifier}/{sanitize_for_s3(job_identifier)}"

    # 如果提供了 SHA 标识，添加到前缀中
    if sha:
        prefix += f"/{sha}"
    # 如果提供了测试配置，经过 S3 清理后添加到前缀中
    if test_config:
        prefix += f"/{sanitize_for_s3(test_config)}"
    # 如果提供了分片信息，添加到前缀中
    if shard:
        prefix += f"/{shard}"

    # 返回构建好的 S3 对象键前缀
    return prefix


def _merge_pytest_caches(
    pytest_cache_dir_to_merge_from: Path, pytest_cache_dir_to_merge_into: Path
) -> None:
    # LASTFAILED_FILE_PATH 是缓存中我们唯一关心的文件，因为它包含所有失败的测试。
    #
    # 其余文件是静态支持文件，对于 pytest 并不重要。它们使缓存文件夹与其他开发人员常用的工具（例如 git）兼容。
    # 但是由于 pytest 不会在 .pytest_cache 文件夹已存在时重新创建这些文件，
    # 我们会复制它们作为一种保护措施，以防未来的 bug 需要这些文件存在才能正常工作（它们的组合文件大小可以忽略不计）。
    static_files_to_copy = [
        ".gitignore",
        "CACHEDIR.TAG",
        "README.md",
    ]

    # 复制静态文件。这些文件从不改变，因此只有在新缓存中不存在时才复制它们。
    for static_file in static_files_to_copy:
        source_file = pytest_cache_dir_to_merge_from / static_file
        if not source_file.is_file():
            continue

        dest_file = pytest_cache_dir_to_merge_into / static_file
        if not dest_file.exists():
            copy_file(source_file, dest_file)

    # 处理 v/cache/lastfailed 文件
    _merge_lastfailed_files(
        pytest_cache_dir_to_merge_from, pytest_cache_dir_to_merge_into
    )

    _merge_additional_failures_files(
        pytest_cache_dir_to_merge_from, pytest_cache_dir_to_merge_into
    )


def _merge_lastfailed_files(source_pytest_cache: Path, dest_pytest_cache: Path) -> None:
    # 简单情况：其中一个文件不存在
    source_lastfailed_file = source_pytest_cache / LASTFAILED_FILE_PATH
    dest_lastfailed_file = dest_pytest_cache / LASTFAILED_FILE_PATH

    if not source_lastfailed_file.exists():
        return
    if not dest_lastfailed_file.exists():
        # 如果目标文件不存在，直接复制源文件到目标位置
        copy_file(source_lastfailed_file, dest_lastfailed_file)
        return

    # 如果两个文件都存在，则需要合并它们
    from_lastfailed = load_json_file(source_lastfailed_file)
    to_lastfailed = load_json_file(dest_lastfailed_file)
    merged_content = _merged_lastfailed_content(from_lastfailed, to_lastfailed)

    # 保存合并后的内容到目标文件
    write_json_file(dest_lastfailed_file, merged_content)
# 合并两个字典，其中 from_lastfailed 是源字典，to_lastfailed 是目标字典
def _merged_lastfailed_content(
    from_lastfailed: Dict[str, bool], to_lastfailed: Dict[str, bool]
) -> Dict[str, bool]:
    """
    The lastfailed files are dictionaries where the key is the test identifier.
    Each entry's value appears to always be `true`, but let's not count on that.
    An empty dictionary is represented with a single value with an empty string as the key.
    """

    # 如果 from_lastfailed 中的条目在 to_lastfailed 中不存在，将其添加到 to_lastfailed 中
    for key in from_lastfailed:
        if key not in to_lastfailed:
            to_lastfailed[key] = from_lastfailed[key]

    if len(to_lastfailed) > 1:
        # 如果存在空条目，删除它，因为现在有实际的条目了
        if "" in to_lastfailed:
            del to_lastfailed[""]

    # 返回合并后的 to_lastfailed 字典
    return to_lastfailed


def _merge_additional_failures_files(
    source_pytest_cache: Path, dest_pytest_cache: Path
) -> None:
    # 简单情况下，其中一个文件不存在
    source_lastfailed_file = (
        source_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL
    )
    dest_lastfailed_file = dest_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL

    # 如果源文件不存在，直接返回
    if not source_lastfailed_file.exists():
        return
    
    # 如果目标文件不存在，复制源文件到目标文件并返回
    if not dest_lastfailed_file.exists():
        copy_file(source_lastfailed_file, dest_lastfailed_file)
        return

    # 如果两个文件都存在，需要合并它们的内容
    from_lastfailed = load_json_file(source_lastfailed_file)
    to_lastfailed = load_json_file(dest_lastfailed_file)
    merged_content = list(set(from_lastfailed + to_lastfailed))

    # 将合并后的内容保存到目标文件中
    write_json_file(dest_lastfailed_file, merged_content)
```