# `.\pytorch\torch\_logging\_registrations.py`

```
# 导入模块内部的注册函数
from ._internal import register_artifact, register_log

# 定义动态日志列表
DYNAMIC = [
    "torch.fx.experimental.symbolic_shapes",
    "torch.fx.experimental.sym_node",
    "torch.fx.experimental.recording",
]

# 定义分布式日志列表
DISTRIBUTED = [
    "torch.distributed",
    "torch._dynamo.backends.distributed",
    "torch.nn.parallel.distributed",
]

# 注册日志 "dynamo"，包括 "torch._dynamo" 和动态日志列表
register_log("dynamo", ["torch._dynamo", *DYNAMIC])

# 注册日志 "aot"，包括 "torch._functorch.aot_autograd" 和 "torch._functorch._aot_autograd"
register_log("aot", ["torch._functorch.aot_autograd", "torch._functorch._aot_autograd"])

# 注册日志 "autograd"，包括 "torch.autograd"
register_log("autograd", "torch.autograd")

# 注册日志 "inductor"，包括 "torch._inductor" 和 "torch._inductor.cudagraph_trees"
register_log("inductor", ["torch._inductor", "torch._inductor.cudagraph_trees"])

# 注册 "cudagraphs" 作为一种工件，描述其生成的 cudagraphs 的信息
register_artifact(
    "cudagraphs",
    "Logs information from wrapping inductor generated code with cudagraphs.",
)

# 注册日志 "dynamic"，包括动态日志列表 DYNAMIC
register_log("dynamic", DYNAMIC)

# 注册日志 "torch"，包括 "torch"
register_log("torch", "torch")

# 注册日志 "distributed"，包括 DISTRIBUTED 列表
register_log("distributed", DISTRIBUTED)

# 注册日志 "c10d"，包括 "torch.distributed.distributed_c10d" 和 "torch.distributed.rendezvous"
register_log(
    "c10d", ["torch.distributed.distributed_c10d", "torch.distributed.rendezvous"]
)

# 注册日志 "ddp"，包括 "torch.nn.parallel.distributed" 和 "torch._dynamo.backends.distributed"
register_log(
    "ddp", ["torch.nn.parallel.distributed", "torch._dynamo.backends.distributed"]
)

# 注册日志 "pp"，包括 "torch.distributed.pipelining"
register_log("pp", ["torch.distributed.pipelining"])

# 注册日志 "fsdp"，包括 "torch.distributed.fsdp" 和 "torch.distributed._composable.fsdp"
register_log("fsdp", ["torch.distributed.fsdp", "torch.distributed._composable.fsdp"])

# 注册日志 "onnx"，包括 "torch.onnx"
register_log("onnx", "torch.onnx")

# 注册日志 "export"，包括 "torch._dynamo"、"torch.export"、动态日志列表 DYNAMIC 和 "torch._export.converter"
register_log(
    "export", ["torch._dynamo", "torch.export", *DYNAMIC, "torch._export.converter"]
)

# 注册工件 "guards"，用于打印每个编译 Dynamo 帧的保护信息，但不显示保护信息的来源
register_artifact(
    "guards",
    "This prints the guards for every compiled Dynamo frame. It does not tell you where the guards come from.",
    visible=True,
)

# 注册工件 "verbose_guards"，默认情况下不显示
register_artifact("verbose_guards", "", off_by_default=True)

# 注册工件 "bytecode"，默认情况下不显示，用于打印 Dynamo 中原始和修改后的字节码，用于调试字节码生成
register_artifact(
    "bytecode",
    "Prints the original and modified bytecode from Dynamo. Mostly useful if you're debugging our bytecode generation in Dynamo.",
    off_by_default=True,
)

# 注册工件 "graph"，用于打印 Dynamo 追踪的图表格形式，在需要 Python 代码时，请使用 "graph_code"
register_artifact(
    "graph",
    "Prints the dynamo traced graph (prior to AOTDispatch) in a table. If you prefer python code use `graph_code` instead. ",
)

# 注册工件 "graph_code"，类似于 "graph"，但提供 Python 代码形式的输出
register_artifact("graph_code", "Like `graph`, but gives you the Python code instead.")

# 注册工件 "graph_sizes"，用于打印 Dynamo 图中所有 FX 节点的大小信息
register_artifact(
    "graph_sizes", "Prints the sizes of all FX nodes in the dynamo graph."
)

# 注册工件 "trace_source"，用于在执行字节码时打印正在处理的文件名/行号和实际源代码，与 "bytecode" 结合使用
register_artifact(
    "trace_source",
    "As we execute bytecode, prints the file name / line number we are processing and the actual source code. Useful with `bytecode`",
)

# 注册工件 "trace_call"，类似于 "trace_source"，但如果 Python 版本足够新，则逐表达式提供详细信息
register_artifact(
    "trace_call",
    "Like trace_source, but it will give you the per-expression blow-by-blow if your Python is recent enough.",
)

# 注册工件 "trace_bytecode"，用于追踪字节码时打印指令和当前堆栈
register_artifact(
    "trace_bytecode",
    "As we trace bytecode, prints the instruction and the current stack.",
)

# 注册工件 "aot_graphs"，用于打印 AOTDispatch 生成的 FX 前向和反向图，用于理解 Inductor 收到的输入内容
register_artifact(
    "aot_graphs",
    "Prints the FX forward and backward graph generated by AOTDispatch, after partitioning. Useful to understand what's being given to Inductor",
    visible=True,
)

# 注册工件 "aot_joint_graph"，用于打印 AOTAutograd 生成的 FX 联合图，在分区之前，用于调试分区过程
register_artifact(
    "aot_joint_graph",
    "Print FX joint graph from AOTAutograd, prior to partitioning. Useful for debugging partitioning",
)

# 注册工件 "post_grad_graphs"，未完整注册，应该是用于打印 Dynamo 图中后处理的梯度图信息
    # 打印由后处理阶段生成的FX图。有助于理解后处理阶段传递给Inductor的内容。
# 注册一个名为 "compiled_autograd" 的工件
register_artifact(
    "compiled_autograd",
    "Prints various logs in compiled_autograd, including but not limited to the graphs. Useful for debugging compiled_autograd.",
    visible=True,
)

# 注册一个名为 "compiled_autograd_verbose" 的工件
register_artifact(
    "compiled_autograd_verbose",
    "Will affect performance. Prints compiled_autograd logs with C++ info e.g. autograd node -> fx node mapping",
    off_by_default=True,
)

# 注册一个名为 "ddp_graphs" 的工件
register_artifact(
    "ddp_graphs",
    "Only relevant for compiling DDP. DDP splits into multiple graphs to trigger comms early. This will print each individual graph here.",
)

# 注册一个名为 "recompiles" 的工件
register_artifact(
    "recompiles",
    "Prints the reason why we recompiled a graph. Very, very useful.",
    visible=True,
)

# 注册一个名为 "recompiles_verbose" 的工件
register_artifact(
    "recompiles_verbose",
    "Prints all guard checks that fail during a recompilation. "
    "At runtime, Dynamo will stop at the first failed check for each failing guard. "
    "So not all logged failing checks are actually ran by Dynamo.",
    visible=True,
    off_by_default=True,
)

# 注册一个名为 "graph_breaks" 的工件
register_artifact(
    "graph_breaks",
    "Prints whenever Dynamo decides that it needs to graph break (i.e. create a new graph). Useful for debugging why torch.compile has poor performance",
    visible=True,
)

# 注册一个名为 "not_implemented" 的工件
register_artifact(
    "not_implemented",
    "Prints log messages whenever we return NotImplemented in a multi-dispatch, letting you trace through each object we attempted to dispatch to",
)

# 注册一个名为 "output_code" 的工件
register_artifact(
    "output_code",
    "Prints the code that Inductor generates (either Triton or C++)",
    off_by_default=True,
    visible=True,
)

# 注册一个名为 "kernel_code" 的工件
register_artifact(
    "kernel_code",
    "Prints the code that Inductor generates (on a per-kernel basis)",
    off_by_default=True,
    visible=True,
)

# 注册一个名为 "schedule" 的工件
register_artifact(
    "schedule",
    "Inductor scheduler information. Useful if working on Inductor fusion algo",
    off_by_default=True,
)

# 注册一个名为 "perf_hints" 的工件，没有提供详细描述信息
register_artifact("perf_hints", "", off_by_default=True)

# 注册一个名为 "onnx_diagnostics" 的工件，没有提供详细描述信息
register_artifact("onnx_diagnostics", "", off_by_default=True)

# 注册一个名为 "fusion" 的工件
register_artifact(
    "fusion",
    "Detailed Inductor fusion decisions. More detailed than 'schedule'",
    off_by_default=True,
)

# 注册一个名为 "overlap" 的工件
register_artifact(
    "overlap",
    "Detailed Inductor compute/comm overlap decisions",
    off_by_default=True,
)

# 注册一个名为 "sym_node" 的工件
register_artifact(
    "sym_node",
    "Logs extra info for various SymNode operations",
    off_by_default=True,
)

# 注册一个名为 "custom_format_test_artifact" 的自定义格式测试工件
register_artifact("custom_format_test_artifact", "Testing only", log_format="")
```