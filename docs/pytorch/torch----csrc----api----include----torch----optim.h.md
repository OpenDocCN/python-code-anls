# `.\pytorch\torch\csrc\api\include\torch\optim.h`

```
#pragma once
// 包含 Torch 库中的 Adagrad 优化器头文件
#include <torch/optim/adagrad.h>
// 包含 Torch 库中的 Adam 优化器头文件
#include <torch/optim/adam.h>
// 包含 Torch 库中的 AdamW 优化器头文件
#include <torch/optim/adamw.h>
// 包含 Torch 库中的 LBFGS 优化器头文件
#include <torch/optim/lbfgs.h>
// 包含 Torch 库中的 optimizer 通用优化器头文件
#include <torch/optim/optimizer.h>
// 包含 Torch 库中的 RMSprop 优化器头文件
#include <torch/optim/rmsprop.h>
// 包含 Torch 库中的 SGD 优化器头文件
#include <torch/optim/sgd.h>

// 包含 Torch 库中的学习率调度器基类头文件
#include <torch/optim/schedulers/lr_scheduler.h>
// 包含 Torch 库中的 ReduceOnPlateauScheduler 学习率调度器头文件
#include <torch/optim/schedulers/reduce_on_plateau_scheduler.h>
// 包含 Torch 库中的 StepLR 学习率调度器头文件
#include <torch/optim/schedulers/step_lr.h>
```