# `.\pytorch\.github\scripts\trymerge.py`

```
#!/usr/bin/env python3

# NB: the following functions are used in Meta-internal workflows
# (github_first_try_merge/my_handler.py) and thus have functionality limitations
# (no `git` command access, no network access besides the strict allow list):
#
# find_matching_merge_rule
# read_merge_rules
#
# Also any signature changes of these functions, as well as changes to the `GitHubPR`
# class, will likely require corresponding changes for the internal workflows.

import base64  # 导入base64模块
import json  # 导入json模块
import os  # 导入os模块
import re  # 导入re模块
import time  # 导入time模块
import urllib.parse  # 导入urllib.parse模块
from collections import defaultdict  # 导入collections模块中的defaultdict类
from dataclasses import dataclass  # 导入dataclasses模块中的dataclass装饰器
from functools import lru_cache  # 导入functools模块中的lru_cache装饰器
from pathlib import Path  # 导入pathlib模块中的Path类
from typing import (  # 导入typing模块中的各种类型标注
    Any,
    Callable,
    cast,
    Dict,
    Iterable,
    List,
    NamedTuple,
    Optional,
    Pattern,
    Tuple,
)
from warnings import warn  # 导入warnings模块中的warn函数

import yaml  # 导入yaml模块
from github_utils import (  # 从github_utils模块导入以下函数
    gh_fetch_json_list,
    gh_fetch_merge_base,
    gh_fetch_url,
    gh_graphql,
    gh_post_commit_comment,
    gh_post_pr_comment,
    gh_update_pr_state,
    GitHubComment,
)

from gitutils import (  # 从gitutils模块导入以下函数
    are_ghstack_branches_in_sync,
    get_git_remote_name,
    get_git_repo_dir,
    GitRepo,
    patterns_to_regex,
    retries_decorator,
)
from label_utils import (  # 从label_utils模块导入以下函数
    gh_add_labels,
    gh_remove_label,
    has_required_labels,
    LABEL_ERR_MSG,
)
from trymerge_explainer import get_revert_message, TryMergeExplainer  # 导入trymerge_explainer模块中的两个函数

# labels
MERGE_IN_PROGRESS_LABEL = "merging"  # 定义标签常量MERGE_IN_PROGRESS_LABEL为"merging"
MERGE_COMPLETE_LABEL = "merged"  # 定义标签常量MERGE_COMPLETE_LABEL为"merged"


class JobCheckState(NamedTuple):
    name: str
    url: str
    status: Optional[str]
    classification: Optional[str]
    job_id: Optional[int]
    title: Optional[str]
    summary: Optional[str]


JobNameToStateDict = Dict[str, JobCheckState]  # 定义类型别名JobNameToStateDict为字典，键为str，值为JobCheckState对象


class WorkflowCheckState:
    def __init__(self, name: str, url: str, run_id: int, status: Optional[str]):
        self.name: str = name
        self.url: str = url
        self.run_id: int = run_id
        self.status: Optional[str] = status
        self.jobs: JobNameToStateDict = {}  # 初始化jobs属性为空字典


GH_PR_REVIEWS_FRAGMENT = """
fragment PRReviews on PullRequestReviewConnection {
  nodes {
    author {
      login
    }
    bodyText
    createdAt
    authorAssociation
    editor {
      login
    }
    databaseId
    url
    state
  }
  pageInfo {
    startCursor
    hasPreviousPage
  }
}
"""

GH_CHECKSUITES_FRAGMENT = """
fragment PRCheckSuites on CheckSuiteConnection {
  edges {
    node {
      app {
        name
        databaseId
      }
      workflowRun {
        workflow {
          name
          databaseId
        }
        databaseId
        url
      }
      checkRuns(first: 50) {
        nodes {
          name
          conclusion
          detailsUrl
          databaseId
          title
          summary
        }
        pageInfo {
          endCursor
          hasNextPage
        }
      }
      conclusion
    }
    cursor
  }
  pageInfo {
    hasNextPage
  }
}
"""

GH_COMMIT_AUTHORS_FRAGMENT = """
fragment CommitAuthors on PullRequestCommitConnection {
  nodes {
    commit {
      authors(first: 2) {
        nodes {
          user {
            login
          }
          email
          name
        }
      }
      oid  # 提交的唯一标识符
    }
  }
  pageInfo {
    endCursor  # 分页信息：结束游标
    hasNextPage  # 分页信息：是否有下一页
  }
}
"""

GH_GET_PR_INFO_QUERY = (
    GH_PR_REVIEWS_FRAGMENT
    + GH_CHECKSUITES_FRAGMENT
    + GH_COMMIT_AUTHORS_FRAGMENT
    + """
query ($owner: String!, $name: String!, $number: Int!) {
  repository(owner: $owner, name: $name) {
    pullRequest(number: $number) {
      closed  # PR 是否已关闭
      isCrossRepository  # 是否跨仓库的 PR
      author {
        login  # 作者的登录名
      }
      title  # PR 标题
      body  # PR 内容
      headRefName  # 源分支引用名称
      headRepository {
        nameWithOwner  # 源分支仓库名称（包括所有者）
      }
      baseRefName  # 目标分支引用名称
      baseRefOid  # 目标分支引用的唯一标识符
      baseRepository {
        nameWithOwner  # 目标分支仓库名称（包括所有者）
        isPrivate  # 目标仓库是否为私有
        defaultBranchRef {
          name  # 目标仓库的默认分支名称
        }
      }
      mergeCommit {
        oid  # 合并提交的唯一标识符
      }
      commits_with_authors: commits(first: 100) {
        ...CommitAuthors  # 使用 CommitAuthors 片段获取提交与作者信息
        totalCount  # 总提交数
      }
      commits(last: 1) {
        nodes {
          commit {
            checkSuites(first: 10) {
              ...PRCheckSuites  # 使用 PRCheckSuites 片段获取检查套件信息
            }
            status {
              contexts {
                context  # 上下文信息
                state  # 状态信息
                targetUrl  # 目标 URL
              }
            }
            oid  # 提交的唯一标识符
          }
        }
      }
      changedFiles  # 修改的文件数
      files(first: 100) {
        nodes {
          path  # 文件路径
        }
        pageInfo {
          endCursor  # 分页信息：结束游标
          hasNextPage  # 分页信息：是否有下一页
        }
      }
      reviews(last: 100) {
        ...PRReviews  # 使用 PRReviews 片段获取审查信息
      }
      comments(last: 5) {
        nodes {
          bodyText  # 评论正文
          createdAt  # 创建时间
          author {
            login  # 作者的登录名
          }
          authorAssociation  # 作者关联信息
          editor {
            login  # 编辑者的登录名
          }
          databaseId  # 数据库 ID
          url  # URL 地址
        }
        pageInfo {
          startCursor  # 分页信息：起始游标
          hasPreviousPage  # 分页信息：是否有上一页
        }
      }
      labels(first: 100) {
        edges {
          node {
            name  # 标签名称
          }
        }
      }
    }
  }
}
"""
)

GH_GET_PR_NEXT_FILES_QUERY = """
query ($owner: String!, $name: String!, $number: Int!, $cursor: String!) {
  repository(name: $name, owner: $owner) {
    pullRequest(number: $number) {
      files(first: 100, after: $cursor) {
        nodes {
          path  # 文件路径
        }
        pageInfo {
          endCursor  # 分页信息：结束游标
          hasNextPage  # 分页信息：是否有下一页
        }
      }
    }
  }
}
"""

GH_GET_PR_NEXT_CHECKSUITES = (
    GH_CHECKSUITES_FRAGMENT
    + """
query ($owner: String!, $name: String!, $number: Int!, $cursor: String!) {
  repository(name: $name, owner: $owner) {
    pullRequest(number: $number) {
      commits(last: 1) {
        nodes {
          commit {
            oid  # 提交的唯一标识符
            checkSuites(first: 10, after: $cursor) {
              ...PRCheckSuites  # 使用 PRCheckSuites 片段获取检查套件信息
            }
          }
        }
      }
    }
  }
}
"""
)

GH_GET_PR_NEXT_CHECK_RUNS = """
# GitHub GraphQL query to fetch information about a specific pull request based on parameters
query ($owner: String!, $name: String!, $number: Int!, $cs_cursor: String, $cr_cursor: String!) {
  repository(name: $name, owner: $owner) {
    # Retrieve repository information based on owner and name
    pullRequest(number: $number) {
      # Fetch details of the specified pull request identified by its number
      commits(last: 1) {
        # Fetch the last commit associated with the pull request
        nodes {
          commit {
            oid  # Unique identifier of the commit
            checkSuites(first: 1, after: $cs_cursor) {
              # Fetch check suites associated with the commit, starting after $cs_cursor
              nodes {
                checkRuns(first: 100, after: $cr_cursor) {
                  # Fetch up to 100 check runs associated with the commit, starting after $cr_cursor
                  nodes {
                    name  # Name of the check run
                    conclusion  # Conclusion of the check run (success, failure, etc.)
                    detailsUrl  # URL providing detailed information about the check run
                    databaseId  # Database ID of the check run
                    title  # Title of the check run
                    summary  # Summary of the check run
                  }
                  pageInfo {
                    endCursor  # Cursor marking the end of the current page of check runs
                    hasNextPage  # Indicates if there are more pages of check runs
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}



# GitHub GraphQL query to retrieve comments on a specific pull request based on parameters
GH_GET_PR_PREV_COMMENTS = """
query ($owner: String!, $name: String!, $number: Int!, $cursor: String!) {
  repository(name: $name, owner: $owner) {
    pullRequest(number: $number) {
      comments(last: 100, before: $cursor) {
        # Retrieve up to 100 comments made on the pull request before the specified cursor
        nodes {
          bodyText  # Body text content of the comment
          createdAt  # Date and time when the comment was created
          author {
            login  # Login username of the comment author
          }
          authorAssociation  # Association level of the comment author (e.g., MEMBER, COLLABORATOR)
          editor {
            login  # Login username of the comment editor
          }
          databaseId  # Database ID of the comment
          url  # URL link to view the comment
        }
        pageInfo {
          startCursor  # Cursor marking the start of the current page of comments
          hasPreviousPage  # Indicates if there are previous pages of comments
        }
      }
    }
  }
}
"""



# GitHub GraphQL query to retrieve team members of an organization's team based on parameters
# This query requires read-org permission
GH_GET_TEAM_MEMBERS_QUERY = """
query($org: String!, $name: String!, $cursor: String) {
  organization(login: $org) {
    team(slug: $name) {
      members(first: 100, after: $cursor) {
        # Retrieve up to 100 members of the specified team, starting after $cursor
        nodes {
          login  # Login username of a team member
        }
        pageInfo {
          hasNextPage  # Indicates if there are more pages of team members
          endCursor  # Cursor marking the end of the current page of team members
        }
      }
    }
  }
}
"""



# GitHub GraphQL query to retrieve authors of commits in a specific pull request based on parameters
GH_GET_PR_NEXT_AUTHORS_QUERY = (
    GH_COMMIT_AUTHORS_FRAGMENT
    + """
query ($owner: String!, $name: String!, $number: Int!, $cursor: String) {
  repository(name: $name, owner: $owner) {
    pullRequest(number: $number) {
      commits_with_authors: commits(first: 100, after: $cursor) {
        ...CommitAuthors  # Include fragment for commit authors
      }
    }
  }
}
"""
)



# GitHub GraphQL query to retrieve reviews of a specific pull request based on parameters
GH_GET_PR_PREV_REVIEWS_QUERY = (
    GH_PR_REVIEWS_FRAGMENT
    + """
query ($owner: String!, $name: String!, $number: Int!, $cursor: String!) {
  repository(name: $name, owner: $owner) {
    pullRequest(number: $number) {
      reviews(last: 100, before: $cursor) {
        ...PRReviews  # Include fragment for pull request reviews
      }
    }
  }
}
"""
)



# GitHub GraphQL query to retrieve submodules of a repository based on parameters
GH_GET_REPO_SUBMODULES = """
query ($owner: String!, $name: String!) {
  repository(owner: $owner, name: $name) {
    submodules(first: 100) {
      # Retrieve up to 100 submodules of the specified repository
      nodes {
        path  # Path of the submodule within the repository
      }
      pageInfo {
        endCursor  # Cursor marking the end of the current page of submodules
        hasNextPage  # Indicates if there are more pages of submodules
      }
    }
  }
}
"""



# Regular expression pattern to match the head reference of a GitHub stack pull request
RE_GHSTACK_HEAD_REF = re.compile(r"^(gh/[^/]+/[0-9]+/)head$")



# Regular expression pattern to match the description of a GitHub stack pull request
RE_GHSTACK_DESC = re.compile(r"Stack.*:\r?\n(\* [^\r\n]+\r?\n)+", re.MULTILINE)



# Regular expression pattern to match a resolved GitHub pull request URL and extract owner, repo, and number
RE_PULL_REQUEST_RESOLVED = re.compile(
    r"Pull Request resolved: "
    r"https://github.com/(?P<owner>[^/]+)/(?P<repo>[^/]+)/pull/(?P<number>[0-9]+)",
    re.MULTILINE,
)
# 匹配以 'cc: @用户名' 开头的行，支持多行匹配模式
RE_PR_CC_LINE = re.compile(r"^cc:? @\w+.*\r?\n?$", re.MULTILINE)

# 匹配以 'Differential Revision: D数字' 开头的行，支持多行匹配模式，提取出Differential Revision号码
RE_DIFF_REV = re.compile(r"^Differential Revision:.+?(D[0-9]+)", re.MULTILINE)

# 匹配以 'ciflow/' 开头的行
CIFLOW_LABEL = re.compile(r"^ciflow/.+")

# 匹配以 'ciflow/trunk' 开头的行
CIFLOW_TRUNK_LABEL = re.compile(r"^ciflow/trunk")

# 定义一个路径对象，指向当前目录下的 .github/merge_rules.yaml 文件
MERGE_RULE_PATH = Path(".github") / "merge_rules.yaml"

# 定义一个字符串常量，表示 Rockset 数据库中的一个集合名称
ROCKSET_MERGES_COLLECTION = "merges"

# 定义一个字符串常量，表示 Rockset 数据库中的一个工作区名称
ROCKSET_MERGES_WORKSPACE = "commons"

# 定义一个字符串常量，表示远程仓库的主分支名称
REMOTE_MAIN_BRANCH = "origin/main"

# 定义一个字符串常量，表示 Dr.CI 的检查任务名称
DRCI_CHECKRUN_NAME = "Dr.CI"

# 定义一个字符串常量，表示 Meta Internal-Only Changes Check 的检查任务名称
INTERNAL_CHANGES_CHECKRUN_NAME = "Meta Internal-Only Changes Check"

# 定义一个字符串常量，表示在没有连接到内部 Diff 时的提示消息
HAS_NO_CONNECTED_DIFF_TITLE = "There is no internal Diff connected, this can be merged now"

# 定义一个整数常量，表示可以忽略的失败检查的阈值，用于决定是否忽略所有不稳定或破损的主干失败
# 在严重情况下，使用类似10这样的较大值可能会有用
IGNORABLE_FAILED_CHECKS_THESHOLD = 10

def gh_get_pr_info(org: str, proj: str, pr_no: int) -> Any:
    # 使用 GraphQL 查询获取指定组织(org)、项目(proj)和 Pull Request 编号(pr_no)的信息
    rc = gh_graphql(GH_GET_PR_INFO_QUERY, name=proj, owner=org, number=pr_no)
    return rc["data"]["repository"]["pullRequest"]

@lru_cache(maxsize=None)
def gh_get_team_members(org: str, name: str) -> List[str]:
    # 使用 GraphQL 查询获取指定组织(org)和团队(name)的成员列表
    rc: List[str] = []
    team_members: Dict[str, Any] = {
        "pageInfo": {"hasNextPage": "true", "endCursor": None}
    }
    while bool(team_members["pageInfo"]["hasNextPage"]):
        # 循环查询下一页的团队成员信息，并将登录名(login)加入结果列表(rc)
        query = gh_graphql(
            GH_GET_TEAM_MEMBERS_QUERY,
            org=org,
            name=name,
            cursor=team_members["pageInfo"]["endCursor"],
        )
        team = query["data"]["organization"]["team"]
        if team is None:
            warn(f"Requested non-existing team {org}/{name}")
            return []
        team_members = team["members"]
        rc += [member["login"] for member in team_members["nodes"]]
    return rc

def get_check_run_name_prefix(workflow_run: Any) -> str:
    # 如果 workflow_run 为 None，则返回空字符串，否则返回格式化后的工作流名称
    if workflow_run is None:
        return ""
    else:
        return f'{workflow_run["workflow"]["name"]} / '

def is_passing_status(status: Optional[str]) -> bool:
    # 判断给定的状态(status)是否为成功、跳过或中性状态
    return status is not None and status.upper() in ["SUCCESS", "SKIPPED", "NEUTRAL"]

def add_workflow_conclusions(
    checksuites: Any,
    get_next_checkruns_page: Callable[[List[Dict[str, Dict[str, Any]]], int, Any], Any],
    get_next_checksuites: Callable[[Any], Any],
) -> JobNameToStateDict:
    # GraphQL 倾向于最近的工作流运行，理论上不需要考虑重新运行，但为了安全起见进行了处理

    # 工作流 -> 作业 -> 作业信息 的映射字典
    workflows: Dict[str, WorkflowCheckState] = {}

    # 对于没有工作流对象的作业，使用默认空的 WorkflowCheckState 对象
    no_workflow_obj: WorkflowCheckState = WorkflowCheckState("", "", 0, None)
    def add_conclusions(edges: Any) -> None:
        # 遍历传入的边缘列表
        for edge_idx, edge in enumerate(edges):
            # 获取边缘中的节点信息
            node = edge["node"]
            # 获取节点中的工作流运行信息和检查运行信息
            workflow_run = node["workflowRun"]
            checkruns = node["checkRuns"]

            # 初始化工作流对象，默认为不存在的工作流对象
            workflow_obj: WorkflowCheckState = no_workflow_obj

            # 如果存在工作流运行信息
            if workflow_run is not None:
                # 获取工作流运行的唯一标识和工作流的名称和ID
                workflow_run_id = workflow_run["databaseId"]
                workflow_name = workflow_run["workflow"]["name"]
                workflow_id = workflow_run["workflow"]["databaseId"]

                # 获取工作流的结论状态
                workflow_conclusion = node["conclusion"]
                # 如果结论为"CANCELLED"并且该工作流名称在已知工作流集合中，则跳过处理
                if workflow_conclusion == "CANCELLED" and workflow_name in workflows:
                    continue

                # 仅保留每个工作流的最新运行，根据运行ID判断
                if (
                    workflow_id not in workflows
                    or workflows[workflow_id].run_id < workflow_run_id
                ):
                    # 更新工作流状态信息
                    workflows[workflow_id] = WorkflowCheckState(
                        name=workflow_name,
                        status=workflow_conclusion,
                        url=workflow_run["url"],
                        run_id=workflow_run_id,
                    )
                workflow_obj = workflows[workflow_id]

            # 处理检查运行信息
            while checkruns is not None:
                for checkrun_node in checkruns["nodes"]:
                    # 如果节点不是字典类型，则输出警告并继续下一个节点处理
                    if not isinstance(checkrun_node, dict):
                        warn(f"Expected dictionary, but got {type(checkrun_node)}")
                        continue
                    # 构建检查运行的名称
                    checkrun_name = f'{get_check_run_name_prefix(workflow_run)}{checkrun_node["name"]}'
                    # 获取现有检查运行信息，若不存在或状态不是通过的，则更新信息
                    existing_checkrun = workflow_obj.jobs.get(checkrun_name)
                    if existing_checkrun is None or not is_passing_status(
                        existing_checkrun.status
                    ):
                        workflow_obj.jobs[checkrun_name] = JobCheckState(
                            checkrun_name,
                            checkrun_node["detailsUrl"],
                            checkrun_node["conclusion"],
                            classification=None,
                            job_id=checkrun_node["databaseId"],
                            title=checkrun_node["title"],
                            summary=checkrun_node["summary"],
                        )

                # 如果还有下一页的检查运行信息，则获取下一页数据
                if bool(checkruns["pageInfo"]["hasNextPage"]):
                    checkruns = get_next_checkruns_page(edges, edge_idx, checkruns)
                else:
                    checkruns = None

    # 复制检查套件中的所有边缘数据
    all_edges = checksuites["edges"].copy()
    # 当还有下一页时继续循环处理检查套件数据
    while bool(checksuites["pageInfo"]["hasNextPage"]):
        # 获取下一页的检查套件数据
        checksuites = get_next_checksuites(checksuites)
        # 将获取的边缘数据扩展到所有边缘列表中
        all_edges.extend(checksuites["edges"])

    # 添加结论到所有边缘数据中
    add_conclusions(all_edges)

    # 初始化结果字典，用于存储作业名称到状态的映射
    res: JobNameToStateDict = {}

    # 遍历工作流字典中的每个工作流对象
    for workflow in workflows.values():
        # 如果工作流对象中有作业存在
        if len(workflow.jobs) > 0:
            # 遍历工作流对象中的每个作业并添加到结果字典中
            for job_name, job in workflow.jobs.items():
                res[job_name] = job
        else:
            # 如果工作流对象中没有作业，创建一个虚拟的作业状态对象并添加到结果字典中
            res[workflow.name] = JobCheckState(
                workflow.name,
                workflow.url,
                workflow.status,
                classification=None,
                job_id=None,
                title=None,
                summary=None,
            )

    # 将未关联到工作流的作业对象也添加到结果字典中
    for job_name, job in no_workflow_obj.jobs.items():
        res[job_name] = job

    # 返回最终的结果字典
    return res
def parse_args() -> Any:
    # 导入 ArgumentParser 类，用于解析命令行参数
    from argparse import ArgumentParser

    # 创建 ArgumentParser 对象，设置描述信息
    parser = ArgumentParser("Merge PR into default branch")
    
    # 添加命令行参数选项
    parser.add_argument("--dry-run", action="store_true")  # 是否执行模拟运行
    parser.add_argument("--revert", action="store_true")   # 是否执行回滚操作
    parser.add_argument("--force", action="store_true")    # 是否强制执行合并
    parser.add_argument("--ignore-current", action="store_true")  # 是否忽略当前分支
    parser.add_argument("--check-mergeability", action="store_true")  # 是否检查可合并性
    parser.add_argument("--comment-id", type=int)           # 评论的 ID
    parser.add_argument("--reason", type=str)               # 操作原因
    parser.add_argument("pr_num", type=int)                 # PR 编号
    # 解析命令行参数并返回结果
    return parser.parse_args()


def can_skip_internal_checks(pr: "GitHubPR", comment_id: Optional[int] = None) -> bool:
    # 如果 comment_id 为 None，则不跳过内部检查
    if comment_id is None:
        return False
    # 获取指定 comment_id 的评论对象
    comment = pr.get_comment_by_id(comment_id)
    # 如果评论编辑者不为空，则不跳过内部检查
    if comment.editor_login is not None:
        return False
    # 返回评论作者是否为 facebook-github-bot
    return comment.author_login == "facebook-github-bot"


def _revlist_to_prs(
    repo: GitRepo,
    pr: "GitHubPR",
    rev_list: Iterable[str],
    should_skip: Optional[Callable[[int, "GitHubPR"], bool]] = None,
) -> List[Tuple["GitHubPR", str]]:
    # 用于存储结果的列表
    rc: List[Tuple[GitHubPR, str]] = []
    # 遍历提交记录列表
    for idx, rev in enumerate(rev_list):
        # 获取提交消息
        msg = repo.commit_message(rev)
        # 在提交消息中查找 PR 解析字符串
        m = RE_PULL_REQUEST_RESOLVED.search(msg)
        # 如果未找到 PR 解析字符串，则抛出运行时错误
        if m is None:
            raise RuntimeError(
                f"Could not find PR-resolved string in {msg} of ghstacked PR {pr.pr_num}"
            )
        # 检查 PR 的所有者和仓库是否匹配
        if pr.org != m.group("owner") or pr.project != m.group("repo"):
            raise RuntimeError(
                f"PR {m.group('number')} resolved to wrong owner/repo pair"
            )
        # 获取 PR 编号
        pr_num = int(m.group("number"))
        # 创建 GitHubPR 对象
        candidate = GitHubPR(pr.org, pr.project, pr_num) if pr_num != pr.pr_num else pr
        # 如果应该跳过该 PR，则继续下一次循环
        if should_skip is not None and should_skip(idx, candidate):
            continue
        # 将结果添加到列表中
        rc.append((candidate, rev))
    # 返回结果列表
    return rc


def get_ghstack_prs(
    repo: GitRepo, pr: "GitHubPR", open_only: bool = True
) -> List[Tuple["GitHubPR", str]]:
    """
    Get the PRs in the stack that are below this PR (inclusive).  Throws error if any of the open PRs are out of sync.
    @:param open_only: Only return open PRs
    """
    # 获取源参考的完整引用
    orig_ref = f"{repo.remote}/{pr.get_ghstack_orig_ref()}"
    # 获取提交记录列表
    rev_list = repo.revlist(f"{pr.default_branch()}..{orig_ref}")

    # 定义用于跳过函数
    def skip_func(idx: int, candidate: "GitHubPR") -> bool:
        # 如果不仅返回开放状态的 PR 或者候选 PR 不是已关闭状态，则不跳过
        if not open_only or not candidate.is_closed():
            return False
        # 打印跳过的信息
        print(
            f"Skipping {idx+1} of {len(rev_list)} PR (#{candidate.pr_num}) as its already been merged"
        )
        return True

    # 断言当前 PR 是 ghstack PR
    assert pr.is_ghstack_pr()
    # 获取整个 PR 堆栈的 PR 列表
    entire_stack = _revlist_to_prs(repo, pr, reversed(rev_list), skip_func)
    # 对于每个堆叠的 Pull Request 和其对应的修订版本进行迭代
    for stacked_pr, rev in entire_stack:
        # 如果堆叠的 Pull Request 已经关闭，则跳过处理
        if stacked_pr.is_closed():
            continue
        
        # 获取堆叠的 Pull Request 的基础引用（基础分支）
        base_ref = stacked_pr.base_ref()
        
        # 如果基础引用与默认分支相同，重新计算基础引用为远程仓库中基础分支与堆叠 PR 头部引用的合并基础
        if base_ref == pr.default_branch():
            base_ref = repo.get_merge_base(
                f"{repo.remote}/{base_ref}", f"{repo.remote}/{stacked_pr.head_ref()}"
            )
        
        # 检查堆叠的 PR 的头部引用和基础引用是否同步
        if not are_ghstack_branches_in_sync(repo, stacked_pr.head_ref(), base_ref):
            # 如果不同步，抛出运行时异常，说明 PR 和对应修订版本在合并到默认分支之前不同步
            raise RuntimeError(
                f"PR {stacked_pr.pr_num} is out of sync with the corresponding revision {rev} on "
                + f"branch {stacked_pr.get_ghstack_orig_ref()} that would be merged into {stacked_pr.default_branch()}.  "
                + "This usually happens because there is a non ghstack change in the PR.  "
                + f"Please sync them and try again (ex. make the changes on {orig_ref} and run ghstack)."
            )
    
    # 返回处理后的堆叠的 Pull Request 列表
    return entire_stack
# 定义 GitHubPR 类，表示 GitHub 上的 Pull Request
class GitHubPR:
    # 初始化方法，接受组织名、项目名和 PR 编号作为参数
    def __init__(self, org: str, project: str, pr_num: int) -> None:
        # 断言 PR 编号是整数类型
        assert isinstance(pr_num, int)
        # 设置实例变量：组织名
        self.org = org
        # 设置实例变量：项目名
        self.project = project
        # 设置实例变量：PR 编号
        self.pr_num = pr_num
        # 获取 PR 的详细信息并保存到实例变量中
        self.info = gh_get_pr_info(org, project, pr_num)
        # 初始化可选的实例变量
        self.changed_files: Optional[List[str]] = None
        self.labels: Optional[List[str]] = None
        self.conclusions: Optional[JobNameToStateDict] = None
        self.comments: Optional[List[GitHubComment]] = None
        self._authors: Optional[List[Tuple[str, str]]] = None
        self._reviews: Optional[List[Tuple[str, str]]] = None
        self.merge_base: Optional[str] = None
        self.submodules: Optional[List[str]] = None

    # 判断 PR 是否已关闭
    def is_closed(self) -> bool:
        return bool(self.info["closed"])

    # 判断 PR 是否跨仓库
    def is_cross_repo(self) -> bool:
        return bool(self.info["isCrossRepository"])

    # 获取 PR 的目标分支名称
    def base_ref(self) -> str:
        return cast(str, self.info["baseRefName"])

    # 获取 PR 所属仓库的默认分支名称
    def default_branch(self) -> str:
        return cast(str, self.info["baseRepository"]["defaultBranchRef"]["name"])

    # 获取 PR 的源分支名称
    def head_ref(self) -> str:
        return cast(str, self.info["headRefName"])

    # 判断 PR 是否是通过 ghstack 工具创建的
    def is_ghstack_pr(self) -> bool:
        return RE_GHSTACK_HEAD_REF.match(self.head_ref()) is not None

    # 获取通过 ghstack 工具创建的 PR 的原始分支名称
    def get_ghstack_orig_ref(self) -> str:
        assert self.is_ghstack_pr()
        return re.sub(r"/head$", "/orig", self.head_ref())

    # 判断 PR 的目标仓库是否为私有仓库
    def is_base_repo_private(self) -> bool:
        return bool(self.info["baseRepository"]["isPrivate"])

    # 获取 PR 中改变的文件数量
    def get_changed_files_count(self) -> int:
        return int(self.info["changedFiles"])

    # 获取 PR 的最后一次提交信息
    def last_commit(self) -> Any:
        return self.info["commits"]["nodes"][-1]["commit"]

    # 获取 PR 的合并基础（merge base）的提交 OID
    def get_merge_base(self) -> str:
        if self.merge_base:
            return self.merge_base

        # 获取 PR 的最后一次提交的 OID
        last_commit_oid = self.last_commit()["oid"]

        # 使用 gh_fetch_merge_base 函数获取 PR 的合并基础
        # 对于普通的 PR，使用 self.base_ref() 是合适的，但是对于 ghstack 创建的 PR，
        # 其基础分支是自定义的，例如 gh/USER/ID/base，因此我们使用 main 作为备用选择
        self.merge_base = gh_fetch_merge_base(
            self.org, self.project, last_commit_oid, self.default_branch()
        )

        # 如果 API 调用失败（例如遇到速率限制），则回退到 baseRefOid
        # baseRefOid 指向 PR 关联的基础引用，或者说 PR 创建或重新基于时的 main 的头部
        # 这不一定是合并基础提交，但在大多数情况下可以作为一个备选项，并且作为 PR 信息的一部分可直接使用
        if not self.merge_base:
            self.merge_base = cast(str, self.info["baseRefOid"])

        return self.merge_base
    # 返回所有已更改文件的路径列表。如果还未获取更改文件列表，则执行获取过程。
    def get_changed_files(self) -> List[str]:
        if self.changed_files is None:
            # 获取 PR 信息
            info = self.info
            unique_changed_files = set()
            # 不超过 100 次迭代，避免获取超过 10,000 个文件
            for _ in range(100):
                # 将当前页文件的路径添加到集合中
                unique_changed_files.update([x["path"] for x in info["files"]["nodes"]])
                # 如果没有下一页，则中断循环
                if not info["files"]["pageInfo"]["hasNextPage"]:
                    break
                # 获取下一页文件信息，并更新 info
                rc = gh_graphql(
                    GH_GET_PR_NEXT_FILES_QUERY,
                    name=self.project,
                    owner=self.org,
                    number=self.pr_num,
                    cursor=info["files"]["pageInfo"]["endCursor"],
                )
                info = rc["data"]["repository"]["pullRequest"]
            # 将集合转换为列表并缓存结果
            self.changed_files = list(unique_changed_files)

        # 如果已更改文件列表长度与更改文件数量不符，则引发运行时错误
        if len(self.changed_files) != self.get_changed_files_count():
            raise RuntimeError("Changed file count mismatch")
        # 返回已更改文件列表
        return self.changed_files

    # 返回所有子模块的路径列表。如果尚未获取子模块列表，则执行获取过程。
    def get_submodules(self) -> List[str]:
        if self.submodules is None:
            # 获取仓库的子模块信息
            rc = gh_graphql(GH_GET_REPO_SUBMODULES, name=self.project, owner=self.org)
            info = rc["data"]["repository"]["submodules"]
            # 提取子模块的路径并缓存结果
            self.submodules = [s["path"] for s in info["nodes"]]
        # 返回子模块路径列表
        return self.submodules

    # 返回所有已更改子模块的路径列表
    def get_changed_submodules(self) -> List[str]:
        # 获取所有子模块路径
        submodules = self.get_submodules()
        # 返回在已更改文件列表中的子模块路径列表
        return [f for f in self.get_changed_files() if f in submodules]

    # 判断 PR 中是否存在无效的子模块更新
    def has_invalid_submodule_updates(self) -> bool:
        # 存在已更改子模块并且在标题、正文、标签中均未提及 "submodule" 关键词，则返回 True，否则返回 False
        return (
            len(self.get_changed_submodules()) > 0
            and "submodule" not in self.get_title().lower()
            and "submodule" not in self.get_body().lower()
            and all("submodule" not in label for label in self.get_labels())
        )

    # 返回 PR 的所有审查的作者和状态列表
    def _get_reviews(self) -> List[Tuple[str, str]]:
        if self._reviews is None:
            self._reviews = []
            info = self.info
            # 不超过 100 次迭代，获取所有审查的作者和状态
            for _ in range(100):
                nodes = info["reviews"]["nodes"]
                # 将每个审查的作者和状态添加到 _reviews 列表中
                self._reviews = [
                    (node["author"]["login"], node["state"]) for node in nodes
                ] + self._reviews
                # 如果没有上一页，则中断循环
                if not info["reviews"]["pageInfo"]["hasPreviousPage"]:
                    break
                # 获取上一页审查信息，并更新 info
                rc = gh_graphql(
                    GH_GET_PR_PREV_REVIEWS_QUERY,
                    name=self.project,
                    owner=self.org,
                    number=self.pr_num,
                    cursor=info["reviews"]["pageInfo"]["startCursor"],
                )
                info = rc["data"]["repository"]["pullRequest"]
        # 筛选掉状态为 "COMMENTED" 的审查，并返回作者和状态组成的列表
        reviews = {}
        for author, state in self._reviews:
            if state != "COMMENTED":
                reviews[author] = state
        return list(reviews.items())
    # 返回所有审批状态为 "APPROVED" 的登录名列表
    def get_approved_by(self) -> List[str]:
        return [login for (login, state) in self._get_reviews() if state == "APPROVED"]

    # 返回提交数量，从 GraphQL 返回的信息中获取
    def get_commit_count(self) -> int:
        return int(self.info["commits_with_authors"]["totalCount"])

    # 返回 Pull Request 的创建者登录名，从 GraphQL 返回的信息中获取
    def get_pr_creator_login(self) -> str:
        return cast(str, self.info["author"]["login"])

    # 获取所有提交的作者信息列表，如果已缓存，则直接返回缓存结果
    def _fetch_authors(self) -> List[Tuple[str, str]]:
        if self._authors is not None:
            return self._authors
        authors: List[Tuple[str, str]] = []

        # 辅助函数，将每个提交的作者信息添加到 authors 列表中
        def add_authors(info: Dict[str, Any]) -> None:
            for node in info["commits_with_authors"]["nodes"]:
                for author_node in node["commit"]["authors"]["nodes"]:
                    user_node = author_node["user"]
                    author = f"{author_node['name']} <{author_node['email']}>"
                    if user_node is None:
                        # 如果作者不是 GitHub 用户，用户节点将为空
                        authors.append(("", author))
                    else:
                        authors.append((cast(str, user_node["login"]), author))

        info = self.info
        # 最多迭代 100 次，以获取所有提交的作者信息
        for _ in range(100):
            add_authors(info)
            if not info["commits_with_authors"]["pageInfo"]["hasNextPage"]:
                break
            # 使用 GraphQL 查询获取下一页的作者信息
            rc = gh_graphql(
                GH_GET_PR_NEXT_AUTHORS_QUERY,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cursor=info["commits_with_authors"]["pageInfo"]["endCursor"],
            )
            info = rc["data"]["repository"]["pullRequest"]
        self._authors = authors
        return authors

    # 返回指定编号（默认为0）的提交者的登录名
    def get_committer_login(self, num: int = 0) -> str:
        return self._fetch_authors()[num][0]

    # 返回指定编号（默认为0）的提交者的作者信息
    def get_committer_author(self, num: int = 0) -> str:
        return self._fetch_authors()[num][1]

    # 返回 Pull Request 的标签列表，如果已缓存，则直接返回缓存结果
    def get_labels(self) -> List[str]:
        if self.labels is not None:
            return self.labels
        # 从 GraphQL 返回的信息中获取标签列表
        labels = (
            [node["node"]["name"] for node in self.info["labels"]["edges"]]
            if "labels" in self.info
            else []
        )
        self.labels = labels
        return self.labels
    # 返回一个字典，包含每个检查运行的名称到其结论和 URL 的映射
    def get_checkrun_conclusions(self) -> JobNameToStateDict:
        """Returns dict of checkrun -> [conclusion, url]"""
        # 如果已经存在结论，则直接返回
        if self.conclusions is not None:
            return self.conclusions
        # 获取最新提交的信息
        orig_last_commit = self.last_commit()

        # 获取 PR 下一个检查运行的信息
        def get_pr_next_check_runs(
            edges: List[Dict[str, Dict[str, Any]]], edge_idx: int, checkruns: Any
        ) -> Any:
            # 使用 GraphQL 获取下一个检查运行的数据
            rc = gh_graphql(
                GH_GET_PR_NEXT_CHECK_RUNS,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cs_cursor=edges[edge_idx - 1]["cursor"] if edge_idx > 0 else None,
                cr_cursor=checkruns["pageInfo"]["endCursor"],
            )
            # 获取最新提交的具体信息
            last_commit = rc["data"]["repository"]["pullRequest"]["commits"]["nodes"][
                -1
            ]["commit"]
            # 获取最新提交的检查套件的运行情况
            checkruns = last_commit["checkSuites"]["nodes"][-1]["checkRuns"]
            return checkruns

        # 获取 PR 下一个检查套件的信息
        def get_pr_next_checksuites(checksuites: Any) -> Any:
            # 使用 GraphQL 获取下一个检查套件的数据
            rc = gh_graphql(
                GH_GET_PR_NEXT_CHECKSUITES,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cursor=checksuites["edges"][-1]["cursor"],
            )
            # 获取仓库中相关 PR 的信息
            info = rc["data"]["repository"]["pullRequest"]
            # 获取最新提交的具体信息
            last_commit = info["commits"]["nodes"][-1]["commit"]
            # 检查最新提交的 OID 是否与原始最后提交的 OID 一致
            if last_commit["oid"] != orig_last_commit["oid"]:
                raise RuntimeError("Last commit changed on PR")
            # 返回最新提交的检查套件
            return last_commit["checkSuites"]

        # 获取原始最后提交的检查套件信息
        checksuites = orig_last_commit["checkSuites"]

        # 添加工作流结论到 self.conclusions
        self.conclusions = add_workflow_conclusions(
            checksuites, get_pr_next_check_runs, get_pr_next_checksuites
        )

        # 将旧的样式状态（例如由 CircleCI 或 EasyCLA 生成的状态）追加到 conclusions 中
        if orig_last_commit["status"] and orig_last_commit["status"]["contexts"]:
            for status in orig_last_commit["status"]["contexts"]:
                name = status["context"]
                # 添加状态到 conclusions
                self.conclusions[name] = JobCheckState(
                    name,
                    status["targetUrl"],
                    status["state"],
                    classification=None,
                    job_id=None,
                    title=None,
                    summary=None,
                )

        # 返回最终的 conclusions
        return self.conclusions

    # 返回一个字典，包含每个提交者的 GitHub 登录名到作者名的映射
    def get_authors(self) -> Dict[str, str]:
        rc = {}
        # 遍历所有提交者，获取他们的 GitHub 登录名和作者名，并添加到字典 rc 中
        for idx in range(len(self._fetch_authors())):
            rc[self.get_committer_login(idx)] = self.get_committer_author(idx)

        # 返回包含 GitHub 登录名到作者名映射的字典 rc
        return rc
    # 获取作者信息的方法，返回一个字符串
    def get_author(self) -> str:
        # 获取所有作者信息的字典
        authors = self.get_authors()
        # 如果只有一个作者，直接返回该作者的值
        if len(authors) == 1:
            return next(iter(authors.values()))
        # 获取提交 Pull Request 的创建者
        creator = self.get_pr_creator_login()
        # 如果 PR 的创建者不在作者列表中，假设由第一个提交者撰写
        if creator not in authors:
            return self.get_committer_author(0)
        # 否则返回 PR 的创建者对应的作者信息
        return authors[creator]

    # 获取标题信息的方法，返回一个字符串
    def get_title(self) -> str:
        # 直接返回存储在 self.info["title"] 中的标题字符串
        return cast(str, self.info["title"])

    # 获取正文信息的方法，返回一个字符串
    def get_body(self) -> str:
        # 直接返回存储在 self.info["body"] 中的正文字符串
        return cast(str, self.info["body"])

    # 获取合并提交的信息的方法，返回一个可选的字符串
    def get_merge_commit(self) -> Optional[str]:
        # 从 self.info["mergeCommit"] 中获取提交的 OID，如果不存在返回 None
        mc = self.info["mergeCommit"]
        return mc["oid"] if mc is not None else None

    # 获取 Pull Request 的 URL 的方法，返回一个字符串
    def get_pr_url(self) -> str:
        # 根据 self.org, self.project 和 self.pr_num 构建 Pull Request 的 URL
        return f"https://github.com/{self.org}/{self.project}/pull/{self.pr_num}"

    # 静态方法：从节点中构造 GitHubComment 对象的方法，返回一个 GitHubComment 实例
    @staticmethod
    def _comment_from_node(node: Any) -> GitHubComment:
        # 从节点中提取编辑者、正文、创建时间、作者登录名、作者关联信息、编辑者登录名、数据库 ID 和 URL
        editor = node["editor"]
        return GitHubComment(
            body_text=node["bodyText"],
            created_at=node["createdAt"] if "createdAt" in node else "",
            author_login=node["author"]["login"],
            author_association=node["authorAssociation"],
            editor_login=editor["login"] if editor else None,
            database_id=node["databaseId"],
            url=node["url"],
        )

    # 获取所有评论的方法，返回一个 GitHubComment 对象的列表
    def get_comments(self) -> List[GitHubComment]:
        # 如果已经获取过评论，则直接返回缓存的评论列表
        if self.comments is not None:
            return self.comments
        # 否则初始化评论列表为空
        self.comments = []
        # 获取评论的详细信息
        info = self.info["comments"]
        # 最多获取一万条评论
        for _ in range(100):
            # 遍历评论节点，创建 GitHubComment 对象并添加到评论列表中
            self.comments = [
                self._comment_from_node(node) for node in info["nodes"]
            ] + self.comments
            # 如果没有上一页评论，则停止获取
            if not info["pageInfo"]["hasPreviousPage"]:
                break
            # 否则继续获取上一页评论的信息
            rc = gh_graphql(
                GH_GET_PR_PREV_COMMENTS,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cursor=info["pageInfo"]["startCursor"],
            )
            info = rc["data"]["repository"]["pullRequest"]["comments"]
        # 返回获取到的所有评论列表
        return self.comments

    # 获取最后一条评论的方法，返回一个 GitHubComment 对象
    def get_last_comment(self) -> GitHubComment:
        # 返回评论节点中的最后一条评论的 GitHubComment 对象
        return self._comment_from_node(self.info["comments"]["nodes"][-1])
    # 根据数据库中评论的唯一标识 ID 获取对应的 GitHubComment 对象
    def get_comment_by_id(self, database_id: int) -> GitHubComment:
        if self.comments is None:
            # 快速路径 - 尝试在部分预取的评论中搜索
            for node in self.info["comments"]["nodes"]:
                comment = self._comment_from_node(node)
                if comment.database_id == database_id:
                    return comment

        # 从全部评论中搜索指定 ID 的评论
        for comment in self.get_comments():
            if comment.database_id == database_id:
                return comment

        # 如果评论实际上是 PR 上的一个审查（与审查同时写入的消息），
        # 检查这些审查评论以确定是否是需要的评论
        for node in self.info["reviews"]["nodes"]:
            # 这些审查评论包含所有常规评论所需的字段
            comment = self._comment_from_node(node)
            if comment.database_id == database_id:
                return comment

        # 如果找不到指定 ID 的评论，则引发运行时错误
        raise RuntimeError(f"Comment with id {database_id} not found")

    # 获取 PR 的差异修订版本号，返回字符串或 None
    def get_diff_revision(self) -> Optional[str]:
        rc = RE_DIFF_REV.search(self.get_body())
        return rc.group(1) if rc is not None else None

    # 检查 PR 是否有内部更改
    def has_internal_changes(self) -> bool:
        checkrun_name = INTERNAL_CHANGES_CHECKRUN_NAME
        if self.get_diff_revision() is None:
            return False
        checks = self.get_checkrun_conclusions()
        if checks is None or checkrun_name not in checks:
            return False
        return checks[checkrun_name].status != "SUCCESS"

    # 检查 PR 是否没有连接的差异
    def has_no_connected_diff(self) -> bool:
        checkrun_name = INTERNAL_CHANGES_CHECKRUN_NAME
        checks = self.get_checkrun_conclusions()
        if checks is None or checkrun_name not in checks:
            return False
        return checks[checkrun_name].title == HAS_NO_CONNECTED_DIFF_TITLE

    # 将当前分支的 GitHub 栈（ghstack）合并到指定的仓库中
    def merge_ghstack_into(
        self,
        repo: GitRepo,
        skip_mandatory_checks: bool,
        comment_id: Optional[int] = None,
        skip_all_rule_checks: bool = False,
        ```
    ) -> List["GitHubPR"]:
        # 确保当前对象对应的 Pull Request 是由 ghstack 生成的
        assert self.is_ghstack_pr()
        # 获取所有相关的 ghstack PRs，包括已关闭的
        ghstack_prs = get_ghstack_prs(
            repo, self, open_only=False
        )  # 如果不同步会引发错误
        # 存储依赖的 PR 列表
        pr_dependencies = []
        # 遍历所有的 ghstack PRs
        for pr, rev in ghstack_prs:
            # 如果 PR 已关闭，则将其加入依赖列表并继续下一个 PR
            if pr.is_closed():
                pr_dependencies.append(pr)
                continue

            # 生成 commit message，过滤掉 ghstack 信息并包含依赖的 PR
            commit_msg = pr.gen_commit_message(
                filter_ghstack=True, ghstack_deps=pr_dependencies
            )
            # 如果当前 PR 不是自身且未跳过所有规则检查，则查找匹配的合并规则
            if pr.pr_num != self.pr_num and not skip_all_rule_checks:
                # 如果找不到匹配的规则会抛出异常
                find_matching_merge_rule(
                    pr,
                    repo,
                    skip_mandatory_checks=skip_mandatory_checks,
                    skip_internal_checks=can_skip_internal_checks(self, comment_id),
                )
            # 对仓库进行 cherry-pick 操作
            repo.cherry_pick(rev)
            # 修改 commit message
            repo.amend_commit_message(commit_msg)
            # 将当前 PR 加入依赖列表
            pr_dependencies.append(pr)
        # 返回所有未关闭的 ghstack PR 对象列表
        return [x for x, _ in ghstack_prs if not x.is_closed()]

    def gen_commit_message(
        self,
        filter_ghstack: bool = False,
        ghstack_deps: Optional[List["GitHubPR"]] = None,
    ) -> str:
        """从 PR 描述中获取标题和正文，添加审阅者信息、解决的 Pull Request，并可选择过滤 ghstack 信息"""
        # 将审批者的 GitHub URL 加入到消息中，使其在 GitHub UI 中可点击
        approved_by_urls = ", ".join(
            prefix_with_github_url(login) for login in self.get_approved_by()
        )
        # 从消息正文中移除 "cc: " 行
        msg_body = re.sub(RE_PR_CC_LINE, "", self.get_body())
        # 如果需要过滤 ghstack 信息，则从消息正文中移除对应内容
        if filter_ghstack:
            msg_body = re.sub(RE_GHSTACK_DESC, "", msg_body)
        # 构建 commit message，包括标题和 PR 编号
        msg = self.get_title() + f" (#{self.pr_num})\n\n"
        msg += msg_body

        # 提及 PR 的共同作者
        for author_login, author_name in self.get_authors().items():
            if author_login != self.get_pr_creator_login():
                msg += f"\nCo-authored-by: {author_name}"

        # 添加解决的 PR 链接和审批者信息到消息中
        msg += f"\nPull Request resolved: {self.get_pr_url()}\n"
        msg += f"Approved by: {approved_by_urls}\n"
        # 如果有 ghstack 依赖，则将其编号添加到消息中
        if ghstack_deps:
            msg += f"ghstack dependencies: {', '.join([f'#{pr.pr_num}' for pr in ghstack_deps])}\n"
        return msg

    def add_numbered_label(self, label_base: str, dry_run: bool) -> None:
        # 获取当前 PR 的标签列表
        labels = self.get_labels() if self.labels is not None else []
        full_label = label_base
        count = 0
        # 遍历现有标签，查找符合基础标签的条目并计数
        for label in labels:
            if label_base in label:
                count += 1
                full_label = f"{label_base}X{count}"
        # 添加带有编号的标签到当前 PR
        gh_add_labels(self.org, self.project, self.pr_num, [full_label], dry_run)

    def merge_into(
        self,
        repo: GitRepo,
        *,
        skip_mandatory_checks: bool = False,
        dry_run: bool = False,
        comment_id: Optional[int] = None,
        ignore_current_checks: Optional[List[str]] = None,
    ) -> None:
        # 如果未找到匹配的规则，则引发异常
        (
            merge_rule,
            pending_checks,
            failed_checks,
            ignorable_checks,
        ) = find_matching_merge_rule(
            self,
            repo,
            skip_mandatory_checks=skip_mandatory_checks,
            skip_internal_checks=can_skip_internal_checks(self, comment_id),
            ignore_current_checks=ignore_current_checks,
        )
        # 合并额外的已合并 PR
        additional_merged_prs = self.merge_changes(
            repo, skip_mandatory_checks, comment_id
        )

        # 将更改推送到默认分支
        repo.push(self.default_branch(), dry_run)
        if not dry_run:
            # 添加“合并完成”标签
            self.add_numbered_label(MERGE_COMPLETE_LABEL, dry_run)
            for pr in additional_merged_prs:
                pr.add_numbered_label(MERGE_COMPLETE_LABEL, dry_run)

        if comment_id and self.pr_num:
            # 当合并过程到达此部分时，可以假定提交已成功推送到主干
            merge_commit_sha = repo.rev_parse(name=REMOTE_MAIN_BRANCH)

            # 最后，将记录上传到 Rockset。合并时的待处理和失败检查列表
            save_merge_record(
                comment_id=comment_id,
                pr_num=self.pr_num,
                owner=self.org,
                project=self.project,
                author=self.get_author(),
                pending_checks=pending_checks,
                failed_checks=failed_checks,
                ignore_current_checks=ignorable_checks.get("IGNORE_CURRENT_CHECK", []),
                broken_trunk_checks=ignorable_checks.get("BROKEN_TRUNK", []),
                flaky_checks=ignorable_checks.get("FLAKY", []),
                unstable_checks=ignorable_checks.get("UNSTABLE", []),
                last_commit_sha=self.last_commit().get("oid", ""),
                merge_base_sha=self.get_merge_base(),
                merge_commit_sha=merge_commit_sha,
                is_failed=False,
                skip_mandatory_checks=skip_mandatory_checks,
                ignore_current=bool(ignore_current_checks),
            )
        else:
            # 打印错误消息，因为缺少评论 ID 或 PR 号码，无法上传到 Rockset
            print("Missing comment ID or PR number, couldn't upload to Rockset")

    def merge_changes(
        self,
        repo: GitRepo,
        skip_mandatory_checks: bool = False,
        comment_id: Optional[int] = None,
        branch: Optional[str] = None,
        skip_all_rule_checks: bool = False,
    ) -> List["GitHubPR"]:
        """
        :param skip_all_rule_checks: If true, skips all rule checks, useful for dry-running merge locally
        """
        # 确定要合并的目标分支，默认为仓库的默认分支
        branch_to_merge_into = self.default_branch() if branch is None else branch
        # 如果当前分支不是目标合并分支，则切换到目标分支
        if repo.current_branch() != branch_to_merge_into:
            repo.checkout(branch_to_merge_into)
        # 如果不是使用 ghstack 工具创建的 Pull Request，则进行普通的合并流程
        if not self.is_ghstack_pr():
            # 生成提交信息
            msg = self.gen_commit_message()
            # 创建用于合并的临时分支名
            pr_branch_name = f"__pull-request-{self.pr_num}__init__"
            # 从远程仓库拉取 Pull Request 的提交到本地临时分支
            repo.fetch(f"pull/{self.pr_num}/head", pr_branch_name)
            # 使用 squash 方式合并临时分支的更改到目标分支
            repo._run_git("merge", "--squash", pr_branch_name)
            # 提交合并后的更改
            repo._run_git("commit", f'--author="{self.get_author()}"', "-m", msg)
            # 返回空列表，表示没有额外的 ghstack PR 需要处理
            return []
        else:
            # 如果是使用 ghstack 工具创建的 PR，则调用特定的合并方法
            return self.merge_ghstack_into(
                repo,
                skip_mandatory_checks,
                comment_id=comment_id,
                skip_all_rule_checks=skip_all_rule_checks,
            )
@dataclass
class MergeRule:
    name: str
    patterns: List[str]
    approved_by: List[str]
    mandatory_checks_name: Optional[List[str]]
    ignore_flaky_failures: bool = True



def gen_new_issue_link(
    org: str, project: str, labels: List[str], template: str = "bug-report.yml"
) -> str:
    # 将标签列表转换为逗号分隔的字符串，生成新问题链接并返回
    labels_str = ",".join(labels)
    return (
        f"https://github.com/{org}/{project}/issues/new?"
        f"labels={urllib.parse.quote(labels_str)}&"
        f"template={urllib.parse.quote(template)}"
    )



def read_merge_rules(
    repo: Optional[GitRepo], org: str, project: str
) -> List[MergeRule]:
    """Returns the list of all merge rules for the repo or project.

    NB: this function is used in Meta-internal workflows, see the comment
    at the top of this file for details.
    """
    # 获取合并规则路径
    repo_relative_rules_path = MERGE_RULE_PATH
    if repo is None:
        # 从 GitHub API 获取 JSON 数据，并解析为 YAML 格式的合并规则
        json_data = gh_fetch_url(
            f"https://api.github.com/repos/{org}/{project}/contents/{repo_relative_rules_path}",
            headers={"Accept": "application/vnd.github.v3+json"},
            reader=json.load,
        )
        content = base64.b64decode(json_data["content"])
        return [MergeRule(**x) for x in yaml.safe_load(content)]
    else:
        # 从本地文件读取 YAML 格式的合并规则
        rules_path = Path(repo.repo_dir) / repo_relative_rules_path
        if not rules_path.exists():
            print(f"{rules_path} does not exist, returning empty rules")
            return []
        with open(rules_path) as fp:
            rc = yaml.safe_load(fp)
        return [MergeRule(**x) for x in rc]



def find_matching_merge_rule(
    pr: GitHubPR,
    repo: Optional[GitRepo] = None,
    skip_mandatory_checks: bool = False,
    skip_internal_checks: bool = False,
    ignore_current_checks: Optional[List[str]] = None,
) -> Tuple[
    MergeRule,
    List[Tuple[str, Optional[str], Optional[int]]],
    List[Tuple[str, Optional[str], Optional[int]]],
    Dict[str, List[Any]],
]:
    """
    Returns merge rule matching to this pr together with the list of associated pending
    and failing jobs OR raises an exception.

    NB: this function is used in Meta-internal workflows, see the comment at the top of
    this file for details.
    """
    # 获取 PR 的修改文件列表和审批者列表
    changed_files = pr.get_changed_files()
    approved_by = set(pr.get_approved_by())

    # 生成新问题链接，用于报告找不到匹配规则的情况
    issue_link = gen_new_issue_link(
        org=pr.org,
        project=pr.project,
        labels=["module: ci"],
    )
    reject_reason = f"No rule found to match PR. Please [report]{issue_link} this issue to DevX team."

    # 读取合并规则
    rules = read_merge_rules(repo, pr.org, pr.project)
    # 如果没有定义规则，则拒绝合并，并给出拒绝理由
    if not rules:
        reject_reason = f"Rejecting the merge as no rules are defined for the repository in {MERGE_RULE_PATH}"
        raise RuntimeError(reject_reason)

    # 获取 pull request 的检查运行结果
    checks = pr.get_checkrun_conclusions()

    # 获取分类结果，用于判断 PR 是否满足合并规则
    checks = get_classifications(
        pr.pr_num,
        pr.project,
        checks,
        ignore_current_checks=ignore_current_checks,
    )

    # 这里存储所有可能批准变更的审批者列表
    all_rule_approvers = {}

    # PR 可能会因为多个合并规则失败，但只需要通过一个规则即可批准。
    # 如果所有规则都失败，需要找出接近通过的规则，并向开发者报告。
    #
    # reject_reason_score 用于评分规则的相关性。分数越高，规则和拒绝理由越相关，
    # 我们只关心最相关的规则和理由。
    #
    # reject_reason_score 评分解释：
    # 分数 0 到 10K - 匹配了多少个文件
    # 分数 10K - 匹配了所有文件，但没有重叠的审批者
    # 分数 20K - 匹配了所有文件和审批者，但有必须通过的检查尚未完成
    # 分数 30K - 匹配了所有文件和审批者，但必须通过的检查失败了
    reject_reason_score = 0

    # 如果 reject_reason_score 等于 20000，则抛出 MandatoryChecksMissingError 异常
    if reject_reason_score == 20000:
        raise MandatoryChecksMissingError(reject_reason, rule)

    # 否则，抛出 MergeRuleFailedError 异常
    raise MergeRuleFailedError(reject_reason, rule)
def checks_to_str(checks: List[Tuple[str, Optional[str]]]) -> str:
    """
    Convert a list of tuples representing checks into a comma-separated string.
    
    Args:
        checks: List of tuples where each tuple contains a name and an optional URL.
        
    Returns:
        str: A comma-separated string of check names, optionally linked to URLs.
    """
    return ", ".join(f"[{c[0]}]({c[1]})" if c[1] is not None else c[0] for c in checks)


def checks_to_markdown_bullets(
    checks: List[Tuple[str, Optional[str], Optional[int]]]
) -> List[str]:
    """
    Convert a list of tuples representing checks into a list of markdown bullet points.

    Args:
        checks: List of tuples where each tuple contains a name, an optional URL, and an optional integer.

    Returns:
        List[str]: A list of markdown bullet points, each formatted with the check name and optional URL.
    """
    return [
        f"- [{c[0]}]({c[1]})" if c[1] is not None else f"- {c[0]}" for c in checks[:5]
    ]


@retries_decorator()
def save_merge_record(
    comment_id: int,
    pr_num: int,
    owner: str,
    project: str,
    author: str,
    pending_checks: List[Tuple[str, Optional[str], Optional[int]]],
    failed_checks: List[Tuple[str, Optional[str], Optional[int]]],
    ignore_current_checks: List[Tuple[str, Optional[str], Optional[int]]],
    broken_trunk_checks: List[Tuple[str, Optional[str], Optional[int]]],
    flaky_checks: List[Tuple[str, Optional[str], Optional[int]]],
    unstable_checks: List[Tuple[str, Optional[str], Optional[int]]],
    last_commit_sha: str,
    merge_base_sha: str,
    merge_commit_sha: str = "",
    is_failed: bool = False,
    skip_mandatory_checks: bool = False,
    ignore_current: bool = False,
    error: str = "",
) -> None:
    """
    Save merge records as JSON data in a file, which can be uploaded to S3 later.

    Args:
        comment_id: ID of the comment associated with the merge.
        pr_num: PR number associated with the merge.
        owner: Owner of the repository.
        project: Project name.
        author: Author of the merge.
        pending_checks: List of pending check tuples.
        failed_checks: List of failed check tuples.
        ignore_current_checks: List of ignored current check tuples.
        broken_trunk_checks: List of broken trunk check tuples.
        flaky_checks: List of flaky check tuples.
        unstable_checks: List of unstable check tuples.
        last_commit_sha: SHA of the last commit in the merge.
        merge_base_sha: SHA of the merge base commit.
        merge_commit_sha: SHA of the merge commit (optional).
        is_failed: Boolean indicating if the merge failed (default False).
        skip_mandatory_checks: Boolean indicating if mandatory checks were skipped (default False).
        ignore_current: Boolean indicating if current checks were ignored (default False).
        error: Any error message associated with the merge (default empty string).
    """
    # Prepare the record to be written into Rockset
    data = [
        {
            "comment_id": comment_id,
            "pr_num": pr_num,
            "owner": owner,
            "project": project,
            "author": author,
            "pending_checks": pending_checks,
            "failed_checks": failed_checks,
            "ignore_current_checks": ignore_current_checks,
            "broken_trunk_checks": broken_trunk_checks,
            "flaky_checks": flaky_checks,
            "unstable_checks": unstable_checks,
            "last_commit_sha": last_commit_sha,
            "merge_base_sha": merge_base_sha,
            "merge_commit_sha": merge_commit_sha,
            "is_failed": is_failed,
            "skip_mandatory_checks": skip_mandatory_checks,
            "ignore_current": ignore_current,
            "error": error,
            # This is a unique identifier for the record for deduping purposes
            # in rockset.  Any unique string would work
            "_id": f"{project}-{pr_num}-{comment_id}-{os.environ.get('GITHUB_RUN_ID')}",
        }
    ]
    repo_root = Path(__file__).resolve().parent.parent.parent

    with open(repo_root / "merge_record.json", "w") as f:
        json.dump(data, f)


@retries_decorator(rc=[])
def get_rockset_results(head_sha: str, merge_base: str) -> List[Dict[str, Any]]:
    """
    Execute a query to retrieve results from Rockset based on given commit SHAs.

    Args:
        head_sha: SHA of the head commit.
        merge_base: SHA of the merge base commit.

    Returns:
        List[Dict[str, Any]]: List of dictionaries containing results from the query.
    """
    query = f"""
SELECT
    w.name as workflow_name,
    j.id,
    j.name,
    j.conclusion,
    j.completed_at,
    j.html_url,
    j.head_sha,
    j.torchci_classification.captures as failure_captures,
    LENGTH(j.steps) as steps,
FROM
    commons.workflow_job j join commons.workflow_run w on w.id = j.run_id
where
    j.head_sha in ('{head_sha}','{merge_base}')
"""
    # 尝试导入 rockset 模块，类型标注为忽略导入错误
    try:
        # 导入 rockset 模块，此处类型标注忽略导入
        import rockset  # type: ignore[import]

        # 使用 RocksetClient 连接到指定的 Rockset API 主机和 API 密钥，执行 SQL 查询
        res = rockset.RocksetClient(
            host="api.usw2a1.rockset.com", api_key=os.environ["ROCKSET_API_KEY"]
        ).sql(query)
        # 返回结果列表，结果类型强制转换为 List[Dict[str, Any]]
        return cast(List[Dict[str, Any]], res.results)
    # 如果 rockset 模块未找到，则捕获 ModuleNotFoundError 异常
    except ModuleNotFoundError:
        # 打印错误消息，指示未能使用 RockSet，因为依赖的 rocket 模块丢失
        print("Could not use RockSet as rocket dependency is missing")
        # 返回空列表作为默认结果
        return []
# 用装饰器 retries_decorator 包装的函数，用于获取与给定 PR 编号相关的失败信息
@retries_decorator()
def get_drci_classifications(pr_num: int, project: str = "pytorch") -> Any:
    """
    Query HUD API to find similar failures to decide if they are flaky
    """
    # 构建 HUD API 的 URL，查询与指定 PR 编号相关的失败信息
    failures = gh_fetch_url(
        f"https://hud.pytorch.org/api/drci/drci?prNumber={pr_num}",
        data=f"repo={project}",
        headers={
            "Authorization": os.getenv("DRCI_BOT_KEY", ""),
            "Accept": "application/vnd.github.v3+json",
        },
        method="POST",
        reader=json.load,
    )

    # 如果查询结果非空，则返回与 PR 编号对应的失败信息；否则返回空字典
    return failures.get(str(pr_num), {}) if failures else {}


# 用于移除作业名称后缀的正则表达式对象
REMOVE_JOB_NAME_SUFFIX_REGEX = re.compile(r", [0-9]+, [0-9]+, .+\)$")


# 函数用于移除作业名称中的特定后缀
def remove_job_name_suffix(name: str, replacement: str = ")") -> str:
    return re.sub(REMOVE_JOB_NAME_SUFFIX_REGEX, replacement, name)


# 判断指定作业是否属于 broken trunk 类型
def is_broken_trunk(
    check: JobCheckState,
    drci_classifications: Any,
) -> bool:
    if not check or not drci_classifications:
        return False

    name = check.name
    job_id = check.job_id

    # 查询 Dr.CI 中的 broken trunk 失败列表，判断指定作业是否属于其中之一
    return any(
        (name == broken_trunk["name"]) or (job_id and job_id == broken_trunk["id"])
        for broken_trunk in drci_classifications.get("BROKEN_TRUNK", [])
    )


# 判断指定作业是否属于 unstable 类型
def is_unstable(
    check: JobCheckState,
    drci_classifications: Any,
) -> bool:
    if not check or not drci_classifications:
        return False

    name = check.name
    job_id = check.job_id

    # 如果作业名称中包含 "unstable" 关键字，则判定为 unstable 类型
    if "unstable" in name:
        return True

    # 查询 Dr.CI 中的 unstable 失败列表，判断指定作业是否属于其中之一
    return any(
        (name == unstable["name"] or (job_id and job_id == unstable["id"]))
        for unstable in drci_classifications.get("UNSTABLE", [])
    )


# 判断指定作业是否属于 flaky 类型
def is_flaky(
    check: JobCheckState,
    drci_classifications: Any,
) -> bool:
    if not check or not drci_classifications:
        return False

    name = check.name
    job_id = check.job_id

    # 查询 Dr.CI 中的 flaky 失败列表，判断指定作业是否属于其中之一
    return any(
        (name == flaky["name"] or (job_id and job_id == flaky["id"]))
        for flaky in drci_classifications.get("FLAKY", [])
    )


# 判断指定作业是否属于 invalid cancel 类型
def is_invalid_cancel(
    name: str,
    conclusion: Optional[str],
    drci_classifications: Any,
) -> bool:
    """
    After https://github.com/pytorch/test-infra/pull/4579, invalid cancelled
    signals have been removed from HUD and Dr.CI. The same needs to be done
    here for consistency
    """
    if (
        not name
        or not drci_classifications
        or not conclusion
        or conclusion.upper() != "CANCELLED"
    ):
        return False

    # 如果作业被标记为 "CANCELLED"，但在 Dr.CI 中未列为失败，则判定为无效取消
    return True
    # 返回一个布尔值，检查是否所有的失败名称都不等于给定的名称
    return all(
        name != failure["name"] for failure in drci_classifications.get("FAILED", [])
    )
# 获取给定 Pull Request 的分类信息，从 Dr.CI 获取最新的结果作为来源。
# 如果可能，首选直接调用 Dr.CI API 来获取最新结果，并更新 Dr.CI 的 PR 评论。
def get_classifications(
    pr_num: int,
    project: str,
    checks: Dict[str, JobCheckState],
    ignore_current_checks: Optional[List[str]],
) -> Dict[str, JobCheckState]:
    # 从 Dr.CI 获取分类结果，作为后续操作的真实来源。
    drci_classifications = get_drci_classifications(pr_num=pr_num, project=project)

    # 定义一个内部函数，用于生成可读的 Dr.CI 结果描述
    def get_readable_drci_results(drci_classifications: Any) -> str:
        try:
            s = f"From Dr.CI API ({pr_num}):\n"
            # 遍历分类和对应的作业，生成详细描述
            for classification, jobs in drci_classifications.items():
                s += f"  {classification}: \n"
                for job in jobs:
                    s += f"    {job['id']} {job['name']}\n"
            return s
        except Exception:
            # 如果发生异常，将返回 Dr.CI 结果的 JSON 格式描述
            return f"From Dr.CI API: {json.dumps(drci_classifications)}"

    # 打印可读的 Dr.CI 结果描述
    print(get_readable_drci_results(drci_classifications))

    # 注意：如果无法从 Dr.CI 获取最新结果（例如在 SandCastle 环境中），则回退到 Dr.CI 检查运行摘要中的任何可用结果。
    if (
        not drci_classifications
        and DRCI_CHECKRUN_NAME in checks
        and checks[DRCI_CHECKRUN_NAME]
        and checks[DRCI_CHECKRUN_NAME].summary
    ):
        # 获取 Dr.CI 检查运行的摘要信息
        drci_summary = checks[DRCI_CHECKRUN_NAME].summary
        try:
            # 尝试解析摘要中的 JSON 数据，并将其赋给 drci_classifications
            print(f"From Dr.CI checkrun summary: {drci_summary}")
            drci_classifications = json.loads(str(drci_summary))
        except json.JSONDecodeError as error:
            # 如果解析失败，记录警告信息，并将 drci_classifications 置为空字典
            warn("Invalid Dr.CI checkrun summary")
            drci_classifications = {}

    # 将 checks 的副本复制到 checks_with_classifications 中
    checks_with_classifications = checks.copy()
    # 遍历检查项及其状态的字典
    for name, check in checks.items():
        # 如果检查状态为"SUCCESS"或"NEUTRAL"，则跳过当前循环，继续下一个检查项
        if check.status == "SUCCESS" or check.status == "NEUTRAL":
            continue
        
        # 如果检查被标记为不稳定，根据分类信息将其添加到带分类的检查项字典中
        if is_unstable(check, drci_classifications):
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                "UNSTABLE",
                check.job_id,
                check.title,
                check.summary,
            )
            continue
        
        # 如果检查被标记为"broken trunk"，根据分类信息将其添加到带分类的检查项字典中
        # 注意：对于 ghstack 和 broken trunk 分类，Dr.CI 使用整个堆栈的基础
        if is_broken_trunk(check, drci_classifications):
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                "BROKEN_TRUNK",
                check.job_id,
                check.title,
                check.summary,
            )
            continue
        
        # 如果检查被标记为"flaky"，根据分类信息将其添加到带分类的检查项字典中
        elif is_flaky(check, drci_classifications):
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                "FLAKY",
                check.job_id,
                check.title,
                check.summary,
            )
            continue
        
        # 如果检查被标记为"invalid cancelled"，创建一个新的分类，并将其添加到带分类的检查项字典中
        elif is_invalid_cancel(name, check.status, drci_classifications):
            # 注意：在这里为无效取消信号创建一个新的分类，因为它们通常在发生时很多。因此，
            # 它们不应计入可忽略失败的阈值
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                "INVALID_CANCEL",
                check.job_id,
                check.title,
                check.summary,
            )
            continue
        
        # 如果忽略当前检查项并且该检查项名称存在于忽略列表中，则将其添加到带分类的检查项字典中
        if ignore_current_checks is not None and name in ignore_current_checks:
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                "IGNORE_CURRENT_CHECK",
                check.job_id,
                check.title,
                check.summary,
            )

    # 返回带分类的检查项字典
    return checks_with_classifications
def filter_checks_with_lambda(
    checks: JobNameToStateDict, status_filter: Callable[[Optional[str]], bool]
) -> List[JobCheckState]:
    # 使用 lambda 函数过滤 checks 中满足 status_filter 条件的 JobCheckState 对象
    return [check for check in checks.values() if status_filter(check.status)]


def get_pr_commit_sha(repo: GitRepo, pr: GitHubPR) -> str:
    # 获取 Pull Request 的合并提交 SHA 值
    commit_sha = pr.get_merge_commit()
    if commit_sha is not None:
        return commit_sha
    # 如果无法获取合并提交 SHA，则获取解析 Pull Request 相关的提交列表，并返回第一个提交的 SHA
    commits = repo.commits_resolving_gh_pr(pr.pr_num)
    if len(commits) == 0:
        raise PostCommentError("Can't find any commits resolving PR")
    return commits[0]


def validate_revert(
    repo: GitRepo, pr: GitHubPR, *, comment_id: Optional[int] = None
) -> Tuple[str, str]:
    # 获取最后一条评论或指定 comment_id 的评论，并检查评论的编辑者登录信息
    comment = (
        pr.get_last_comment()
        if comment_id is None
        else pr.get_comment_by_id(comment_id)
    )
    if comment.editor_login is not None:
        raise PostCommentError("Don't want to revert based on edited command")
    # 检查评论作者的关联权限和登录名，并定义允许进行 revert 操作的角色列表
    author_association = comment.author_association
    author_login = comment.author_login
    allowed_reverters = ["COLLABORATOR", "MEMBER", "OWNER"]
    # 如果基础仓库是私有的，则允许 CONTRIBUTOR 角色进行 revert 操作
    if pr.is_base_repo_private():
        allowed_reverters.append("CONTRIBUTOR")
    # 如果评论作者的关联权限不在允许列表中，则抛出异常
    if author_association not in allowed_reverters:
        raise PostCommentError(
            f"Will not revert as @{author_login} is not one of "
            f"[{', '.join(allowed_reverters)}], but instead is {author_association}."
        )

    # 调用函数查找匹配的合并规则，跳过所有的强制性检查和内部检查
    find_matching_merge_rule(
        pr, repo, skip_mandatory_checks=True, skip_internal_checks=True
    )
    # 获取 Pull Request 的合并提交 SHA 值
    commit_sha = get_pr_commit_sha(repo, pr)
    return (author_login, commit_sha)


def get_ghstack_dependent_prs(
    repo: GitRepo, pr: GitHubPR, only_closed: bool = True
) -> List[Tuple[str, GitHubPR]]:
    """
    获取位于堆栈中、位于当前 PR 之上（包括当前 PR）的其他 PR。
    如果堆栈分支或原始分支已经不存在，则抛出错误。
    """
    assert pr.is_ghstack_pr()
    # 获取原始参考的远程引用，并通过 revision 列表获取相关提交
    orig_ref = f"{repo.remote}/{pr.get_ghstack_orig_ref()}"
    rev_list = repo.revlist(f"{pr.default_branch()}..{orig_ref}")
    if len(rev_list) == 0:
        raise RuntimeError(
            f"PR {pr.pr_num} does not have any revisions associated with it"
        )
    skip_len = len(rev_list) - 1
    # 遍历包含原始引用的分支列表，验证每个候选分支是否是 rev_list 的子集
    for branch in repo.branches_containing_ref(orig_ref):
        candidate = repo.revlist(f"{pr.default_branch()}..{branch}")
        # 选择最长的候选列表
        if len(candidate) > len(rev_list):
            candidate, rev_list = rev_list, candidate
        # 验证候选列表是否始终是 rev_list 的子集
        if rev_list[-len(candidate) :] != candidate:
            raise RuntimeError(
                f"Branch {branch} revlist {', '.join(candidate)} is not a subset of {', '.join(rev_list)}"
            )
    # 如果存在跳过的提交数，则从 rev_list 中移除依赖的提交
    if skip_len > 0:
        rev_list = rev_list[:-skip_len]
    # 创建一个空列表 rc，用于存储元组 (提交SHA, GitHubPR) 的结果
    rc: List[Tuple[str, GitHubPR]] = []
    
    # 遍历通过 _revlist_to_prs 函数获取的每个元组 (pr_, sha)
    for pr_, sha in _revlist_to_prs(repo, pr, rev_list):
        # 检查 pull request 是否未关闭
        if not pr_.is_closed():
            # 如果未关闭且不仅需要已关闭的 PR，将空字符串和 pr_ 添加到 rc 列表中
            if not only_closed:
                rc.append(("", pr_))
            # 跳过当前 PR 继续下一个
            continue
        
        # 获取 PR 的提交 SHA
        commit_sha = get_pr_commit_sha(repo, pr_)
        
        # 将 (commit_sha, pr_) 元组添加到 rc 列表中
        rc.append((commit_sha, pr_))
    
    # 返回包含所有 (提交SHA, GitHubPR) 元组的列表 rc
    return rc
# 函数定义：执行 PR 的回滚操作
def do_revert_prs(
    repo: GitRepo,
    shas_and_prs: List[Tuple[str, GitHubPR]],
    *,
    author_login: str,
    extra_msg: str = "",
    skip_internal_checks: bool = False,
    dry_run: bool = False,
) -> None:
    # 准备并推送回滚提交
    commit_shas: List[str] = []
    for commit_sha, pr in shas_and_prs:
        # 构建回滚提交的消息
        revert_msg = f"\nReverted {pr.get_pr_url()} on behalf of {prefix_with_github_url(author_login)}"
        revert_msg += extra_msg
        # 切换到 PR 的默认分支
        repo.checkout(pr.default_branch())
        # 执行回滚操作
        repo.revert(commit_sha)
        # 获取当前提交的提交消息
        msg = repo.commit_message("HEAD")
        # 移除已解决的拉取请求信息
        msg = re.sub(RE_PULL_REQUEST_RESOLVED, "", msg)
        # 将回滚消息添加到提交消息中
        msg += revert_msg
        # 修改提交消息
        repo.amend_commit_message(msg)
    # 推送回滚后的提交到远程仓库
    repo.push(shas_and_prs[0][1].default_branch(), dry_run)

    # 评论/重新打开 PR
    for commit_sha, pr in shas_and_prs:
        # 构建回滚消息
        revert_message = (
            f"@{pr.get_pr_creator_login()} your PR has been successfully reverted."
        )
        # 如果 PR 包含内部变更且未连接的差异，并且未跳过内部检查，则添加警告信息
        if (
            pr.has_internal_changes()
            and not pr.has_no_connected_diff()
            and not skip_internal_checks
        ):
            revert_message += "\n:warning: This PR might contain internal changes"
            revert_message += "\ncc: @pytorch/pytorch-dev-infra"
        # 发布 PR 评论
        gh_post_pr_comment(
            pr.org, pr.project, pr.pr_num, revert_message, dry_run=dry_run
        )

        # 添加已回滚的标签到 PR
        pr.add_numbered_label("reverted", dry_run)
        # 如果非 dry_run 模式，则发布提交评论
        if not dry_run:
            gh_post_commit_comment(pr.org, pr.project, commit_sha, revert_msg)
            # 更新 PR 状态
            gh_update_pr_state(pr.org, pr.project, pr.pr_num)


# 函数定义：尝试执行 PR 的回滚操作
def try_revert(
    repo: GitRepo,
    pr: GitHubPR,
    *,
    dry_run: bool = False,
    comment_id: Optional[int] = None,
    reason: Optional[str] = None,
) -> None:
    try:
        # 验证并获取回滚操作的作者和提交 SHA
        author_login, commit_sha = validate_revert(repo, pr, comment_id=comment_id)
    except PostCommentError as e:
        # 如果发生评论发布错误，则发布错误信息到 PR 上
        gh_post_pr_comment(pr.org, pr.project, pr.pr_num, str(e), dry_run=dry_run)
        return

    # 构建额外的消息，包含原因和相关评论链接（如果存在）
    extra_msg = f" due to {reason}" if reason is not None else ""
    extra_msg += (
        f" ([comment]({pr.get_comment_by_id(comment_id).url}))\n"
        if comment_id is not None
        else "\n"
    )
    # 获取需要回滚的 PR 列表
    shas_and_prs = [(commit_sha, pr)]
    if pr.is_ghstack_pr():
        try:
            # 如果是 GHStack PR，则获取其依赖的 PR 列表
            shas_and_prs = get_ghstack_dependent_prs(repo, pr)
            prs_to_revert = " ".join([t[1].get_pr_url() for t in shas_and_prs])
            print(f"About to stack of PRs: {prs_to_revert}")
        except Exception as e:
            # 如果获取依赖 PR 列表失败，则输出错误信息并继续单一回滚操作
            print(
                f"Failed to fetch dependent PRs: {str(e)}, fall over to single revert"
            )

    # 执行 PR 回滚操作
    do_revert_prs(
        repo,
        shas_and_prs,
        author_login=author_login,
        extra_msg=extra_msg,
        dry_run=dry_run,
        skip_internal_checks=can_skip_internal_checks(pr, comment_id),
    )


# 函数定义：为给定的字符串添加 GitHub URL 前缀
def prefix_with_github_url(suffix_str: str) -> str:
    return f"https://github.com/{suffix_str}"
# 检查是否需要跳过强制检查，如果是，则函数提前返回，不执行后续操作
def check_for_sev(org: str, project: str, skip_mandatory_checks: bool) -> None:
    if skip_mandatory_checks:
        return
    
    # 使用 GitHub API 查询指定仓库中带有标签"ci: sev"的开放问题
    response = cast(
        Dict[str, Any],
        gh_fetch_json_list(
            "https://api.github.com/search/issues",
            params={"q": f'repo:{org}/{project} is:open is:issue label:"ci: sev"'},
        ),
    )
    
    # 如果存在带有"MERGE BLOCKING"关键字的问题，则抛出运行时错误
    if response["total_count"] != 0:
        for item in response["items"]:
            if "MERGE BLOCKING" in item["body"]:
                raise RuntimeError(
                    "Not merging any PRs at the moment because there is a "
                    + "merge blocking https://github.com/pytorch/pytorch/labels/ci:%20sev issue open at: \n"
                    + f"{item['html_url']}"
                )
    return


# 检查标签列表中是否存在与给定模式匹配的标签
def has_label(labels: List[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:
    return len(list(filter(pattern.match, labels))) > 0


# 将作业状态分类为待处理和失败的作业列表，忽略所有已知的易错故障和断层主干
def categorize_checks(
    check_runs: JobNameToStateDict,
    required_checks: List[str],
    ok_failed_checks_threshold: Optional[int] = None,
) -> Tuple[
    List[Tuple[str, Optional[str], Optional[int]]],
    List[Tuple[str, Optional[str], Optional[int]]],
    Dict[str, List[Any]],
]:
    """
    Categories all jobs into the list of pending and failing jobs. All known flaky
    failures and broken trunk are ignored by defaults when ok_failed_checks_threshold
    is not set (unlimited)
    """
    # 待处理的作业列表
    pending_checks: List[Tuple[str, Optional[str], Optional[int]]] = []
    # 失败的作业列表
    failed_checks: List[Tuple[str, Optional[str], Optional[int]]] = []

    # 用于存储在保存合并记录到Rockset时所有可忽略失败的分类
    failed_checks_categorization: Dict[str, List[Any]] = defaultdict(list)

    # 如果未设置或空的 required_checks，则认为所有作业名称都相关
    relevant_checknames = [
        name
        for name in check_runs.keys()
        if not required_checks or any(x in name for x in required_checks)
    ]

    # 对于每个 required_checks 中的作业名，如果该作业名不在 check_runs 中，则加入到待处理列表中
    for checkname in required_checks:
        if all(checkname not in x for x in check_runs.keys()):
            pending_checks.append((checkname, None, None))
    # 遍历与关键检查相关的检查名称列表
    for checkname in relevant_checknames:
        # 获取当前检查的状态
        status = check_runs[checkname].status
        # 获取当前检查的 URL
        url = check_runs[checkname].url
        # 获取当前检查的分类
        classification = check_runs[checkname].classification
        # 获取当前检查的作业 ID
        job_id = check_runs[checkname].job_id

        # 如果状态为空且分类不是 "UNSTABLE"
        if status is None and classification != "UNSTABLE":
            # 将当前检查加入待处理检查列表，以及其 URL 和作业 ID
            # 注意：如果作业分类为不稳定，就没有必要等待，因为它将被忽略
            # 这对于不需要等待稀缺资源（如 ROCm，经常处于不稳定模式）很有用
            pending_checks.append((checkname, url, job_id))
        # 如果分类为 "INVALID_CANCEL"，则跳过当前检查
        elif classification == "INVALID_CANCEL":
            continue
        # 如果当前检查状态不是通过状态
        elif not is_passing_status(check_runs[checkname].status):
            # 根据分类将当前检查添加到对应的失败检查列表中
            target = (
                failed_checks_categorization[classification]
                if classification
                in ("IGNORE_CURRENT_CHECK", "BROKEN_TRUNK", "FLAKY", "UNSTABLE")
                else failed_checks
            )
            target.append((checkname, url, job_id))

    # 计算 "BROKEN_TRUNK" 和 "FLAKY" 类型的失败检查的总数
    flaky_or_broken_trunk = (
        failed_checks_categorization["BROKEN_TRUNK"]
        + failed_checks_categorization["FLAKY"]
    )

    # 如果存在 "BROKEN_TRUNK" 或 "FLAKY" 类型的失败检查
    if flaky_or_broken_trunk:
        # 发出警告，提醒用户可能由于不稳定性或受损代码主干导致的失败检查
        warn(
            f"The following {len(flaky_or_broken_trunk)} checks failed but were likely due flakiness or broken trunk: "
            + ", ".join([x[0] for x in flaky_or_broken_trunk])
            + (
                f" but this is greater than the threshold of {ok_failed_checks_threshold} so merge will fail"
                if ok_failed_checks_threshold is not None
                and len(flaky_or_broken_trunk) > ok_failed_checks_threshold
                else ""
            )
        )

    # 如果设置了合并失败检查的阈值，并且 "BROKEN_TRUNK" 或 "FLAKY" 类型的失败检查超过了该阈值
    if (
        ok_failed_checks_threshold is not None
        and len(flaky_or_broken_trunk) > ok_failed_checks_threshold
    ):
        # 将 "BROKEN_TRUNK" 和 "FLAKY" 类型的失败检查添加到总失败检查列表中
        failed_checks = failed_checks + flaky_or_broken_trunk

    # 将待处理检查列表、总失败检查列表和失败检查分类字典返回
    # 以便保存到 Rockset 合并记录中
    return (pending_checks, failed_checks, failed_checks_categorization)
def merge(
    pr: GitHubPR,  # GitHubPR 类型的参数，表示 GitHub 上的 Pull Request 对象
    repo: GitRepo,  # GitRepo 类型的参数，表示 Git 仓库对象
    dry_run: bool = False,  # 布尔类型的可选参数，指示是否运行测试，缺省为 False
    skip_mandatory_checks: bool = False,  # 布尔类型的可选参数，是否跳过强制检查，缺省为 False
    comment_id: Optional[int] = None,  # 可选的整数类型参数，评论 ID，默认为 None
    timeout_minutes: int = 400,  # 整数类型参数，超时时间，缺省为 400 分钟
    stale_pr_days: int = 3,  # 整数类型参数，过期 PR 天数，缺省为 3 天
    ignore_current: bool = False,  # 布尔类型的可选参数，是否忽略当前检查，缺省为 False
) -> None:  # 函数返回 None

    initial_commit_sha = pr.last_commit()["oid"]  # 获取 PR 的最后一个提交的 SHA 值
    pr_link = f"https://github.com/{pr.org}/{pr.project}/pull/{pr.pr_num}"  # PR 的链接地址
    print(f"Attempting merge of {initial_commit_sha} ({pr_link})")  # 打印尝试合并的提示信息

    if MERGE_IN_PROGRESS_LABEL not in pr.get_labels():  # 如果 MERGE_IN_PROGRESS_LABEL 不在 PR 的标签中
        gh_add_labels(pr.org, pr.project, pr.pr_num, [MERGE_IN_PROGRESS_LABEL], dry_run)  # 添加 MERGE_IN_PROGRESS_LABEL 标签

    explainer = TryMergeExplainer(
        skip_mandatory_checks,
        pr.get_labels(),
        pr.pr_num,
        pr.org,
        pr.project,
        ignore_current,
    )  # 创建 TryMergeExplainer 对象，用于解释合并过程的详细信息

    # probably a bad name, but this is a list of current checks that should be
    # ignored and is toggled by the --ignore-current flag
    ignore_current_checks_info = []  # 初始化一个空列表，用于存储应该忽略的当前检查信息

    if pr.is_ghstack_pr():  # 如果是通过 ghstack 提交的 PR
        get_ghstack_prs(repo, pr)  # 获取与该 PR 相关的 ghstack PR，如果不同步则引发错误

    check_for_sev(pr.org, pr.project, skip_mandatory_checks)  # 检查严重性

    if skip_mandatory_checks:  # 如果跳过强制检查
        gh_post_pr_comment(
            pr.org,
            pr.project,
            pr.pr_num,
            explainer.get_merge_message(),  # 发布合并消息的评论
            dry_run=dry_run,
        )
        return pr.merge_into(
            repo,
            dry_run=dry_run,
            skip_mandatory_checks=skip_mandatory_checks,
            comment_id=comment_id,
        )  # 调用 PR 对象的 merge_into 方法，执行合并操作

    # Check for approvals
    find_matching_merge_rule(pr, repo, skip_mandatory_checks=True)  # 查找匹配的合并规则

    if not has_required_labels(pr):  # 如果没有必需的标签
        raise RuntimeError(LABEL_ERR_MSG.lstrip(" #"))  # 抛出运行时错误，错误消息为 LABEL_ERR_MSG

    if ignore_current:  # 如果忽略当前检查
        checks = pr.get_checkrun_conclusions()  # 获取检查运行的结果
        _, failing, _ = categorize_checks(
            checks,
            list(checks.keys()),
            ok_failed_checks_threshold=IGNORABLE_FAILED_CHECKS_THESHOLD,
        )  # 对检查结果进行分类
        ignore_current_checks_info = failing  # 将失败的检查信息存储到 ignore_current_checks_info 中

    gh_post_pr_comment(
        pr.org,
        pr.project,
        pr.pr_num,
        explainer.get_merge_message(ignore_current_checks_info),  # 发布合并消息的评论，包括需要忽略的当前检查信息
        dry_run=dry_run,
    )

    start_time = time.time()  # 记录开始时间
    last_exception = ""  # 初始化最后的异常信息为空字符串
    elapsed_time = 0.0  # 初始化已经过去的时间为 0.0

    ignore_current_checks = [
        x[0] for x in ignore_current_checks_info
    ]  # 从 ignore_current_checks_info 中提取需要忽略的检查，转换为 List[str] 方便处理

    # Finally report timeout back
    msg = f"Merged timed out after {timeout_minutes} minutes. Please contact the pytorch_dev_infra team."
    msg += f"The last exception was: {last_exception}"
    gh_add_labels(pr.org, pr.project, pr.pr_num, ["land-failed"], dry_run)  # 添加 "land-failed" 标签
    raise RuntimeError(msg)  # 抛出运行时错误，报告超时信息
    def handle_exception(e: Exception, title: str = "Merge failed") -> None:
        # 构造异常消息
        exception = f"**Reason**: {e}"

        # 如果异常是 MergeRuleFailedError 类型，获取失败的合并规则名
        failing_rule = None
        if isinstance(e, MergeRuleFailedError):
            failing_rule = e.rule.name if e.rule else None

        # 初始化内部调试信息为空字符串
        internal_debugging = ""
        # 获取环境变量中的 GitHub 运行 URL
        run_url = os.getenv("GH_RUN_URL")
        if run_url is not None:
            # 如果存在运行 URL，构建详细信息块，但默认收起显示以避免大部分开发者不必要的信息
            internal_debugging = "\n".join(
                line
                for line in (
                    "<details><summary>Details for Dev Infra team</summary>",
                    f'Raised by <a href="{run_url}">workflow job</a>\n',
                    f"Failing merge rule: {failing_rule}" if failing_rule else "",
                    "</details>",
                )
                if line
            )  # 在连接时忽略空行

        # 构建要发布的消息
        msg = "\n".join((f"## {title}", f"{exception}", "", f"{internal_debugging}"))

        # 调用 GitHub API 发布 PR 评论
        gh_post_pr_comment(org, project, args.pr_num, msg, dry_run=args.dry_run)
        import traceback

        # 打印异常的堆栈信息
        traceback.print_exc()

    # 如果需要回滚 PR
    if args.revert:
        try:
            # 发布回滚消息
            gh_post_pr_comment(
                org,
                project,
                args.pr_num,
                get_revert_message(org, project, pr.pr_num),
                args.dry_run,
            )
            # 尝试回滚 PR
            try_revert(
                repo,
                pr,
                dry_run=args.dry_run,
                comment_id=args.comment_id,
                reason=args.reason,
            )
        except Exception as e:
            # 处理异常
            handle_exception(e, f"Reverting PR {args.pr_num} failed")
        return

    # 如果 PR 已关闭
    if pr.is_closed():
        # 发布无法合并已关闭的 PR 的评论
        gh_post_pr_comment(
            org,
            project,
            args.pr_num,
            f"Can't merge closed PR #{args.pr_num}",
            dry_run=args.dry_run,
        )
        return

    # 如果 PR 是跨仓库 PR 并且是通过 ghstack 创建的
    if pr.is_cross_repo() and pr.is_ghstack_pr():
        # 发布不支持跨仓库 ghstack 合并的评论
        gh_post_pr_comment(
            org,
            project,
            args.pr_num,
            "Cross-repo ghstack merges are not supported",
            dry_run=args.dry_run,
        )
        return
    
    # 如果 PR 不是通过 ghstack 创建并且目标分支不是默认分支
    if not pr.is_ghstack_pr() and pr.base_ref() != pr.default_branch():
        # 发布不支持非默认分支目标的合并请求评论
        gh_post_pr_comment(
            org,
            project,
            args.pr_num,
            f"PR targets {pr.base_ref()} rather than {pr.default_branch()}, refusing merge request",
            dry_run=args.dry_run,
        )
        return

    # 如果需要检查可合并性
    if args.check_mergeability:
        # 如果是 ghstack PR，获取相关的 ghstack PRs 并在不同步时引发错误
        if pr.is_ghstack_pr():
            get_ghstack_prs(repo, pr)
        # 合并 PR 的更改，跳过所有强制检查和规则检查
        pr.merge_changes(
            repo,
            skip_mandatory_checks=True,
            skip_all_rule_checks=True,
        )
        return
    # 如果没有强制选项并且 pull request 包含无效的子模块更新
    if not args.force and pr.has_invalid_submodule_updates():
        # 构建消息，说明这个 PR 更新了哪些子模块
        message = (
            f"This PR updates submodules {', '.join(pr.get_changed_submodules())}\n"
        )
        # 添加额外的说明，提示如果这些更新是有意的，请在 PR 的标题或描述中添加 "submodule" 关键字
        message += '\nIf those updates are intentional, please add "submodule" keyword to PR title/description.'
        # 在 GitHub 上发布评论，提醒用户更新 PR 的标题或描述
        gh_post_pr_comment(org, project, args.pr_num, message, dry_run=args.dry_run)
        # 返回，不再继续执行后续的代码
        return

    try:
        # 尝试合并 pull request
        merge(
            pr,
            repo,
            dry_run=args.dry_run,
            skip_mandatory_checks=args.force,
            comment_id=args.comment_id,
            ignore_current=args.ignore_current,
        )
    except Exception as e:
        # 处理合并过程中的异常
        handle_exception(e)

        # 如果提供了评论 ID 和 PR 编号
        if args.comment_id and args.pr_num:
            # 最后，将记录上传到 Rockset，这里我们无法获取到待处理和失败的检查列表，但目前并不需要它们
            save_merge_record(
                comment_id=args.comment_id,
                pr_num=args.pr_num,
                owner=org,
                project=project,
                author=pr.get_author(),
                pending_checks=[],
                failed_checks=[],
                ignore_current_checks=[],
                broken_trunk_checks=[],
                flaky_checks=[],
                unstable_checks=[],
                last_commit_sha=pr.last_commit().get("oid", ""),
                merge_base_sha=pr.get_merge_base(),
                is_failed=True,
                skip_mandatory_checks=args.force,
                ignore_current=args.ignore_current,
                error=str(e),
            )
        else:
            # 如果缺少评论 ID 或 PR 编号，无法上传到 Rockset
            print("Missing comment ID or PR number, couldn't upload to Rockset")
    finally:
        # 最终，如果不需要检查可合并性，从 GitHub 上移除指定的标签
        if not args.check_mergeability:
            gh_remove_label(
                org, project, args.pr_num, MERGE_IN_PROGRESS_LABEL, args.dry_run
            )
# 如果当前脚本作为主程序执行
if __name__ == "__main__":
    # 调用主函数 main()
    main()
```