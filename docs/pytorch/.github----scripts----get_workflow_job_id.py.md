# `.\pytorch\.github\scripts\get_workflow_job_id.py`

```
# 导入必要的模块和库
import argparse  # 用于解析命令行参数
import json  # 用于处理 JSON 数据
import operator  # 操作符模块，通常不用于此代码段
import os  # 系统操作模块，用于操作文件路径等
import re  # 正则表达式模块，用于字符串匹配和处理
import sys  # 系统相关模块，提供了对 Python 运行时的访问
import time  # 时间模块，用于等待和延时操作
import urllib  # 用于处理 URL 相关操作
import urllib.parse  # URL 解析模块，用于解析 URL 字符串

from typing import Any, Callable, Dict, List, Optional, Tuple  # 类型提示相关的模块和函数
from urllib.request import Request, urlopen  # 用于发送 HTTP 请求的模块


def parse_json_and_links(conn: Any) -> Tuple[Any, Dict[str, Dict[str, str]]]:
    links = {}
    # 提取 GitHub 用于分页的链接，参考：https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Link
    if "Link" in conn.headers:
        # 拆分链接头部信息并解析每个链接
        for elem in re.split(", *<", conn.headers["Link"]):
            try:
                url, params_ = elem.split(";", 1)
            except ValueError:
                continue
            url = urllib.parse.unquote(url.strip("<> "))
            qparams = urllib.parse.parse_qs(params_.strip(), separator=";")
            # 提取并整理链接参数
            params = {
                k: v[0].strip('"')
                for k, v in qparams.items()
                if type(v) is list and len(v) > 0
            }
            params["url"] = url
            if "rel" in params:
                links[params["rel"]] = params

    # 返回从连接中加载的 JSON 数据和解析的链接字典
    return json.load(conn), links


def fetch_url(
    url: str,
    *,
    headers: Optional[Dict[str, str]] = None,
    reader: Callable[[Any], Any] = lambda x: x.read(),
    retries: Optional[int] = 3,
    backoff_timeout: float = 0.5,
) -> Any:
    # 如果未提供头部信息，则初始化为空字典
    if headers is None:
        headers = {}
    try:
        # 使用 urllib 发送 HTTP 请求并获取响应
        with urlopen(Request(url, headers=headers)) as conn:
            return reader(conn)
    except urllib.error.HTTPError as err:
        # 如果是 HTTP 错误，且仍有重试次数，则等待一段时间后重新尝试请求
        if isinstance(retries, (int, float)) and retries > 0:
            time.sleep(backoff_timeout)
            return fetch_url(
                url,
                headers=headers,
                reader=reader,
                retries=retries - 1,
                backoff_timeout=backoff_timeout,
            )
        # 如果重试次数用尽，则抛出运行时错误并包含详细信息
        exception_message = (
            "Is github alright?",
            f"Recieved status code '{err.code}' when attempting to retrieve {url}:\n",
            f"{err.reason}\n\nheaders={err.headers}",
        )
        raise RuntimeError(exception_message) from err


def parse_args() -> Any:
    # 创建解析命令行参数的 ArgumentParser 对象
    parser = argparse.ArgumentParser()
    # 添加必须参数：工作流运行的 ID
    parser.add_argument(
        "workflow_run_id", help="The id of the workflow run, should be GITHUB_RUN_ID"
    )
    # 添加必须参数：执行工作的运行器名称
    parser.add_argument(
        "runner_name",
        help="The name of the runner to retrieve the job id, should be RUNNER_NAME",
    )

    # 解析并返回命令行参数对象
    return parser.parse_args()


def fetch_jobs(url: str, headers: Dict[str, str]) -> List[Dict[str, str]]:
    # 调用 fetch_url 函数获取 URL 响应和链接字典
    response, links = fetch_url(url, headers=headers, reader=parse_json_and_links)
    # 获取响应中的 jobs 列表，确保其为列表类型
    jobs = response["jobs"]
    assert type(jobs) is list  # 确保 jobs 是列表类型
    # 当在链接字典中找到键 "next" 时，执行循环
    while "next" in links.keys():
        # 使用 fetch_url 函数获取指定 URL 的响应和新的链接信息，
        # 并使用 parse_json_and_links 函数解析响应数据和链接信息
        response, links = fetch_url(
            links["next"]["url"], headers=headers, reader=parse_json_and_links
        )
        # 将响应中的 "jobs" 列表扩展到 jobs 列表中
        jobs.extend(response["jobs"])
    
    # 返回包含所有作业的 jobs 列表
    return jobs
# Our strategy is to retrieve the parent workflow run, then filter its jobs on
# RUNNER_NAME to figure out which job we're currently running.
#
# Why RUNNER_NAME? Because it's the only thing that uniquely identifies a job within a workflow.
# GITHUB_JOB doesn't work, as it corresponds to the job yaml id
# (https://bit.ly/37e78oI), which has two problems:
# 1. It's not present in the workflow job JSON object, so we can't use it as a filter.
# 2. It isn't unique; for matrix jobs the job yaml id is the same for all jobs in the matrix.
#
# RUNNER_NAME on the other hand is unique across the pool of runners. Also,
# since only one job can be scheduled on a runner at a time, we know that
# looking for RUNNER_NAME will uniquely identify the job we're currently
# running.

def find_job_id_name(args: Any) -> Tuple[str, str]:
    # From https://docs.github.com/en/actions/learn-github-actions/environment-variables
    PYTORCH_REPO = os.environ.get("GITHUB_REPOSITORY", "pytorch/pytorch")
    # Construct GitHub API URL for the repository
    PYTORCH_GITHUB_API = f"https://api.github.com/repos/{PYTORCH_REPO}"
    # Retrieve GitHub token from environment variable
    GITHUB_TOKEN = os.environ["GITHUB_TOKEN"]
    # Define headers for GitHub API request
    REQUEST_HEADERS = {
        "Accept": "application/vnd.github.v3+json",
        "Authorization": "token " + GITHUB_TOKEN,
    }

    # Construct URL to fetch jobs associated with the specified workflow run ID
    url = f"{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100"
    # Fetch jobs data from GitHub API using the provided URL and headers
    jobs = fetch_jobs(url, REQUEST_HEADERS)

    # Sort the jobs list by start time, in descending order to get the most recent job
    jobs.sort(key=operator.itemgetter("started_at"), reverse=True)

    # Iterate through sorted jobs to find a job with matching RUNNER_NAME
    for job in jobs:
        if job["runner_name"] == args.runner_name:
            return (job["id"], job["name"])

    # Raise an error if no job with the specified RUNNER_NAME is found
    raise RuntimeError(f"Can't find job id for runner {args.runner_name}")


def set_output(name: str, val: Any) -> None:
    # Check if GITHUB_OUTPUT environment variable is set
    if os.getenv("GITHUB_OUTPUT"):
        # Append the output to the specified file
        with open(str(os.getenv("GITHUB_OUTPUT")), "a") as env:
            print(f"{name}={val}", file=env)
        # Print a message indicating the output is being set
        print(f"setting {name}={val}")
    else:
        # Set the GitHub action output using the '::set-output' command
        print(f"::set-output name={name}::{val}")


def main() -> None:
    # Parse command line arguments
    args = parse_args()
    try:
        # Get both the job ID and job name based on the provided arguments
        job_id, job_name = find_job_id_name(args)
        # Set GitHub action outputs for job ID and job name
        set_output("job-id", job_id)
        set_output("job-name", job_name)
    except Exception as e:
        # Print the exception traceback to standard error
        print(repr(e), file=sys.stderr)
        # Print a fallback message indicating the workflow run ID
        print(f"workflow-{args.workflow_run_id}")


if __name__ == "__main__":
    # Execute the main function when the script is run directly
    main()
```