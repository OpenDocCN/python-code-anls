# CogVideo & CogVideoX å¾®è°ƒä»£ç æºç è§£æï¼ˆä¸€ï¼‰



#  Raise valuable PR / æå‡ºæœ‰ä»·å€¼çš„PR

## Caution / æ³¨æ„äº‹é¡¹:
Users should keep the following points in mind when submitting PRs:

1. Ensure that your code meets the requirements in the [specification](../../resources/contribute.md).
2. the proposed PR should be relevant, if there are multiple ideas and optimizations, they should be assigned to different PRs.

ç”¨æˆ·åœ¨æäº¤PRæ—¶å€™åº”è¯¥æ³¨æ„ä»¥ä¸‹å‡ ç‚¹:

1. ç¡®ä¿æ‚¨çš„ä»£ç ç¬¦åˆ [è§„èŒƒ](../../resources/contribute_zh.md) ä¸­çš„è¦æ±‚ã€‚
2. æå‡ºçš„PRåº”è¯¥å…·æœ‰é’ˆå¯¹æ€§ï¼Œå¦‚æœå…·æœ‰å¤šä¸ªä¸åŒçš„æƒ³æ³•å’Œä¼˜åŒ–æ–¹æ¡ˆï¼Œåº”è¯¥åˆ†é…åˆ°ä¸åŒçš„PRä¸­ã€‚

## ä¸åº”è¯¥æå‡ºçš„PR / PRs that should not be proposed

If a developer proposes a PR about any of the following, it may be closed or Rejected.

1. those that don't describe improvement options.
2. multiple issues of different types combined in one PR.
3. The proposed PR is highly duplicative of already existing PRs.

å¦‚æœå¼€å‘è€…æå‡ºå…³äºä»¥ä¸‹æ–¹é¢çš„PRï¼Œåˆ™å¯èƒ½ä¼šè¢«ç›´æ¥å…³é—­æˆ–æ‹’ç»é€šè¿‡ã€‚

1. æ²¡æœ‰è¯´æ˜æ”¹è¿›æ–¹æ¡ˆçš„ã€‚
2. å¤šä¸ªä¸åŒç±»å‹çš„é—®é¢˜åˆå¹¶åœ¨ä¸€ä¸ªPRä¸­çš„ã€‚
3. æå‡ºçš„PRä¸å·²ç»å­˜åœ¨çš„PRé«˜åº¦é‡å¤çš„ã€‚


# æ£€æŸ¥æ‚¨çš„PR
- [ ] Have you read the Contributor Guidelines, Pull Request section? / æ‚¨æ˜¯å¦é˜…è¯»äº†è´¡çŒ®è€…æŒ‡å—ã€Pull Request éƒ¨åˆ†ï¼Ÿ
- [ ] Has this been discussed/approved via a Github issue or forum? If so, add a link. / æ˜¯å¦é€šè¿‡ Github é—®é¢˜æˆ–è®ºå›è®¨è®º/æ‰¹å‡†è¿‡ï¼Ÿå¦‚æœæ˜¯ï¼Œè¯·æ·»åŠ é“¾æ¥ã€‚
- [ ] Did you make sure you updated the documentation with your changes? Here are the Documentation Guidelines, and here are the Documentation Formatting Tips. /æ‚¨æ˜¯å¦ç¡®ä¿æ ¹æ®æ‚¨çš„æ›´æ”¹æ›´æ–°äº†æ–‡æ¡£ï¼Ÿè¿™é‡Œæ˜¯æ–‡æ¡£æŒ‡å—ï¼Œè¿™é‡Œæ˜¯æ–‡æ¡£æ ¼å¼åŒ–æŠ€å·§ã€‚
- [ ] Did you write new required tests? / æ‚¨æ˜¯å¦ç¼–å†™äº†æ–°çš„å¿…è¦æµ‹è¯•ï¼Ÿ
- [ ]  Are your PRs for only one issue / æ‚¨çš„PRæ˜¯å¦ä»…é’ˆå¯¹ä¸€ä¸ªé—®é¢˜

# CogVideoX diffusers Fine-tuning Guide

[ä¸­æ–‡é˜…è¯»](./README_zh.md)

[æ—¥æœ¬èªã§èª­ã‚€](./README_ja.md)

This feature is not fully complete yet. If you want to check the fine-tuning for the SAT version, please
see [here](../sat/README_zh.md). The dataset format is different from this version.

## Hardware Requirements

+ CogVideoX-2B / 5B LoRA: 1 * A100 (5B need to use `--use_8bit_adam`)
+ CogVideoX-2B SFT:  8 * A100 (Working)
+ CogVideoX-5B-I2V is not supported yet.

## Install Dependencies

Since the related code has not been merged into the diffusers release, you need to base your fine-tuning on the
diffusers branch. Please follow the steps below to install dependencies:

```py
git clone https://github.com/huggingface/diffusers.git
cd diffusers # Now in Main branch
pip install -e .
```

## Prepare the Dataset

First, you need to prepare the dataset. The dataset format should be as follows, with `videos.txt` containing the list
of videos in the `videos` directory:

```py
.
â”œâ”€â”€ prompts.txt
â”œâ”€â”€ videos
â””â”€â”€ videos.txt
```

You can download
the [Disney Steamboat Willie](https://huggingface.co/datasets/Wild-Heart/Disney-VideoGeneration-Dataset) dataset from
here.

This video fine-tuning dataset is used as a test for fine-tuning.

## Configuration Files and Execution

The `accelerate` configuration files are as follows:

+ `accelerate_config_machine_multi.yaml`: Suitable for multi-GPU use
+ `accelerate_config_machine_single.yaml`: Suitable for single-GPU use

The configuration for the `finetune` script is as follows:

```py
accelerate launch --config_file accelerate_config_machine_single.yaml --multi_gpu \  # Use accelerate to launch multi-GPU training with the config file accelerate_config_machine_single.yaml
  train_cogvideox_lora.py \  # Training script train_cogvideox_lora.py for LoRA fine-tuning on CogVideoX model
  --gradient_checkpointing \  # Enable gradient checkpointing to reduce memory usage
  --pretrained_model_name_or_path $MODEL_PATH \  # Path to the pretrained model, specified by $MODEL_PATH
  --cache_dir $CACHE_PATH \  # Cache directory for model files, specified by $CACHE_PATH
  --enable_tiling \  # Enable tiling technique to process videos in chunks, saving memory
  --enable_slicing \  # Enable slicing to further optimize memory by slicing inputs
  --instance_data_root $DATASET_PATH \  # Dataset path specified by $DATASET_PATH
  --caption_column prompts.txt \  # Specify the file prompts.txt for video descriptions used in training
  --video_column videos.txt \  # Specify the file videos.txt for video paths used in training
  --validation_prompt "" \  # Prompt used for generating validation videos during training
  --validation_prompt_separator ::: \  # Set ::: as the separator for validation prompts
  --num_validation_videos 1 \  # Generate 1 validation video per validation round
  --validation_epochs 100 \  # Perform validation every 100 training epochs
  --seed 42 \  # Set random seed to 42 for reproducibility
  --rank 128 \  # Set the rank for LoRA parameters to 128
  --lora_alpha 64 \  # Set the alpha parameter for LoRA to 64, adjusting LoRA learning rate
  --mixed_precision bf16 \  # Use bf16 mixed precision for training to save memory
  --output_dir $OUTPUT_PATH \  # Specify the output directory for the model, defined by $OUTPUT_PATH
  --height 480 \  # Set video height to 480 pixels
  --width 720 \  # Set video width to 720 pixels
  --fps 8 \  # Set video frame rate to 8 frames per second
  --max_num_frames 49 \  # Set the maximum number of frames per video to 49
  --skip_frames_start 0 \  # Skip 0 frames at the start of the video
  --skip_frames_end 0 \  # Skip 0 frames at the end of the video
  --train_batch_size 4 \  # Set training batch size to 4
  --num_train_epochs 30 \  # Total number of training epochs set to 30
  --checkpointing_steps 1000 \  # Save model checkpoint every 1000 steps
  --gradient_accumulation_steps 1 \  # Accumulate gradients for 1 step, updating after each batch
  --learning_rate 1e-3 \  # Set learning rate to 0.001
  --lr_scheduler cosine_with_restarts \  # Use cosine learning rate scheduler with restarts
  --lr_warmup_steps 200 \  # Warm up the learning rate for the first 200 steps
  --lr_num_cycles 1 \  # Set the number of learning rate cycles to 1
  --optimizer AdamW \  # Use the AdamW optimizer
  --adam_beta1 0.9 \  # Set Adam optimizer beta1 parameter to 0.9
  --adam_beta2 0.95 \  # Set Adam optimizer beta2 parameter to 0.95
  --max_grad_norm 1.0 \  # Set maximum gradient clipping value to 1.0
  --allow_tf32 \  # Enable TF32 to speed up training
  --report_to wandb  # Use Weights and Biases (wandb) for logging and monitoring the training
```

## Running the Script to Start Fine-tuning

Single Node (One GPU or Multi GPU) fine-tuning:

```py
bash finetune_single_rank.sh
```

Multi-Node fine-tuning:

```py
bash finetune_multi_rank.sh # Needs to be run on each node
```

## Loading the Fine-tuned Model

+ Please refer to [cli_demo.py](../inference/cli_demo.py) for how to load the fine-tuned model.

## Best Practices

+ Includes 70 training videos with a resolution of `200 x 480 x 720` (frames x height x width). By skipping frames in
  the data preprocessing, we created two smaller datasets with 49 and 16 frames to speed up experimentation, as the
  maximum frame limit recommended by the CogVideoX team is 49 frames. We split the 70 videos into three groups of 10,
  25, and 50 videos, with similar conceptual nature.
+ Using 25 or more videos works best when training new concepts and styles.
+ It works better to train using identifier tokens specified with `--id_token`. This is similar to Dreambooth training,
  but regular fine-tuning without such tokens also works.
+ The original repository used `lora_alpha` set to 1. We found this value ineffective across multiple runs, likely due
  to differences in the backend and training setup. Our recommendation is to set `lora_alpha` equal to rank or rank //
    2.
+ We recommend using a rank of 64 or higher.


# CogVideoX diffusers å¾®èª¿æ•´æ–¹æ³•

[Read this in English.](./README_zh)

[ä¸­æ–‡é˜…è¯»](./README_zh.md)


ã“ã®æ©Ÿèƒ½ã¯ã¾ã å®Œå…¨ã«å®Œæˆã—ã¦ã„ã¾ã›ã‚“ã€‚SATãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å¾®èª¿æ•´ã‚’ç¢ºèªã—ãŸã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../sat/README_ja.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚æœ¬ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã¯ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

## ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

+ CogVideoX-2B / 5B T2V LORA: 1 * A100  (5B need to use `--use_8bit_adam`)
+ CogVideoX-2B SFT:  8 * A100 ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
+ CogVideoX-5B-I2V ã¾ã ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“

## ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

é–¢é€£ã‚³ãƒ¼ãƒ‰ã¯ã¾ã diffusersã®ãƒªãƒªãƒ¼ã‚¹ç‰ˆã«çµ±åˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€diffusersãƒ–ãƒ©ãƒ³ãƒã‚’ä½¿ç”¨ã—ã¦å¾®èª¿æ•´ã‚’è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã«å¾“ã£ã¦ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š

```py
git clone https://github.com/huggingface/diffusers.git
cd diffusers # Now in Main branch
pip install -e .
```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

ã¾ãšã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢å¼ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```py
.
â”œâ”€â”€ prompts.txt
â”œâ”€â”€ videos
â””â”€â”€ videos.txt
```

[ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ã‚¹ãƒãƒ¼ãƒ ãƒœãƒ¼ãƒˆã‚¦ã‚£ãƒªãƒ¼](https://huggingface.co/datasets/Wild-Heart/Disney-VideoGeneration-Dataset)ã‚’ã“ã“ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚

ãƒ“ãƒ‡ã‚ªå¾®èª¿æ•´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ãƒ†ã‚¹ãƒˆç”¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨å®Ÿè¡Œ

`accelerate` è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

+ accelerate_config_machine_multi.yaml è¤‡æ•°GPUå‘ã‘
+ accelerate_config_machine_single.yaml å˜ä¸€GPUå‘ã‘

`finetune` ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹ï¼š

```py
accelerate launch --config_file accelerate_config_machine_single.yaml --multi_gpu \  # accelerateã‚’ä½¿ç”¨ã—ã¦multi-GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’èµ·å‹•ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯accelerate_config_machine_single.yaml
  train_cogvideox_lora.py \  # LoRAã®å¾®èª¿æ•´ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆtrain_cogvideox_lora.pyã‚’å®Ÿè¡Œ
  --gradient_checkpointing \  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸›ã‚‰ã™ãŸã‚ã«gradient checkpointingã‚’æœ‰åŠ¹åŒ–
  --pretrained_model_name_or_path $MODEL_PATH \  # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’$MODEL_PATHã§æŒ‡å®š
  --cache_dir $CACHE_PATH \  # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’$CACHE_PATHã§æŒ‡å®š
  --enable_tiling \  # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã«ã‚¿ã‚¤ãƒ«å‡¦ç†ã‚’æœ‰åŠ¹åŒ–ã—ã€å‹•ç”»ã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†ã‘ã—ã¦å‡¦ç†
  --enable_slicing \  # å…¥åŠ›ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦ã•ã‚‰ã«ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
  --instance_data_root $DATASET_PATH \  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã‚’$DATASET_PATHã§æŒ‡å®š
  --caption_column prompts.txt \  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ç”¨ã™ã‚‹å‹•ç”»ã®èª¬æ˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’prompts.txtã§æŒ‡å®š
  --video_column videos.txt \  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ç”¨ã™ã‚‹å‹•ç”»ã®ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’videos.txtã§æŒ‡å®š
  --validation_prompt "" \  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«æ¤œè¨¼ç”¨ã®å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹éš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
  --validation_prompt_separator ::: \  # æ¤œè¨¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’:::ã«è¨­å®š
  --num_validation_videos 1 \  # å„æ¤œè¨¼ãƒ©ã‚¦ãƒ³ãƒ‰ã§1æœ¬ã®å‹•ç”»ã‚’ç”Ÿæˆ
  --validation_epochs 100 \  # 100ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«æ¤œè¨¼ã‚’å®Ÿæ–½
  --seed 42 \  # å†ç¾æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã«ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’42ã«è¨­å®š
  --rank 128 \  # LoRAã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ©ãƒ³ã‚¯ã‚’128ã«è¨­å®š
  --lora_alpha 64 \  # LoRAã®alphaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’64ã«è¨­å®šã—ã€LoRAã®å­¦ç¿’ç‡ã‚’èª¿æ•´
  --mixed_precision bf16 \  # bf16æ··åˆç²¾åº¦ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„
  --output_dir $OUTPUT_PATH \  # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’$OUTPUT_PATHã§æŒ‡å®š
  --height 480 \  # å‹•ç”»ã®é«˜ã•ã‚’480ãƒ”ã‚¯ã‚»ãƒ«ã«è¨­å®š
  --width 720 \  # å‹•ç”»ã®å¹…ã‚’720ãƒ”ã‚¯ã‚»ãƒ«ã«è¨­å®š
  --fps 8 \  # å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã‚’1ç§’ã‚ãŸã‚Š8ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¨­å®š
  --max_num_frames 49 \  # å„å‹•ç”»ã®æœ€å¤§ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’49ã«è¨­å®š
  --skip_frames_start 0 \  # å‹•ç”»ã®æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’0ã‚¹ã‚­ãƒƒãƒ—
  --skip_frames_end 0 \  # å‹•ç”»ã®æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’0ã‚¹ã‚­ãƒƒãƒ—
  --train_batch_size 4 \  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’4ã«è¨­å®š
  --num_train_epochs 30 \  # ç·ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯æ•°ã‚’30ã«è¨­å®š
  --checkpointing_steps 1000 \  # 1000ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
  --gradient_accumulation_steps 1 \  # 1ã‚¹ãƒ†ãƒƒãƒ—ã®å‹¾é…ç´¯ç©ã‚’è¡Œã„ã€å„ãƒãƒƒãƒå¾Œã«æ›´æ–°
  --learning_rate 1e-3 \  # å­¦ç¿’ç‡ã‚’0.001ã«è¨­å®š
  --lr_scheduler cosine_with_restarts \  # ãƒªã‚¹ã‚¿ãƒ¼ãƒˆä»˜ãã®ã‚³ã‚µã‚¤ãƒ³å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨
  --lr_warmup_steps 200 \  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€åˆã®200ã‚¹ãƒ†ãƒƒãƒ—ã§å­¦ç¿’ç‡ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
  --lr_num_cycles 1 \  # å­¦ç¿’ç‡ã®ã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’1ã«è¨­å®š
  --optimizer AdamW \  # AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
  --adam_beta1 0.9 \  # Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®beta1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’0.9ã«è¨­å®š
  --adam_beta2 0.95 \  # Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®beta2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’0.95ã«è¨­å®š
  --max_grad_norm 1.0 \  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®æœ€å¤§å€¤ã‚’1.0ã«è¨­å®š
  --allow_tf32 \  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«TF32ã‚’æœ‰åŠ¹åŒ–
  --report_to wandb  # Weights and Biasesã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¨˜éŒ²ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†
```

## å¾®èª¿æ•´ã‚’é–‹å§‹

å˜ä¸€ãƒã‚·ãƒ³ (ã‚·ãƒ³ã‚°ãƒ«GPUã€ãƒãƒ«ãƒGPU) ã§ã®å¾®èª¿æ•´:

```py
bash finetune_single_rank.sh
```

è¤‡æ•°ãƒã‚·ãƒ³ãƒ»ãƒãƒ«ãƒGPUã§ã®å¾®èª¿æ•´ï¼š

```py
bash finetune_multi_rank.sh # å„ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
```

## å¾®èª¿æ•´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

+ å¾®èª¿æ•´æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[cli_demo.py](../inference/cli_demo.py) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

+ è§£åƒåº¦ãŒ `200 x 480 x 720`ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•° x é«˜ã• x å¹…ï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ“ãƒ‡ã‚ªãŒ70æœ¬å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã§ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§ã€49ãƒ•ãƒ¬ãƒ¼ãƒ ã¨16ãƒ•ãƒ¬ãƒ¼ãƒ ã®å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚ã“ã‚Œã¯å®Ÿé¨“ã‚’åŠ é€Ÿã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã€CogVideoXãƒãƒ¼ãƒ ãŒæ¨å¥¨ã™ã‚‹æœ€å¤§ãƒ•ãƒ¬ãƒ¼ãƒ æ•°åˆ¶é™ã¯49ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã™ã€‚
+ 25æœ¬ä»¥ä¸Šã®ãƒ“ãƒ‡ã‚ªãŒæ–°ã—ã„æ¦‚å¿µã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«æœ€é©ã§ã™ã€‚
+ ç¾åœ¨ã€`--id_token` ã‚’æŒ‡å®šã—ã¦è­˜åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹ãŒåŠ¹æœçš„ã§ã™ã€‚ã“ã‚Œã¯Dreamboothãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä¼¼ã¦ã„ã¾ã™ãŒã€é€šå¸¸ã®å¾®èª¿æ•´ã§ã‚‚æ©Ÿèƒ½ã—ã¾ã™ã€‚
+ å…ƒã®ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ `lora_alpha` ã‚’1ã«è¨­å®šã—ã¦ã„ã¾ã—ãŸãŒã€è¤‡æ•°ã®å®Ÿè¡Œã§ã“ã®å€¤ãŒåŠ¹æœçš„ã§ãªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã«ã‚ˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ç§ãŸã¡ã®ææ¡ˆã¯ã€lora_alphaã‚’rankã¨åŒã˜ã‹ã€rank // 2ã«è¨­å®šã™ã‚‹ã“ã¨ã§ã™ã€‚
+ Rank 64ä»¥ä¸Šã®è¨­å®šã‚’æ¨å¥¨ã—ã¾ã™ã€‚


# CogVideoX diffusers å¾®è°ƒæ–¹æ¡ˆ

[Read this in English](./README_zh.md)

[æ—¥æœ¬èªã§èª­ã‚€](./README_ja.md)

æœ¬åŠŸèƒ½å°šæœªå®Œå…¨å®Œå–„ï¼Œå¦‚æœæ‚¨æƒ³æŸ¥çœ‹SATç‰ˆæœ¬å¾®è°ƒï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](../sat/README_zh.md)ã€‚å…¶æ•°æ®é›†æ ¼å¼ä¸æœ¬ç‰ˆæœ¬ä¸åŒã€‚

## ç¡¬ä»¶è¦æ±‚

+ CogVideoX-2B / 5B T2V LORA: 1 * A100  (5B need to use `--use_8bit_adam`)
+ CogVideoX-2B SFT:  8 * A100 (åˆ¶ä½œä¸­)
+ CogVideoX-5B-I2V æš‚æœªæ”¯æŒ

## å®‰è£…ä¾èµ–

ç”±äºç›¸å…³ä»£ç è¿˜æ²¡æœ‰è¢«åˆå¹¶åˆ°diffuserså‘è¡Œç‰ˆï¼Œä½ éœ€è¦åŸºäºdiffusersåˆ†æ”¯è¿›è¡Œå¾®è°ƒã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…ä¾èµ–ï¼š

```py
git clone https://github.com/huggingface/diffusers.git
cd diffusers # Now in Main branch
pip install -e .
```

## å‡†å¤‡æ•°æ®é›†

é¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡æ•°æ®é›†ï¼Œæ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ï¼Œvideos.txt å­˜æ”¾ videos ä¸­çš„è§†é¢‘ã€‚

```py
.
â”œâ”€â”€ prompts.txt
â”œâ”€â”€ videos
â””â”€â”€ videos.txt
```

ä½ å¯ä»¥ä»è¿™é‡Œä¸‹è½½ [è¿ªå£«å°¼æ±½èˆ¹å¨åˆ©å·](https://huggingface.co/datasets/Wild-Heart/Disney-VideoGeneration-Dataset)

è§†é¢‘å¾®è°ƒæ•°æ®é›†ä½œä¸ºæµ‹è¯•å¾®è°ƒã€‚

## é…ç½®æ–‡ä»¶å’Œè¿è¡Œ

`accelerate` é…ç½®æ–‡ä»¶å¦‚ä¸‹:

+ accelerate_config_machine_multi.yaml é€‚åˆå¤šGPUä½¿ç”¨
+ accelerate_config_machine_single.yaml é€‚åˆå•GPUä½¿ç”¨

`finetune` è„šæœ¬é…ç½®æ–‡ä»¶å¦‚ä¸‹ï¼š

```py

accelerate launch --config_file accelerate_config_machine_single.yaml --multi_gpu \  # ä½¿ç”¨ accelerate å¯åŠ¨å¤šGPUè®­ç»ƒï¼Œé…ç½®æ–‡ä»¶ä¸º accelerate_config_machine_single.yaml
  train_cogvideox_lora.py \  # è¿è¡Œçš„è®­ç»ƒè„šæœ¬ä¸º train_cogvideox_lora.pyï¼Œç”¨äºåœ¨ CogVideoX æ¨¡å‹ä¸Šè¿›è¡Œ LoRA å¾®è°ƒ
  --gradient_checkpointing \  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½ï¼Œä»¥å‡å°‘æ˜¾å­˜ä½¿ç”¨
  --pretrained_model_name_or_path $MODEL_PATH \  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼Œé€šè¿‡ $MODEL_PATH æŒ‡å®š
  --cache_dir $CACHE_PATH \  # æ¨¡å‹ç¼“å­˜è·¯å¾„ï¼Œç”± $CACHE_PATH æŒ‡å®š
  --enable_tiling \  # å¯ç”¨tilingæŠ€æœ¯ï¼Œä»¥åˆ†ç‰‡å¤„ç†è§†é¢‘ï¼ŒèŠ‚çœæ˜¾å­˜
  --enable_slicing \  # å¯ç”¨slicingæŠ€æœ¯ï¼Œå°†è¾“å…¥åˆ‡ç‰‡ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜
  --instance_data_root $DATASET_PATH \  # æ•°æ®é›†è·¯å¾„ï¼Œç”± $DATASET_PATH æŒ‡å®š
  --caption_column prompts.txt \  # æŒ‡å®šç”¨äºè®­ç»ƒçš„è§†é¢‘æè¿°æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸º prompts.txt
  --video_column videos.txt \  # æŒ‡å®šç”¨äºè®­ç»ƒçš„è§†é¢‘è·¯å¾„æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸º videos.txt
  --validation_prompt "" \  # éªŒè¯é›†çš„æç¤ºè¯­ (prompt)ï¼Œç”¨äºåœ¨è®­ç»ƒæœŸé—´ç”ŸæˆéªŒè¯è§†é¢‘
  --validation_prompt_separator ::: \  # è®¾ç½®éªŒè¯æç¤ºè¯­çš„åˆ†éš”ç¬¦ä¸º :::
  --num_validation_videos 1 \  # æ¯ä¸ªéªŒè¯å›åˆç”Ÿæˆ 1 ä¸ªè§†é¢‘
  --validation_epochs 100 \  # æ¯ 100 ä¸ªè®­ç»ƒepochè¿›è¡Œä¸€æ¬¡éªŒè¯
  --seed 42 \  # è®¾ç½®éšæœºç§å­ä¸º 42ï¼Œä»¥ä¿è¯ç»“æœçš„å¯å¤ç°æ€§
  --rank 128 \  # è®¾ç½® LoRA å‚æ•°çš„ç§© (rank) ä¸º 128
  --lora_alpha 64 \  # è®¾ç½® LoRA çš„ alpha å‚æ•°ä¸º 64ï¼Œç”¨äºè°ƒæ•´LoRAçš„å­¦ä¹ ç‡
  --mixed_precision bf16 \  # ä½¿ç”¨ bf16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒï¼Œå‡å°‘æ˜¾å­˜ä½¿ç”¨
  --output_dir $OUTPUT_PATH \  # æŒ‡å®šæ¨¡å‹è¾“å‡ºç›®å½•ï¼Œç”± $OUTPUT_PATH å®šä¹‰
  --height 480 \  # è§†é¢‘é«˜åº¦ä¸º 480 åƒç´ 
  --width 720 \  # è§†é¢‘å®½åº¦ä¸º 720 åƒç´ 
  --fps 8 \  # è§†é¢‘å¸§ç‡è®¾ç½®ä¸º 8 å¸§æ¯ç§’
  --max_num_frames 49 \  # æ¯ä¸ªè§†é¢‘çš„æœ€å¤§å¸§æ•°ä¸º 49 å¸§
  --skip_frames_start 0 \  # è·³è¿‡è§†é¢‘å¼€å¤´çš„å¸§æ•°ä¸º 0
  --skip_frames_end 0 \  # è·³è¿‡è§†é¢‘ç»“å°¾çš„å¸§æ•°ä¸º 0
  --train_batch_size 4 \  # è®­ç»ƒæ—¶çš„ batch size è®¾ç½®ä¸º 4
  --num_train_epochs 30 \  # æ€»è®­ç»ƒepochæ•°ä¸º 30
  --checkpointing_steps 1000 \  # æ¯ 1000 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹
  --gradient_accumulation_steps 1 \  # æ¢¯åº¦ç´¯è®¡æ­¥æ•°ä¸º 1ï¼Œå³æ¯ä¸ª batch åéƒ½ä¼šæ›´æ–°æ¢¯åº¦
  --learning_rate 1e-3 \  # å­¦ä¹ ç‡è®¾ç½®ä¸º 0.001
  --lr_scheduler cosine_with_restarts \  # ä½¿ç”¨å¸¦é‡å¯çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨
  --lr_warmup_steps 200 \  # åœ¨è®­ç»ƒçš„å‰ 200 æ­¥è¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­
  --lr_num_cycles 1 \  # å­¦ä¹ ç‡å‘¨æœŸè®¾ç½®ä¸º 1
  --optimizer AdamW \  # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
  --adam_beta1 0.9 \  # è®¾ç½® Adam ä¼˜åŒ–å™¨çš„ beta1 å‚æ•°ä¸º 0.9
  --adam_beta2 0.95 \  # è®¾ç½® Adam ä¼˜åŒ–å™¨çš„ beta2 å‚æ•°ä¸º 0.95
  --max_grad_norm 1.0 \  # æœ€å¤§æ¢¯åº¦è£å‰ªå€¼è®¾ç½®ä¸º 1.0
  --allow_tf32 \  # å¯ç”¨ TF32 ä»¥åŠ é€Ÿè®­ç»ƒ
  --report_to wandb  # ä½¿ç”¨ Weights and Biases è¿›è¡Œè®­ç»ƒè®°å½•ä¸ç›‘æ§
```

## è¿è¡Œè„šæœ¬ï¼Œå¼€å§‹å¾®è°ƒ

å•æœº(å•å¡ï¼Œå¤šå¡)å¾®è°ƒï¼š

```py
bash finetune_single_rank.sh
```

å¤šæœºå¤šå¡å¾®è°ƒï¼š

```py
bash finetune_multi_rank.sh #éœ€è¦åœ¨æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ
```

## è½½å…¥å¾®è°ƒçš„æ¨¡å‹

+ è¯·å…³æ³¨[cli_demo.py](../inference/cli_demo.py) ä»¥äº†è§£å¦‚ä½•åŠ è½½å¾®è°ƒçš„æ¨¡å‹ã€‚

## æœ€ä½³å®è·µ

+ åŒ…å«70ä¸ªåˆ†è¾¨ç‡ä¸º `200 x 480 x 720`ï¼ˆå¸§æ•° x é«˜ x
  å®½ï¼‰çš„è®­ç»ƒè§†é¢‘ã€‚é€šè¿‡æ•°æ®é¢„å¤„ç†ä¸­çš„å¸§è·³è¿‡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªè¾ƒå°çš„49å¸§å’Œ16å¸§æ•°æ®é›†ï¼Œä»¥åŠ å¿«å®éªŒé€Ÿåº¦ï¼Œå› ä¸ºCogVideoXå›¢é˜Ÿå»ºè®®çš„æœ€å¤§å¸§æ•°é™åˆ¶æ˜¯49å¸§ã€‚æˆ‘ä»¬å°†70ä¸ªè§†é¢‘åˆ†æˆä¸‰ç»„ï¼Œåˆ†åˆ«ä¸º10ã€25å’Œ50ä¸ªè§†é¢‘ã€‚è¿™äº›è§†é¢‘çš„æ¦‚å¿µæ€§è´¨ç›¸ä¼¼ã€‚
+ 25ä¸ªåŠä»¥ä¸Šçš„è§†é¢‘åœ¨è®­ç»ƒæ–°æ¦‚å¿µå’Œé£æ ¼æ—¶æ•ˆæœæœ€ä½³ã€‚
+ ç°ä½¿ç”¨å¯ä»¥é€šè¿‡ `--id_token` æŒ‡å®šçš„æ ‡è¯†ç¬¦tokenè¿›è¡Œè®­ç»ƒæ•ˆæœæ›´å¥½ã€‚è¿™ç±»ä¼¼äº Dreambooth è®­ç»ƒï¼Œä½†ä¸ä½¿ç”¨è¿™ç§tokençš„å¸¸è§„å¾®è°ƒä¹Ÿå¯ä»¥å·¥ä½œã€‚
+ åŸå§‹ä»“åº“ä½¿ç”¨ `lora_alpha` è®¾ç½®ä¸º 1ã€‚æˆ‘ä»¬å‘ç°è¿™ä¸ªå€¼åœ¨å¤šæ¬¡è¿è¡Œä¸­æ•ˆæœä¸ä½³ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹åç«¯å’Œè®­ç»ƒè®¾ç½®çš„ä¸åŒã€‚æˆ‘ä»¬çš„å»ºè®®æ˜¯å°†
  lora_alpha è®¾ç½®ä¸ºä¸ rank ç›¸åŒæˆ– rank // 2ã€‚
+ å»ºè®®ä½¿ç”¨ rank ä¸º 64 åŠä»¥ä¸Šçš„è®¾ç½®ã€‚



# `.\cogvideo-finetune\finetune\train_cogvideox_image_to_video_lora.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œæ ‡æ˜ç‰ˆæƒå½’å±äºCogViewå›¢é˜Ÿã€æ¸…åå¤§å­¦ã€ZhipuAIå’ŒHuggingFaceå›¢é˜Ÿï¼Œæ‰€æœ‰æƒåˆ©ä¿ç•™ã€‚
# 
# æ ¹æ®Apacheè®¸å¯è¯ç¬¬2.0ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›
# é™¤ééµå¾ªè¯¥è®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œè½¯ä»¶åœ¨è®¸å¯è¯ä¸‹ä»¥â€œæŒ‰åŸæ ·â€æ–¹å¼åˆ†å‘ï¼Œ
# ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚
# æœ‰å…³è®¸å¯è¯ä¸‹æƒé™å’Œé™åˆ¶çš„å…·ä½“ä¿¡æ¯ï¼Œè¯·å‚è§è®¸å¯è¯ã€‚

# å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£æåº“
import argparse
# å¯¼å…¥æ—¥å¿—è®°å½•åº“
import logging
# å¯¼å…¥æ•°å­¦åº“
import math
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£åº“
import os
# å¯¼å…¥éšæœºæ•°ç”Ÿæˆåº“
import random
# å¯¼å…¥æ–‡ä»¶å’Œç›®å½•æ“ä½œåº“
import shutil
# å¯¼å…¥æ—¶é—´å¤„ç†åº“
from datetime import timedelta
# å¯¼å…¥è·¯å¾„æ“ä½œåº“
from pathlib import Path
# å¯¼å…¥ç±»å‹æ³¨è§£ç›¸å…³çš„åº“
from typing import List, Optional, Tuple, Union

# å¯¼å…¥PyTorchåº“
import torch
# å¯¼å…¥transformersåº“ï¼Œç”¨äºå¤„ç†é¢„è®­ç»ƒæ¨¡å‹
import transformers
# ä»accelerateåº“å¯¼å…¥åŠ é€Ÿå™¨ç±»
from accelerate import Accelerator
# ä»accelerateåº“å¯¼å…¥æ—¥å¿—è®°å½•å‡½æ•°
from accelerate.logging import get_logger
# ä»accelerateåº“å¯¼å…¥åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç›¸å…³å‚æ•°
from accelerate.utils import DistributedDataParallelKwargs, InitProcessGroupKwargs, ProjectConfiguration, set_seed
# ä»huggingface_hubåº“å¯¼å…¥åˆ›å»ºå’Œä¸Šä¼ æ¨¡å‹åº“çš„å‡½æ•°
from huggingface_hub import create_repo, upload_folder
# ä»peftåº“å¯¼å…¥Loraé…ç½®åŠæ¨¡å‹çŠ¶æ€å­—å…¸ç›¸å…³å‡½æ•°
from peft import LoraConfig, get_peft_model_state_dict, set_peft_model_state_dict
# ä»torch.utils.dataåº“å¯¼å…¥æ•°æ®åŠ è½½å™¨å’Œæ•°æ®é›†ç±»
from torch.utils.data import DataLoader, Dataset
# ä»torchvisionåº“å¯¼å…¥å›¾åƒå˜æ¢å‡½æ•°
from torchvision import transforms
# ä»tqdmåº“å¯¼å…¥è¿›åº¦æ¡æ˜¾ç¤º
from tqdm.auto import tqdm
# ä»transformersåº“å¯¼å…¥è‡ªåŠ¨æ ‡è®°å™¨å’Œæ¨¡å‹
from transformers import AutoTokenizer, T5EncoderModel, T5Tokenizer

# å¯¼å…¥diffusersåº“
import diffusers
# ä»diffusersåº“å¯¼å…¥ä¸åŒæ¨¡å‹å’Œè°ƒåº¦å™¨
from diffusers import (
    AutoencoderKLCogVideoX,
    CogVideoXDPMScheduler,
    CogVideoXImageToVideoPipeline,
    CogVideoXTransformer3DModel,
)
# ä»diffusers.models.embeddingsåº“å¯¼å…¥3Dæ—‹è½¬ä½ç½®åµŒå…¥å‡½æ•°
from diffusers.models.embeddings import get_3d_rotary_pos_embed
# ä»diffusers.optimizationåº“å¯¼å…¥è°ƒåº¦å™¨è·å–å‡½æ•°
from diffusers.optimization import get_scheduler
# ä»diffusers.pipelines.cogvideoåº“å¯¼å…¥å›¾åƒç¼©æ”¾è£å‰ªåŒºåŸŸè·å–å‡½æ•°
from diffusers.pipelines.cogvideo.pipeline_cogvideox import get_resize_crop_region_for_grid
# ä»diffusers.training_utilsåº“å¯¼å…¥è®­ç»ƒå‚æ•°å¤„ç†å’Œå†…å­˜é‡Šæ”¾å‡½æ•°
from diffusers.training_utils import cast_training_params, free_memory
# ä»diffusers.utilsåº“å¯¼å…¥å¤šç§å·¥å…·å‡½æ•°
from diffusers.utils import (
    check_min_version,
    convert_unet_state_dict_to_peft,
    export_to_video,
    is_wandb_available,
    load_image,
)
# ä»diffusers.utils.hub_utilsåº“å¯¼å…¥æ¨¡å‹å¡åŠ è½½å’Œå¡«å……å‡½æ•°
from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card
# ä»diffusers.utils.torch_utilsåº“å¯¼å…¥æ£€æŸ¥æ¨¡å—ç¼–è¯‘çŠ¶æ€çš„å‡½æ•°
from diffusers.utils.torch_utils import is_compiled_module
# ä»torchvision.transforms.functionalåº“å¯¼å…¥ä¸­å¿ƒè£å‰ªå’Œè°ƒæ•´å¤§å°å‡½æ•°
from torchvision.transforms.functional import center_crop, resize
# ä»torchvision.transformsåº“å¯¼å…¥æ’å€¼æ¨¡å¼
from torchvision.transforms import InterpolationMode
# å¯¼å…¥torchvision.transformsåº“
import torchvision.transforms as TT
# å¯¼å…¥NumPyåº“
import numpy as np
# ä»diffusers.image_processoråº“å¯¼å…¥å›¾åƒå¤„ç†å™¨
from diffusers.image_processor import VaeImageProcessor

# å¦‚æœWandBåº“å¯ç”¨ï¼Œåˆ™å¯¼å…¥WandB
if is_wandb_available():
    import wandb

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æœ€å°ç‰ˆæœ¬çš„diffusersåº“ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°†ä¼šæŠ¥é”™ã€‚é£é™©è‡ªè´Ÿã€‚
check_min_version("0.31.0.dev0")

# è·å–æ—¥å¿—è®°å½•å™¨å®ä¾‹ï¼Œä½¿ç”¨å½“å‰æ¨¡å—çš„åç§°
logger = get_logger(__name__)

# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºå‚æ•°è§£æå™¨ï¼Œæè¿°ä¸ºCogVideoXçš„è®­ç»ƒè„šæœ¬ç¤ºä¾‹
    parser = argparse.ArgumentParser(description="Simple example of a training script for CogVideoX.")

    # æ·»åŠ æ¨¡å‹ä¿¡æ¯çš„å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        default=None,
        required=True,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„æˆ–Hugging Faceæ¨¡å‹æ ‡è¯†ç¬¦
        help="Path to pretrained model or model identifier from huggingface.co/models.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹çš„ä¿®è®¢ç‰ˆ
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        required=False,
        # å¸®åŠ©ä¿¡æ¯ï¼šæ¥è‡ª huggingface.co/models çš„é¢„è®­ç»ƒæ¨¡å‹æ ‡è¯†ç¬¦çš„ä¿®è®¢ç‰ˆ
        help="Revision of pretrained model identifier from huggingface.co/models.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹çš„å˜ä½“
    parser.add_argument(
        "--variant",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šé¢„è®­ç»ƒæ¨¡å‹æ ‡è¯†ç¬¦çš„æ¨¡å‹æ–‡ä»¶çš„å˜ä½“ï¼Œä¾‹å¦‚ fp16
        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šç¼“å­˜ç›®å½•
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šä¸‹è½½çš„æ¨¡å‹å’Œæ•°æ®é›†å°†å­˜å‚¨çš„ç›®å½•
        help="The directory where the downloaded models and datasets will be stored.",
    )

    # æ•°æ®é›†ä¿¡æ¯
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ•°æ®é›†åç§°
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šåŒ…å«å®ä¾‹å›¾åƒè®­ç»ƒæ•°æ®çš„æ•°æ®é›†åç§°ï¼Œå¯ä»¥æ˜¯æœ¬åœ°æ•°æ®é›†è·¯å¾„
        help=(
            "The name of the Dataset (from the HuggingFace hub) containing the training data of instance images (could be your own, possibly private,"
            " dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,"
            " or to a folder containing files that ğŸ¤— Datasets can understand."
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ•°æ®é›†é…ç½®åç§°
    parser.add_argument(
        "--dataset_config_name",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šæ•°æ®é›†çš„é…ç½®ï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªé…ç½®åˆ™ä¿ç•™ä¸º None
        help="The config of the Dataset, leave as None if there's only one config.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šå®ä¾‹æ•°æ®æ ¹ç›®å½•
    parser.add_argument(
        "--instance_data_root",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šåŒ…å«è®­ç»ƒæ•°æ®çš„æ–‡ä»¶å¤¹
        help=("A folder containing the training data."),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šè§†é¢‘åˆ—å
    parser.add_argument(
        "--video_column",
        type=str,
        default="video",
        # å¸®åŠ©ä¿¡æ¯ï¼šæ•°æ®é›†ä¸­åŒ…å«è§†é¢‘çš„åˆ—åï¼Œæˆ–åŒ…å«è§†é¢‘æ•°æ®è·¯å¾„çš„æ–‡ä»¶å
        help="The column of the dataset containing videos. Or, the name of the file in `--instance_data_root` folder containing the line-separated path to video data.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæç¤ºæ–‡æœ¬åˆ—å
    parser.add_argument(
        "--caption_column",
        type=str,
        default="text",
        # å¸®åŠ©ä¿¡æ¯ï¼šæ•°æ®é›†ä¸­æ¯ä¸ªè§†é¢‘çš„å®ä¾‹æç¤ºçš„åˆ—åï¼Œæˆ–åŒ…å«å®ä¾‹æç¤ºçš„æ–‡ä»¶å
        help="The column of the dataset containing the instance prompt for each video. Or, the name of the file in `--instance_data_root` folder containing the line-separated instance prompts.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ ‡è¯†ç¬¦ä»¤ç‰Œ
    parser.add_argument(
        "--id_token", type=str, default=None, 
        # å¸®åŠ©ä¿¡æ¯ï¼šå¦‚æœæä¾›ï¼Œå°†é™„åŠ åˆ°æ¯ä¸ªæç¤ºçš„å¼€å¤´çš„æ ‡è¯†ç¬¦ä»¤ç‰Œ
        help="Identifier token appended to the start of each prompt if provided."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ•°æ®åŠ è½½å™¨çš„å·¥ä½œè¿›ç¨‹æ•°
    parser.add_argument(
        "--dataloader_num_workers",
        type=int,
        default=0,
        # å¸®åŠ©ä¿¡æ¯ï¼šç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°é‡ã€‚0 è¡¨ç¤ºåœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ•°æ®
        help=(
            "Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process."
        ),
    )

    # éªŒè¯
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šéªŒè¯æç¤º
    parser.add_argument(
        "--validation_prompt",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼šåœ¨éªŒè¯æœŸé—´ä½¿ç”¨çš„ä¸€ä¸ªæˆ–å¤šä¸ªæç¤ºï¼Œä»¥éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨å­¦ä¹ 
        help="One or more prompt(s) that is used during validation to verify that the model is learning. Multiple validation prompts should be separated by the '--validation_prompt_seperator' string.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šéªŒè¯å›¾åƒè·¯å¾„
    parser.add_argument(
        "--validation_images",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º None
        default=None,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="One or more image path(s) that is used during validation to verify that the model is learning. Multiple validation paths should be separated by the '--validation_prompt_seperator' string. These should correspond to the order of the validation prompts.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šéªŒè¯æç¤ºåˆ†éš”ç¬¦
    parser.add_argument(
        "--validation_prompt_separator",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º ':::'
        default=":::",
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="String that separates multiple validation prompts",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šç”ŸæˆéªŒè¯è§†é¢‘çš„æ•°é‡
    parser.add_argument(
        "--num_validation_videos",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 1
        default=1,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="Number of videos that should be generated during validation per `validation_prompt`.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šæ¯ X ä¸ªè®­ç»ƒå‘¨æœŸè¿›è¡ŒéªŒè¯
    parser.add_argument(
        "--validation_epochs",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 50
        default=50,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=(
            "Run validation every X epochs. Validation consists of running the prompt `args.validation_prompt` multiple times: `args.num_validation_videos`."
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šå¼•å¯¼å°ºåº¦
    parser.add_argument(
        "--guidance_scale",
        # å‚æ•°ç±»å‹ä¸ºæµ®ç‚¹æ•°
        type=float,
        # é»˜è®¤å€¼ä¸º 6
        default=6,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="The guidance scale to use while sampling validation videos.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šæ˜¯å¦ä½¿ç”¨åŠ¨æ€é…ç½®
    parser.add_argument(
        "--use_dynamic_cfg",
        # å‚æ•°ç±»å‹ä¸ºå¸ƒå°”å€¼ï¼Œè®¾ç½®ä¸ºçœŸæ—¶å¯ç”¨åŠ¨æ€é…ç½®
        action="store_true",
        # é»˜è®¤å€¼ä¸º False
        default=False,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="Whether or not to use the default cosine dynamic guidance schedule when sampling validation videos.",
    )

    # è®­ç»ƒä¿¡æ¯
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šéšæœºç§å­
    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®š LoRA æ›´æ–°çŸ©é˜µçš„ç»´åº¦
    parser.add_argument(
        "--rank",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 128
        default=128,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=("The dimension of the LoRA update matrices."),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®š LoRA çš„ç¼©æ”¾å› å­
    parser.add_argument(
        "--lora_alpha",
        # å‚æ•°ç±»å‹ä¸ºæµ®ç‚¹æ•°
        type=float,
        # é»˜è®¤å€¼ä¸º 128
        default=128,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=("The scaling factor to scale LoRA weight update. The actual scaling factor is `lora_alpha / rank`"),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šæ··åˆç²¾åº¦è®¾ç½®
    parser.add_argument(
        "--mixed_precision",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º None
        default=None,
        # å¯é€‰å€¼åŒ…æ‹¬ "no", "fp16", "bf16"
        choices=["no", "fp16", "bf16"],
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help=(
            "Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
            " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
            " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šè¾“å‡ºç›®å½•
    parser.add_argument(
        "--output_dir",
        # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        type=str,
        # é»˜è®¤å€¼ä¸º 'cogvideox-i2v-lora'
        default="cogvideox-i2v-lora",
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="The output directory where the model predictions and checkpoints will be written.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šè¾“å…¥è§†é¢‘çš„é«˜åº¦
    parser.add_argument(
        "--height",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 480
        default=480,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="All input videos are resized to this height.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ç”¨äºæŒ‡å®šè¾“å…¥è§†é¢‘çš„å®½åº¦
    parser.add_argument(
        "--width",
        # å‚æ•°ç±»å‹ä¸ºæ•´æ•°
        type=int,
        # é»˜è®¤å€¼ä¸º 720
        default=720,
        # å‚æ•°å¸®åŠ©ä¿¡æ¯è¯´æ˜ç”¨é€”
        help="All input videos are resized to this width.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è§†é¢‘é‡å¡‘æ¨¡å¼ï¼Œæ¥å—çš„å€¼æœ‰ ['center', 'random', 'none']
    parser.add_argument(
        "--video_reshape_mode",
        type=str,
        default="center",
        help="All input videos are reshaped to this mode. Choose between ['center', 'random', 'none']",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è¾“å…¥è§†é¢‘çš„å¸§ç‡ï¼Œé»˜è®¤ä¸º 8
    parser.add_argument("--fps", type=int, default=8, help="All input videos will be used at this FPS.")
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è¾“å…¥è§†é¢‘çš„æœ€å¤§å¸§æ•°ï¼Œé»˜è®¤ä¸º 49
    parser.add_argument(
        "--max_num_frames", type=int, default=49, help="All input videos will be truncated to these many frames."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®ä»æ¯ä¸ªè¾“å…¥è§†é¢‘å¼€å§‹è·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
    parser.add_argument(
        "--skip_frames_start",
        type=int,
        default=0,
        help="Number of frames to skip from the beginning of each input video. Useful if training data contains intro sequences.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®ä»æ¯ä¸ªè¾“å…¥è§†é¢‘ç»“æŸè·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
    parser.add_argument(
        "--skip_frames_end",
        type=int,
        default=0,
        help="Number of frames to skip from the end of each input video. Useful if training data contains outro sequences.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ˜¯å¦éšæœºæ°´å¹³ç¿»è½¬è§†é¢‘
    parser.add_argument(
        "--random_flip",
        action="store_true",
        help="whether to randomly flip videos horizontally",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º 4
    parser.add_argument(
        "--train_batch_size", type=int, default=4, help="Batch size (per device) for the training dataloader."
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®è®­ç»ƒçš„æ€»å‘¨æœŸæ•°ï¼Œé»˜è®¤ä¸º 1
    parser.add_argument("--num_train_epochs", type=int, default=1)
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ€»è®­ç»ƒæ­¥éª¤æ•°ï¼Œé»˜è®¤ä¸º Noneï¼Œè¦†ç›–å‘¨æœŸè®¾ç½®
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="Total number of training steps to perform. If provided, overrides `--num_train_epochs`.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ¯ X æ¬¡æ›´æ–°ä¿å­˜è®­ç»ƒçŠ¶æ€æ£€æŸ¥ç‚¹çš„æ­¥æ•°ï¼Œé»˜è®¤ä¸º 500
    parser.add_argument(
        "--checkpointing_steps",
        type=int,
        default=500,
        help=(
            "Save a checkpoint of the training state every X updates. These checkpoints can be used both as final"
            " checkpoints in case they are better than the last checkpoint, and are also suitable for resuming"
            " training using `--resume_from_checkpoint`."
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®å­˜å‚¨çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡ï¼Œé»˜è®¤ä¸º None
    parser.add_argument(
        "--checkpoints_total_limit",
        type=int,
        default=None,
        help=("Max number of checkpoints to store."),
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ˜¯å¦ä»å…ˆå‰çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œé»˜è®¤ä¸º None
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help=(
            "Whether training should be resumed from a previous checkpoint. Use a path saved by"
            ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
        ),
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®åœ¨æ‰§è¡Œåå‘ä¼ æ’­/æ›´æ–°å‰è¦ç´¯ç§¯çš„æ›´æ–°æ­¥éª¤æ•°ï¼Œé»˜è®¤ä¸º 1
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ¥èŠ‚çœå†…å­˜ï¼Œé»˜è®¤ä¸º False
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        help="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.",
    )
    # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºè®¾ç½®åˆå§‹å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º 1e-4
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --scale_lrï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œé»˜è®¤å€¼ä¸º False
    parser.add_argument(
        "--scale_lr",
        action="store_true",
        default=False,
        help="Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_schedulerï¼ŒæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º "constant"
    parser.add_argument(
        "--lr_scheduler",
        type=str,
        default="constant",
        help=(
            'The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
            ' "constant", "constant_with_warmup"]'
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_warmup_stepsï¼ŒæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„é¢„çƒ­æ­¥éª¤æ•°ï¼Œé»˜è®¤å€¼ä¸º 500
    parser.add_argument(
        "--lr_warmup_steps", type=int, default=500, help="Number of steps for the warmup in the lr scheduler."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_num_cyclesï¼ŒæŒ‡å®šåœ¨ cosine_with_restarts è°ƒåº¦å™¨ä¸­å­¦ä¹ ç‡çš„ç¡¬é‡ç½®æ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º 1
    parser.add_argument(
        "--lr_num_cycles",
        type=int,
        default=1,
        help="Number of hard resets of the lr in cosine_with_restarts scheduler.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --lr_powerï¼ŒæŒ‡å®šå¤šé¡¹å¼è°ƒåº¦å™¨çš„å¹‚å› å­ï¼Œé»˜è®¤å€¼ä¸º 1.0
    parser.add_argument("--lr_power", type=float, default=1.0, help="Power factor of the polynomial scheduler.")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --enable_slicingï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œé»˜è®¤å€¼ä¸º Falseï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ VAE åˆ‡ç‰‡ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_slicing",
        action="store_true",
        default=False,
        help="Whether or not to use VAE slicing for saving memory.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --enable_tilingï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œé»˜è®¤å€¼ä¸º Falseï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ VAE ç“·ç –ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_tiling",
        action="store_true",
        default=False,
        help="Whether or not to use VAE tiling for saving memory.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --noised_image_dropoutï¼ŒæŒ‡å®šå›¾åƒæ¡ä»¶çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œé»˜è®¤å€¼ä¸º 0.05
    parser.add_argument(
        "--noised_image_dropout",
        type=float,
        default=0.05,
        help="Image condition dropout probability.",
    )

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --optimizerï¼ŒæŒ‡å®šä¼˜åŒ–å™¨ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º "adam"
    parser.add_argument(
        "--optimizer",
        type=lambda s: s.lower(),
        default="adam",
        choices=["adam", "adamw", "prodigy"],
        help=("The optimizer type to use."),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --use_8bit_adamï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ bitsandbytes çš„ 8 ä½ Adam
    parser.add_argument(
        "--use_8bit_adam",
        action="store_true",
        help="Whether or not to use 8-bit Adam from bitsandbytes. Ignored if optimizer is not set to AdamW",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_beta1ï¼ŒæŒ‡å®š Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ beta1 å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 0.9
    parser.add_argument(
        "--adam_beta1", type=float, default=0.9, help="The beta1 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_beta2ï¼ŒæŒ‡å®š Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ beta2 å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 0.95
    parser.add_argument(
        "--adam_beta2", type=float, default=0.95, help="The beta2 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --prodigy_beta3ï¼ŒæŒ‡å®š Prodigy ä¼˜åŒ–å™¨çš„æ­¥é•¿ç³»æ•°ï¼Œé»˜è®¤å€¼ä¸º None
    parser.add_argument(
        "--prodigy_beta3",
        type=float,
        default=None,
        help="Coefficients for computing the Prodigy optimizer's stepsize using running averages. If set to None, uses the value of square root of beta2.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --prodigy_decoupleï¼Œä½œä¸ºå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ AdamW é£æ ¼çš„è§£è€¦æƒé‡è¡°å‡
    parser.add_argument("--prodigy_decouple", action="store_true", help="Use AdamW style decoupled weight decay")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_weight_decayï¼ŒæŒ‡å®š UNet å‚æ•°ä½¿ç”¨çš„æƒé‡è¡°å‡ï¼Œé»˜è®¤å€¼ä¸º 1e-04
    parser.add_argument("--adam_weight_decay", type=float, default=1e-04, help="Weight decay to use for unet params")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --adam_epsilonï¼ŒæŒ‡å®š Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ epsilon å€¼ï¼Œé»˜è®¤å€¼ä¸º 1e-08
    parser.add_argument(
        "--adam_epsilon",
        type=float,
        default=1e-08,
        help="Epsilon value for the Adam optimizer and Prodigy optimizers.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --max_grad_normï¼ŒæŒ‡å®šæœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼Œé»˜è®¤å€¼ä¸º 1.0
    parser.add_argument("--max_grad_norm", default=1.0, type=float, help="Max gradient norm.")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºå¼€å¯ Adam çš„åå·®ä¿®æ­£
        parser.add_argument("--prodigy_use_bias_correction", action="store_true", help="Turn on Adam's bias correction.")
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºåœ¨çƒ­èº«é˜¶æ®µç§»é™¤å­¦ä¹ ç‡ï¼Œä»¥é¿å… D ä¼°è®¡çš„é—®é¢˜
        parser.add_argument(
            "--prodigy_safeguard_warmup",
            action="store_true",
            help="Remove lr from the denominator of D estimate to avoid issues during warm-up stage.",
        )
    
        # æ·»åŠ é¡¹ç›®è¿½è¸ªå™¨åç§°çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
        parser.add_argument("--tracker_name", type=str, default=None, help="Project tracker name")
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° Hub
        parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
        # æ·»åŠ  Hub è®¿é—®ä»¤ç‰Œçš„å‘½ä»¤è¡Œå‚æ•°ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
        parser.add_argument("--hub_token", type=str, default=None, help="The token to use to push to the Model Hub.")
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šè¦ä¸æœ¬åœ°è¾“å‡ºç›®å½•åŒæ­¥çš„å­˜å‚¨åº“åç§°
        parser.add_argument(
            "--hub_model_id",
            type=str,
            default=None,
            help="The name of the repository to keep in sync with the local `output_dir`.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ—¥å¿—æ–‡ä»¶å­˜å‚¨ç›®å½•ï¼Œé»˜è®¤ä¸º "logs"
        parser.add_argument(
            "--logging_dir",
            type=str,
            default="logs",
            help="Directory where logs are stored.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦å…è®¸åœ¨ Ampere GPU ä¸Šä½¿ç”¨ TF32ï¼Œä»¥åŠ é€Ÿè®­ç»ƒ
        parser.add_argument(
            "--allow_tf32",
            action="store_true",
            help=(
                "Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see"
                " https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices"
            ),
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ŒæŒ‡å®šå°†ç»“æœå’Œæ—¥å¿—æŠ¥å‘Šåˆ°çš„é›†æˆå¹³å°
        parser.add_argument(
            "--report_to",
            type=str,
            default=None,
            help=(
                'The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
                ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
            ),
        )
        # æ·»åŠ  NCCL åç«¯è¶…æ—¶çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œå•ä½ä¸ºç§’ï¼Œé»˜è®¤ä¸º 600
        parser.add_argument("--nccl_timeout", type=int, default=600, help="NCCL backend timeout in seconds.")
    
        # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ç»“æœ
        return parser.parse_args()
# å®šä¹‰ä¸€ä¸ªè§†é¢‘æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ª Dataset åŸºç±»
class VideoDataset(Dataset):
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—å¤šä¸ªå‚æ•°ä»¥é…ç½®æ•°æ®é›†
    def __init__(
        # æ•°æ®æ ¹ç›®å½•ï¼Œå¯é€‰
        self,
        instance_data_root: Optional[str] = None,
        # æ•°æ®é›†åç§°ï¼Œå¯é€‰
        dataset_name: Optional[str] = None,
        # æ•°æ®é›†é…ç½®åç§°ï¼Œå¯é€‰
        dataset_config_name: Optional[str] = None,
        # ç”¨äºæ–‡æœ¬æè¿°çš„åˆ—å
        caption_column: str = "text",
        # è§†é¢‘åˆ—å
        video_column: str = "video",
        # è§†é¢‘é«˜åº¦ï¼Œé»˜è®¤ 480
        height: int = 480,
        # è§†é¢‘å®½åº¦ï¼Œé»˜è®¤ 720
        width: int = 720,
        # è§†é¢‘é‡å¡‘æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨ä¸­å¿ƒæ¨¡å¼
        video_reshape_mode: str = "center",
        # å¸§ç‡ï¼Œé»˜è®¤ 8 å¸§æ¯ç§’
        fps: int = 8,
        # æœ€å¤§å¸§æ•°ï¼Œé»˜è®¤ 49
        max_num_frames: int = 49,
        # å¼€å§‹è·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ 0
        skip_frames_start: int = 0,
        # ç»“æŸè·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ 0
        skip_frames_end: int = 0,
        # ç¼“å­˜ç›®å½•ï¼Œå¯é€‰
        cache_dir: Optional[str] = None,
        # ID ä»¤ç‰Œï¼Œå¯é€‰
        id_token: Optional[str] = None,
    ) -> None:
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()

        # å°†æ•°æ®æ ¹ç›®å½•è½¬æ¢ä¸º Path å¯¹è±¡ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä¸º None
        self.instance_data_root = Path(instance_data_root) if instance_data_root is not None else None
        # è®¾ç½®æ•°æ®é›†åç§°
        self.dataset_name = dataset_name
        # è®¾ç½®æ•°æ®é›†é…ç½®åç§°
        self.dataset_config_name = dataset_config_name
        # è®¾ç½®æ–‡æœ¬æè¿°åˆ—å
        self.caption_column = caption_column
        # è®¾ç½®è§†é¢‘åˆ—å
        self.video_column = video_column
        # è®¾ç½®è§†é¢‘é«˜åº¦
        self.height = height
        # è®¾ç½®è§†é¢‘å®½åº¦
        self.width = width
        # è®¾ç½®è§†é¢‘é‡å¡‘æ¨¡å¼
        self.video_reshape_mode = video_reshape_mode
        # è®¾ç½®å¸§ç‡
        self.fps = fps
        # è®¾ç½®æœ€å¤§å¸§æ•°
        self.max_num_frames = max_num_frames
        # è®¾ç½®å¼€å§‹è·³è¿‡çš„å¸§æ•°
        self.skip_frames_start = skip_frames_start
        # è®¾ç½®ç»“æŸè·³è¿‡çš„å¸§æ•°
        self.skip_frames_end = skip_frames_end
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        # è®¾ç½® ID ä»¤ç‰Œï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
        self.id_token = id_token or ""

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œåˆ™ä» hub åŠ è½½æ•°æ®é›†
        if dataset_name is not None:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_hub()
        # å¦åˆ™ï¼Œä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†
        else:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_local_path()

        # å°† ID ä»¤ç‰Œæ·»åŠ åˆ°æ¯ä¸ªæç¤ºå‰
        self.instance_prompts = [self.id_token + prompt for prompt in self.instance_prompts]

        # è®¡ç®—å®ä¾‹è§†é¢‘çš„æ•°é‡
        self.num_instance_videos = len(self.instance_video_paths)
        # ç¡®ä¿è§†é¢‘å’Œæç¤ºæ•°é‡åŒ¹é…ï¼Œä¸åŒ¹é…åˆ™å¼•å‘é”™è¯¯
        if self.num_instance_videos != len(self.instance_prompts):
            raise ValueError(
                f"Expected length of instance prompts and videos to be the same but found {len(self.instance_prompts)=} and {len(self.instance_video_paths)=}. Please ensure that the number of caption prompts and videos match in your dataset."
            )

        # é¢„å¤„ç†æ•°æ®å¹¶å­˜å‚¨å¤„ç†åçš„å®ä¾‹è§†é¢‘
        self.instance_videos = self._preprocess_data()

    # è¿”å›æ•°æ®é›†ä¸­çš„å®ä¾‹æ•°é‡
    def __len__(self):
        return self.num_instance_videos

    # æ ¹æ®ç´¢å¼•è·å–æ•°æ®é¡¹
    def __getitem__(self, index):
        return {
            # è¿”å›å¯¹åº”çš„å®ä¾‹æç¤º
            "instance_prompt": self.instance_prompts[index],
            # è¿”å›å¯¹åº”çš„å®ä¾‹è§†é¢‘
            "instance_video": self.instance_videos[index],
        }
    # ä»æ•°æ®é›†ä¸­åŠ è½½æ•°æ®çš„ç§æœ‰æ–¹æ³•
        def _load_dataset_from_hub(self):
            # å°è¯•å¯¼å…¥ datasets åº“ä»¥åŠ è½½æ•°æ®é›†
            try:
                from datasets import load_dataset
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æŠ›å‡º ImportError
            except ImportError:
                raise ImportError(
                    "You are trying to load your data using the datasets library. If you wish to train using custom "
                    "captions please install the datasets library: `pip install datasets`. If you wish to load a "
                    "local folder containing images only, specify --instance_data_root instead."
                )
    
            # ä»æ•°æ®é›†ä¸­å¿ƒä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è§æ–‡æ¡£é“¾æ¥
            dataset = load_dataset(
                self.dataset_name,  # æ•°æ®é›†åç§°
                self.dataset_config_name,  # æ•°æ®é›†é…ç½®åç§°
                cache_dir=self.cache_dir,  # ç¼“å­˜ç›®å½•
            )
            # è·å–è®­ç»ƒé›†çš„åˆ—å
            column_names = dataset["train"].column_names
    
            # å¦‚æœæœªæŒ‡å®šè§†é¢‘åˆ—ï¼Œåˆ™é»˜è®¤ä¸ºåˆ—åçš„ç¬¬ä¸€ä¸ª
            if self.video_column is None:
                video_column = column_names[0]
                logger.info(f"`video_column` defaulting to {video_column}")
            else:
                video_column = self.video_column
                # æ£€æŸ¥æŒ‡å®šçš„è§†é¢‘åˆ—æ˜¯å¦å­˜åœ¨äºåˆ—åä¸­
                if video_column not in column_names:
                    raise ValueError(
                        f"`--video_column` value '{video_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                    )
    
            # å¦‚æœæœªæŒ‡å®šå­—å¹•åˆ—ï¼Œåˆ™é»˜è®¤ä¸ºåˆ—åçš„ç¬¬äºŒä¸ª
            if self.caption_column is None:
                caption_column = column_names[1]
                logger.info(f"`caption_column` defaulting to {caption_column}")
            else:
                caption_column = self.caption_column
                # æ£€æŸ¥æŒ‡å®šçš„å­—å¹•åˆ—æ˜¯å¦å­˜åœ¨äºåˆ—åä¸­
                if self.caption_column not in column_names:
                    raise ValueError(
                        f"`--caption_column` value '{self.caption_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                    )
    
            # è·å–è®­ç»ƒé›†ä¸­çš„å®ä¾‹æç¤ºï¼ˆå­—å¹•ï¼‰
            instance_prompts = dataset["train"][caption_column]
            # è·å–è®­ç»ƒé›†ä¸­è§†é¢‘æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨
            instance_videos = [Path(self.instance_data_root, filepath) for filepath in dataset["train"][video_column]]
    
            # è¿”å›å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„
            return instance_prompts, instance_videos
    # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†
        def _load_dataset_from_local_path(self):
            # æ£€æŸ¥å®ä¾‹æ•°æ®æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.instance_data_root.exists():
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜æ ¹æ–‡ä»¶å¤¹ä¸å­˜åœ¨
                raise ValueError("Instance videos root folder does not exist")
    
            # æ„å»ºæç¤ºæ–‡æœ¬æ–‡ä»¶è·¯å¾„
            prompt_path = self.instance_data_root.joinpath(self.caption_column)
            # æ„å»ºè§†é¢‘æ–‡ä»¶è·¯å¾„
            video_path = self.instance_data_root.joinpath(self.video_column)
    
            # æ£€æŸ¥æç¤ºæ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæ–‡ä»¶
            if not prompt_path.exists() or not prompt_path.is_file():
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜æç¤ºæ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®
                raise ValueError(
                    "Expected `--caption_column` to be path to a file in `--instance_data_root` containing line-separated text prompts."
                )
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæ–‡ä»¶
            if not video_path.exists() or not video_path.is_file():
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®
                raise ValueError(
                    "Expected `--video_column` to be path to a file in `--instance_data_root` containing line-separated paths to video data in the same directory."
                )
    
            # è¯»å–æç¤ºæ–‡æœ¬æ–‡ä»¶ï¼ŒæŒ‰è¡Œå»é™¤ç©ºç™½å¹¶è¿”å›åˆ—è¡¨
            with open(prompt_path, "r", encoding="utf-8") as file:
                instance_prompts = [line.strip() for line in file.readlines() if len(line.strip()) > 0]
            # è¯»å–è§†é¢‘æ–‡ä»¶ï¼ŒæŒ‰è¡Œå»é™¤ç©ºç™½å¹¶æ„å»ºè§†é¢‘è·¯å¾„åˆ—è¡¨
            with open(video_path, "r", encoding="utf-8") as file:
                instance_videos = [
                    self.instance_data_root.joinpath(line.strip()) for line in file.readlines() if len(line.strip()) > 0
                ]
    
            # æ£€æŸ¥è§†é¢‘è·¯å¾„åˆ—è¡¨ä¸­æ˜¯å¦å­˜åœ¨æ— æ•ˆæ–‡ä»¶è·¯å¾„
            if any(not path.is_file() for path in instance_videos):
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜è‡³å°‘ä¸€ä¸ªè·¯å¾„ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶
                raise ValueError(
                    "Expected '--video_column' to be a path to a file in `--instance_data_root` containing line-separated paths to video data but found atleast one path that is not a valid file."
                )
    
            # è¿”å›æç¤ºæ–‡æœ¬å’Œè§†é¢‘è·¯å¾„åˆ—è¡¨
            return instance_prompts, instance_videos
    
        # æ ¹æ®é•¿å®½è°ƒæ•´æ•°ç»„ä»¥é€‚åº”çŸ©å½¢è£å‰ª
        def _resize_for_rectangle_crop(self, arr):
            # è·å–ç›®æ ‡å›¾åƒå°ºå¯¸
            image_size = self.height, self.width
            # è·å–é‡å¡‘æ¨¡å¼
            reshape_mode = self.video_reshape_mode
            # æ£€æŸ¥æ•°ç»„å®½é«˜æ¯”ä¸ç›®æ ‡å›¾åƒå®½é«˜æ¯”
            if arr.shape[3] / arr.shape[2] > image_size[1] / image_size[0]:
                # è°ƒæ•´æ•°ç»„å°ºå¯¸ä»¥åŒ¹é…ç›®æ ‡å›¾åƒå®½åº¦
                arr = resize(
                    arr,
                    size=[image_size[0], int(arr.shape[3] * image_size[0] / arr.shape[2])],
                    interpolation=InterpolationMode.BICUBIC,
                )
            else:
                # è°ƒæ•´æ•°ç»„å°ºå¯¸ä»¥åŒ¹é…ç›®æ ‡å›¾åƒé«˜åº¦
                arr = resize(
                    arr,
                    size=[int(arr.shape[2] * image_size[1] / arr.shape[3]), image_size[1]],
                    interpolation=InterpolationMode.BICUBIC,
                )
    
            # è·å–è°ƒæ•´åæ•°ç»„çš„é«˜åº¦å’Œå®½åº¦
            h, w = arr.shape[2], arr.shape[3]
            # å»æ‰æ•°ç»„çš„ç¬¬ä¸€ç»´
            arr = arr.squeeze(0)
    
            # è®¡ç®—é«˜åº¦å’Œå®½åº¦çš„å·®å€¼
            delta_h = h - image_size[0]
            delta_w = w - image_size[1]
    
            # æ ¹æ®é‡å¡‘æ¨¡å¼è®¡ç®—è£å‰ªçš„èµ·å§‹ç‚¹
            if reshape_mode == "random" or reshape_mode == "none":
                # éšæœºç”Ÿæˆè£å‰ªèµ·å§‹ç‚¹
                top = np.random.randint(0, delta_h + 1)
                left = np.random.randint(0, delta_w + 1)
            elif reshape_mode == "center":
                # è®¡ç®—ä¸­å¿ƒè£å‰ªèµ·å§‹ç‚¹
                top, left = delta_h // 2, delta_w // 2
            else:
                # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡æ˜é‡å¡‘æ¨¡å¼æœªå®ç°
                raise NotImplementedError
            # è£å‰ªæ•°ç»„åˆ°æŒ‡å®šçš„é«˜åº¦å’Œå®½åº¦
            arr = TT.functional.crop(arr, top=top, left=left, height=image_size[0], width=image_size[1])
            # è¿”å›è£å‰ªåçš„æ•°ç»„
            return arr
    # æ•°æ®é¢„å¤„ç†å‡½æ•°
    def _preprocess_data(self):
        # å°è¯•å¯¼å…¥ decord åº“
        try:
            import decord
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æŠ›å‡º ImportError å¼‚å¸¸ï¼Œå¹¶æç¤ºç”¨æˆ·å®‰è£… decord
        except ImportError:
            raise ImportError(
                "The `decord` package is required for loading the video dataset. Install with `pip install decord`"
            )

        # è®¾ç½® decord ä½¿ç”¨ PyTorch ä½œä¸ºæ¡¥æ¥åº“
        decord.bridge.set_bridge("torch")

        # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºè§†é¢‘åŠ è½½ã€è°ƒæ•´å¤§å°å’Œè£å‰ªçš„è¿›åº¦
        progress_dataset_bar = tqdm(
            range(0, len(self.instance_video_paths)),
            desc="Loading progress resize and crop videos",
        )

        # åˆå§‹åŒ–è§†é¢‘åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å¤„ç†åçš„è§†é¢‘å¸§
        videos = []

        # éå†æ¯ä¸ªè§†é¢‘æ–‡ä»¶çš„è·¯å¾„
        for filename in self.instance_video_paths:
            # ä½¿ç”¨ decord.VideoReader è¯»å–è§†é¢‘æ–‡ä»¶
            video_reader = decord.VideoReader(uri=filename.as_posix())
            # è·å–è§†é¢‘å¸§çš„æ•°é‡
            video_num_frames = len(video_reader)

            # ç¡®å®šå¼€å§‹å’Œç»“æŸå¸§çš„ç´¢å¼•
            start_frame = min(self.skip_frames_start, video_num_frames)
            end_frame = max(0, video_num_frames - self.skip_frames_end)
            # å¦‚æœç»“æŸå¸§ç´¢å¼•å°äºç­‰äºå¼€å§‹å¸§ç´¢å¼•ï¼Œåˆ™åªè·å–å¼€å§‹å¸§
            if end_frame <= start_frame:
                frames = video_reader.get_batch([start_frame])
            # å¦‚æœå¸§æ•°åœ¨å¼€å§‹å’Œç»“æŸå¸§ä¹‹é—´å°äºç­‰äºæœ€å¤§å¸§æ•°ï¼Œåˆ™è·å–å…¨éƒ¨å¸§
            elif end_frame - start_frame <= self.max_num_frames:
                frames = video_reader.get_batch(list(range(start_frame, end_frame)))
            # å¦åˆ™ï¼Œå‡åŒ€é€‰æ‹©å¸§çš„ç´¢å¼•
            else:
                indices = list(range(start_frame, end_frame, (end_frame - start_frame) // self.max_num_frames))
                frames = video_reader.get_batch(indices)

            # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§å¸§æ•°é™åˆ¶
            frames = frames[: self.max_num_frames]
            # è·å–é€‰ä¸­çš„å¸§æ•°
            selected_num_frames = frames.shape[0]

            # é€‰æ‹©å‰ (4k + 1) å¸§ï¼Œç¡®ä¿å¸§æ•°æ»¡è¶³ VAE çš„è¦æ±‚
            remainder = (3 + (selected_num_frames % 4)) % 4
            # å¦‚æœæœ‰å¤šä½™å¸§ï¼Œå»æ‰è¿™äº›å¸§
            if remainder != 0:
                frames = frames[:-remainder]
            # æ›´æ–°é€‰ä¸­çš„å¸§æ•°
            selected_num_frames = frames.shape[0]

            # æ–­è¨€é€‰ä¸­çš„å¸§æ•°å‡å» 1 èƒ½è¢« 4 æ•´é™¤
            assert (selected_num_frames - 1) % 4 == 0

            # è¿›è¡Œè®­ç»ƒå˜æ¢ï¼Œå°†å¸§å€¼å½’ä¸€åŒ–åˆ° [-1, 1]
            frames = (frames - 127.5) / 127.5
            # è°ƒæ•´å¸§çš„ç»´åº¦é¡ºåºä¸º [F, C, H, W]
            frames = frames.permute(0, 3, 1, 2) # [F, C, H, W]
            # æ›´æ–°è¿›åº¦æ¡æè¿°ï¼Œæ˜¾ç¤ºå½“å‰è§†é¢‘çš„å°ºå¯¸
            progress_dataset_bar.set_description(
                f"Loading progress Resizing video from {frames.shape[2]}x{frames.shape[3]} to {self.height}x{self.width}"
            )
            # è°ƒæ•´å¸§çš„å°ºå¯¸ä»¥é€‚åº”çŸ©å½¢è£å‰ª
            frames = self._resize_for_rectangle_crop(frames)
            # å°†å¤„ç†åçš„å¸§æ·»åŠ åˆ°è§†é¢‘åˆ—è¡¨ä¸­
            videos.append(frames.contiguous())  # [F, C, H, W]
            # æ›´æ–°è¿›åº¦æ¡
            progress_dataset_bar.update(1)

        # å…³é—­è¿›åº¦æ¡
        progress_dataset_bar.close()

        # è¿”å›å¤„ç†åçš„æ‰€æœ‰è§†é¢‘å¸§
        return videos
# ä¿å­˜æ¨¡å‹å¡ç‰‡ï¼ŒåŒ…å«æ¨¡å‹ä¿¡æ¯å’Œè§†é¢‘éªŒè¯
def save_model_card(
    # ä»“åº“æ ‡è¯†
    repo_id: str,
    # è§†é¢‘åˆ—è¡¨ï¼Œé»˜è®¤å€¼ä¸º None
    videos=None,
    # åŸºç¡€æ¨¡å‹åç§°ï¼Œé»˜è®¤å€¼ä¸º None
    base_model: str = None,
    # éªŒè¯æç¤ºï¼Œé»˜è®¤å€¼ä¸º None
    validation_prompt=None,
    # ä»“åº“æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤å€¼ä¸º None
    repo_folder=None,
    # å¸§ç‡ï¼Œé»˜è®¤å€¼ä¸º 8
    fps=8,
):
    # åˆå§‹åŒ–å°éƒ¨ä»¶å­—å…¸
    widget_dict = []
    # æ£€æŸ¥æ˜¯å¦æä¾›è§†é¢‘
    if videos is not None:
        # éå†è§†é¢‘åˆ—è¡¨åŠå…¶ç´¢å¼•
        for i, video in enumerate(videos):
            # ä¸ºæ¯ä¸ªè§†é¢‘ç”Ÿæˆæ–‡ä»¶å
            video_path = f"final_video_{i}.mp4"
            # å¯¼å‡ºè§†é¢‘åˆ°æŒ‡å®šè·¯å¾„
            export_to_video(video, os.path.join(repo_folder, video_path, fps=fps))
            # å°†è§†é¢‘ä¿¡æ¯æ·»åŠ åˆ°å°éƒ¨ä»¶å­—å…¸ä¸­
            widget_dict.append(
                {"text": validation_prompt if validation_prompt else " ", "output": {"url": video_path}},
            )

    # å®šä¹‰æ¨¡å‹æè¿°æ–‡æœ¬
    model_description = f"""
# CogVideoX LoRA - {repo_id}

<Gallery />

## Model description

These are {repo_id} LoRA weights for {base_model}.

The weights were trained using the [CogVideoX Diffusers trainer](https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/train_cogvideox_image_to_video_lora.py).

Was LoRA for the text encoder enabled? No.

## Download model

[Download the *.safetensors LoRA]({repo_id}/tree/main) in the Files & versions tab.

## Use it with the [ğŸ§¨ diffusers library](https://github.com/huggingface/diffusers)


import torch
from diffusers import CogVideoXImageToVideoPipeline
from diffusers.utils import load_image, export_to_video

pipe = CogVideoXImageToVideoPipeline.from_pretrained("THUDM/CogVideoX-5b", torch_dtype=torch.bfloat16).to("cuda")
pipe.load_lora_weights("{repo_id}", weight_name="pytorch_lora_weights.safetensors", adapter_name=["cogvideox-i2v-lora"])

# The LoRA adapter weights are determined by what was used for training.
# In this case, we assume `--lora_alpha` is 32 and `--rank` is 64.
# It can be made lower or higher from what was used in training to decrease or amplify the effect
# of the LoRA upto a tolerance, beyond which one might notice no effect at all or overflows.
pipe.set_adapters(["cogvideox-i2v-lora"], [32 / 64])

image = load_image("/path/to/image")
video = pipe(image=image, "{validation_prompt}", guidance_scale=6, use_dynamic_cfg=True).frames[0]
export_to_video(video, "output.mp4", fps=8)


For more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)

## License

Please adhere to the licensing terms as described [here](https://huggingface.co/THUDM/CogVideoX-5b-I2V/blob/main/LICENSE).
"""
    # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡
    model_card = load_or_create_model_card(
        # ä»“åº“ ID æˆ–è·¯å¾„
        repo_id_or_path=repo_id,
        # æŒ‡ç¤ºæ˜¯å¦ä»è®­ç»ƒä¸­åˆ›å»º
        from_training=True,
        # è®¸å¯è¯ç±»å‹
        license="other",
        # åŸºç¡€æ¨¡å‹åç§°
        base_model=base_model,
        # éªŒè¯æç¤º
        prompt=validation_prompt,
        # æ¨¡å‹æè¿°
        model_description=model_description,
        # å°éƒ¨ä»¶ä¿¡æ¯
        widget=widget_dict,
    )
    # å®šä¹‰æ ‡ç­¾åˆ—è¡¨
    tags = [
        "image-to-video",
        "diffusers-training",
        "diffusers",
        "lora",
        "cogvideox",
        "cogvideox-diffusers",
        "template:sd-lora",
    ]

    # å¡«å……æ¨¡å‹å¡ç‰‡çš„æ ‡ç­¾
    model_card = populate_model_card(model_card, tags=tags)
    # ä¿å­˜æ¨¡å‹å¡ç‰‡åˆ°æŒ‡å®šè·¯å¾„
    model_card.save(os.path.join(repo_folder, "README.md"))


# è®°å½•éªŒè¯è¿‡ç¨‹
def log_validation(
    # ç®¡é“å¯¹è±¡
    pipe,
    # å‚æ•°
    args,
    # åŠ é€Ÿå™¨å¯¹è±¡
    accelerator,
    # ç®¡é“å‚æ•°ï¼Œç”¨äºé…ç½®å’Œç®¡ç†æ•°æ®å¤„ç†æµç¨‹
        pipeline_args,
        # å½“å‰è®­ç»ƒçš„è½®æ¬¡ï¼Œé€šå¸¸ç”¨äºæ§åˆ¶è®­ç»ƒè¿‡ç¨‹
        epoch,
        # æŒ‡ç¤ºæ˜¯å¦è¿›è¡Œæœ€ç»ˆéªŒè¯çš„å¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸º False
        is_final_validation: bool = False,
# æ—¥å¿—è®°å½•å½“å‰éªŒè¯è¿è¡Œçš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”Ÿæˆè§†é¢‘çš„æ•°é‡å’Œæç¤ºå†…å®¹
):
    logger.info(
        f"Running validation... \n Generating {args.num_validation_videos} videos with prompt: {pipeline_args['prompt']}."
    )
    # åˆå§‹åŒ–è°ƒåº¦å™¨å‚æ•°å­—å…¸
    scheduler_args = {}

    # æ£€æŸ¥è°ƒåº¦å™¨é…ç½®ä¸­æ˜¯å¦åŒ…å«æ–¹å·®ç±»å‹
    if "variance_type" in pipe.scheduler.config:
        # è·å–æ–¹å·®ç±»å‹
        variance_type = pipe.scheduler.config.variance_type

        # å¦‚æœæ–¹å·®ç±»å‹æ˜¯å·²å­¦ä¹ çš„ç±»å‹ï¼Œè®¾ç½®ä¸ºå›ºå®šå°
        if variance_type in ["learned", "learned_range"]:
            variance_type = "fixed_small"

        # æ›´æ–°è°ƒåº¦å™¨å‚æ•°å­—å…¸ä¸­çš„æ–¹å·®ç±»å‹
        scheduler_args["variance_type"] = variance_type

    # ä½¿ç”¨è°ƒåº¦å™¨é…ç½®å’Œå‚æ•°åˆå§‹åŒ–è°ƒåº¦å™¨
    pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, **scheduler_args)
    # å°†ç®¡é“ç§»åŠ¨åˆ°æŒ‡å®šçš„åŠ é€Ÿå™¨è®¾å¤‡ä¸Š
    pipe = pipe.to(accelerator.device)
    # å…³é—­è¿›åº¦æ¡é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨è¿›åº¦æ¡ï¼‰

    # è¿è¡Œæ¨ç†
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶è®¾ç½®éšæœºç§å­ï¼Œå¦‚æœæœªæŒ‡å®šç§å­ï¼Œåˆ™ä¸º None
    generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None

    # åˆå§‹åŒ–è§†é¢‘åˆ—è¡¨
    videos = []
    # å¾ªç¯ç”ŸæˆæŒ‡å®šæ•°é‡çš„è§†é¢‘
    for _ in range(args.num_validation_videos):
        # é€šè¿‡ç®¡é“ç”Ÿæˆè§†é¢‘å¸§
        pt_images = pipe(**pipeline_args, generator=generator, output_type="pt").frames[0]
        # å°†ç”Ÿæˆçš„å¸§å †å æˆä¸€ä¸ªå¼ é‡
        pt_images = torch.stack([pt_images[i] for i in range(pt_images.shape[0])])

        # å°† PyTorch å›¾åƒè½¬æ¢ä¸º NumPy æ•°ç»„
        image_np = VaeImageProcessor.pt_to_numpy(pt_images)
        # å°† NumPy æ•°ç»„è½¬æ¢ä¸º PIL å›¾åƒ
        image_pil = VaeImageProcessor.numpy_to_pil(image_np)

        # å°†ç”Ÿæˆçš„ PIL å›¾åƒæ·»åŠ åˆ°è§†é¢‘åˆ—è¡¨ä¸­
        videos.append(image_pil)

    # éå†åŠ é€Ÿå™¨çš„è·Ÿè¸ªå™¨
    for tracker in accelerator.trackers:
        # ç¡®å®šå½“å‰é˜¶æ®µåç§°
        phase_name = "test" if is_final_validation else "validation"
        # æ£€æŸ¥æ˜¯å¦ä¸º WandB è·Ÿè¸ªå™¨
        if tracker.name == "wandb":
            # åˆå§‹åŒ–è§†é¢‘æ–‡ä»¶ååˆ—è¡¨
            video_filenames = []
            # éå†ç”Ÿæˆçš„è§†é¢‘
            for i, video in enumerate(videos):
                # æ ¼å¼åŒ–æç¤ºå†…å®¹å¹¶æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
                prompt = (
                    pipeline_args["prompt"][:25]
                    .replace(" ", "_")
                    .replace(" ", "_")
                    .replace("'", "_")
                    .replace('"', "_")
                    .replace("/", "_")
                )
                # ç”Ÿæˆè§†é¢‘æ–‡ä»¶å
                filename = os.path.join(args.output_dir, f"{phase_name}_video_{i}_{prompt}.mp4")
                # å°†è§†é¢‘å¯¼å‡ºä¸ºæ–‡ä»¶
                export_to_video(video, filename, fps=8)
                # å°†æ–‡ä»¶åæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                video_filenames.append(filename)

            # è®°å½•è§†é¢‘åˆ°è·Ÿè¸ªå™¨
            tracker.log(
                {
                    phase_name: [
                        wandb.Video(filename, caption=f"{i}: {pipeline_args['prompt']}")
                        for i, filename in enumerate(video_filenames)
                    ]
                }
            )

    # åˆ é™¤ç®¡é“å¯¹è±¡ä»¥é‡Šæ”¾å†…å­˜
    del pipe
    # é‡Šæ”¾å†…å­˜èµ„æº
    free_memory()

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘åˆ—è¡¨
    return videos


# å®šä¹‰è·å– T5 æç¤ºåµŒå…¥çš„å‡½æ•°
def _get_t5_prompt_embeds(
    tokenizer: T5Tokenizer,
    text_encoder: T5EncoderModel,
    prompt: Union[str, List[str]],
    num_videos_per_prompt: int = 1,
    max_sequence_length: int = 226,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = None,
    text_input_ids=None,
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è·å–æ‰¹å¤„ç†å¤§å°ï¼Œå³æç¤ºçš„æ•°é‡
    batch_size = len(prompt)
    # æ£€æŸ¥ tokenizer æ˜¯å¦è¢«æŒ‡å®š
        if tokenizer is not None:
            # ä½¿ç”¨ tokenizer å¯¹æç¤ºæ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå¼ é‡å½¢å¼çš„è¾“å…¥
            text_inputs = tokenizer(
                prompt,
                padding="max_length",  # å¡«å……è‡³æœ€å¤§é•¿åº¦
                max_length=max_sequence_length,  # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
                truncation=True,  # å…è®¸æˆªæ–­è¶…å‡ºæœ€å¤§é•¿åº¦çš„è¾“å…¥
                add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šæ ‡è®°
                return_tensors="pt",  # è¿”å› PyTorch å¼ é‡
            )
            # æå–ç¼–ç åçš„è¾“å…¥ ID
            text_input_ids = text_inputs.input_ids
        else:
            # å¦‚æœæœªæä¾› tokenizerï¼Œæ£€æŸ¥è¾“å…¥ ID æ˜¯å¦ä¸º None
            if text_input_ids is None:
                # å¼•å‘é”™è¯¯ï¼Œæç¤ºå¿…é¡»æä¾› text_input_ids
                raise ValueError("`text_input_ids` must be provided when the tokenizer is not specified.")
    
        # ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæç¤ºçš„åµŒå…¥
        prompt_embeds = text_encoder(text_input_ids.to(device))[0]
        # å°†åµŒå…¥è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
    
        # ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆé‡å¤çš„æ–‡æœ¬åµŒå…¥ï¼Œä½¿ç”¨å…¼å®¹ MPS çš„æ–¹æ³•
        _, seq_len, _ = prompt_embeds.shape  # è·å–åµŒå…¥çš„å½¢çŠ¶
        # é‡å¤åµŒå…¥ä»¥åŒ¹é…ç”Ÿæˆæ•°é‡
        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)
        # è°ƒæ•´åµŒå…¥çš„å½¢çŠ¶ä»¥é€‚åº”æ‰¹å¤„ç†
        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)
    
        # è¿”å›å¤„ç†åçš„æ–‡æœ¬åµŒå…¥
        return prompt_embeds
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºç¼–ç æç¤ºæ–‡æœ¬ï¼Œå‚æ•°åŒ…æ‹¬åˆ†è¯å™¨ã€æ–‡æœ¬ç¼–ç å™¨ã€æç¤ºå†…å®¹ç­‰
def encode_prompt(
    tokenizer: T5Tokenizer,  # ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼çš„åˆ†è¯å™¨
    text_encoder: T5EncoderModel,  # æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹
    prompt: Union[str, List[str]],  # æç¤ºæ–‡æœ¬ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
    num_videos_per_prompt: int = 1,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡ï¼Œé»˜è®¤ä¸º1
    max_sequence_length: int = 226,  # è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ä¸º226
    device: Optional[torch.device] = None,  # æŒ‡å®šè¿è¡Œè®¾å¤‡ï¼ˆå¦‚GPUï¼‰ï¼Œé»˜è®¤ä¸ºNone
    dtype: Optional[torch.dtype] = None,  # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚float32ï¼‰ï¼Œé»˜è®¤ä¸ºNone
    text_input_ids=None,  # é¢„å…ˆæä¾›çš„æ–‡æœ¬è¾“å…¥IDï¼Œé»˜è®¤ä¸ºNone
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºå•å…ƒç´ åˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è·å–æç¤ºçš„åµŒå…¥è¡¨ç¤ºï¼Œè°ƒç”¨è‡ªå®šä¹‰å‡½æ•°
    prompt_embeds = _get_t5_prompt_embeds(
        tokenizer,  # åˆ†è¯å™¨
        text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
        prompt=prompt,  # æç¤ºæ–‡æœ¬
        num_videos_per_prompt=num_videos_per_prompt,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
        max_sequence_length=max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
        device=device,  # è¿è¡Œè®¾å¤‡
        dtype=dtype,  # æ•°æ®ç±»å‹
        text_input_ids=text_input_ids,  # æ–‡æœ¬è¾“å…¥ID
    )
    # è¿”å›æç¤ºåµŒå…¥è¡¨ç¤º
    return prompt_embeds


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºè®¡ç®—æç¤ºçš„åµŒå…¥è¡¨ç¤ºï¼Œæ¥å—å¤šä¸ªå‚æ•°
def compute_prompt_embeddings(
    tokenizer,  # åˆ†è¯å™¨
    text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
    prompt,  # æç¤ºæ–‡æœ¬
    max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
    device,  # è¿è¡Œè®¾å¤‡
    dtype,  # æ•°æ®ç±»å‹
    requires_grad: bool = False  # æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œé»˜è®¤ä¸ºFalse
):
    # å¦‚æœéœ€è¦è®¡ç®—æ¢¯åº¦
    if requires_grad:
        # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
        prompt_embeds = encode_prompt(
            tokenizer,  # åˆ†è¯å™¨
            text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
            prompt,  # æç¤ºæ–‡æœ¬
            num_videos_per_prompt=1,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
            max_sequence_length=max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
            device=device,  # è¿è¡Œè®¾å¤‡
            dtype=dtype,  # æ•°æ®ç±»å‹
        )
    else:
        # å¦‚æœä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¦æ­¢æ¢¯åº¦è®¡ç®—
        with torch.no_grad():
            # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
            prompt_embeds = encode_prompt(
                tokenizer,  # åˆ†è¯å™¨
                text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
                prompt,  # æç¤ºæ–‡æœ¬
                num_videos_per_prompt=1,  # æ¯ä¸ªæç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
                max_sequence_length=max_sequence_length,  # æœ€å¤§åºåˆ—é•¿åº¦
                device=device,  # è¿è¡Œè®¾å¤‡
                dtype=dtype,  # æ•°æ®ç±»å‹
            )
    # è¿”å›è®¡ç®—å¾—åˆ°çš„æç¤ºåµŒå…¥
    return prompt_embeds


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºå‡†å¤‡æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œæ¥å—å¤šä¸ªå‚æ•°
def prepare_rotary_positional_embeddings(
    height: int,  # è¾“å…¥å›¾åƒçš„é«˜åº¦
    width: int,  # è¾“å…¥å›¾åƒçš„å®½åº¦
    num_frames: int,  # å¸§æ•°
    vae_scale_factor_spatial: int = 8,  # VAEç©ºé—´ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º8
    patch_size: int = 2,  # æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼Œé»˜è®¤ä¸º2
    attention_head_dim: int = 64,  # æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º64
    device: Optional[torch.device] = None,  # æŒ‡å®šè¿è¡Œè®¾å¤‡ï¼ˆå¦‚GPUï¼‰ï¼Œé»˜è®¤ä¸ºNone
    base_height: int = 480,  # åŸºç¡€é«˜åº¦ï¼Œé»˜è®¤ä¸º480
    base_width: int = 720,  # åŸºç¡€å®½åº¦ï¼Œé»˜è®¤ä¸º720
) -> Tuple[torch.Tensor, torch.Tensor]:  # è¿”å›ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„
    # è®¡ç®—ç½‘æ ¼é«˜åº¦
    grid_height = height // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—ç½‘æ ¼å®½åº¦
    grid_width = width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€å®½åº¦
    base_size_width = base_width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€é«˜åº¦
    base_size_height = base_height // (vae_scale_factor_spatial * patch_size)

    # è·å–ç½‘æ ¼çš„è£å‰ªåŒºåŸŸåæ ‡
    grid_crops_coords = get_resize_crop_region_for_grid((grid_height, grid_width), base_size_width, base_size_height)
    # è·å–æ—‹è½¬ä½ç½®åµŒå…¥çš„æ­£å¼¦å’Œä½™å¼¦é¢‘ç‡
    freqs_cos, freqs_sin = get_3d_rotary_pos_embed(
        embed_dim=attention_head_dim,  # åµŒå…¥ç»´åº¦
        crops_coords=grid_crops_coords,  # ç½‘æ ¼è£å‰ªåæ ‡
        grid_size=(grid_height, grid_width),  # ç½‘æ ¼å¤§å°
        temporal_size=num_frames,  # æ—¶é—´ç»´åº¦å¤§å°
    )

    # å°†ä½™å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_cos = freqs_cos.to(device=device)
    # å°†æ­£å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_sin = freqs_sin.to(device=device)
    # è¿”å›ä½™å¼¦å’Œæ­£å¼¦é¢‘ç‡å¼ é‡
    return freqs_cos, freqs_sin


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºè·å–ä¼˜åŒ–å™¨ï¼Œæ¥å—å¤šä¸ªå‚æ•°
def get_optimizer(args, params_to_optimize, use_deepspeed: bool = False):
    # å¦‚æœä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    if use_deepspeed:
        # ä» accelerate åº“å¯¼å…¥ DummyOptim
        from accelerate.utils import DummyOptim

        # è¿”å› DeepSpeed ä¼˜åŒ–å™¨çš„å®ä¾‹
        return DummyOptim(
            params_to_optimize,  # å¾…ä¼˜åŒ–çš„å‚æ•°
            lr=args.learning_rate,  # å­¦ä¹ ç‡
            betas=(args.adam_beta1, args.adam_beta2),  # Adamä¼˜åŒ–å™¨çš„åŠ¨é‡å‚æ•°
            eps=args.adam_epsilon,  # Adamä¼˜åŒ–å™¨çš„ epsilon
            weight_decay=args.adam_weight_decay,  # æƒé‡è¡°å‡
        )

    # ä¼˜åŒ–å™¨åˆ›å»º
    # å®šä¹‰æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹åˆ—è¡¨
    supported_optimizers = ["adam", "adamw", "prodigy"]
    # æ£€æŸ¥ç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯å¦åœ¨æ”¯æŒçš„åˆ—è¡¨ä¸­
    if args.optimizer not in supported_optimizers:
        # è®°å½•ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨è­¦å‘Šä¿¡æ¯
        logger.warning(
            f"Unsupported choice of optimizer: {args.optimizer}. Supported optimizers include {supported_optimizers}. Defaulting to AdamW"
        )
        # å°†ä¼˜åŒ–å™¨é»˜è®¤è®¾ç½®ä¸º "adamw"
        args.optimizer = "adamw"

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ 8 ä½ Adam ä¼˜åŒ–å™¨ï¼Œå¹¶ä¸”å½“å‰ä¼˜åŒ–å™¨ä¸æ˜¯ Adam æˆ– AdamW
    if args.use_8bit_adam and args.optimizer.lower() not in ["adam", "adamw"]:
        # è®°å½•è­¦å‘Šï¼Œè¯´æ˜ä½¿ç”¨ 8 ä½ Adam æ—¶ä¼˜åŒ–å™¨å¿…é¡»ä¸º Adam æˆ– AdamW
        logger.warning(
            f"use_8bit_adam is ignored when optimizer is not set to 'Adam' or 'AdamW'. Optimizer was "
            f"set to {args.optimizer.lower()}"
        )

    # å¦‚æœç”¨æˆ·é€‰æ‹©ä½¿ç”¨ 8 ä½ Adam ä¼˜åŒ–å™¨
    if args.use_8bit_adam:
        try:
            # å°è¯•å¯¼å…¥ bitsandbytes åº“
            import bitsandbytes as bnb
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯æç¤ºç”¨æˆ·å®‰è£… bitsandbytes åº“
            raise ImportError(
                "To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`."
            )

    # å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯ AdamW
    if args.optimizer.lower() == "adamw":
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨ 8 ä½ Adam é€‰æ‹©ç›¸åº”çš„ä¼˜åŒ–å™¨ç±»
        optimizer_class = bnb.optim.AdamW8bit if args.use_8bit_adam else torch.optim.AdamW

        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¼ å…¥ä¼˜åŒ–å‚æ•°å’Œç›¸å…³è¶…å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯ Adam
    elif args.optimizer.lower() == "adam":
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨ 8 ä½ Adam é€‰æ‹©ç›¸åº”çš„ä¼˜åŒ–å™¨ç±»
        optimizer_class = bnb.optim.Adam8bit if args.use_8bit_adam else torch.optim.Adam

        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¼ å…¥ä¼˜åŒ–å‚æ•°å’Œç›¸å…³è¶…å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¼˜åŒ–å™¨æ˜¯ Prodigy
    elif args.optimizer.lower() == "prodigy":
        try:
            # å°è¯•å¯¼å…¥ prodigyopt åº“
            import prodigyopt
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯æç¤ºç”¨æˆ·å®‰è£… prodigyopt åº“
            raise ImportError("To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`")

        # è®¾ç½® Prodigy ä¼˜åŒ–å™¨ç±»
        optimizer_class = prodigyopt.Prodigy

        # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡ä½ï¼Œå¹¶è®°å½•è­¦å‘Š
        if args.learning_rate <= 0.1:
            logger.warning(
                "Learning rate is too low. When using prodigy, it's generally better to set learning rate around 1.0"
            )

        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¼ å…¥ä¼˜åŒ–å‚æ•°å’Œç›¸å…³è¶…å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            lr=args.learning_rate,
            betas=(args.adam_beta1, args.adam_beta2),
            beta3=args.prodigy_beta3,
            weight_decay=args.adam_weight_decay,
            eps=args.adam_epsilon,
            decouple=args.prodigy_decouple,
            use_bias_correction=args.prodigy_use_bias_correction,
            safeguard_warmup=args.prodigy_safeguard_warmup,
        )

    # è¿”å›åˆ›å»ºçš„ä¼˜åŒ–å™¨å®ä¾‹
    return optimizer
# ä¸»å‡½æ•°ï¼Œæ¥æ”¶å‘½ä»¤è¡Œå‚æ•°
def main(args):
    # æ£€æŸ¥æ˜¯å¦åŒæ—¶ä½¿ç”¨ wandb å’Œ hub_tokenï¼Œè‹¥æ˜¯åˆ™æŠ›å‡ºé”™è¯¯
    if args.report_to == "wandb" and args.hub_token is not None:
        raise ValueError(
            "You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token."
            " Please use `huggingface-cli login` to authenticate with the Hub."
        )

    # æ£€æŸ¥ MPS æ˜¯å¦å¯ç”¨ä¸”æ··åˆç²¾åº¦ä¸º bf16ï¼Œè‹¥æ˜¯åˆ™æŠ›å‡ºé”™è¯¯
    if torch.backends.mps.is_available() and args.mixed_precision == "bf16":
        # due to pytorch#99272, MPS does not yet support bfloat16.
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )

    # ç”Ÿæˆæ—¥å¿—ç›®å½•çš„è·¯å¾„
    logging_dir = Path(args.output_dir, args.logging_dir)

    # åˆå§‹åŒ–é¡¹ç›®é…ç½®
    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
    # è®¾ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œçš„å‚æ•°
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    # åˆå§‹åŒ–è¿›ç¨‹ç»„çš„å‚æ•°
    init_kwargs = InitProcessGroupKwargs(backend="nccl", timeout=timedelta(seconds=args.nccl_timeout))
    # åˆ›å»ºåŠ é€Ÿå™¨å®ä¾‹
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
        kwargs_handlers=[ddp_kwargs, init_kwargs],
    )

    # å¦‚æœ MPS å¯ç”¨ï¼Œç¦ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
    if torch.backends.mps.is_available():
        accelerator.native_amp = False

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ wandb è¿›è¡ŒæŠ¥å‘Š
    if args.report_to == "wandb":
        # å¦‚æœ wandb ä¸å¯ç”¨ï¼Œåˆ™æŠ›å‡ºå¯¼å…¥é”™è¯¯
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # é…ç½®æ—¥å¿—è®°å½•ä»¥ä¾¿äºè°ƒè¯•
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    # è®°å½•åŠ é€Ÿå™¨çš„çŠ¶æ€ä¿¡æ¯
    logger.info(accelerator.state, main_process_only=False)
    # å¦‚æœæ˜¯æœ¬åœ°ä¸»è¿›ç¨‹ï¼Œè®¾ç½®ä¸åŒçš„æ—¥å¿—è¯¦ç»†çº§åˆ«
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # å¦‚æœæä¾›äº†ç§å­ï¼Œåˆ™è®¾ç½®è®­ç»ƒç§å­
    if args.seed is not None:
        set_seed(args.seed)

    # å¤„ç†ä»“åº“åˆ›å»º
    if accelerator.is_main_process:
        # å¦‚æœè¾“å‡ºç›®å½•ä¸ä¸ºç©ºï¼Œåˆ›å»ºè¯¥ç›®å½•
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        # å¦‚æœéœ€è¦æ¨é€åˆ° Hubï¼Œåˆ›å»ºä»“åº“
        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
            ).repo_id

    # å‡†å¤‡æ¨¡å‹å’Œè°ƒåº¦å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision
    )

    text_encoder = T5EncoderModel.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision
    )

    # CogVideoX-2b æƒé‡ä»¥ float16 å­˜å‚¨
    # CogVideoX-5b å’Œ CogVideoX-5b-I2V çš„æƒé‡ä»¥ bfloat16 å­˜å‚¨
    load_dtype = torch.bfloat16 if "5b" in args.pretrained_model_name_or_path.lower() else torch.float16
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½ 3D Transformer æ¨¡å‹
    transformer = CogVideoXTransformer3DModel.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="transformer",  # æŒ‡å®šå­æ–‡ä»¶å¤¹ä¸º transformer
        torch_dtype=load_dtype,  # è®¾ç½®åŠ è½½çš„æƒé‡æ•°æ®ç±»å‹
        revision=args.revision,  # ä½¿ç”¨æŒ‡å®šçš„ä¿®è®¢ç‰ˆæœ¬
        variant=args.variant,  # ä½¿ç”¨æŒ‡å®šçš„å˜ä½“
    )
    
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½ VAE æ¨¡å‹
    vae = AutoencoderKLCogVideoX.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision, variant=args.variant
    )
    
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½è°ƒåº¦å™¨
    scheduler = CogVideoXDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
    
    # å¦‚æœå¯ç”¨äº†åˆ‡ç‰‡ï¼Œåˆ™å¯ç”¨ VAE çš„åˆ‡ç‰‡åŠŸèƒ½
    if args.enable_slicing:
        vae.enable_slicing()
    # å¦‚æœå¯ç”¨äº†å¹³é“ºï¼Œåˆ™å¯ç”¨ VAE çš„å¹³é“ºåŠŸèƒ½
    if args.enable_tiling:
        vae.enable_tiling()
    
    # ä»…è®­ç»ƒé™„åŠ çš„é€‚é…å™¨ LoRA å±‚
    text_encoder.requires_grad_(False)  # ç¦ç”¨æ–‡æœ¬ç¼–ç å™¨çš„æ¢¯åº¦è®¡ç®—
    transformer.requires_grad_(False)  # ç¦ç”¨ Transformer çš„æ¢¯åº¦è®¡ç®—
    vae.requires_grad_(False)  # ç¦ç”¨ VAE çš„æ¢¯åº¦è®¡ç®—
    
    # å¯¹äºæ··åˆç²¾åº¦è®­ç»ƒï¼Œå°†æ‰€æœ‰ä¸å¯è®­ç»ƒæƒé‡ï¼ˆvaeã€text_encoder å’Œ transformerï¼‰è½¬æ¢ä¸ºåŠç²¾åº¦
    weight_dtype = torch.float32  # é»˜è®¤æƒé‡æ•°æ®ç±»å‹ä¸º float32
    if accelerator.state.deepspeed_plugin:
        # DeepSpeed å¤„ç†ç²¾åº¦ï¼Œä½¿ç”¨ DeepSpeed é…ç½®ä¸­çš„è®¾ç½®
        if (
            "fp16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["fp16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¯ç”¨ fp16 æ—¶è®¾ç½®æƒé‡æ•°æ®ç±»å‹ä¸º float16
        if (
            "bf16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["bf16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¯ç”¨ bf16 æ—¶ä¹Ÿè®¾ç½®ä¸º float16
    else:
        if accelerator.mixed_precision == "fp16":
            weight_dtype = torch.float16  # å¦‚æœä½¿ç”¨ fp16ï¼Œè®¾ç½®æƒé‡æ•°æ®ç±»å‹ä¸º float16
        elif accelerator.mixed_precision == "bf16":
            weight_dtype = torch.bfloat16  # å¦‚æœä½¿ç”¨ bf16ï¼Œè®¾ç½®ä¸º bfloat16
    
    # æ£€æŸ¥ MPS æ˜¯å¦å¯ç”¨ï¼Œä¸”æƒé‡æ•°æ®ç±»å‹ä¸º bfloat16
    if torch.backends.mps.is_available() and weight_dtype == torch.bfloat16:
        # ç”±äº pytorch#99272ï¼ŒMPS å°šä¸æ”¯æŒ bfloat16ã€‚
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )
    
    # å°†æ–‡æœ¬ç¼–ç å™¨ã€Transformer å’Œ VAE è½¬ç§»åˆ°åŠ é€Ÿå™¨è®¾å¤‡ï¼ŒæŒ‡å®šæ•°æ®ç±»å‹
    text_encoder.to(accelerator.device, dtype=weight_dtype)
    transformer.to(accelerator.device, dtype=weight_dtype)
    vae.to(accelerator.device, dtype=weight_dtype)
    
    # å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™å¯ç”¨ Transformer çš„æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()
    
    # ç°åœ¨æˆ‘ä»¬å°†æ–° LoRA æƒé‡æ·»åŠ åˆ°æ³¨æ„åŠ›å±‚
    transformer_lora_config = LoraConfig(
        r=args.rank,  # è®¾ç½® LoRA çš„ç§©
        lora_alpha=args.lora_alpha,  # è®¾ç½® LoRA çš„ alpha å€¼
        init_lora_weights=True,  # åˆå§‹åŒ– LoRA æƒé‡
        target_modules=["to_k", "to_q", "to_v", "to_out.0"],  # ç›®æ ‡æ¨¡å—åˆ—è¡¨
    )
    # å°† LoRA é€‚é…å™¨æ·»åŠ åˆ° Transformer
    transformer.add_adapter(transformer_lora_config)
    # è§£åŒ…æ¨¡å‹ï¼Œä»¥ä¾¿äºå¤„ç†
        def unwrap_model(model):
            # ä½¿ç”¨åŠ é€Ÿå™¨è§£åŒ…æ¨¡å‹
            model = accelerator.unwrap_model(model)
            # å¦‚æœæ˜¯ç¼–è¯‘çš„æ¨¡å—ï¼Œè·å–åŸå§‹æ¨¡å‹ï¼Œå¦åˆ™è¿”å›å½“å‰æ¨¡å‹
            model = model._orig_mod if is_compiled_module(model) else model
            # è¿”å›å¤„ç†åçš„æ¨¡å‹
            return model
    
        # åˆ›å»ºè‡ªå®šä¹‰ä¿å­˜å’ŒåŠ è½½é’©å­ï¼Œä»¥ä¾¿åŠ é€Ÿå™¨ä»¥è‰¯å¥½æ ¼å¼åºåˆ—åŒ–çŠ¶æ€
        def save_model_hook(models, weights, output_dir):
            # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
            if accelerator.is_main_process:
                # åˆå§‹åŒ–å¾…ä¿å­˜çš„å±‚ä¸º None
                transformer_lora_layers_to_save = None
    
                # éå†æ¨¡å‹åˆ—è¡¨
                for model in models:
                    # æ£€æŸ¥æ¨¡å‹ç±»å‹æ˜¯å¦ä¸è§£åŒ…åçš„ transformer ç›¸åŒ
                    if isinstance(model, type(unwrap_model(transformer))):
                        # è·å–æ¨¡å‹çš„çŠ¶æ€å­—å…¸
                        transformer_lora_layers_to_save = get_peft_model_state_dict(model)
                    else:
                        # æŠ›å‡ºå¼‚å¸¸ä»¥å¤„ç†æ„å¤–æ¨¡å‹ç±»å‹
                        raise ValueError(f"unexpected save model: {model.__class__}")
    
                    # ç¡®ä¿ä»æƒé‡ä¸­ç§»é™¤å·²å¤„ç†çš„æ¨¡å‹
                    weights.pop()
    
                # ä¿å­˜ LoRA æƒé‡
                CogVideoXImageToVideoPipeline.save_lora_weights(
                    output_dir,
                    transformer_lora_layers=transformer_lora_layers_to_save,
                )
    
        # åˆ›å»ºåŠ è½½æ¨¡å‹çš„é’©å­
        def load_model_hook(models, input_dir):
            # åˆå§‹åŒ– transformer ä¸º None
            transformer_ = None
    
            # å½“æ¨¡å‹åˆ—è¡¨ä¸ä¸ºç©ºæ—¶
            while len(models) > 0:
                # ä»æ¨¡å‹åˆ—è¡¨ä¸­å¼¹å‡ºæ¨¡å‹
                model = models.pop()
    
                # æ£€æŸ¥æ¨¡å‹ç±»å‹
                if isinstance(model, type(unwrap_model(transformer))):
                    # å°† transformer è®¾ç½®ä¸ºå½“å‰æ¨¡å‹
                    transformer_ = model
                else:
                    # æŠ›å‡ºå¼‚å¸¸ä»¥å¤„ç†æ„å¤–æ¨¡å‹ç±»å‹
                    raise ValueError(f"Unexpected save model: {model.__class__}")
    
            # ä»è¾“å…¥ç›®å½•è·å– LoRA çŠ¶æ€å­—å…¸
            lora_state_dict = CogVideoXImageToVideoPipeline.lora_state_dict(input_dir)
    
            # åˆ›å»ºè½¬æ¢åçš„ transformer çŠ¶æ€å­—å…¸
            transformer_state_dict = {
                f'{k.replace("transformer.", "")}': v for k, v in lora_state_dict.items() if k.startswith("transformer.")
            }
            # å°†çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºé€‚åˆ PEFT çš„æ ¼å¼
            transformer_state_dict = convert_unet_state_dict_to_peft(transformer_state_dict)
            # è®¾ç½® PEFT æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå¹¶è·å–ä¸å…¼å®¹çš„é”®
            incompatible_keys = set_peft_model_state_dict(transformer_, transformer_state_dict, adapter_name="default")
            # å¦‚æœå­˜åœ¨ä¸å…¼å®¹çš„é”®ï¼Œæ£€æŸ¥æ„å¤–çš„é”®
            if incompatible_keys is not None:
                # è·å–æ„å¤–çš„é”®
                unexpected_keys = getattr(incompatible_keys, "unexpected_keys", None)
                if unexpected_keys:
                    # è®°å½•è­¦å‘Šä¿¡æ¯
                    logger.warning(
                        f"Loading adapter weights from state_dict led to unexpected keys not found in the model: "
                        f" {unexpected_keys}. "
                    )
    
            # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
            if args.mixed_precision == "fp16":
                # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰è½¬ä¸º fp32 ç±»å‹
                cast_training_params([transformer_])
    
        # æ³¨å†Œä¿å­˜çŠ¶æ€å‰é’©å­
        accelerator.register_save_state_pre_hook(save_model_hook)
        # æ³¨å†ŒåŠ è½½çŠ¶æ€å‰é’©å­
        accelerator.register_load_state_pre_hook(load_model_hook)
    
        # å¯ç”¨ TF32 ä»¥åŠ é€Ÿ Ampere GPU çš„è®­ç»ƒ
        if args.allow_tf32 and torch.cuda.is_available():
            # å…è®¸ä½¿ç”¨ TF32
            torch.backends.cuda.matmul.allow_tf32 = True
    # å¦‚æœæŒ‡å®šäº†ç¼©æ”¾å­¦ä¹ ç‡çš„æ ‡å¿—
    if args.scale_lr:
        # æ ¹æ®æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ã€è®­ç»ƒæ‰¹é‡å¤§å°å’Œè¿›ç¨‹æ•°ç¼©æ”¾å­¦ä¹ ç‡
        args.learning_rate = (
            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes
        )

    # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
    if args.mixed_precision == "fp16":
        # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰æå‡ä¸º fp32 ç±»å‹
        cast_training_params([transformer], dtype=torch.float32)

    # è·å–æ‰€æœ‰å¯è®­ç»ƒçš„ LoRA å‚æ•°
    transformer_lora_parameters = list(filter(lambda p: p.requires_grad, transformer.parameters()))

    # ä¼˜åŒ–å‚æ•°
    transformer_parameters_with_lr = {"params": transformer_lora_parameters, "lr": args.learning_rate}
    # å°†å‚æ•°æ”¾å…¥å¾…ä¼˜åŒ–çš„åˆ—è¡¨ä¸­
    params_to_optimize = [transformer_parameters_with_lr]

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    use_deepspeed_optimizer = (
        accelerator.state.deepspeed_plugin is not None
        and "optimizer" in accelerator.state.deepspeed_plugin.deepspeed_config
    )
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨
    use_deepspeed_scheduler = (
        accelerator.state.deepspeed_plugin is not None
        and "scheduler" not in accelerator.state.deepspeed_plugin.deepspeed_config
    )

    # è·å–ä¼˜åŒ–å™¨
    optimizer = get_optimizer(args, params_to_optimize, use_deepspeed=use_deepspeed_optimizer)

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = VideoDataset(
        # å®ä¾‹æ•°æ®çš„æ ¹ç›®å½•
        instance_data_root=args.instance_data_root,
        # æ•°æ®é›†åç§°
        dataset_name=args.dataset_name,
        # æ•°æ®é›†é…ç½®åç§°
        dataset_config_name=args.dataset_config_name,
        # æè¿°æ€§æ–‡æœ¬åˆ—åç§°
        caption_column=args.caption_column,
        # è§†é¢‘åˆ—åç§°
        video_column=args.video_column,
        # è§†é¢‘é«˜åº¦
        height=args.height,
        # è§†é¢‘å®½åº¦
        width=args.width,
        # è§†é¢‘é‡å¡‘æ¨¡å¼
        video_reshape_mode=args.video_reshape_mode,
        # å¸§ç‡
        fps=args.fps,
        # æœ€å¤§å¸§æ•°
        max_num_frames=args.max_num_frames,
        # å¼€å§‹è·³è¿‡çš„å¸§æ•°
        skip_frames_start=args.skip_frames_start,
        # ç»“æŸè·³è¿‡çš„å¸§æ•°
        skip_frames_end=args.skip_frames_end,
        # ç¼“å­˜ç›®å½•
        cache_dir=args.cache_dir,
        # èº«ä»½ä»¤ç‰Œ
        id_token=args.id_token,
    )

    # å®šä¹‰ç¼–ç è§†é¢‘çš„å‡½æ•°
    def encode_video(video, bar):
        # æ›´æ–°è¿›åº¦æ¡
        bar.update(1)
        # å°†è§†é¢‘è½¬æ¢ä¸ºæŒ‡å®šè®¾å¤‡å¹¶å¢åŠ ä¸€ä¸ªç»´åº¦
        video = video.to(accelerator.device, dtype=vae.dtype).unsqueeze(0)
        # è°ƒæ•´è§†é¢‘ç»´åº¦é¡ºåºä¸º [B, C, F, H, W]
        video = video.permute(0, 2, 1, 3, 4)  # [B, C, F, H, W]
        # å…‹éš†ç¬¬ä¸€ä¸ªå¸§ä½œä¸ºå›¾åƒ
        image = video[:, :, :1].clone()

        # ç¼–ç è§†é¢‘ä»¥è·å–æ½œåœ¨åˆ†å¸ƒ
        latent_dist = vae.encode(video).latent_dist

        # ç”Ÿæˆå›¾åƒå™ªå£°æ ‡å‡†å·®
        image_noise_sigma = torch.normal(mean=-3.0, std=0.5, size=(1,), device=image.device)
        # å–æŒ‡æ•°å¹¶è½¬æ¢ä¸ºå›¾åƒæ•°æ®ç±»å‹
        image_noise_sigma = torch.exp(image_noise_sigma).to(dtype=image.dtype)
        # ç”Ÿæˆä¸å›¾åƒå¤§å°ç›¸åŒçš„å™ªå£°å›¾åƒ
        noisy_image = torch.randn_like(image) * image_noise_sigma[:, None, None, None, None]
        # å¯¹å™ªå£°å›¾åƒè¿›è¡Œç¼–ç ä»¥è·å–æ½œåœ¨åˆ†å¸ƒ
        image_latent_dist = vae.encode(noisy_image).latent_dist

        # è¿”å›æ½œåœ¨åˆ†å¸ƒ
        return latent_dist, image_latent_dist

    # è®¡ç®—å®ä¾‹æç¤ºçš„åµŒå…¥
    train_dataset.instance_prompts = [
        compute_prompt_embeddings(
            tokenizer,
            text_encoder,
            [prompt],
            transformer.config.max_text_seq_length,
            accelerator.device,
            weight_dtype,
            requires_grad=False,
        )
        for prompt in train_dataset.instance_prompts
    ]

    # åˆ›å»ºè¿›åº¦æ¡ä»¥æ˜¾ç¤ºç¼–ç è§†é¢‘çš„åŠ è½½è¿›åº¦
    progress_encode_bar = tqdm(
        range(0, len(train_dataset.instance_videos)),
        desc="Loading Encode videos",
    )
    # å¯¹è®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸ªå®ä¾‹è§†é¢‘è¿›è¡Œç¼–ç ï¼Œå¹¶æ›´æ–°æ•°æ®é›†çš„å®ä¾‹è§†é¢‘åˆ—è¡¨
    train_dataset.instance_videos = [encode_video(video, progress_encode_bar) for video in train_dataset.instance_videos]
    # å…³é—­è¿›åº¦ç¼–ç æ¡
    progress_encode_bar.close()

    # å®šä¹‰ç”¨äºåˆå¹¶æ ·æœ¬çš„å‡½æ•°
    def collate_fn(examples):
        # åˆå§‹åŒ–è§†é¢‘å’Œå›¾åƒçš„åˆ—è¡¨
        videos = []
        images = []
        # éå†æ‰€æœ‰ç¤ºä¾‹
        for example in examples:
            # è·å–å®ä¾‹è§†é¢‘çš„æ½œåœ¨åˆ†å¸ƒå’Œå›¾åƒæ½œåœ¨åˆ†å¸ƒ
            latent_dist, image_latent_dist = example["instance_video"]

            # ä»æ½œåœ¨åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œå¹¶åº”ç”¨ç¼©æ”¾å› å­
            video_latents = latent_dist.sample() * vae.config.scaling_factor
            image_latents = image_latent_dist.sample() * vae.config.scaling_factor
            # è°ƒæ•´è§†é¢‘æ½œåœ¨è¡¨ç¤ºçš„ç»´åº¦é¡ºåº
            video_latents = video_latents.permute(0, 2, 1, 3, 4)
            # è°ƒæ•´å›¾åƒæ½œåœ¨è¡¨ç¤ºçš„ç»´åº¦é¡ºåº
            image_latents = image_latents.permute(0, 2, 1, 3, 4)

            # è®¡ç®—å¡«å……çš„å½¢çŠ¶ï¼Œä»¥ä¾¿ä¸ºè§†é¢‘æ½œåœ¨è¡¨ç¤ºä¿ç•™æ—¶é—´æ­¥é•¿
            padding_shape = (video_latents.shape[0], video_latents.shape[1] - 1, *video_latents.shape[2:])
            # åˆ›å»ºæ–°çš„é›¶å¡«å……å¼ é‡
            latent_padding = image_latents.new_zeros(padding_shape)
            # å°†å¡«å……å¼ é‡é™„åŠ åˆ°å›¾åƒæ½œåœ¨è¡¨ç¤º
            image_latents = torch.cat([image_latents, latent_padding], dim=1)

            # æ ¹æ®éšæœºå€¼å†³å®šæ˜¯å¦å°†å›¾åƒæ½œåœ¨è¡¨ç¤ºç½®ä¸ºé›¶ï¼ˆæ·»åŠ å™ªå£°ï¼‰
            if random.random() < args.noised_image_dropout:
                image_latents = torch.zeros_like(image_latents)

            # å°†è§†é¢‘å’Œå›¾åƒæ½œåœ¨è¡¨ç¤ºæ·»åŠ åˆ°åˆ—è¡¨ä¸­
            videos.append(video_latents)
            images.append(image_latents)

        # å°†è§†é¢‘å’Œå›¾åƒåˆ—è¡¨åˆå¹¶æˆå•ä¸€å¼ é‡
        videos = torch.cat(videos)
        images = torch.cat(images)
        # å°†å¼ é‡è½¬æ¢ä¸ºè¿ç»­æ ¼å¼å¹¶è½¬ä¸ºæµ®ç‚¹å‹
        videos = videos.to(memory_format=torch.contiguous_format).float()
        images = images.to(memory_format=torch.contiguous_format).float()

        # æå–æ¯ä¸ªç¤ºä¾‹çš„æç¤ºä¿¡æ¯
        prompts = [example["instance_prompt"] for example in examples]
        # å°†æç¤ºä¿¡æ¯åˆå¹¶ä¸ºä¸€ä¸ªå¼ é‡
        prompts = torch.cat(prompts)

        # è¿”å›åŒ…å«è§†é¢‘ã€å›¾åƒå’Œæç¤ºçš„å­—å…¸
        return {
            "videos": (videos, images),
            "prompts": prompts,
        }

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ä»¥ä¾¿äºæ‰¹é‡åŠ è½½è®­ç»ƒæ•°æ®
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.train_batch_size,  # è®¾ç½®æ‰¹é‡å¤§å°
        shuffle=True,  # æ‰“ä¹±æ•°æ®
        collate_fn=collate_fn,  # ä½¿ç”¨è‡ªå®šä¹‰çš„åˆå¹¶å‡½æ•°
        num_workers=args.dataloader_num_workers,  # è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
    )

    # è®¡ç®—è®­ç»ƒæ­¥éª¤æ•°çš„è°ƒåº¦å™¨åŠç›¸å…³æ•°å­¦
    overrode_max_train_steps = False  # åˆå§‹åŒ–æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦è¦†ç›–æœ€å¤§è®­ç»ƒæ­¥éª¤
    # è®¡ç®—æ¯ä¸ªè®­ç»ƒå‘¨æœŸçš„æ›´æ–°æ­¥éª¤æ•°
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    # å¦‚æœæ²¡æœ‰è®¾ç½®æœ€å¤§è®­ç»ƒæ­¥éª¤ï¼Œåˆ™æ ¹æ®è®­ç»ƒå‘¨æœŸå’Œæ›´æ–°æ­¥éª¤è®¡ç®—æœ€å¤§è®­ç»ƒæ­¥éª¤
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨é€‰æ‹©ç›¸åº”çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    if use_deepspeed_scheduler:
        from accelerate.utils import DummyScheduler

        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè°ƒåº¦å™¨
        lr_scheduler = DummyScheduler(
            name=args.lr_scheduler,  # å­¦ä¹ ç‡è°ƒåº¦å™¨åç§°
            optimizer=optimizer,  # å…³è”çš„ä¼˜åŒ–å™¨
            total_num_steps=args.max_train_steps * accelerator.num_processes,  # æ€»è®­ç»ƒæ­¥éª¤
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,  # é¢„çƒ­æ­¥éª¤æ•°
        )
    else:
        # åˆ›å»ºæ ‡å‡†å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = get_scheduler(
            args.lr_scheduler,  # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
            optimizer=optimizer,  # å…³è”çš„ä¼˜åŒ–å™¨
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,  # é¢„çƒ­æ­¥éª¤æ•°
            num_training_steps=args.max_train_steps * accelerator.num_processes,  # æ€»è®­ç»ƒæ­¥éª¤
            num_cycles=args.lr_num_cycles,  # å¾ªç¯æ¬¡æ•°
            power=args.lr_power,  # å­¦ä¹ ç‡è°ƒæ•´çš„æŒ‡æ•°
        )

    # ä½¿ç”¨åŠ é€Ÿå™¨å‡†å¤‡æ‰€æœ‰ç»„ä»¶
    transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        transformer,  # è½¬æ¢å™¨æ¨¡å‹
        optimizer,  # ä¼˜åŒ–å™¨
        train_dataloader,  # æ•°æ®åŠ è½½å™¨
        lr_scheduler  # å­¦ä¹ ç‡è°ƒåº¦å™¨
    )
    # ç”±äºè®­ç»ƒæ•°æ®åŠ è½½å™¨çš„å¤§å°å¯èƒ½å·²ç»æ”¹å˜ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®—æ€»çš„è®­ç»ƒæ­¥éª¤
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)  
    # å¦‚æœè¦†ç›–äº†æœ€å¤§è®­ç»ƒæ­¥éª¤ï¼Œåˆ™æ›´æ–°æœ€å¤§è®­ç»ƒæ­¥éª¤ä¸ºè®­ç»ƒè½®æ•°ä¹˜ä»¥æ¯è½®çš„æ›´æ–°æ­¥éª¤
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch  
    # ä¹‹åæˆ‘ä»¬é‡æ–°è®¡ç®—è®­ç»ƒè½®æ•°
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)  

    # æˆ‘ä»¬éœ€è¦åˆå§‹åŒ–è¿½è¸ªå™¨ï¼Œå¹¶å­˜å‚¨æˆ‘ä»¬çš„é…ç½®
    # è¿½è¸ªå™¨ä¼šåœ¨ä¸»è¿›ç¨‹ä¸­è‡ªåŠ¨åˆå§‹åŒ–
    if accelerator.is_main_process:
        # è·å–è¿½è¸ªå™¨åç§°ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤åç§°
        tracker_name = args.tracker_name or "cogvideox-i2v-lora"  
        # åˆå§‹åŒ–è¿½è¸ªå™¨ï¼Œå¹¶ä¼ å…¥é…ç½®å‚æ•°
        accelerator.init_trackers(tracker_name, config=vars(args))  

    # å¼€å§‹è®­ç»ƒï¼
    # è®¡ç®—æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ€»æ‰¹é‡å¤§å°
    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps  
    # è®¡ç®—å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°
    num_trainable_parameters = sum(param.numel() for model in params_to_optimize for param in model["params"])  

    # è®°å½•è®­ç»ƒä¿¡æ¯
    logger.info("***** Running training *****")  
    logger.info(f"  Num trainable parameters = {num_trainable_parameters}")  # è®°å½•å¯è®­ç»ƒå‚æ•°æ•°é‡
    logger.info(f"  Num examples = {len(train_dataset)}")  # è®°å½•è®­ç»ƒæ ·æœ¬æ•°é‡
    logger.info(f"  Num batches each epoch = {len(train_dataloader)}")  # è®°å½•æ¯è½®çš„æ‰¹æ¬¡æ•°
    logger.info(f"  Num epochs = {args.num_train_epochs}")  # è®°å½•æ€»è®­ç»ƒè½®æ•°
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")  # è®°å½•æ¯ä¸ªè®¾å¤‡çš„å³æ—¶æ‰¹é‡å¤§å°
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")  # è®°å½•æ€»æ‰¹é‡å¤§å°
    logger.info(f"  Gradient accumulation steps = {args.gradient_accumulation_steps}")  # è®°å½•æ¢¯åº¦ç´¯ç§¯æ­¥éª¤æ•°
    logger.info(f"  Total optimization steps = {args.max_train_steps}")  # è®°å½•æ€»ä¼˜åŒ–æ­¥éª¤æ•°
    global_step = 0  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤
    first_epoch = 0  # åˆå§‹åŒ–ç¬¬ä¸€è½®

    # å¯èƒ½åŠ è½½æ¥è‡ªä¹‹å‰ä¿å­˜çš„æƒé‡å’ŒçŠ¶æ€
    if not args.resume_from_checkpoint:
        initial_global_step = 0  # å¦‚æœä¸ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œåˆå§‹å…¨å±€æ­¥éª¤è®¾ä¸º0
    else:
        # å¦‚æœæŒ‡å®šçš„æ£€æŸ¥ç‚¹ä¸æ˜¯"latest"ï¼Œåˆ™æå–è·¯å¾„
        if args.resume_from_checkpoint != "latest":
            path = os.path.basename(args.resume_from_checkpoint)  
        else:
            # è·å–æœ€è¿‘çš„æ£€æŸ¥ç‚¹
            dirs = os.listdir(args.output_dir)  # åˆ—å‡ºè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
            dirs = [d for d in dirs if d.startswith("checkpoint")]  # è¿‡æ»¤å‡ºæ£€æŸ¥ç‚¹æ–‡ä»¶
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))  # æŒ‰æ£€æŸ¥ç‚¹ç¼–å·æ’åº
            path = dirs[-1] if len(dirs) > 0 else None  # è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™è®¾ä¸ºNone

        # æ£€æŸ¥ç‚¹è·¯å¾„ä¸ºç©ºï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯å¹¶å¼€å§‹æ–°çš„è®­ç»ƒ
        if path is None:
            accelerator.print(
                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
            )  
            args.resume_from_checkpoint = None  # å°†æ¢å¤æ£€æŸ¥ç‚¹è®¾ä¸ºNone
            initial_global_step = 0  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤ä¸º0
        else:
            # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
            accelerator.print(f"Resuming from checkpoint {path}")  
            # åŠ è½½æ£€æŸ¥ç‚¹çŠ¶æ€
            accelerator.load_state(os.path.join(args.output_dir, path))  
            # æå–å…¨å±€æ­¥éª¤æ•°
            global_step = int(path.split("-")[1])  

            initial_global_step = global_step  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤ä¸ºå½“å‰æ­¥éª¤
            first_epoch = global_step // num_update_steps_per_epoch  # è®¡ç®—ç¬¬ä¸€è½®
    # åˆ›å»ºè¿›åº¦æ¡ï¼ŒèŒƒå›´ä¸ºæœ€å¤§è®­ç»ƒæ­¥éª¤ï¼Œåˆå§‹å€¼ä¸ºå…¨å±€æ­¥æ•°
        progress_bar = tqdm(
            range(0, args.max_train_steps),
            initial=initial_global_step,
            desc="Steps",
            # ä»…åœ¨æ¯å°æœºå™¨ä¸Šæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡
            disable=not accelerator.is_local_main_process,
        )
        # è®¡ç®— VAE ç©ºé—´ç¼©æ”¾å› å­ï¼Œæ ¹æ®å—è¾“å‡ºé€šé“çš„æ•°é‡
        vae_scale_factor_spatial = 2 ** (len(vae.config.block_out_channels) - 1)
    
        # è·å–æ¨¡å‹é…ç½®ï¼Œæ”¯æŒ DeepSpeed è®­ç»ƒ
        model_config = transformer.module.config if hasattr(transformer, "module") else transformer.config
    
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å‡†å¤‡å®Œæ¯•
        accelerator.wait_for_everyone()
        # ç»“æŸè®­ç»ƒ
        accelerator.end_training()
# åˆ¤æ–­å½“å‰æ¨¡å—æ˜¯å¦æ˜¯ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()
    # è°ƒç”¨ä¸»å‡½æ•°å¹¶ä¼ é€’å‚æ•°
    main(args)
```

# `.\cogvideo-finetune\finetune\train_cogvideox_lora.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œæ ‡æ˜ä»£ç çš„ç‰ˆæƒæ‰€æœ‰è€…åŠç›¸å…³ä¿¡æ¯
# Copyright 2024 The CogView team, Tsinghua University & ZhipuAI and The HuggingFace Team. All rights reserved.
#
# æŒ‰ç…§ Apache 2.0 è®¸å¯åè®®è¿›è¡Œæˆæƒ
# Licensed under the Apache License, Version 2.0 (the "License");
# ä½ ä¸å¾—åœ¨æœªéµå¾ªè®¸å¯çš„æƒ…å†µä¸‹ä½¿ç”¨æ­¤æ–‡ä»¶
# you may not use this file except in compliance with the License.
# ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰è§„å®šï¼Œè½¯ä»¶æ ¹æ®è®¸å¯è¯åˆ†å‘æ˜¯åŸºäºâ€œæŒ‰ç°çŠ¶â€åŸåˆ™
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯æˆ–æ¡ä»¶
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# è¯·å‚è§è®¸å¯è¯ä»¥è·å–ç‰¹å®šè¯­è¨€é€‚ç”¨çš„æƒé™å’Œé™åˆ¶
# See the License for the specific language governing permissions and
# limitations under the License.

# å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import argparse
# å¯¼å…¥æ—¥å¿—è®°å½•æ¨¡å—
import logging
# å¯¼å…¥æ•°å­¦è¿ç®—æ¨¡å—
import math
# å¯¼å…¥æ“ä½œç³»ç»Ÿç›¸å…³çš„æ¨¡å—
import os
# å¯¼å…¥æ–‡ä»¶å’Œç›®å½•æ“ä½œæ¨¡å—
import shutil
# å¯¼å…¥è·¯å¾„å¤„ç†æ¨¡å—
from pathlib import Path
# å¯¼å…¥ç±»å‹æç¤ºç›¸å…³çš„æ¨¡å—
from typing import List, Optional, Tuple, Union

# å¯¼å…¥ PyTorch åº“
import torch
# å¯¼å…¥ Transformers åº“
import transformers
# ä» accelerate åº“å¯¼å…¥åŠ é€Ÿå™¨
from accelerate import Accelerator
# ä» accelerate.logging å¯¼å…¥è·å–æ—¥å¿—è®°å½•å™¨
from accelerate.logging import get_logger
# ä» accelerate.utils å¯¼å…¥åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç›¸å…³çš„å·¥å…·
from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed
# ä» huggingface_hub å¯¼å…¥åˆ›å»ºå’Œä¸Šä¼ æ¨¡å‹çš„å·¥å…·
from huggingface_hub import create_repo, upload_folder
# ä» peft åº“å¯¼å…¥ Lora é…ç½®å’Œæ¨¡å‹çŠ¶æ€å­—å…¸å¤„ç†å·¥å…·
from peft import LoraConfig, get_peft_model_state_dict, set_peft_model_state_dict
# ä» PyTorch çš„å·¥å…·æ•°æ®é›†æ¨¡å—å¯¼å…¥ DataLoader å’Œ Dataset
from torch.utils.data import DataLoader, Dataset
# ä» torchvision å¯¼å…¥æ•°æ®é¢„å¤„ç†å·¥å…·
from torchvision import transforms
# å¯¼å…¥è¿›åº¦æ¡å·¥å…·
from tqdm.auto import tqdm
# ä» Transformers åº“å¯¼å…¥è‡ªåŠ¨æ ‡è®°å™¨å’Œ T5 æ¨¡å‹
from transformers import AutoTokenizer, T5EncoderModel, T5Tokenizer

# å¯¼å…¥ Diffusers åº“åŠå…¶ç›¸å…³æ¨¡å—
import diffusers
# å¯¼å…¥ CogVideoX ç›¸å…³çš„æ¨¡å‹å’Œè°ƒåº¦å™¨
from diffusers import AutoencoderKLCogVideoX, CogVideoXDPMScheduler, CogVideoXPipeline, CogVideoXTransformer3DModel
# ä» Diffusers å¯¼å…¥è·å– 3D æ—‹è½¬ä½ç½®åµŒå…¥çš„å·¥å…·
from diffusers.models.embeddings import get_3d_rotary_pos_embed
# ä» Diffusers å¯¼å…¥è·å–è°ƒåº¦å™¨çš„å·¥å…·
from diffusers.optimization import get_scheduler
# ä» Diffusers çš„ CogVideoX ç®¡é“å¯¼å…¥è°ƒæ•´åŒºåŸŸçš„å·¥å…·
from diffusers.pipelines.cogvideo.pipeline_cogvideox import get_resize_crop_region_for_grid
# ä» Diffusers å¯¼å…¥è®­ç»ƒç›¸å…³çš„å·¥å…·
from diffusers.training_utils import (
    cast_training_params,  # è½¬æ¢è®­ç»ƒå‚æ•°çš„å·¥å…·
    free_memory,           # é‡Šæ”¾å†…å­˜çš„å·¥å…·
)
# ä» Diffusers å¯¼å…¥å·¥å…·é›†
from diffusers.utils import check_min_version, convert_unet_state_dict_to_peft, export_to_video, is_wandb_available
# ä» Diffusers çš„ Hub å·¥å…·å¯¼å…¥æ¨¡å‹å¡åŠ è½½ä¸åˆ›å»ºå·¥å…·
from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card
# ä» Diffusers å¯¼å…¥ä¸ PyTorch ç›¸å…³çš„å·¥å…·
from diffusers.utils.torch_utils import is_compiled_module

# å¦‚æœå¯ç”¨ï¼Œå¯¼å…¥ Weights & Biases åº“
if is_wandb_available():
    import wandb

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æœ€å°ç‰ˆæœ¬çš„ Diffusersï¼Œå¦‚æœæœªå®‰è£…å°†å¼•å‘é”™è¯¯
# Will error if the minimal version of diffusers is not installed. Remove at your own risks.
check_min_version("0.31.0.dev0")

# è·å–æ—¥å¿—è®°å½•å™¨ï¼Œè®°å½•å½“å‰æ¨¡å—çš„æ—¥å¿—ä¿¡æ¯
logger = get_logger(__name__)


# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºä¸€ä¸ªè§£æå™¨ï¼Œæè¿°è®­ç»ƒè„šæœ¬çš„ç®€å•ç¤ºä¾‹
    parser = argparse.ArgumentParser(description="Simple example of a training script for CogVideoX.")

    # æ·»åŠ é¢„è®­ç»ƒæ¨¡å‹ä¿¡æ¯å‚æ•°
    parser.add_argument(
        "--pretrained_model_name_or_path",  # å‚æ•°å
        type=str,                            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        default=None,                        # é»˜è®¤å€¼ä¸º None
        required=True,                       # è¯¥å‚æ•°ä¸ºå¿…éœ€é¡¹
        help="Path to pretrained model or model identifier from huggingface.co/models.",  # å‚æ•°å¸®åŠ©ä¿¡æ¯
    )
    # æ·»åŠ æ¨¡å‹ä¿®è®¢ç‰ˆæœ¬å‚æ•°
    parser.add_argument(
        "--revision",                        # å‚æ•°å
        type=str,                            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        default=None,                        # é»˜è®¤å€¼ä¸º None
        required=False,                      # è¯¥å‚æ•°ä¸ºå¯é€‰é¡¹
        help="Revision of pretrained model identifier from huggingface.co/models.",  # å‚æ•°å¸®åŠ©ä¿¡æ¯
    )
    # æ·»åŠ æ¨¡å‹å˜ä½“å‚æ•°
    parser.add_argument(
        "--variant",                         # å‚æ•°å
        type=str,                            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
        default=None,                        # é»˜è®¤å€¼ä¸º None
        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",  # å‚æ•°å¸®åŠ©ä¿¡æ¯
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --cache_dirï¼ŒæŒ‡å®šç¼“å­˜ç›®å½•
        parser.add_argument(
            "--cache_dir",
            type=str,
            default=None,
            help="The directory where the downloaded models and datasets will be stored.",
        )
    
        # æ•°æ®é›†ä¿¡æ¯
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --dataset_nameï¼ŒæŒ‡å®šæ•°æ®é›†åç§°
        parser.add_argument(
            "--dataset_name",
            type=str,
            default=None,
            help=(
                "The name of the Dataset (from the HuggingFace hub) containing the training data of instance images (could be your own, possibly private,"
                " dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,"
                " or to a folder containing files that ğŸ¤— Datasets can understand."
            ),
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --dataset_config_nameï¼ŒæŒ‡å®šæ•°æ®é›†é…ç½®åç§°
        parser.add_argument(
            "--dataset_config_name",
            type=str,
            default=None,
            help="The config of the Dataset, leave as None if there's only one config.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --instance_data_rootï¼ŒæŒ‡å®šè®­ç»ƒæ•°æ®æ ¹ç›®å½•
        parser.add_argument(
            "--instance_data_root",
            type=str,
            default=None,
            help=("A folder containing the training data."),
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --video_columnï¼ŒæŒ‡å®šåŒ…å«è§†é¢‘çš„åˆ—åç§°
        parser.add_argument(
            "--video_column",
            type=str,
            default="video",
            help="The column of the dataset containing videos. Or, the name of the file in `--instance_data_root` folder containing the line-separated path to video data.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --caption_columnï¼ŒæŒ‡å®šåŒ…å«æç¤ºæ–‡æœ¬çš„åˆ—åç§°
        parser.add_argument(
            "--caption_column",
            type=str,
            default="text",
            help="The column of the dataset containing the instance prompt for each video. Or, the name of the file in `--instance_data_root` folder containing the line-separated instance prompts.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --id_tokenï¼ŒæŒ‡å®šæ ‡è¯†ç¬¦ä»¤ç‰Œ
        parser.add_argument(
            "--id_token", type=str, default=None, help="Identifier token appended to the start of each prompt if provided."
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --dataloader_num_workersï¼ŒæŒ‡å®šæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°é‡
        parser.add_argument(
            "--dataloader_num_workers",
            type=int,
            default=0,
            help=(
                "Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process."
            ),
        )
    
        # éªŒè¯
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --validation_promptï¼ŒæŒ‡å®šéªŒè¯æ—¶ä½¿ç”¨çš„æç¤º
        parser.add_argument(
            "--validation_prompt",
            type=str,
            default=None,
            help="One or more prompt(s) that is used during validation to verify that the model is learning. Multiple validation prompts should be separated by the '--validation_prompt_seperator' string.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --validation_prompt_separatorï¼ŒæŒ‡å®šéªŒè¯æç¤ºçš„åˆ†éš”ç¬¦
        parser.add_argument(
            "--validation_prompt_separator",
            type=str,
            default=":::",
            help="String that separates multiple validation prompts",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --num_validation_videosï¼ŒæŒ‡å®šæ¯ä¸ªéªŒè¯æç¤ºç”Ÿæˆçš„è§†é¢‘æ•°é‡
        parser.add_argument(
            "--num_validation_videos",
            type=int,
            default=1,
            help="Number of videos that should be generated during validation per `validation_prompt`.",
        )
        # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° --validation_epochsï¼ŒæŒ‡å®šæ¯éš”å¤šå°‘ä¸ªå‘¨æœŸè¿›è¡Œä¸€æ¬¡éªŒè¯
        parser.add_argument(
            "--validation_epochs",
            type=int,
            default=50,
            help=(
                "Run validation every X epochs. Validation consists of running the prompt `args.validation_prompt` multiple times: `args.num_validation_videos`."
            ),
        )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæŒ‡å¯¼å°ºåº¦ï¼Œç”¨äºé‡‡æ ·éªŒè¯è§†é¢‘
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=6,
        help="The guidance scale to use while sampling validation videos.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦ä½¿ç”¨åŠ¨æ€é…ç½®æ ‡å¿—
    parser.add_argument(
        "--use_dynamic_cfg",
        action="store_true",
        default=False,
        help="Whether or not to use the default cosine dynamic guidance schedule when sampling validation videos.",
    )

    # è®­ç»ƒä¿¡æ¯
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šéšæœºç§å­ä»¥ç¡®ä¿è®­ç»ƒçš„å¯é‡å¤æ€§
    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šLoRAæ›´æ–°çŸ©é˜µçš„ç»´åº¦
    parser.add_argument(
        "--rank",
        type=int,
        default=128,
        help=("The dimension of the LoRA update matrices."),
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šLoRAæƒé‡æ›´æ–°çš„ç¼©æ”¾å› å­
    parser.add_argument(
        "--lora_alpha",
        type=float,
        default=128,
        help=("The scaling factor to scale LoRA weight update. The actual scaling factor is `lora_alpha / rank`"),
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    parser.add_argument(
        "--mixed_precision",
        type=str,
        default=None,
        choices=["no", "fp16", "bf16"],
        help=(
            "Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
            " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
            " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
        ),
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ¨¡å‹é¢„æµ‹å’Œæ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•
    parser.add_argument(
        "--output_dir",
        type=str,
        default="cogvideox-lora",
        help="The output directory where the model predictions and checkpoints will be written.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘çš„é«˜åº¦
    parser.add_argument(
        "--height",
        type=int,
        default=480,
        help="All input videos are resized to this height.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘çš„å®½åº¦
    parser.add_argument(
        "--width",
        type=int,
        default=720,
        help="All input videos are resized to this width.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘çš„å¸§ç‡
    parser.add_argument("--fps", type=int, default=8, help="All input videos will be used at this FPS.")
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ‰€æœ‰è¾“å…¥è§†é¢‘å°†è¢«æˆªæ–­åˆ°çš„å¸§æ•°
    parser.add_argument(
        "--max_num_frames", type=int, default=49, help="All input videos will be truncated to these many frames."
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šä»æ¯ä¸ªè¾“å…¥è§†é¢‘å¼€å§‹è·³è¿‡çš„å¸§æ•°
    parser.add_argument(
        "--skip_frames_start",
        type=int,
        default=0,
        help="Number of frames to skip from the beginning of each input video. Useful if training data contains intro sequences.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šä»æ¯ä¸ªè¾“å…¥è§†é¢‘æœ«å°¾è·³è¿‡çš„å¸§æ•°
    parser.add_argument(
        "--skip_frames_end",
        type=int,
        default=0,
        help="Number of frames to skip from the end of each input video. Useful if training data contains outro sequences.",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šæ˜¯å¦éšæœºæ°´å¹³ç¿»è½¬è§†é¢‘
    parser.add_argument(
        "--random_flip",
        action="store_true",
        help="whether to randomly flip videos horizontally",
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šè®­ç»ƒæ•°æ®åŠ è½½å™¨çš„æ‰¹é‡å¤§å°ï¼ˆæ¯ä¸ªè®¾å¤‡ï¼‰
    parser.add_argument(
        "--train_batch_size", type=int, default=4, help="Batch size (per device) for the training dataloader."
    )
    # æ·»åŠ å‚æ•°ï¼ŒæŒ‡å®šè®­ç»ƒçš„å‘¨æœŸæ•°
    parser.add_argument("--num_train_epochs", type=int, default=1)
    # æ·»åŠ å‚æ•° `--max_train_steps`ï¼Œç”¨äºæŒ‡å®šè®­ç»ƒçš„æœ€å¤§æ­¥æ•°
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Total number of training steps to perform. If provided, overrides `--num_train_epochs`.",
    )
    # æ·»åŠ å‚æ•° `--checkpointing_steps`ï¼Œç”¨äºæŒ‡å®šæ¯ X æ¬¡æ›´æ–°ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    parser.add_argument(
        "--checkpointing_steps",
        type=int,
        default=500,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help=(
            "Save a checkpoint of the training state every X updates. These checkpoints can be used both as final"
            " checkpoints in case they are better than the last checkpoint, and are also suitable for resuming"
            " training using `--resume_from_checkpoint`."
        ),
    )
    # æ·»åŠ å‚æ•° `--checkpoints_total_limit`ï¼Œç”¨äºæŒ‡å®šè¦å­˜å‚¨çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡
    parser.add_argument(
        "--checkpoints_total_limit",
        type=int,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help=("Max number of checkpoints to store."),
    )
    # æ·»åŠ å‚æ•° `--resume_from_checkpoint`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦ä»ä¹‹å‰çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help=(
            "Whether training should be resumed from a previous checkpoint. Use a path saved by"
            ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
        ),
    )
    # æ·»åŠ å‚æ•° `--gradient_accumulation_steps`ï¼Œç”¨äºæŒ‡å®šåœ¨æ‰§è¡Œåå‘ä¼ æ’­å’Œæ›´æ–°ä¹‹å‰ç§¯ç´¯çš„æ›´æ–°æ­¥æ•°
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    # æ·»åŠ å‚æ•° `--gradient_checkpointing`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.",
    )
    # æ·»åŠ å‚æ•° `--learning_rate`ï¼Œç”¨äºæŒ‡å®šåˆå§‹å­¦ä¹ ç‡
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    # æ·»åŠ å‚æ•° `--scale_lr`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦æŒ‰ GPU æ•°é‡ã€æ¢¯åº¦ç§¯ç´¯æ­¥æ•°å’Œæ‰¹é‡å¤§å°ç¼©æ”¾å­¦ä¹ ç‡
    parser.add_argument(
        "--scale_lr",
        action="store_true",
        default=False,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.",
    )
    # æ·»åŠ å‚æ•° `--lr_scheduler`ï¼Œç”¨äºæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„ç±»å‹
    parser.add_argument(
        "--lr_scheduler",
        type=str,
        default="constant",
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„å¯é€‰å€¼
        help=(
            'The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
            ' "constant", "constant_with_warmup"]'
        ),
    )
    # æ·»åŠ å‚æ•° `--lr_warmup_steps`ï¼Œç”¨äºæŒ‡å®šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„é¢„çƒ­æ­¥æ•°
    parser.add_argument(
        "--lr_warmup_steps", type=int, default=500, 
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Number of steps for the warmup in the lr scheduler."
    )
    # æ·»åŠ å‚æ•° `--lr_num_cycles`ï¼Œç”¨äºæŒ‡å®šåœ¨ `cosine_with_restarts` è°ƒåº¦å™¨ä¸­çš„ç¡¬é‡ç½®æ¬¡æ•°
    parser.add_argument(
        "--lr_num_cycles",
        type=int,
        default=1,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Number of hard resets of the lr in cosine_with_restarts scheduler.",
    )
    # æ·»åŠ å‚æ•° `--lr_power`ï¼Œç”¨äºæŒ‡å®šå¤šé¡¹å¼è°ƒåº¦å™¨çš„å¹‚å› å­
    parser.add_argument("--lr_power", type=float, default=1.0, 
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Power factor of the polynomial scheduler."
    )
    # æ·»åŠ å‚æ•° `--enable_slicing`ï¼Œç”¨äºæŒ‡å®šæ˜¯å¦ä½¿ç”¨ VAE åˆ‡ç‰‡ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_slicing",
        action="store_true",
        default=False,
        # å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜è¯¥å‚æ•°çš„ç”¨é€”
        help="Whether or not to use VAE slicing for saving memory.",
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºå¯ç”¨æˆ–ç¦ç”¨ VAE ç“¦ç‰‡åŠŸèƒ½ä»¥èŠ‚çœå†…å­˜
    parser.add_argument(
        "--enable_tiling",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º False
        default=False,
        help="Whether or not to use VAE tiling for saving memory.",  # å‚æ•°è¯´æ˜
    )

    # ä¼˜åŒ–å™¨é…ç½®
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œé€‰æ‹©ä¼˜åŒ–å™¨ç±»å‹
    parser.add_argument(
        "--optimizer",
        type=lambda s: s.lower(),  # å°†è¾“å…¥è½¬ä¸ºå°å†™
        default="adam",  # é»˜è®¤ä½¿ç”¨ Adam ä¼˜åŒ–å™¨
        choices=["adam", "adamw", "prodigy"],  # å¯é€‰çš„ä¼˜åŒ–å™¨ç±»å‹
        help=("The optimizer type to use."),  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ 8-bit Adam ä¼˜åŒ–å™¨
    parser.add_argument(
        "--use_8bit_adam",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹
        help="Whether or not to use 8-bit Adam from bitsandbytes. Ignored if optimizer is not set to AdamW",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam ä¼˜åŒ–å™¨çš„ beta1 å‚æ•°
    parser.add_argument(
        "--adam_beta1", type=float, default=0.9, help="The beta1 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam ä¼˜åŒ–å™¨çš„ beta2 å‚æ•°
    parser.add_argument(
        "--adam_beta2", type=float, default=0.95, help="The beta2 parameter for the Adam and Prodigy optimizers."
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Prodigy ä¼˜åŒ–å™¨çš„ beta3 å‚æ•°
    parser.add_argument(
        "--prodigy_beta3",
        type=float,  # å‚æ•°ç±»å‹ä¸ºæµ®ç‚¹æ•°
        default=None,  # é»˜è®¤å€¼ä¸º None
        help="Coefficients for computing the Prodigy optimizer's stepsize using running averages. If set to None, uses the value of square root of beta2.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ AdamW é£æ ¼çš„è§£è€¦æƒé‡è¡°å‡
    parser.add_argument("--prodigy_decouple", action="store_true", help="Use AdamW style decoupled weight decay")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡
    parser.add_argument("--adam_weight_decay", type=float, default=1e-04, help="Weight decay to use for unet params")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½® Adam å’Œ Prodigy ä¼˜åŒ–å™¨çš„ epsilon å€¼
    parser.add_argument(
        "--adam_epsilon",
        type=float,
        default=1e-08,  # é»˜è®¤ epsilon å€¼
        help="Epsilon value for the Adam optimizer and Prodigy optimizers.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®æœ€å¤§æ¢¯åº¦èŒƒæ•°
    parser.add_argument("--max_grad_norm", default=1.0, type=float, help="Max gradient norm.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦å¼€å¯ Adam çš„åå·®ä¿®æ­£
    parser.add_argument("--prodigy_use_bias_correction", action="store_true", help="Turn on Adam's bias correction.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦åœ¨æš–å¯åŠ¨é˜¶æ®µç§»é™¤ lr åœ¨ D ä¼°è®¡çš„åˆ†æ¯ä¸­
    parser.add_argument(
        "--prodigy_safeguard_warmup",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹
        help="Remove lr from the denominator of D estimate to avoid issues during warm-up stage.",  # å‚æ•°è¯´æ˜
    )

    # å…¶ä»–ä¿¡æ¯
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®é¡¹ç›®è¿½è¸ªå™¨åç§°
    parser.add_argument("--tracker_name", type=str, default=None, help="Project tracker name")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦å°†æ¨¡å‹æ¨é€åˆ° Hub
    parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®æ¨é€åˆ°æ¨¡å‹ Hub æ—¶ä½¿ç”¨çš„ token
    parser.add_argument("--hub_token", type=str, default=None, help="The token to use to push to the Model Hub.")
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®ä¸æœ¬åœ° output_dir åŒæ­¥çš„ä»“åº“åç§°
    parser.add_argument(
        "--hub_model_id",
        type=str,
        default=None,  # é»˜è®¤å€¼ä¸º None
        help="The name of the repository to keep in sync with the local `output_dir`.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œè®¾ç½®æ—¥å¿—å­˜å‚¨ç›®å½•
    parser.add_argument(
        "--logging_dir",
        type=str,
        default="logs",  # é»˜è®¤æ—¥å¿—ç›®å½•
        help="Directory where logs are stored.",  # å‚æ•°è¯´æ˜
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šæ˜¯å¦å…è®¸åœ¨ Ampere GPU ä¸Šä½¿ç”¨ TF32
    parser.add_argument(
        "--allow_tf32",
        action="store_true",  # æŒ‡å®šè¯¥å‚æ•°ä¸ºå¸ƒå°”ç±»å‹
        help=(
            "Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see"
            " https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices"  # å‚æ•°è¯´æ˜
        ),
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•° '--report_to' çš„é…ç½®
        parser.add_argument(
            # å‚æ•°åç§°
            "--report_to",
            # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
            type=str,
            # é»˜è®¤å€¼ä¸º None
            default=None,
            # å‚æ•°å¸®åŠ©ä¿¡æ¯ï¼Œè§£é‡Šè¯¥å‚æ•°çš„ç”¨é€”
            help=(
                # æä¾›æ”¯æŒçš„å¹³å°å’Œé»˜è®¤å€¼è¯´æ˜
                'The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
                # ç»§ç»­å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜å¯é€‰çš„å¹³å°
                ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
            ),
        )
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ç»“æœ
        return parser.parse_args()
# å®šä¹‰ä¸€ä¸ªè§†é¢‘æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ª Dataset
class VideoDataset(Dataset):
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®æ•°æ®é›†çš„å„ç§å‚æ•°
    def __init__(
        self,
        instance_data_root: Optional[str] = None,  # å®ä¾‹æ•°æ®æ ¹ç›®å½•ï¼Œå¯é€‰
        dataset_name: Optional[str] = None,  # æ•°æ®é›†åç§°ï¼Œå¯é€‰
        dataset_config_name: Optional[str] = None,  # æ•°æ®é›†é…ç½®åç§°ï¼Œå¯é€‰
        caption_column: str = "text",  # æè¿°æ–‡æœ¬æ‰€åœ¨åˆ—åï¼Œé»˜è®¤ä¸º "text"
        video_column: str = "video",  # è§†é¢‘æ•°æ®æ‰€åœ¨åˆ—åï¼Œé»˜è®¤ä¸º "video"
        height: int = 480,  # è§†é¢‘é«˜åº¦ï¼Œé»˜è®¤ä¸º 480
        width: int = 720,  # è§†é¢‘å®½åº¦ï¼Œé»˜è®¤ä¸º 720
        fps: int = 8,  # æ¯ç§’å¸§æ•°ï¼Œé»˜è®¤ä¸º 8
        max_num_frames: int = 49,  # æœ€å¤§å¸§æ•°ï¼Œé»˜è®¤ä¸º 49
        skip_frames_start: int = 0,  # å¼€å§‹è·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
        skip_frames_end: int = 0,  # ç»“æŸè·³è¿‡çš„å¸§æ•°ï¼Œé»˜è®¤ä¸º 0
        cache_dir: Optional[str] = None,  # ç¼“å­˜ç›®å½•ï¼Œå¯é€‰
        id_token: Optional[str] = None,  # ID ä»¤ç‰Œï¼Œå¯é€‰
    ) -> None:
        super().__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•

        # å¦‚æœæä¾›äº†å®ä¾‹æ•°æ®æ ¹ç›®å½•ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸º Path å¯¹è±¡
        self.instance_data_root = Path(instance_data_root) if instance_data_root is not None else None
        # è®¾ç½®æ•°æ®é›†åç§°
        self.dataset_name = dataset_name
        # è®¾ç½®æ•°æ®é›†é…ç½®åç§°
        self.dataset_config_name = dataset_config_name
        # è®¾ç½®æè¿°æ–‡æœ¬æ‰€åœ¨åˆ—å
        self.caption_column = caption_column
        # è®¾ç½®è§†é¢‘æ•°æ®æ‰€åœ¨åˆ—å
        self.video_column = video_column
        # è®¾ç½®è§†é¢‘é«˜åº¦
        self.height = height
        # è®¾ç½®è§†é¢‘å®½åº¦
        self.width = width
        # è®¾ç½®æ¯ç§’å¸§æ•°
        self.fps = fps
        # è®¾ç½®æœ€å¤§å¸§æ•°
        self.max_num_frames = max_num_frames
        # è®¾ç½®å¼€å§‹è·³è¿‡çš„å¸§æ•°
        self.skip_frames_start = skip_frames_start
        # è®¾ç½®ç»“æŸè·³è¿‡çš„å¸§æ•°
        self.skip_frames_end = skip_frames_end
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        # è®¾ç½® ID ä»¤ç‰Œï¼Œå¦‚æœæœªæä¾›åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
        self.id_token = id_token or ""

        # å¦‚æœæä¾›äº†æ•°æ®é›†åç§°ï¼Œåˆ™ä»æ•°æ®é›†ä¸­åŠ è½½å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„
        if dataset_name is not None:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_hub()
        # å¦åˆ™ä»æœ¬åœ°è·¯å¾„åŠ è½½å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„
        else:
            self.instance_prompts, self.instance_video_paths = self._load_dataset_from_local_path()

        # è®¡ç®—å®ä¾‹è§†é¢‘çš„æ•°é‡
        self.num_instance_videos = len(self.instance_video_paths)
        # æ£€æŸ¥å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„æ•°é‡æ˜¯å¦åŒ¹é…
        if self.num_instance_videos != len(self.instance_prompts):
            raise ValueError(
                # æŠ›å‡ºé”™è¯¯ï¼Œæç¤ºå®ä¾‹æç¤ºå’Œè§†é¢‘æ•°é‡ä¸åŒ¹é…
                f"Expected length of instance prompts and videos to be the same but found {len(self.instance_prompts)=} and {len(self.instance_video_paths)=}. Please ensure that the number of caption prompts and videos match in your dataset."
            )

        # é¢„å¤„ç†æ•°æ®ä»¥è·å–å®ä¾‹è§†é¢‘
        self.instance_videos = self._preprocess_data()

    # è¿”å›æ•°æ®é›†çš„é•¿åº¦
    def __len__(self):
        return self.num_instance_videos

    # æ ¹æ®ç´¢å¼•è·å–å®ä¾‹æ•°æ®
    def __getitem__(self, index):
        return {
            # è¿”å›ç»„åˆåçš„å®ä¾‹æç¤º
            "instance_prompt": self.id_token + self.instance_prompts[index],
            # è¿”å›å¯¹åº”çš„å®ä¾‹è§†é¢‘
            "instance_video": self.instance_videos[index],
        }
    # ä»æ•°æ®é›†ä¸­å¿ƒåŠ è½½æ•°æ®é›†çš„ç§æœ‰æ–¹æ³•
    def _load_dataset_from_hub(self):
        try:
            # å°è¯•å¯¼å…¥ datasets åº“
            from datasets import load_dataset
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡º ImportErrorï¼Œå¹¶æä¾›å®‰è£…æç¤º
            raise ImportError(
                "You are trying to load your data using the datasets library. If you wish to train using custom "
                "captions please install the datasets library: `pip install datasets`. If you wish to load a "
                "local folder containing images only, specify --instance_data_root instead."
            )

        # ä»æ•°æ®é›†ä¸­å¿ƒä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†ï¼Œå…³äºå¦‚ä½•åŠ è½½è‡ªå®šä¹‰å›¾åƒçš„ä¿¡æ¯è§é“¾æ¥
        # https://huggingface.co/docs/datasets/v2.0.0/en/dataset_script
        dataset = load_dataset(
            self.dataset_name,  # æ•°æ®é›†åç§°
            self.dataset_config_name,  # æ•°æ®é›†é…ç½®åç§°
            cache_dir=self.cache_dir,  # ç¼“å­˜ç›®å½•
        )
        # è·å–è®­ç»ƒé›†çš„åˆ—å
        column_names = dataset["train"].column_names

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè§†é¢‘åˆ—
        if self.video_column is None:
            # é»˜è®¤ä½¿ç”¨åˆ—ååˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªåˆ—åä½œä¸ºè§†é¢‘åˆ—
            video_column = column_names[0]
            # è®°å½•ä½¿ç”¨é»˜è®¤è§†é¢‘åˆ—çš„ä¿¡æ¯
            logger.info(f"`video_column` defaulting to {video_column}")
        else:
            # å¦‚æœå·²æŒ‡å®šè§†é¢‘åˆ—ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„åˆ—å
            video_column = self.video_column
            # æ£€æŸ¥æŒ‡å®šçš„è§†é¢‘åˆ—æ˜¯å¦åœ¨åˆ—åä¸­
            if video_column not in column_names:
                # å¦‚æœä¸åœ¨ï¼ŒæŠ›å‡º ValueError
                raise ValueError(
                    f"`--video_column` value '{video_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                )

        # å¦‚æœæ²¡æœ‰æŒ‡å®šå­—å¹•åˆ—
        if self.caption_column is None:
            # é»˜è®¤ä½¿ç”¨åˆ—ååˆ—è¡¨ä¸­çš„ç¬¬äºŒä¸ªåˆ—åä½œä¸ºå­—å¹•åˆ—
            caption_column = column_names[1]
            # è®°å½•ä½¿ç”¨é»˜è®¤å­—å¹•åˆ—çš„ä¿¡æ¯
            logger.info(f"`caption_column` defaulting to {caption_column}")
        else:
            # å¦‚æœå·²æŒ‡å®šå­—å¹•åˆ—ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„åˆ—å
            caption_column = self.caption_column
            # æ£€æŸ¥æŒ‡å®šçš„å­—å¹•åˆ—æ˜¯å¦åœ¨åˆ—åä¸­
            if self.caption_column not in column_names:
                # å¦‚æœä¸åœ¨ï¼ŒæŠ›å‡º ValueError
                raise ValueError(
                    f"`--caption_column` value '{self.caption_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}"
                )

        # ä»è®­ç»ƒé›†ä¸­æå–å®ä¾‹æç¤ºï¼ˆå­—å¹•ï¼‰
        instance_prompts = dataset["train"][caption_column]
        # æ ¹æ®è§†é¢‘åˆ—çš„æ–‡ä»¶è·¯å¾„åˆ›å»ºè§†é¢‘å®ä¾‹åˆ—è¡¨
        instance_videos = [Path(self.instance_data_root, filepath) for filepath in dataset["train"][video_column]]

        # è¿”å›å®ä¾‹æç¤ºå’Œè§†é¢‘å®ä¾‹åˆ—è¡¨
        return instance_prompts, instance_videos
    # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†
        def _load_dataset_from_local_path(self):
            # æ£€æŸ¥å®ä¾‹æ•°æ®æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.instance_data_root.exists():
                # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError("Instance videos root folder does not exist")
    
            # æ„å»ºæç¤ºæ–‡ä»¶è·¯å¾„
            prompt_path = self.instance_data_root.joinpath(self.caption_column)
            # æ„å»ºè§†é¢‘æ–‡ä»¶è·¯å¾„
            video_path = self.instance_data_root.joinpath(self.video_column)
    
            # æ£€æŸ¥æç¤ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ˜¯æ–‡ä»¶
            if not prompt_path.exists() or not prompt_path.is_file():
                # å¦‚æœä¸æ˜¯ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError(
                    "Expected `--caption_column` to be path to a file in `--instance_data_root` containing line-separated text prompts."
                )
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ˜¯æ–‡ä»¶
            if not video_path.exists() or not video_path.is_file():
                # å¦‚æœä¸æ˜¯ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError(
                    "Expected `--video_column` to be path to a file in `--instance_data_root` containing line-separated paths to video data in the same directory."
                )
    
            # æ‰“å¼€æç¤ºæ–‡ä»¶å¹¶è¯»å–æ¯è¡Œï¼Œå»é™¤é¦–å°¾ç©ºç™½ï¼Œå½¢æˆæç¤ºåˆ—è¡¨
            with open(prompt_path, "r", encoding="utf-8") as file:
                instance_prompts = [line.strip() for line in file.readlines() if len(line.strip()) > 0]
            # æ‰“å¼€è§†é¢‘æ–‡ä»¶å¹¶è¯»å–æ¯è¡Œï¼Œå»é™¤é¦–å°¾ç©ºç™½ï¼Œå½¢æˆè§†é¢‘è·¯å¾„åˆ—è¡¨
            with open(video_path, "r", encoding="utf-8") as file:
                instance_videos = [
                    self.instance_data_root.joinpath(line.strip()) for line in file.readlines() if len(line.strip()) > 0
                ]
    
            # æ£€æŸ¥è§†é¢‘è·¯å¾„åˆ—è¡¨ä¸­æ˜¯å¦æœ‰æ— æ•ˆæ–‡ä»¶è·¯å¾„
            if any(not path.is_file() for path in instance_videos):
                # å¦‚æœæœ‰ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
                raise ValueError(
                    "Expected '--video_column' to be a path to a file in `--instance_data_root` containing line-separated paths to video data but found atleast one path that is not a valid file."
                )
    
            # è¿”å›å®ä¾‹æç¤ºå’Œè§†é¢‘è·¯å¾„åˆ—è¡¨
            return instance_prompts, instance_videos
    # å®šä¹‰æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•
    def _preprocess_data(self):
        # å°è¯•å¯¼å…¥ decord åº“
        try:
            import decord
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™æŠ›å‡ºé”™è¯¯ï¼Œæç¤ºéœ€è¦å®‰è£… decord
        except ImportError:
            raise ImportError(
                "The `decord` package is required for loading the video dataset. Install with `pip install decord`"
            )

        # è®¾ç½® decord ä½¿ç”¨ PyTorch ä½œä¸ºåç«¯
        decord.bridge.set_bridge("torch")

        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨è§†é¢‘æ•°æ®
        videos = []
        # å®šä¹‰è®­ç»ƒæ—¶çš„è½¬æ¢æ“ä½œ
        train_transforms = transforms.Compose(
            [
                # å°†åƒç´ å€¼å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
                transforms.Lambda(lambda x: x / 255.0 * 2.0 - 1.0),
            ]
        )

        # éå†æ¯ä¸ªè§†é¢‘æ–‡ä»¶çš„è·¯å¾„
        for filename in self.instance_video_paths:
            # ä½¿ç”¨ decord è¯»å–è§†é¢‘ï¼ŒæŒ‡å®šå®½åº¦å’Œé«˜åº¦
            video_reader = decord.VideoReader(uri=filename.as_posix(), width=self.width, height=self.height)
            # è·å–è§†é¢‘çš„å¸§æ•°
            video_num_frames = len(video_reader)

            # è®¡ç®—å¼€å§‹å¸§å’Œç»“æŸå¸§çš„ç´¢å¼•
            start_frame = min(self.skip_frames_start, video_num_frames)
            end_frame = max(0, video_num_frames - self.skip_frames_end)
            # å¦‚æœç»“æŸå¸§å°äºç­‰äºå¼€å§‹å¸§ï¼Œåªè·å–å¼€å§‹å¸§çš„å¸§æ•°æ®
            if end_frame <= start_frame:
                frames = video_reader.get_batch([start_frame])
            # å¦‚æœè¦è·å–çš„å¸§æ•°é‡åœ¨å…è®¸çš„æœ€å¤§èŒƒå›´å†…
            elif end_frame - start_frame <= self.max_num_frames:
                frames = video_reader.get_batch(list(range(start_frame, end_frame)))
            # å¦‚æœè¦è·å–çš„å¸§æ•°é‡è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œåˆ™æŒ‰æ­¥é•¿è·å–
            else:
                indices = list(range(start_frame, end_frame, (end_frame - start_frame) // self.max_num_frames))
                frames = video_reader.get_batch(indices)

            # ç¡®ä¿å¸§æ•°é‡ä¸è¶…è¿‡æœ€å¤§é™åˆ¶
            frames = frames[: self.max_num_frames]
            # è·å–å½“å‰é€‰æ‹©çš„å¸§æ•°é‡
            selected_num_frames = frames.shape[0]

            # é€‰æ‹©å‰ (4k + 1) å¸§ï¼Œä»¥æ»¡è¶³ VAE çš„éœ€æ±‚
            remainder = (3 + (selected_num_frames % 4)) % 4
            # å¦‚æœå¸§æ•°é‡ä¸æ˜¯ 4 çš„å€æ•°ï¼Œå»æ‰å¤šä½™çš„å¸§
            if remainder != 0:
                frames = frames[:-remainder]
            # æ›´æ–°é€‰æ‹©çš„å¸§æ•°é‡
            selected_num_frames = frames.shape[0]

            # ç¡®ä¿é€‰æ‹©çš„å¸§æ•°é‡å‡ 1 æ˜¯ 4 çš„å€æ•°
            assert (selected_num_frames - 1) % 4 == 0

            # åº”ç”¨è®­ç»ƒè½¬æ¢æ“ä½œ
            frames = frames.float()
            # å°†æ¯ä¸€å¸§åº”ç”¨è½¬æ¢å¹¶å †å æˆä¸€ä¸ªæ–°çš„å¼ é‡
            frames = torch.stack([train_transforms(frame) for frame in frames], dim=0)
            # å°†å¤„ç†åçš„å¸§æŒ‰ç…§ [F, C, H, W] çš„é¡ºåºæ’åˆ—å¹¶å­˜å…¥è§†é¢‘åˆ—è¡¨
            videos.append(frames.permute(0, 3, 1, 2).contiguous())  # [F, C, H, W]

        # è¿”å›å¤„ç†åçš„è§†é¢‘æ•°æ®
        return videos
# ä¿å­˜æ¨¡å‹å¡ç‰‡ä¿¡æ¯
def save_model_card(
    repo_id: str,  # æ¨¡å‹ä»“åº“çš„æ ‡è¯†
    videos=None,  # å¯é€‰çš„è§†é¢‘åˆ—è¡¨
    base_model: str = None,  # åŸºç¡€æ¨¡å‹çš„åç§°ï¼Œé»˜è®¤ä¸º None
    validation_prompt=None,  # éªŒè¯æ—¶ä½¿ç”¨çš„æç¤ºè¯­
    repo_folder=None,  # æ¨¡å‹å­˜å‚¨çš„æ–‡ä»¶å¤¹è·¯å¾„
    fps=8,  # è§†é¢‘å¸§ç‡ï¼Œé»˜è®¤ä¸º 8
):
    widget_dict = []  # åˆå§‹åŒ–å°éƒ¨ä»¶å­—å…¸ï¼Œç”¨äºå­˜å‚¨è§†é¢‘ä¿¡æ¯
    if videos is not None:  # æ£€æŸ¥è§†é¢‘åˆ—è¡¨æ˜¯å¦ä¸ä¸ºç©º
        for i, video in enumerate(videos):  # éå†è§†é¢‘åˆ—è¡¨ï¼Œè·å–ç´¢å¼•å’Œè§†é¢‘å¯¹è±¡
            # å°†è§†é¢‘å¯¼å‡ºåˆ°æŒ‡å®šè·¯å¾„ï¼Œå¹¶è®¾ç½®å¸§ç‡
            export_to_video(video, os.path.join(repo_folder, f"final_video_{i}.mp4", fps=fps))
            # å°†è§†é¢‘ä¿¡æ¯æ·»åŠ åˆ°å°éƒ¨ä»¶å­—å…¸ä¸­
            widget_dict.append(
                {"text": validation_prompt if validation_prompt else " ", "output": {"url": f"video_{i}.mp4"}}
            )

    # å®šä¹‰æ¨¡å‹æè¿°ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹ ID å’ŒåŸºç¡€æ¨¡å‹åç§°
    model_description = f"""
# CogVideoX LoRA - {repo_id}

<Gallery />

## Model description

These are {repo_id} LoRA weights for {base_model}.

The weights were trained using the [CogVideoX Diffusers trainer](https://github.com/huggingface/diffusers/blob/main/examples/cogvideo/train_cogvideox_lora.py).

Was LoRA for the text encoder enabled? No.

## Download model

[Download the *.safetensors LoRA]({repo_id}/tree/main) in the Files & versions tab.

## Use it with the [ğŸ§¨ diffusers library](https://github.com/huggingface/diffusers)


from diffusers import CogVideoXPipeline  # å¯¼å…¥ CogVideoXPipeline ç±»
import torch  # å¯¼å…¥ PyTorch åº“

# ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ç®¡é“ï¼Œè®¾ç½®æ•°æ®ç±»å‹ä¸º bfloat16ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ° CUDA
pipe = CogVideoXPipeline.from_pretrained("THUDM/CogVideoX-5b", torch_dtype=torch.bfloat16).to("cuda")
# åŠ è½½ LoRA æƒé‡ï¼ŒæŒ‡å®šæƒé‡æ–‡ä»¶åå’Œé€‚é…å™¨åç§°
pipe.load_lora_weights("{repo_id}", weight_name="pytorch_lora_weights.safetensors", adapter_name=["cogvideox-lora"])

# LoRA é€‚é…å™¨æƒé‡æ˜¯åŸºäºè®­ç»ƒæ—¶ä½¿ç”¨çš„å‚æ•°ç¡®å®šçš„ã€‚
# åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡è®¾ `--lora_alpha` æ˜¯ 32ï¼Œ`--rank` æ˜¯ 64ã€‚
# å¯ä»¥æ ¹æ®è®­ç»ƒä¸­ä½¿ç”¨çš„å€¼è¿›è¡Œè°ƒæ•´ï¼Œä»¥å‡å°æˆ–æ”¾å¤§ LoRA çš„æ•ˆæœ
# è¶…è¿‡ä¸€å®šçš„å®¹å¿åº¦ï¼Œå¯èƒ½ä¼šæ³¨æ„åˆ°æ²¡æœ‰æ•ˆæœæˆ–æº¢å‡ºã€‚
pipe.set_adapters(["cogvideox-lora"], [32 / 64])

# ä½¿ç”¨ç®¡é“ç”Ÿæˆè§†é¢‘ï¼Œä¼ å…¥éªŒè¯æç¤ºï¼Œè®¾ç½®æŒ‡å¯¼æ¯”ä¾‹ï¼Œå¹¶å¯ç”¨åŠ¨æ€é…ç½®
video = pipe("{validation_prompt}", guidance_scale=6, use_dynamic_cfg=True).frames[0]


# æ›´å¤šç»†èŠ‚ï¼ŒåŒ…æ‹¬æƒé‡ã€åˆå¹¶å’Œèåˆ LoRAï¼Œè¯·æŸ¥çœ‹ [diffusers ä¸­åŠ è½½ LoRA çš„æ–‡æ¡£](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)

## License

è¯·éµå®ˆ [æ­¤å¤„](https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE) å’Œ [æ­¤å¤„](https://huggingface.co/THUDM/CogVideoX-2b/blob/main/LICENSE) ä¸­æè¿°çš„è®¸å¯æ¡æ¬¾ã€‚
"""
    # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡ï¼Œä¼ å…¥å¿…è¦å‚æ•°
    model_card = load_or_create_model_card(
        repo_id_or_path=repo_id,  # æ¨¡å‹ ID æˆ–è·¯å¾„
        from_training=True,  # æŒ‡ç¤ºä»è®­ç»ƒç”Ÿæˆ
        license="other",  # è®¾ç½®è®¸å¯è¯ç±»å‹
        base_model=base_model,  # åŸºç¡€æ¨¡å‹åç§°
        prompt=validation_prompt,  # éªŒè¯æç¤º
        model_description=model_description,  # æ¨¡å‹æè¿°
        widget=widget_dict,  # å°éƒ¨ä»¶ä¿¡æ¯
    )
    # å®šä¹‰æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºæ ‡è¯†æ¨¡å‹ç‰¹æ€§
    tags = [
        "text-to-video",  # æ–‡æœ¬è½¬è§†é¢‘
        "diffusers-training",  # Diffusers è®­ç»ƒ
        "diffusers",  # Diffusers
        "lora",  # LoRA
        "cogvideox",  # CogVideoX
        "cogvideox-diffusers",  # CogVideoX Diffusers
        "template:sd-lora",  # æ¨¡æ¿ç±»å‹
    ]

    # å¡«å……æ¨¡å‹å¡ç‰‡çš„æ ‡ç­¾
    model_card = populate_model_card(model_card, tags=tags)
    # ä¿å­˜æ¨¡å‹å¡ç‰‡åˆ°æŒ‡å®šè·¯å¾„
    model_card.save(os.path.join(repo_folder, "README.md"))


# è®°å½•éªŒè¯ç»“æœ
def log_validation(
    pipe,  # è§†é¢‘ç”Ÿæˆç®¡é“
    args,  # å…¶ä»–å‚æ•°
    accelerator,  # åŠ é€Ÿå™¨å®ä¾‹
    pipeline_args,  # ç®¡é“å‚æ•°
    epoch,  # å½“å‰è®­ç»ƒçš„è½®æ¬¡
    is_final_validation: bool = False,  # æ˜¯å¦ä¸ºæœ€ç»ˆéªŒè¯
):
    # è®°å½•æ­£åœ¨è¿è¡ŒéªŒè¯çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”Ÿæˆè§†é¢‘çš„æ•°é‡å’Œæç¤ºå†…å®¹
        logger.info(
            f"Running validation... \n Generating {args.num_validation_videos} videos with prompt: {pipeline_args['prompt']}."
        )
        # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨è°ƒåº¦å™¨çš„å‚æ•°
        scheduler_args = {}
    
        # æ£€æŸ¥è°ƒåº¦å™¨é…ç½®ä¸­æ˜¯å¦åŒ…å«æ–¹å·®ç±»å‹
        if "variance_type" in pipe.scheduler.config:
            # è·å–æ–¹å·®ç±»å‹
            variance_type = pipe.scheduler.config.variance_type
    
            # å¦‚æœæ–¹å·®ç±»å‹æ˜¯â€œlearnedâ€æˆ–â€œlearned_rangeâ€ï¼Œåˆ™å°†å…¶æ›´æ”¹ä¸ºâ€œfixed_smallâ€
            if variance_type in ["learned", "learned_range"]:
                variance_type = "fixed_small"
    
            # å°†æ–¹å·®ç±»å‹æ·»åŠ åˆ°è°ƒåº¦å™¨å‚æ•°ä¸­
            scheduler_args["variance_type"] = variance_type
    
        # æ ¹æ®è°ƒåº¦å™¨é…ç½®å’Œå‚æ•°åˆ›å»ºæ–°çš„è°ƒåº¦å™¨
        pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, **scheduler_args)
        # å°†ç®¡é“ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š
        pipe = pipe.to(accelerator.device)
        # å…³é—­è¿›åº¦æ¡é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼‰
        # pipe.set_progress_bar_config(disable=True)
    
        # è¿è¡Œæ¨ç†ï¼Œåˆ›å»ºéšæœºæ•°ç”Ÿæˆå™¨ï¼Œè®¾ç½®ç§å­
        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
    
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ä»¥å­˜å‚¨ç”Ÿæˆçš„è§†é¢‘
        videos = []
        # æ ¹æ®éœ€è¦ç”ŸæˆæŒ‡å®šæ•°é‡çš„è§†é¢‘
        for _ in range(args.num_validation_videos):
            # è°ƒç”¨ç®¡é“ç”Ÿæˆè§†é¢‘ï¼Œè·å–ç¬¬ä¸€å¸§
            video = pipe(**pipeline_args, generator=generator, output_type="np").frames[0]
            # å°†ç”Ÿæˆçš„è§†é¢‘æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            videos.append(video)
    
        # éå†æ‰€æœ‰è·Ÿè¸ªå™¨
        for tracker in accelerator.trackers:
            # æ ¹æ®æ˜¯å¦ä¸ºæœ€ç»ˆéªŒè¯é€‰æ‹©é˜¶æ®µåç§°
            phase_name = "test" if is_final_validation else "validation"
            # æ£€æŸ¥è·Ÿè¸ªå™¨åç§°æ˜¯å¦ä¸ºâ€œwandbâ€
            if tracker.name == "wandb":
                # åˆå§‹åŒ–è§†é¢‘æ–‡ä»¶ååˆ—è¡¨
                video_filenames = []
                # éå†ç”Ÿæˆçš„è§†é¢‘åˆ—è¡¨
                for i, video in enumerate(videos):
                    # å¤„ç†æç¤ºæ–‡æœ¬ä»¥åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
                    prompt = (
                        pipeline_args["prompt"][:25]
                        .replace(" ", "_")
                        .replace(" ", "_")
                        .replace("'", "_")
                        .replace('"', "_")
                        .replace("/", "_")
                    )
                    # åˆ›å»ºè§†é¢‘æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                    filename = os.path.join(args.output_dir, f"{phase_name}_video_{i}_{prompt}.mp4")
                    # å°†è§†é¢‘å¯¼å‡ºä¸ºæ–‡ä»¶
                    export_to_video(video, filename, fps=8)
                    # å°†æ–‡ä»¶åæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                    video_filenames.append(filename)
    
                # è®°å½•è§†é¢‘åˆ° wandb
                tracker.log(
                    {
                        phase_name: [
                            wandb.Video(filename, caption=f"{i}: {pipeline_args['prompt']}")
                            for i, filename in enumerate(video_filenames)
                        ]
                    }
                )
    
        # é‡Šæ”¾å†…å­˜
        free_memory()
    
        # è¿”å›ç”Ÿæˆçš„è§†é¢‘åˆ—è¡¨
        return videos
# è·å– T5 æ¨¡å‹çš„æç¤ºåµŒå…¥
def _get_t5_prompt_embeds(
    # å®šä¹‰ T5 ä»¤ç‰ŒåŒ–å™¨
    tokenizer: T5Tokenizer,
    # å®šä¹‰ T5 ç¼–ç å™¨æ¨¡å‹
    text_encoder: T5EncoderModel,
    # æç¤ºæ–‡æœ¬ï¼Œå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
    prompt: Union[str, List[str]],
    # æ¯ä¸ªæç¤ºç”Ÿæˆè§†é¢‘çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 1
    num_videos_per_prompt: int = 1,
    # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸º 226
    max_sequence_length: int = 226,
    # æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ï¼Œå¯é€‰
    device: Optional[torch.device] = None,
    # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚ float32ï¼‰ï¼Œå¯é€‰
    dtype: Optional[torch.dtype] = None,
    # é¢„å…ˆæä¾›çš„æ–‡æœ¬è¾“å…¥ IDï¼Œå¯é€‰
    text_input_ids=None,
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è·å–æç¤ºçš„æ‰¹å¤„ç†å¤§å°
    batch_size = len(prompt)

    # å¦‚æœæä¾›äº†ä»¤ç‰ŒåŒ–å™¨
    if tokenizer is not None:
        # ä½¿ç”¨ä»¤ç‰ŒåŒ–å™¨å¯¹æç¤ºè¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå¼ é‡
        text_inputs = tokenizer(
            prompt,
            padding="max_length",  # å¡«å……åˆ°æœ€å¤§é•¿åº¦
            max_length=max_sequence_length,  # æœ€å¤§é•¿åº¦
            truncation=True,  # è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æˆªæ–­
            add_special_tokens=True,  # æ·»åŠ ç‰¹æ®Šä»¤ç‰Œ
            return_tensors="pt",  # è¿”å› PyTorch å¼ é‡
        )
        # è·å–æ–‡æœ¬è¾“å…¥ ID
        text_input_ids = text_inputs.input_ids
    else:
        # å¦‚æœæ²¡æœ‰ä»¤ç‰ŒåŒ–å™¨ä¸”æœªæä¾›æ–‡æœ¬è¾“å…¥ IDï¼ŒæŠ›å‡ºé”™è¯¯
        if text_input_ids is None:
            raise ValueError("`text_input_ids` must be provided when the tokenizer is not specified.")

    # å°†æ–‡æœ¬è¾“å…¥ ID è¾“å…¥ç¼–ç å™¨ä»¥è·å–æç¤ºåµŒå…¥
    prompt_embeds = text_encoder(text_input_ids.to(device))[0]
    # å°†åµŒå…¥è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
    prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)

    # ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆçš„æ¯ä¸ªè§†é¢‘å¤åˆ¶æ–‡æœ¬åµŒå…¥ï¼Œä½¿ç”¨é€‚åˆ MPS çš„æ–¹æ³•
    _, seq_len, _ = prompt_embeds.shape  # è·å–åµŒå…¥çš„å½¢çŠ¶
    # é‡å¤åµŒå…¥ä»¥åŒ¹é…æ¯ä¸ªæç¤ºçš„è§†é¢‘æ•°é‡
    prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)
    # å°†åµŒå…¥è°ƒæ•´ä¸ºæ–°çš„å½¢çŠ¶
    prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)

    # è¿”å›æœ€ç»ˆçš„æç¤ºåµŒå…¥
    return prompt_embeds


# ç¼–ç æç¤ºï¼Œç”Ÿæˆå…¶åµŒå…¥
def encode_prompt(
    # å®šä¹‰ T5 ä»¤ç‰ŒåŒ–å™¨
    tokenizer: T5Tokenizer,
    # å®šä¹‰ T5 ç¼–ç å™¨æ¨¡å‹
    text_encoder: T5EncoderModel,
    # æç¤ºæ–‡æœ¬ï¼Œå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
    prompt: Union[str, List[str]],
    # æ¯ä¸ªæç¤ºç”Ÿæˆè§†é¢‘çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 1
    num_videos_per_prompt: int = 1,
    # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸º 226
    max_sequence_length: int = 226,
    # æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ï¼Œå¯é€‰
    device: Optional[torch.device] = None,
    # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚ float32ï¼‰ï¼Œå¯é€‰
    dtype: Optional[torch.dtype] = None,
    # é¢„å…ˆæä¾›çš„æ–‡æœ¬è¾“å…¥ IDï¼Œå¯é€‰
    text_input_ids=None,
):
    # å¦‚æœæç¤ºæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
    prompt = [prompt] if isinstance(prompt, str) else prompt
    # è°ƒç”¨å†…éƒ¨å‡½æ•°è·å–æç¤ºåµŒå…¥
    prompt_embeds = _get_t5_prompt_embeds(
        tokenizer,
        text_encoder,
        prompt=prompt,
        num_videos_per_prompt=num_videos_per_prompt,
        max_sequence_length=max_sequence_length,
        device=device,
        dtype=dtype,
        text_input_ids=text_input_ids,
    )
    # è¿”å›æç¤ºåµŒå…¥
    return prompt_embeds


# è®¡ç®—æç¤ºçš„åµŒå…¥
def compute_prompt_embeddings(
    # å®šä¹‰ T5 ä»¤ç‰ŒåŒ–å™¨
    tokenizer, 
    # å®šä¹‰ T5 ç¼–ç å™¨æ¨¡å‹
    text_encoder, 
    # æç¤ºæ–‡æœ¬
    prompt, 
    # æœ€å¤§åºåˆ—é•¿åº¦
    max_sequence_length, 
    # æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰
    device, 
    # æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆå¦‚ float32ï¼‰
    dtype, 
    # æ˜¯å¦éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œé»˜è®¤ä¸º False
    requires_grad: bool = False
):
    # å¦‚æœéœ€è¦è®¡ç®—æ¢¯åº¦
    if requires_grad:
        # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
        prompt_embeds = encode_prompt(
            tokenizer,
            text_encoder,
            prompt,
            num_videos_per_prompt=1,  # é»˜è®¤ä¸º 1
            max_sequence_length=max_sequence_length,
            device=device,
            dtype=dtype,
        )
    else:
        # å¦‚æœä¸éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œä½¿ç”¨ no_grad ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        with torch.no_grad():
            # è°ƒç”¨ encode_prompt å‡½æ•°è·å–æç¤ºåµŒå…¥
            prompt_embeds = encode_prompt(
                tokenizer,
                text_encoder,
                prompt,
                num_videos_per_prompt=1,  # é»˜è®¤ä¸º 1
                max_sequence_length=max_sequence_length,
                device=device,
                dtype=dtype,
            )
    # è¿”å›è®¡ç®—å¾—åˆ°çš„æç¤ºåµŒå…¥
    return prompt_embeds


# å‡†å¤‡æ—‹è½¬ä½ç½®åµŒå…¥
def prepare_rotary_positional_embeddings(
    # åµŒå…¥çš„é«˜åº¦
    height: int,
    # åµŒå…¥çš„å®½åº¦
    width: int,
    # å¸§çš„æ•°é‡
    num_frames: int,
    # ç©ºé—´ VAE ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 8
    vae_scale_factor_spatial: int = 8,
    # è´´ç‰‡å¤§å°ï¼Œé»˜è®¤ä¸º 2
    patch_size: int = 2,
    # æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 64
    attention_head_dim: int = 64,
    # å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šè®¾å¤‡ç±»å‹ï¼ˆå¦‚ CPU æˆ– GPUï¼‰ï¼Œé»˜è®¤ä¸º None
    device: Optional[torch.device] = None,
    # åŸºç¡€é«˜åº¦ï¼Œé»˜è®¤ä¸º 480 åƒç´ 
    base_height: int = 480,
    # åŸºç¡€å®½åº¦ï¼Œé»˜è®¤ä¸º 720 åƒç´ 
    base_width: int = 720,
# å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„
) -> Tuple[torch.Tensor, torch.Tensor]:
    # è®¡ç®—ç½‘æ ¼çš„é«˜åº¦
    grid_height = height // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—ç½‘æ ¼çš„å®½åº¦
    grid_width = width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€å®½åº¦
    base_size_width = base_width // (vae_scale_factor_spatial * patch_size)
    # è®¡ç®—åŸºç¡€é«˜åº¦
    base_size_height = base_height // (vae_scale_factor_spatial * patch_size)

    # è·å–ç½‘æ ¼çš„è£å‰ªåŒºåŸŸåæ ‡
    grid_crops_coords = get_resize_crop_region_for_grid((grid_height, grid_width), base_size_width, base_size_height)
    # è®¡ç®—3Dæ—‹è½¬ä½ç½®åµŒå…¥çš„ä½™å¼¦å’Œæ­£å¼¦é¢‘ç‡
    freqs_cos, freqs_sin = get_3d_rotary_pos_embed(
        embed_dim=attention_head_dim,
        crops_coords=grid_crops_coords,
        grid_size=(grid_height, grid_width),
        temporal_size=num_frames,
    )

    # å°†ä½™å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_cos = freqs_cos.to(device=device)
    # å°†æ­£å¼¦é¢‘ç‡å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    freqs_sin = freqs_sin.to(device=device)
    # è¿”å›ä½™å¼¦å’Œæ­£å¼¦é¢‘ç‡å¼ é‡
    return freqs_cos, freqs_sin


# åˆ›å»ºä¼˜åŒ–å™¨çš„å‡½æ•°ï¼Œæ¥å—å‚æ•°å’Œä¼˜åŒ–å‚æ•°
def get_optimizer(args, params_to_optimize, use_deepspeed: bool = False):
    # ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    if use_deepspeed:
        from accelerate.utils import DummyOptim

        # è¿”å›ä¸€ä¸ªè™šæ‹Ÿä¼˜åŒ–å™¨ä»¥ä¾›ä½¿ç”¨
        return DummyOptim(
            params_to_optimize,
            lr=args.learning_rate,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )

    # ä¼˜åŒ–å™¨åˆ›å»ºéƒ¨åˆ†
    supported_optimizers = ["adam", "adamw", "prodigy"]
    # æ£€æŸ¥æ‰€é€‰ä¼˜åŒ–å™¨æ˜¯å¦å—æ”¯æŒ
    if args.optimizer not in supported_optimizers:
        # è®°å½•ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨è­¦å‘Š
        logger.warning(
            f"Unsupported choice of optimizer: {args.optimizer}. Supported optimizers include {supported_optimizers}. Defaulting to AdamW"
        )
        # é»˜è®¤ä¼˜åŒ–å™¨è®¾ç½®ä¸º AdamW
        args.optimizer = "adamw"

    # æ£€æŸ¥8ä½Adamçš„ä½¿ç”¨æ¡ä»¶
    if args.use_8bit_adam and not (args.optimizer.lower() not in ["adam", "adamw"]):
        logger.warning(
            f"use_8bit_adam is ignored when optimizer is not set to 'Adam' or 'AdamW'. Optimizer was "
            f"set to {args.optimizer.lower()}"
        )

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨8ä½Adamä¼˜åŒ–å™¨
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            # å¦‚æœæœªå®‰è£…bitsandbytesï¼ŒæŠ›å‡ºå¯¼å…¥é”™è¯¯
            raise ImportError(
                "To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`."
            )

    # åˆ›å»ºAdamWä¼˜åŒ–å™¨
    if args.optimizer.lower() == "adamw":
        optimizer_class = bnb.optim.AdamW8bit if args.use_8bit_adam else torch.optim.AdamW

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # åˆ›å»ºAdamä¼˜åŒ–å™¨
    elif args.optimizer.lower() == "adam":
        optimizer_class = bnb.optim.Adam8bit if args.use_8bit_adam else torch.optim.Adam

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = optimizer_class(
            params_to_optimize,
            betas=(args.adam_beta1, args.adam_beta2),
            eps=args.adam_epsilon,
            weight_decay=args.adam_weight_decay,
        )
    # æ£€æŸ¥ä¼˜åŒ–å™¨å‚æ•°æ˜¯å¦ä¸º "prodigy"ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    elif args.optimizer.lower() == "prodigy":
        # å°è¯•å¯¼å…¥ prodigyopt åº“
        try:
            import prodigyopt
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ŒæŠ›å‡º ImportError å¹¶æç¤ºå®‰è£…å‘½ä»¤
        except ImportError:
            raise ImportError("To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`")

        # è®¾ç½®ä¼˜åŒ–å™¨ç±»ä¸º Prodigy
        optimizer_class = prodigyopt.Prodigy

        # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡ä½ï¼Œå¹¶å‘å‡ºè­¦å‘Š
        if args.learning_rate <= 0.1:
            logger.warning(
                "Learning rate is too low. When using prodigy, it's generally better to set learning rate around 1.0"
            )

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å¯¹è±¡ï¼Œä¼ å…¥æ‰€éœ€å‚æ•°
        optimizer = optimizer_class(
            params_to_optimize,
            lr=args.learning_rate,
            betas=(args.adam_beta1, args.adam_beta2),
            beta3=args.prodigy_beta3,
            weight_decay=args.adam_weight_decay,
            eps=args.adam_epsilon,
            decouple=args.prodigy_decouple,
            use_bias_correction=args.prodigy_use_bias_correction,
            safeguard_warmup=args.prodigy_safeguard_warmup,
        )

    # è¿”å›åˆ›å»ºçš„ä¼˜åŒ–å™¨å¯¹è±¡
    return optimizer
# ä¸»å‡½æ•°ï¼Œæ¥æ”¶å‘½ä»¤è¡Œå‚æ•°
def main(args):
    # å¦‚æœæŠ¥å‘Šç›®æ ‡æ˜¯ "wandb" ä¸”æä¾›äº† hub_tokenï¼Œåˆ™æŠ›å‡ºé”™è¯¯
    if args.report_to == "wandb" and args.hub_token is not None:
        raise ValueError(
            "You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token."
            " Please use `huggingface-cli login` to authenticate with the Hub."
        )

    # æ£€æŸ¥ MPS åç«¯æ˜¯å¦å¯ç”¨ï¼Œå¹¶ä¸”æ··åˆç²¾åº¦è®¾ç½®ä¸º bf16ï¼Œè‹¥æ˜¯åˆ™æŠ›å‡ºé”™è¯¯
    if torch.backends.mps.is_available() and args.mixed_precision == "bf16":
        # ç”±äº pytorch#99272ï¼ŒMPS ç›®å‰ä¸æ”¯æŒ bfloat16ã€‚
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )

    # åˆ›å»ºæ—¥å¿—ç›®å½•çš„è·¯å¾„
    logging_dir = Path(args.output_dir, args.logging_dir)

    # åˆ›å»ºé¡¹ç›®é…ç½®ï¼ŒåŒ…æ‹¬é¡¹ç›®ç›®å½•å’Œæ—¥å¿—ç›®å½•
    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
    # é…ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œçš„å‚æ•°
    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    # åˆ›å»ºåŠ é€Ÿå™¨å®ä¾‹ï¼Œé…ç½®å…¶å‚æ•°
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
        kwargs_handlers=[kwargs],
    )

    # ç¦ç”¨ MPS çš„è‡ªåŠ¨æ··åˆç²¾åº¦
    if torch.backends.mps.is_available():
        accelerator.native_amp = False

    # å¦‚æœæŠ¥å‘Šç›®æ ‡æ˜¯ "wandb"ï¼Œæ£€æŸ¥å…¶æ˜¯å¦å¯ç”¨
    if args.report_to == "wandb":
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # é…ç½®æ—¥å¿—ï¼Œç¡®ä¿æ¯ä¸ªè¿›ç¨‹éƒ½èƒ½è®°å½•è°ƒè¯•ä¿¡æ¯
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    # è®°å½•åŠ é€Ÿå™¨çš„çŠ¶æ€ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½èƒ½çœ‹åˆ°
    logger.info(accelerator.state, main_process_only=False)
    # è®¾ç½®ä¸»è¿›ç¨‹çš„æ—¥å¿—çº§åˆ«
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # å¦‚æœæä¾›äº†ç§å­ï¼Œåˆ™è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_seed(args.seed)

    # å¤„ç†ä»“åº“çš„åˆ›å»º
    if accelerator.is_main_process:
        # å¦‚æœè¾“å‡ºç›®å½•ä¸ä¸ºç©ºï¼Œåˆ›å»ºè¾“å‡ºç›®å½•
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        # å¦‚æœéœ€è¦æ¨é€åˆ° hubï¼Œåˆ›å»ºä»“åº“
        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
            ).repo_id

    # å‡†å¤‡æ¨¡å‹å’Œè°ƒåº¦å™¨
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision
    )

    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½æ–‡æœ¬ç¼–ç å™¨
    text_encoder = T5EncoderModel.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision
    )

    # CogVideoX-2b æƒé‡å­˜å‚¨ä¸º float16
    # CogVideoX-5b å’Œ CogVideoX-5b-I2V æƒé‡å­˜å‚¨ä¸º bfloat16
    # æ ¹æ®é¢„è®­ç»ƒæ¨¡å‹åç§°é€‰æ‹©åŠ è½½çš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒ bfloat16 æˆ– float16
    load_dtype = torch.bfloat16 if "5b" in args.pretrained_model_name_or_path.lower() else torch.float16
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ 3D å˜æ¢å™¨æ¨¡å‹
    transformer = CogVideoXTransformer3DModel.from_pretrained(
        args.pretrained_model_name_or_path,  # é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        subfolder="transformer",  # æŒ‡å®šå­æ–‡ä»¶å¤¹
        torch_dtype=load_dtype,  # è®¾ç½®æ•°æ®ç±»å‹
        revision=args.revision,  # æŒ‡å®šç‰ˆæœ¬
        variant=args.variant,  # æŒ‡å®šå˜ä½“
    )

    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ VAE æ¨¡å‹
    vae = AutoencoderKLCogVideoX.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision, variant=args.variant
    )

    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½è°ƒåº¦å™¨
    scheduler = CogVideoXDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")

    # å¦‚æœå¯ç”¨äº†åˆ‡ç‰‡åŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„åˆ‡ç‰‡
    if args.enable_slicing:
        vae.enable_slicing()
    # å¦‚æœå¯ç”¨äº†å¹³é“ºåŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„å¹³é“º
    if args.enable_tiling:
        vae.enable_tiling()

    # åªè®­ç»ƒé¢å¤–çš„é€‚é…å™¨ LoRA å±‚
    text_encoder.requires_grad_(False)  # ç¦ç”¨æ–‡æœ¬ç¼–ç å™¨çš„æ¢¯åº¦è®¡ç®—
    transformer.requires_grad_(False)  # ç¦ç”¨å˜æ¢å™¨çš„æ¢¯åº¦è®¡ç®—
    vae.requires_grad_(False)  # ç¦ç”¨ VAE çš„æ¢¯åº¦è®¡ç®—

    # å¯¹äºæ··åˆç²¾åº¦è®­ç»ƒï¼Œå°†æ‰€æœ‰éå¯è®­ç»ƒæƒé‡ï¼ˆVAEã€æ–‡æœ¬ç¼–ç å™¨å’Œå˜æ¢å™¨ï¼‰è½¬æ¢ä¸ºåŠç²¾åº¦
    # å› ä¸ºè¿™äº›æƒé‡ä»…ç”¨äºæ¨ç†ï¼Œå› æ­¤ä¸éœ€è¦ä¿æŒå…¨ç²¾åº¦
    weight_dtype = torch.float32  # é»˜è®¤æƒé‡æ•°æ®ç±»å‹ä¸º float32
    if accelerator.state.deepspeed_plugin:  # å¦‚æœä½¿ç”¨ DeepSpeed
        # DeepSpeed å¤„ç†ç²¾åº¦ï¼Œä½¿ç”¨ DeepSpeed é…ç½®ä¸­çš„è®¾ç½®
        if (
            "fp16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["fp16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¦‚æœå¯ç”¨ fp16ï¼Œåˆ™è®¾ç½®æƒé‡ä¸º float16
        if (
            "bf16" in accelerator.state.deepspeed_plugin.deepspeed_config
            and accelerator.state.deepspeed_plugin.deepspeed_config["bf16"]["enabled"]
        ):
            weight_dtype = torch.float16  # å¦‚æœå¯ç”¨ bf16ï¼Œåˆ™è®¾ç½®æƒé‡ä¸º float16
    else:  # å¦‚æœä¸ä½¿ç”¨ DeepSpeed
        if accelerator.mixed_precision == "fp16":  # å¦‚æœæ··åˆç²¾åº¦ä¸º fp16
            weight_dtype = torch.float16  # è®¾ç½®æƒé‡ä¸º float16
        elif accelerator.mixed_precision == "bf16":  # å¦‚æœæ··åˆç²¾åº¦ä¸º bf16
            weight_dtype = torch.bfloat16  # è®¾ç½®æƒé‡ä¸º bfloat16

    # å¦‚æœ MPS å¯ç”¨ä¸”æƒé‡ç±»å‹ä¸º bfloat16ï¼ŒæŠ›å‡ºé”™è¯¯
    if torch.backends.mps.is_available() and weight_dtype == torch.bfloat16:
        # ç”±äº pytorch#99272ï¼ŒMPS ç›®å‰ä¸æ”¯æŒ bfloat16ã€‚
        raise ValueError(
            "Mixed precision training with bfloat16 is not supported on MPS. Please use fp16 (recommended) or fp32 instead."
        )

    # å°†æ–‡æœ¬ç¼–ç å™¨ã€å˜æ¢å™¨å’Œ VAE è½¬ç§»åˆ°åŠ é€Ÿå™¨è®¾å¤‡ï¼Œå¹¶è®¾ç½®æƒé‡æ•°æ®ç±»å‹
    text_encoder.to(accelerator.device, dtype=weight_dtype)
    transformer.to(accelerator.device, dtype=weight_dtype)
    vae.to(accelerator.device, dtype=weight_dtype)

    # å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™å¯ç”¨å˜æ¢å™¨çš„æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()

    # ç°åœ¨å°†æ–°çš„ LoRA æƒé‡æ·»åŠ åˆ°æ³¨æ„åŠ›å±‚
    transformer_lora_config = LoraConfig(
        r=args.rank,  # LoRA çš„ç§©
        lora_alpha=args.lora_alpha,  # LoRA çš„ alpha å€¼
        init_lora_weights=True,  # åˆå§‹åŒ– LoRA æƒé‡
        target_modules=["to_k", "to_q", "to_v", "to_out.0"],  # ç›®æ ‡æ¨¡å—
    )
    # å°† LoRA é€‚é…å™¨æ·»åŠ åˆ°å˜æ¢å™¨ä¸­
    transformer.add_adapter(transformer_lora_config)

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè§£åŒ…æ¨¡å‹
    def unwrap_model(model):
        # è§£åŒ…åŠ é€Ÿå™¨ä¸­çš„æ¨¡å‹
        model = accelerator.unwrap_model(model)
        # å¦‚æœæ˜¯ç¼–è¯‘æ¨¡å—ï¼Œåˆ™è¿”å›å…¶åŸå§‹æ¨¡å—
        model = model._orig_mod if is_compiled_module(model) else model
        return model  # è¿”å›è§£åŒ…åçš„æ¨¡å‹
    # åˆ›å»ºè‡ªå®šä¹‰çš„ä¿å­˜å’ŒåŠ è½½é’©å­ï¼Œä»¥ä¾¿ `accelerator.save_state(...)` å¯ä»¥åºåˆ—åŒ–ä¸ºè‰¯å¥½çš„æ ¼å¼
    def save_model_hook(models, weights, output_dir):
        # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
        if accelerator.is_main_process:
            # åˆå§‹åŒ–è¦ä¿å­˜çš„å˜æ¢å™¨ LoRA å±‚å˜é‡
            transformer_lora_layers_to_save = None
    
            # éå†æ‰€æœ‰æ¨¡å‹
            for model in models:
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ˜¯å˜æ¢å™¨çš„å®ä¾‹
                if isinstance(model, type(unwrap_model(transformer))):
                    # è·å–å˜æ¢å™¨æ¨¡å‹çš„çŠ¶æ€å­—å…¸
                    transformer_lora_layers_to_save = get_peft_model_state_dict(model)
                else:
                    # å¦‚æœæ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise ValueError(f"unexpected save model: {model.__class__}")
    
                # ç¡®ä¿ä»æƒé‡ä¸­ç§»é™¤ç›¸åº”çš„æƒé‡ï¼Œä»¥é¿å…é‡å¤ä¿å­˜
                weights.pop()
    
            # ä¿å­˜ LoRA æƒé‡åˆ°æŒ‡å®šè¾“å‡ºç›®å½•
            CogVideoXPipeline.save_lora_weights(
                output_dir,
                transformer_lora_layers=transformer_lora_layers_to_save,
            )
    
        # å®šä¹‰åŠ è½½æ¨¡å‹çš„é’©å­
        def load_model_hook(models, input_dir):
            # åˆå§‹åŒ–å˜æ¢å™¨å˜é‡
            transformer_ = None
    
            # å½“æ¨¡å‹åˆ—è¡¨éç©ºæ—¶æŒç»­æ‰§è¡Œ
            while len(models) > 0:
                # å¼¹å‡ºæ¨¡å‹
                model = models.pop()
    
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ˜¯å˜æ¢å™¨çš„å®ä¾‹
                if isinstance(model, type(unwrap_model(transformer))):
                    transformer_ = model
                else:
                    # å¦‚æœæ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise ValueError(f"Unexpected save model: {model.__class__}")
    
            # ä»æŒ‡å®šè¾“å…¥ç›®å½•è·å– LoRA çŠ¶æ€å­—å…¸
            lora_state_dict = CogVideoXPipeline.lora_state_dict(input_dir)
    
            # åˆ›å»ºå˜æ¢å™¨çŠ¶æ€å­—å…¸ï¼Œä»…ä¿ç•™ä»¥ "transformer." å¼€å¤´çš„é”®
            transformer_state_dict = {
                f'{k.replace("transformer.", "")}': v for k, v in lora_state_dict.items() if k.startswith("transformer.")
            }
            # è½¬æ¢ UNet çŠ¶æ€å­—å…¸ä¸º PEFT æ ¼å¼
            transformer_state_dict = convert_unet_state_dict_to_peft(transformer_state_dict)
            # è®¾ç½® PEFT æ¨¡å‹çŠ¶æ€å­—å…¸å¹¶è·å–ä¸å…¼å®¹çš„é”®
            incompatible_keys = set_peft_model_state_dict(transformer_, transformer_state_dict, adapter_name="default")
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ„å¤–çš„é”®
            if incompatible_keys is not None:
                # è·å–æ„å¤–çš„é”®
                unexpected_keys = getattr(incompatible_keys, "unexpected_keys", None)
                if unexpected_keys:
                    # è®°å½•è­¦å‘Šæ—¥å¿—
                    logger.warning(
                        f"Loading adapter weights from state_dict led to unexpected keys not found in the model: "
                        f" {unexpected_keys}. "
                    )
    
            # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
            if args.mixed_precision == "fp16":
                # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰æå‡ä¸º fp32
                cast_training_params([transformer_])
    
        # æ³¨å†Œä¿å­˜çŠ¶æ€å‰é’©å­
        accelerator.register_save_state_pre_hook(save_model_hook)
        # æ³¨å†ŒåŠ è½½çŠ¶æ€å‰é’©å­
        accelerator.register_load_state_pre_hook(load_model_hook)
    
        # å¦‚æœå…è®¸ä½¿ç”¨ TF32ï¼Œåˆ™åœ¨ Ampere GPU ä¸Šå¯ç”¨æ›´å¿«çš„è®­ç»ƒ
        if args.allow_tf32 and torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = True
    
        # å¦‚æœéœ€è¦ç¼©æ”¾å­¦ä¹ ç‡ï¼Œåˆ™è¿›è¡Œç›¸åº”è°ƒæ•´
        if args.scale_lr:
            args.learning_rate = (
                args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes
            )
    # ç¡®ä¿å¯è®­ç»ƒå‚æ•°ä¸º float32 ç±»å‹
    if args.mixed_precision == "fp16":
        # ä»…å°†å¯è®­ç»ƒå‚æ•°ï¼ˆLoRAï¼‰ä¸Šå‡ä¸º fp32 ç±»å‹
        cast_training_params([transformer], dtype=torch.float32)

    # è¿‡æ»¤å‡ºéœ€è¦æ¢¯åº¦æ›´æ–°çš„å‚æ•°
    transformer_lora_parameters = list(filter(lambda p: p.requires_grad, transformer.parameters()))

    # ä¼˜åŒ–å™¨çš„å‚æ•°å­—å…¸
    transformer_parameters_with_lr = {"params": transformer_lora_parameters, "lr": args.learning_rate}
    # å°†ä¼˜åŒ–å‚æ•°æ”¾å…¥åˆ—è¡¨ä¸­
    params_to_optimize = [transformer_parameters_with_lr]

    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
    use_deepspeed_optimizer = (
        accelerator.state.deepspeed_plugin is not None
        and "optimizer" in accelerator.state.deepspeed_plugin.deepspeed_config
    )
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨
    use_deepspeed_scheduler = (
        accelerator.state.deepspeed_plugin is not None
        and "scheduler" not in accelerator.state.deepspeed_plugin.deepspeed_config
    )

    # è·å–ä¼˜åŒ–å™¨å®ä¾‹
    optimizer = get_optimizer(args, params_to_optimize, use_deepspeed=use_deepspeed_optimizer)

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = VideoDataset(
        instance_data_root=args.instance_data_root,  # å®ä¾‹æ•°æ®æ ¹ç›®å½•
        dataset_name=args.dataset_name,  # æ•°æ®é›†åç§°
        dataset_config_name=args.dataset_config_name,  # æ•°æ®é›†é…ç½®åç§°
        caption_column=args.caption_column,  # æè¿°åˆ—åç§°
        video_column=args.video_column,  # è§†é¢‘åˆ—åç§°
        height=args.height,  # è§†é¢‘é«˜åº¦
        width=args.width,  # è§†é¢‘å®½åº¦
        fps=args.fps,  # å¸§ç‡
        max_num_frames=args.max_num_frames,  # æœ€å¤§å¸§æ•°
        skip_frames_start=args.skip_frames_start,  # å¼€å§‹è·³è¿‡çš„å¸§æ•°
        skip_frames_end=args.skip_frames_end,  # ç»“æŸè·³è¿‡çš„å¸§æ•°
        cache_dir=args.cache_dir,  # ç¼“å­˜ç›®å½•
        id_token=args.id_token,  # ID ä»¤ç‰Œ
    )

    # å®šä¹‰ç¼–ç è§†é¢‘çš„å‡½æ•°
    def encode_video(video):
        # å°†è§†é¢‘è½¬ç§»åˆ°è®¾å¤‡å¹¶å¢åŠ ç»´åº¦
        video = video.to(accelerator.device, dtype=vae.dtype).unsqueeze(0)
        # è°ƒæ•´è§†é¢‘ç»´åº¦é¡ºåº
        video = video.permute(0, 2, 1, 3, 4)  # [B, C, F, H, W]
        # ä½¿ç”¨ VAE ç¼–ç è§†é¢‘å¹¶è·å–æ½œåœ¨åˆ†å¸ƒ
        latent_dist = vae.encode(video).latent_dist
        return latent_dist

    # å¯¹æ•°æ®é›†ä¸­æ¯ä¸ªå®ä¾‹è§†é¢‘è¿›è¡Œç¼–ç 
    train_dataset.instance_videos = [encode_video(video) for video in train_dataset.instance_videos]

    # å®šä¹‰æ•´ç†å‡½æ•°ä»¥ç»„åˆæ•°æ®
    def collate_fn(examples):
        # æå–è§†é¢‘æ ·æœ¬å¹¶è¿›è¡Œç¼©æ”¾
        videos = [example["instance_video"].sample() * vae.config.scaling_factor for example in examples]
        # æå–å¯¹åº”çš„æç¤ºæ–‡æœ¬
        prompts = [example["instance_prompt"] for example in examples]

        # å°†è§†é¢‘å¼ é‡åˆå¹¶
        videos = torch.cat(videos)
        # ç¡®ä¿è§†é¢‘å¼ é‡è¿ç»­å¹¶è½¬æ¢ä¸º float ç±»å‹
        videos = videos.to(memory_format=torch.contiguous_format).float()

        return {
            "videos": videos,  # è¿”å›è§†é¢‘å¼ é‡
            "prompts": prompts,  # è¿”å›æç¤ºæ–‡æœ¬
        }

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(
        train_dataset,  # ä½¿ç”¨çš„æ•°æ®é›†
        batch_size=args.train_batch_size,  # æ¯æ‰¹æ¬¡çš„å¤§å°
        shuffle=True,  # æ˜¯å¦æ‰“ä¹±æ•°æ®
        collate_fn=collate_fn,  # è‡ªå®šä¹‰æ•´ç†å‡½æ•°
        num_workers=args.dataloader_num_workers,  # ä½¿ç”¨çš„å·¥ä½œçº¿ç¨‹æ•°
    )

    # è°ƒåº¦å™¨å’Œè®­ç»ƒæ­¥éª¤çš„æ•°å­¦è®¡ç®—
    overrode_max_train_steps = False  # æ ‡è®°æ˜¯å¦è¦†ç›–æœ€å¤§è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)  # æ¯ä¸ª epoch çš„æ›´æ–°æ­¥éª¤æ•°
    if args.max_train_steps is None:
        # å¦‚æœæœªæŒ‡å®šæœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œåˆ™æ ¹æ®è®­ç»ƒå‘¨æœŸè®¡ç®—
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True  # è®¾ç½®æ ‡è®°ä¸º True
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeed è°ƒåº¦å™¨
    if use_deepspeed_scheduler:
        # ä» accelerate.utils å¯¼å…¥ DummyScheduler ç±»
        from accelerate.utils import DummyScheduler

        # åˆ›å»ºä¸€ä¸ª DummyScheduler å®ä¾‹ï¼Œç”¨äºå­¦ä¹ ç‡è°ƒåº¦
        lr_scheduler = DummyScheduler(
            # è®¾ç½®è°ƒåº¦å™¨åç§°
            name=args.lr_scheduler,
            # ä¼ å…¥ä¼˜åŒ–å™¨
            optimizer=optimizer,
            # è®¾ç½®æ€»è®­ç»ƒæ­¥æ•°
            total_num_steps=args.max_train_steps * accelerator.num_processes,
            # è®¾ç½®é¢„çƒ­æ­¥æ•°
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
        )
    else:
        # å¦‚æœä¸ä½¿ç”¨ DeepSpeedï¼Œè°ƒç”¨ get_scheduler å‡½æ•°è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = get_scheduler(
            # ä¼ å…¥è°ƒåº¦å™¨åç§°
            args.lr_scheduler,
            # ä¼ å…¥ä¼˜åŒ–å™¨
            optimizer=optimizer,
            # è®¾ç½®é¢„çƒ­æ­¥æ•°
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
            # è®¾ç½®æ€»è®­ç»ƒæ­¥æ•°
            num_training_steps=args.max_train_steps * accelerator.num_processes,
            # è®¾ç½®å­¦ä¹ ç‡å¾ªç¯æ¬¡æ•°
            num_cycles=args.lr_num_cycles,
            # è®¾ç½®å­¦ä¹ ç‡è¡°å‡çš„å¹‚
            power=args.lr_power,
        )

    # ä½¿ç”¨ accelerator å‡†å¤‡æ‰€æœ‰ç»„ä»¶
    transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        # å‡†å¤‡å˜æ¢å™¨ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒæ•°æ®åŠ è½½å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        transformer, optimizer, train_dataloader, lr_scheduler
    )

    # éœ€è¦é‡æ–°è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®åŠ è½½å™¨çš„å¤§å°å¯èƒ½å·²ç»æ”¹å˜
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    # å¦‚æœè¦†ç›–äº†æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if overrode_max_train_steps:
        # æ ¹æ®è®­ç»ƒå‘¨æœŸå’Œæ›´æ–°æ­¥éª¤è®¡ç®—æœ€å¤§è®­ç»ƒæ­¥æ•°
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    # éšåé‡æ–°è®¡ç®—è®­ç»ƒå‘¨æœŸæ•°
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # åˆå§‹åŒ–è·Ÿè¸ªå™¨å¹¶å­˜å‚¨é…ç½®
    # è·Ÿè¸ªå™¨åœ¨ä¸»è¿›ç¨‹ä¸­è‡ªåŠ¨åˆå§‹åŒ–
    if accelerator.is_main_process:
        # è·å–è·Ÿè¸ªå™¨åç§°ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤åç§°
        tracker_name = args.tracker_name or "cogvideox-lora"
        # åˆå§‹åŒ–è·Ÿè¸ªå™¨ï¼Œå¹¶ä¼ å…¥é…ç½®
        accelerator.init_trackers(tracker_name, config=vars(args))

    # å¼€å§‹è®­ç»ƒï¼
    # è®¡ç®—æ€»æ‰¹é‡å¤§å°
    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
    # è®¡ç®—å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
    num_trainable_parameters = sum(param.numel() for model in params_to_optimize for param in model["params"])

    # è®°å½•è®­ç»ƒå¼€å§‹çš„ä¿¡æ¯
    logger.info("***** Running training *****")
    # è®°å½•å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
    logger.info(f"  Num trainable parameters = {num_trainable_parameters}")
    # è®°å½•æ ·æœ¬æ•°é‡
    logger.info(f"  Num examples = {len(train_dataset)}")
    # è®°å½•æ¯ä¸ªå‘¨æœŸçš„æ‰¹æ¬¡æ•°
    logger.info(f"  Num batches each epoch = {len(train_dataloader)}")
    # è®°å½•è®­ç»ƒå‘¨æœŸæ•°
    logger.info(f"  Num epochs = {args.num_train_epochs}")
    # è®°å½•æ¯ä¸ªè®¾å¤‡çš„ç¬æ—¶æ‰¹é‡å¤§å°
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")
    # è®°å½•æ€»æ‰¹é‡å¤§å°ï¼ˆåŒ…æ‹¬å¹¶è¡Œã€åˆ†å¸ƒå¼å’Œç§¯ç´¯ï¼‰
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    # è®°å½•æ¢¯åº¦ç§¯ç´¯æ­¥æ•°
    logger.info(f"  Gradient accumulation steps = {args.gradient_accumulation_steps}")
    # è®°å½•æ€»ä¼˜åŒ–æ­¥éª¤
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    # åˆå§‹åŒ–å…¨å±€æ­¥æ•°
    global_step = 0
    # åˆå§‹åŒ–é¦–ä¸ªå‘¨æœŸ
    first_epoch = 0

    # å¯èƒ½ä»ä¹‹å‰çš„ä¿å­˜ä¸­åŠ è½½æƒé‡å’ŒçŠ¶æ€
    if not args.resume_from_checkpoint:
        # å¦‚æœæœªä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œè®¾ç½®åˆå§‹å…¨å±€æ­¥æ•°ä¸º0
        initial_global_step = 0
    else:  # å¦‚æœå‰é¢çš„æ¡ä»¶ä¸æ»¡è¶³ï¼Œæ‰§è¡Œä»¥ä¸‹ä»£ç 
        if args.resume_from_checkpoint != "latest":  # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†éæœ€æ–°çš„æ£€æŸ¥ç‚¹
            path = os.path.basename(args.resume_from_checkpoint)  # è·å–æŒ‡å®šæ£€æŸ¥ç‚¹çš„åŸºæœ¬æ–‡ä»¶å
        else:  # å¦‚æœæ²¡æœ‰æŒ‡å®šéæœ€æ–°æ£€æŸ¥ç‚¹
            # è·å–æœ€è¿‘çš„æ£€æŸ¥ç‚¹
            dirs = os.listdir(args.output_dir)  # åˆ—å‡ºè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œç›®å½•
            dirs = [d for d in dirs if d.startswith("checkpoint")]  # è¿‡æ»¤å‡ºä»¥ "checkpoint" å¼€å¤´çš„ç›®å½•
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))  # æ ¹æ®æ£€æŸ¥ç‚¹çš„æ•°å­—éƒ¨åˆ†æ’åº
            path = dirs[-1] if len(dirs) > 0 else None  # å¦‚æœæœ‰æ£€æŸ¥ç‚¹ï¼Œå–æœ€æ–°çš„ä¸€ä¸ªï¼Œå¦åˆ™ä¸º None

        if path is None:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹
            accelerator.print(  # è¾“å‡ºä¿¡æ¯
                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."  # æç¤ºæ£€æŸ¥ç‚¹ä¸å­˜åœ¨ï¼Œå¼€å§‹æ–°çš„è®­ç»ƒ
            )
            args.resume_from_checkpoint = None  # å°†æ¢å¤æ£€æŸ¥ç‚¹å‚æ•°è®¾ç½®ä¸º None
            initial_global_step = 0  # åˆå§‹åŒ–å…¨å±€æ­¥éª¤ä¸º 0
        else:  # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹
            accelerator.print(f"Resuming from checkpoint {path}")  # è¾“å‡ºæ¢å¤æ£€æŸ¥ç‚¹çš„ä¿¡æ¯
            accelerator.load_state(os.path.join(args.output_dir, path))  # åŠ è½½æŒ‡å®šæ£€æŸ¥ç‚¹çš„çŠ¶æ€
            global_step = int(path.split("-")[1])  # ä»æ£€æŸ¥ç‚¹çš„æ–‡ä»¶åä¸­æå–å…¨å±€æ­¥éª¤

            initial_global_step = global_step  # å°†åˆå§‹å…¨å±€æ­¥éª¤è®¾ç½®ä¸ºæå–çš„å€¼
            first_epoch = global_step // num_update_steps_per_epoch  # è®¡ç®—å½“å‰æ˜¯ç¬¬å‡ ä¸ª epoch

    progress_bar = tqdm(  # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
        range(0, args.max_train_steps),  # è®¾ç½®è¿›åº¦æ¡çš„èŒƒå›´
        initial=initial_global_step,  # è®¾ç½®è¿›åº¦æ¡çš„åˆå§‹å€¼
        desc="Steps",  # è®¾ç½®è¿›åº¦æ¡çš„æè¿°
        # ä»…åœ¨æ¯å°æœºå™¨ä¸Šæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡ã€‚
        disable=not accelerator.is_local_main_process,  # å¦‚æœä¸æ˜¯æœ¬åœ°ä¸»è¿›ç¨‹ï¼Œåˆ™ç¦ç”¨è¿›åº¦æ¡
    )
    vae_scale_factor_spatial = 2 ** (len(vae.config.block_out_channels) - 1)  # è®¡ç®— VAE çš„ç©ºé—´ç¼©æ”¾å› å­

    # ç”¨äº DeepSpeed è®­ç»ƒ
    model_config = transformer.module.config if hasattr(transformer, "module") else transformer.config  # è·å–æ¨¡å‹é…ç½®ï¼Œè€ƒè™‘æ¨¡å—å±æ€§

    # ä¿å­˜ LoRA å±‚
    accelerator.wait_for_everyone()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
        if accelerator.is_main_process:
            # è§£åŒ…æ¨¡å‹ä»¥è·å–ä¸»æ¨¡å‹
            transformer = unwrap_model(transformer)
            # æ ¹æ®æ··åˆç²¾åº¦è®¾ç½®é€‰æ‹©æ•°æ®ç±»å‹
            dtype = (
                torch.float16
                if args.mixed_precision == "fp16"
                else torch.bfloat16
                if args.mixed_precision == "bf16"
                else torch.float32
            )
            # å°†æ¨¡å‹è½¬æ¢ä¸ºæ‰€é€‰çš„æ•°æ®ç±»å‹
            transformer = transformer.to(dtype)
            # è·å–æ¨¡å‹çš„ LoRA å±‚çŠ¶æ€å­—å…¸
            transformer_lora_layers = get_peft_model_state_dict(transformer)
    
            # ä¿å­˜ LoRA æƒé‡åˆ°æŒ‡å®šç›®å½•
            CogVideoXPipeline.save_lora_weights(
                save_directory=args.output_dir,
                transformer_lora_layers=transformer_lora_layers,
            )
    
            # æœ€ç»ˆæµ‹è¯•æ¨ç†
            pipe = CogVideoXPipeline.from_pretrained(
                args.pretrained_model_name_or_path,
                revision=args.revision,
                variant=args.variant,
                torch_dtype=weight_dtype,
            )
            # ä½¿ç”¨é…ç½®åˆ›å»ºè°ƒåº¦å™¨
            pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config)
    
            # å¦‚æœå¯ç”¨åˆ‡ç‰‡åŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„åˆ‡ç‰‡
            if args.enable_slicing:
                pipe.vae.enable_slicing()
            # å¦‚æœå¯ç”¨å¹³é“ºåŠŸèƒ½ï¼Œåˆ™å¯ç”¨ VAE çš„å¹³é“º
            if args.enable_tiling:
                pipe.vae.enable_tiling()
    
            # åŠ è½½ LoRA æƒé‡
            lora_scaling = args.lora_alpha / args.rank
            # ä»è¾“å‡ºç›®å½•åŠ è½½ LoRA æƒé‡
            pipe.load_lora_weights(args.output_dir, adapter_name="cogvideox-lora")
            # è®¾ç½®é€‚é…å™¨åŠå…¶ç¼©æ”¾å› å­
            pipe.set_adapters(["cogvideox-lora"], [lora_scaling])
    
            # è¿è¡Œæ¨ç†å¹¶è¿›è¡ŒéªŒè¯
            validation_outputs = []
            # å¦‚æœæœ‰éªŒè¯æç¤ºä¸”æ•°é‡å¤§äºé›¶ï¼Œåˆ™è¿›è¡ŒéªŒè¯
            if args.validation_prompt and args.num_validation_videos > 0:
                validation_prompts = args.validation_prompt.split(args.validation_prompt_separator)
                # éå†æ¯ä¸ªéªŒè¯æç¤º
                for validation_prompt in validation_prompts:
                    # å‡†å¤‡æ¨ç†å‚æ•°
                    pipeline_args = {
                        "prompt": validation_prompt,
                        "guidance_scale": args.guidance_scale,
                        "use_dynamic_cfg": args.use_dynamic_cfg,
                        "height": args.height,
                        "width": args.width,
                    }
    
                    # è®°å½•éªŒè¯è¾“å‡º
                    video = log_validation(
                        pipe=pipe,
                        args=args,
                        accelerator=accelerator,
                        pipeline_args=pipeline_args,
                        epoch=epoch,
                        is_final_validation=True,
                    )
                    # æ‰©å±•éªŒè¯è¾“å‡ºåˆ—è¡¨
                    validation_outputs.extend(video)
    
            # å¦‚æœéœ€è¦ä¸Šä¼ åˆ°ä¸­å¿ƒ
            if args.push_to_hub:
                # ä¿å­˜æ¨¡å‹å¡ä¿¡æ¯åˆ°æŒ‡å®šçš„åº“
                save_model_card(
                    repo_id,
                    videos=validation_outputs,
                    base_model=args.pretrained_model_name_or_path,
                    validation_prompt=args.validation_prompt,
                    repo_folder=args.output_dir,
                    fps=args.fps,
                )
                # ä¸Šä¼ è¾“å‡ºç›®å½•åˆ°æŒ‡å®šçš„åº“
                upload_folder(
                    repo_id=repo_id,
                    folder_path=args.output_dir,
                    commit_message="End of training",
                    ignore_patterns=["step_*", "epoch_*"],
                )
    
        # ç»“æŸè®­ç»ƒè¿‡ç¨‹
        accelerator.end_training()
# å¦‚æœè¯¥è„šæœ¬æ˜¯ä¸»ç¨‹åºï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç å—
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()
    # è°ƒç”¨ä¸»å‡½æ•°ï¼Œå¹¶å°†å‚æ•°ä¼ é€’ç»™å®ƒ
    main(args)
```