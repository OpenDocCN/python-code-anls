# `.\diffusers\models\unets\unet_2d_condition.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œæ ‡æ˜ç‰ˆæƒä¿¡æ¯å’Œä½¿ç”¨è®¸å¯
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# æŒ‰ç…§ Apache License 2.0 ç‰ˆæœ¬è¿›è¡Œè®¸å¯
# Licensed under the Apache License, Version 2.0 (the "License");
# ä½ ä¸å¾—åœ¨æœªéµå®ˆè®¸å¯çš„æƒ…å†µä¸‹ä½¿ç”¨æ­¤æ–‡ä»¶
# you may not use this file except in compliance with the License.
# ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å¾—è®¸å¯è¯çš„å‰¯æœ¬
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éæ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œè½¯ä»¶ä»¥â€œæŒ‰åŸæ ·â€æ–¹å¼åˆ†å‘ï¼Œä¸æä¾›ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# æŸ¥çœ‹è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€æ‰€é€‚ç”¨çš„æƒé™å’Œé™åˆ¶
# See the License for the specific language governing permissions and
# limitations under the License.

# ä» dataclasses æ¨¡å—å¯¼å…¥ dataclass è£…é¥°å™¨ï¼Œç”¨äºç®€åŒ–ç±»çš„å®šä¹‰
from dataclasses import dataclass
# å¯¼å…¥æ‰€éœ€çš„ç±»å‹æ³¨é‡Š
from typing import Any, Dict, List, Optional, Tuple, Union

# å¯¼å…¥ PyTorch åº“å’Œç›¸å…³æ¨¡å—
import torch
import torch.nn as nn
import torch.utils.checkpoint

# ä»é…ç½®å’ŒåŠ è½½å™¨æ¨¡å—ä¸­å¯¼å…¥æ‰€éœ€çš„ç±»å’Œå‡½æ•°
from ...configuration_utils import ConfigMixin, register_to_config
from ...loaders import PeftAdapterMixin, UNet2DConditionLoadersMixin
from ...loaders.single_file_model import FromOriginalModelMixin
from ...utils import USE_PEFT_BACKEND, BaseOutput, deprecate, logging, scale_lora_layers, unscale_lora_layers
from ..activations import get_activation
from ..attention_processor import (
    ADDED_KV_ATTENTION_PROCESSORS,  # å¯¼å…¥ä¸æ³¨æ„åŠ›æœºåˆ¶ç›¸å…³çš„å¤„ç†å™¨
    CROSS_ATTENTION_PROCESSORS,
    Attention,
    AttentionProcessor,
    AttnAddedKVProcessor,
    AttnProcessor,
    FusedAttnProcessor2_0,
)
from ..embeddings import (
    GaussianFourierProjection,  # å¯¼å…¥å¤šç§åµŒå…¥æ–¹æ³•
    GLIGENTextBoundingboxProjection,
    ImageHintTimeEmbedding,
    ImageProjection,
    ImageTimeEmbedding,
    TextImageProjection,
    TextImageTimeEmbedding,
    TextTimeEmbedding,
    TimestepEmbedding,
    Timesteps,
)
from ..modeling_utils import ModelMixin  # å¯¼å…¥æ¨¡å‹æ··åˆç±»
from .unet_2d_blocks import (
    get_down_block,  # å¯¼å…¥ä¸‹é‡‡æ ·å—çš„æ„é€ å‡½æ•°
    get_mid_block,   # å¯¼å…¥ä¸­é—´å—çš„æ„é€ å‡½æ•°
    get_up_block,    # å¯¼å…¥ä¸Šé‡‡æ ·å—çš„æ„é€ å‡½æ•°
)

# åˆ›å»ºä¸€ä¸ªæ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•æ¨¡å‹ç›¸å…³ä¿¡æ¯
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name

# å®šä¹‰ UNet2DConditionOutput æ•°æ®ç±»ï¼Œç”¨äºå­˜å‚¨ UNet2DConditionModel çš„è¾“å‡º
@dataclass
class UNet2DConditionOutput(BaseOutput):
    """
    UNet2DConditionModel çš„è¾“å‡ºã€‚

    å‚æ•°:
        sample (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`):
            åŸºäº `encoder_hidden_states` è¾“å…¥çš„éšè—çŠ¶æ€è¾“å‡ºï¼Œæ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºã€‚
    """

    sample: torch.Tensor = None  # å®šä¹‰ä¸€ä¸ªæ ·æœ¬å±æ€§ï¼Œé»˜è®¤ä¸º None

# å®šä¹‰ UNet2DConditionModel ç±»ï¼Œè¡¨ç¤ºä¸€ä¸ªæ¡ä»¶ 2D UNet æ¨¡å‹
class UNet2DConditionModel(
    ModelMixin, ConfigMixin, FromOriginalModelMixin, UNet2DConditionLoadersMixin, PeftAdapterMixin
):
    r"""
    ä¸€ä¸ªæ¡ä»¶ 2D UNet æ¨¡å‹ï¼Œæ¥å—ä¸€ä¸ªå™ªå£°æ ·æœ¬ã€æ¡ä»¶çŠ¶æ€å’Œæ—¶é—´æ­¥ï¼Œå¹¶è¿”å›æ ·æœ¬å½¢çŠ¶çš„è¾“å‡ºã€‚

    è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [`ModelMixin`]ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–å…¶ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•
    ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼‰ã€‚
    """

    _supports_gradient_checkpointing = True  # è¡¨ç¤ºè¯¥æ¨¡å‹æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹
    _no_split_modules = ["BasicTransformerBlock", "ResnetBlock2D", "CrossAttnUpBlock2D"]  # ä¸è¿›è¡Œæ‹†åˆ†çš„æ¨¡å—åˆ—è¡¨

    @register_to_config  # å°†è¯¥æ–¹æ³•æ³¨å†Œåˆ°é…ç½®ä¸­
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®ç±»çš„åŸºæœ¬å±æ€§
        def __init__(
            # æ ·æœ¬å¤§å°ï¼Œé»˜è®¤ä¸º None
            self,
            sample_size: Optional[int] = None,
            # è¾“å…¥é€šé“æ•°ï¼Œé»˜è®¤ä¸º 4
            in_channels: int = 4,
            # è¾“å‡ºé€šé“æ•°ï¼Œé»˜è®¤ä¸º 4
            out_channels: int = 4,
            # æ˜¯å¦å°†è¾“å…¥æ ·æœ¬ä¸­å¿ƒåŒ–ï¼Œé»˜è®¤ä¸º False
            center_input_sample: bool = False,
            # æ˜¯å¦å°†æ­£å¼¦å‡½æ•°ç¿»è½¬ä¸ºä½™å¼¦å‡½æ•°ï¼Œé»˜è®¤ä¸º True
            flip_sin_to_cos: bool = True,
            # é¢‘ç‡åç§»é‡ï¼Œé»˜è®¤ä¸º 0
            freq_shift: int = 0,
            # å‘ä¸‹é‡‡æ ·çš„å—ç±»å‹ï¼ŒåŒ…å«å¤šç§å—ç±»å‹
            down_block_types: Tuple[str] = (
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            # ä¸­é—´å—çš„ç±»å‹ï¼Œé»˜è®¤ä¸º UNet çš„ä¸­é—´å—ç±»å‹
            mid_block_type: Optional[str] = "UNetMidBlock2DCrossAttn",
            # å‘ä¸Šé‡‡æ ·çš„å—ç±»å‹ï¼ŒåŒ…å«å¤šç§å—ç±»å‹
            up_block_types: Tuple[str] = ("UpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D"),
            # æ˜¯å¦ä»…ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œé»˜è®¤ä¸º False
            only_cross_attention: Union[bool, Tuple[bool]] = False,
            # æ¯ä¸ªå—çš„è¾“å‡ºé€šé“æ•°
            block_out_channels: Tuple[int] = (320, 640, 1280, 1280),
            # æ¯ä¸ªå—çš„å±‚æ•°ï¼Œé»˜è®¤ä¸º 2
            layers_per_block: Union[int, Tuple[int]] = 2,
            # ä¸‹é‡‡æ ·æ—¶çš„å¡«å……å¤§å°ï¼Œé»˜è®¤ä¸º 1
            downsample_padding: int = 1,
            # ä¸­é—´å—çš„ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 1
            mid_block_scale_factor: float = 1,
            # dropout æ¦‚ç‡ï¼Œé»˜è®¤ä¸º 0.0
            dropout: float = 0.0,
            # æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œé»˜è®¤ä¸º "silu"
            act_fn: str = "silu",
            # å½’ä¸€åŒ–çš„ç»„æ•°ï¼Œé»˜è®¤ä¸º 32
            norm_num_groups: Optional[int] = 32,
            # å½’ä¸€åŒ–çš„ epsilon å€¼ï¼Œé»˜è®¤ä¸º 1e-5
            norm_eps: float = 1e-5,
            # äº¤å‰æ³¨æ„åŠ›çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 1280
            cross_attention_dim: Union[int, Tuple[int]] = 1280,
            # æ¯ä¸ªå—çš„å˜æ¢å±‚æ•°ï¼Œé»˜è®¤ä¸º 1
            transformer_layers_per_block: Union[int, Tuple[int], Tuple[Tuple]] = 1,
            # åå‘å˜æ¢å±‚çš„å—æ•°ï¼Œé»˜è®¤ä¸º None
            reverse_transformer_layers_per_block: Optional[Tuple[Tuple[int]]] = None,
            # ç¼–ç å™¨éšè—å±‚çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º None
            encoder_hid_dim: Optional[int] = None,
            # ç¼–ç å™¨éšè—å±‚ç±»å‹ï¼Œé»˜è®¤ä¸º None
            encoder_hid_dim_type: Optional[str] = None,
            # æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 8
            attention_head_dim: Union[int, Tuple[int]] = 8,
            # æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤ä¸º None
            num_attention_heads: Optional[Union[int, Tuple[int]]] = None,
            # æ˜¯å¦ä½¿ç”¨åŒäº¤å‰æ³¨æ„åŠ›ï¼Œé»˜è®¤ä¸º False
            dual_cross_attention: bool = False,
            # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±ï¼Œé»˜è®¤ä¸º False
            use_linear_projection: bool = False,
            # ç±»åµŒå…¥ç±»å‹ï¼Œé»˜è®¤ä¸º None
            class_embed_type: Optional[str] = None,
            # é™„åŠ åµŒå…¥ç±»å‹ï¼Œé»˜è®¤ä¸º None
            addition_embed_type: Optional[str] = None,
            # é™„åŠ æ—¶é—´åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º None
            addition_time_embed_dim: Optional[int] = None,
            # ç±»åµŒå…¥æ•°é‡ï¼Œé»˜è®¤ä¸º None
            num_class_embeds: Optional[int] = None,
            # æ˜¯å¦ä¸Šæº¯æ³¨æ„åŠ›ï¼Œé»˜è®¤ä¸º False
            upcast_attention: bool = False,
            # ResNet æ—¶é—´ç¼©æ”¾åç§»ç±»å‹ï¼Œé»˜è®¤ä¸º "default"
            resnet_time_scale_shift: str = "default",
            # ResNet æ˜¯å¦è·³è¿‡æ—¶é—´æ¿€æ´»ï¼Œé»˜è®¤ä¸º False
            resnet_skip_time_act: bool = False,
            # ResNet è¾“å‡ºç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 1.0
            resnet_out_scale_factor: float = 1.0,
            # æ—¶é—´åµŒå…¥ç±»å‹ï¼Œé»˜è®¤ä¸º "positional"
            time_embedding_type: str = "positional",
            # æ—¶é—´åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º None
            time_embedding_dim: Optional[int] = None,
            # æ—¶é—´åµŒå…¥æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸º None
            time_embedding_act_fn: Optional[str] = None,
            # æ—¶é—´æ­¥åæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸º None
            timestep_post_act: Optional[str] = None,
            # æ—¶é—´æ¡ä»¶æŠ•å½±ç»´åº¦ï¼Œé»˜è®¤ä¸º None
            time_cond_proj_dim: Optional[int] = None,
            # è¾“å…¥å·ç§¯æ ¸å¤§å°ï¼Œé»˜è®¤ä¸º 3
            conv_in_kernel: int = 3,
            # è¾“å‡ºå·ç§¯æ ¸å¤§å°ï¼Œé»˜è®¤ä¸º 3
            conv_out_kernel: int = 3,
            # æŠ•å½±ç±»åµŒå…¥è¾“å…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º None
            projection_class_embeddings_input_dim: Optional[int] = None,
            # æ³¨æ„åŠ›ç±»å‹ï¼Œé»˜è®¤ä¸º "default"
            attention_type: str = "default",
            # ç±»åµŒå…¥æ˜¯å¦æ‹¼æ¥ï¼Œé»˜è®¤ä¸º False
            class_embeddings_concat: bool = False,
            # ä¸­é—´å—æ˜¯å¦ä»…ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œé»˜è®¤ä¸º None
            mid_block_only_cross_attention: Optional[bool] = None,
            # äº¤å‰æ³¨æ„åŠ›å½’ä¸€åŒ–ç±»å‹ï¼Œé»˜è®¤ä¸º None
            cross_attention_norm: Optional[str] = None,
            # é™„åŠ åµŒå…¥ç±»å‹çš„å¤´æ•°é‡ï¼Œé»˜è®¤ä¸º 64
            addition_embed_type_num_heads: int = 64,
    # å®šä¹‰ä¸€ä¸ªç§æœ‰æ–¹æ³•ï¼Œç”¨äºæ£€æŸ¥é…ç½®å‚æ•°
        def _check_config(
            self,
            # å®šä¹‰ä¸‹è¡Œå—ç±»å‹çš„å…ƒç»„ï¼Œè¡¨ç¤ºæ¨¡å‹çš„ç»“æ„
            down_block_types: Tuple[str],
            # å®šä¹‰ä¸Šè¡Œå—ç±»å‹çš„å…ƒç»„ï¼Œè¡¨ç¤ºæ¨¡å‹çš„ç»“æ„
            up_block_types: Tuple[str],
            # å®šä¹‰ä»…ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›çš„æ ‡å¿—ï¼Œå¯ä»¥æ˜¯å¸ƒå°”å€¼æˆ–å¸ƒå°”å€¼çš„å…ƒç»„
            only_cross_attention: Union[bool, Tuple[bool]],
            # å®šä¹‰æ¯ä¸ªå—çš„è¾“å‡ºé€šé“æ•°çš„å…ƒç»„ï¼Œè¡¨ç¤ºå±‚çš„å®½åº¦
            block_out_channels: Tuple[int],
            # å®šä¹‰æ¯ä¸ªå—çš„å±‚æ•°ï¼Œå¯ä»¥æ˜¯æ•´æ•°æˆ–æ•´æ•°çš„å…ƒç»„
            layers_per_block: Union[int, Tuple[int]],
            # å®šä¹‰äº¤å‰æ³¨æ„åŠ›ç»´åº¦ï¼Œå¯ä»¥æ˜¯æ•´æ•°æˆ–æ•´æ•°çš„å…ƒç»„
            cross_attention_dim: Union[int, Tuple[int]],
            # å®šä¹‰æ¯ä¸ªå—çš„å˜æ¢å™¨å±‚æ•°ï¼Œå¯ä»¥æ˜¯æ•´æ•°ã€æ•´æ•°çš„å…ƒç»„æˆ–å…ƒç»„çš„å…ƒç»„
            transformer_layers_per_block: Union[int, Tuple[int], Tuple[Tuple[int]]],
            # å®šä¹‰æ˜¯å¦åè½¬å˜æ¢å™¨å±‚çš„å¸ƒå°”å€¼
            reverse_transformer_layers_per_block: bool,
            # å®šä¹‰æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œè¡¨ç¤ºæ³¨æ„åŠ›çš„åˆ†è¾¨ç‡
            attention_head_dim: int,
            # å®šä¹‰æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œå¯ä»¥æ˜¯å¯é€‰çš„æ•´æ•°æˆ–æ•´æ•°çš„å…ƒç»„
            num_attention_heads: Optional[Union[int, Tuple[int]],
    ):
        # æ£€æŸ¥ down_block_types å’Œ up_block_types çš„é•¿åº¦æ˜¯å¦ç›¸åŒ
        if len(down_block_types) != len(up_block_types):
            # å¦‚æœä¸åŒï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ValueError(
                f"Must provide the same number of `down_block_types` as `up_block_types`. `down_block_types`: {down_block_types}. `up_block_types`: {up_block_types}."
            )

        # æ£€æŸ¥ block_out_channels å’Œ down_block_types çš„é•¿åº¦æ˜¯å¦ç›¸åŒ
        if len(block_out_channels) != len(down_block_types):
            # å¦‚æœä¸åŒï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ValueError(
                f"Must provide the same number of `block_out_channels` as `down_block_types`. `block_out_channels`: {block_out_channels}. `down_block_types`: {down_block_types}."
            )

        # æ£€æŸ¥ only_cross_attention æ˜¯å¦ä¸ºå¸ƒå°”å€¼ä¸”é•¿åº¦ä¸ down_block_types ç›¸åŒ
        if not isinstance(only_cross_attention, bool) and len(only_cross_attention) != len(down_block_types):
            # å¦‚æœä¸æ»¡è¶³æ¡ä»¶ï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ValueError(
                f"Must provide the same number of `only_cross_attention` as `down_block_types`. `only_cross_attention`: {only_cross_attention}. `down_block_types`: {down_block_types}."
            )

        # æ£€æŸ¥ num_attention_heads æ˜¯å¦ä¸ºæ•´æ•°ä¸”é•¿åº¦ä¸ down_block_types ç›¸åŒ
        if not isinstance(num_attention_heads, int) and len(num_attention_heads) != len(down_block_types):
            # å¦‚æœä¸æ»¡è¶³æ¡ä»¶ï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ValueError(
                f"Must provide the same number of `num_attention_heads` as `down_block_types`. `num_attention_heads`: {num_attention_heads}. `down_block_types`: {down_block_types}."
            )

        # æ£€æŸ¥ attention_head_dim æ˜¯å¦ä¸ºæ•´æ•°ä¸”é•¿åº¦ä¸ down_block_types ç›¸åŒ
        if not isinstance(attention_head_dim, int) and len(attention_head_dim) != len(down_block_types):
            # å¦‚æœä¸æ»¡è¶³æ¡ä»¶ï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ValueError(
                f"Must provide the same number of `attention_head_dim` as `down_block_types`. `attention_head_dim`: {attention_head_dim}. `down_block_types`: {down_block_types}."
            )

        # æ£€æŸ¥ cross_attention_dim æ˜¯å¦ä¸ºåˆ—è¡¨ä¸”é•¿åº¦ä¸ down_block_types ç›¸åŒ
        if isinstance(cross_attention_dim, list) and len(cross_attention_dim) != len(down_block_types):
            # å¦‚æœä¸æ»¡è¶³æ¡ä»¶ï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ValueError(
                f"Must provide the same number of `cross_attention_dim` as `down_block_types`. `cross_attention_dim`: {cross_attention_dim}. `down_block_types`: {down_block_types}."
            )

        # æ£€æŸ¥ layers_per_block æ˜¯å¦ä¸ºæ•´æ•°ä¸”é•¿åº¦ä¸ down_block_types ç›¸åŒ
        if not isinstance(layers_per_block, int) and len(layers_per_block) != len(down_block_types):
            # å¦‚æœä¸æ»¡è¶³æ¡ä»¶ï¼ŒæŠ›å‡ºå€¼é”™è¯¯å¹¶æä¾›è¯¦ç»†ä¿¡æ¯
            raise ValueError(
                f"Must provide the same number of `layers_per_block` as `down_block_types`. `layers_per_block`: {layers_per_block}. `down_block_types`: {down_block_types}."
            )
        # æ£€æŸ¥ transformer_layers_per_block æ˜¯å¦ä¸ºåˆ—è¡¨ä¸” reverse_transformer_layers_per_block ä¸º None
        if isinstance(transformer_layers_per_block, list) and reverse_transformer_layers_per_block is None:
            # éå† transformer_layers_per_block ä¸­çš„æ¯ä¸ªå±‚
            for layer_number_per_block in transformer_layers_per_block:
                # æ£€æŸ¥æ¯ä¸ªå±‚æ˜¯å¦ä¸ºåˆ—è¡¨
                if isinstance(layer_number_per_block, list):
                    # å¦‚æœæ˜¯ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯ï¼Œæç¤ºéœ€è¦æä¾› reverse_transformer_layers_per_block
                    raise ValueError("Must provide 'reverse_transformer_layers_per_block` if using asymmetrical UNet.")

    # å®šä¹‰è®¾ç½®æ—¶é—´æŠ•å½±çš„ç§æœ‰æ–¹æ³•
    def _set_time_proj(
        self,
        # æ—¶é—´åµŒå…¥ç±»å‹
        time_embedding_type: str,
        # å—è¾“å‡ºé€šé“æ•°
        block_out_channels: int,
        # æ˜¯å¦ç¿»è½¬æ­£å¼¦å’Œä½™å¼¦
        flip_sin_to_cos: bool,
        # é¢‘ç‡åç§»
        freq_shift: float,
        # æ—¶é—´åµŒå…¥ç»´åº¦
        time_embedding_dim: int,
    # è¿”å›æ—¶é—´åµŒå…¥ç»´åº¦å’Œæ—¶é—´æ­¥è¾“å…¥ç»´åº¦çš„å…ƒç»„
    ) -> Tuple[int, int]:
        # åˆ¤æ–­æ—¶é—´åµŒå…¥ç±»å‹æ˜¯å¦ä¸ºå‚…é‡Œå¶
        if time_embedding_type == "fourier":
            # è®¡ç®—æ—¶é—´åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º block_out_channels[0] * 2
            time_embed_dim = time_embedding_dim or block_out_channels[0] * 2
            # ç¡®ä¿æ—¶é—´åµŒå…¥ç»´åº¦ä¸ºå¶æ•°
            if time_embed_dim % 2 != 0:
                raise ValueError(f"`time_embed_dim` should be divisible by 2, but is {time_embed_dim}.")
            # åˆå§‹åŒ–é«˜æ–¯å‚…é‡Œå¶æŠ•å½±ï¼Œè®¾å®šç›¸å…³å‚æ•°
            self.time_proj = GaussianFourierProjection(
                time_embed_dim // 2, set_W_to_weight=False, log=False, flip_sin_to_cos=flip_sin_to_cos
            )
            # è®¾ç½®æ—¶é—´æ­¥è¾“å…¥ç»´åº¦ä¸ºæ—¶é—´åµŒå…¥ç»´åº¦
            timestep_input_dim = time_embed_dim
        # åˆ¤æ–­æ—¶é—´åµŒå…¥ç±»å‹æ˜¯å¦ä¸ºä½ç½®ç¼–ç 
        elif time_embedding_type == "positional":
            # è®¡ç®—æ—¶é—´åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º block_out_channels[0] * 4
            time_embed_dim = time_embedding_dim or block_out_channels[0] * 4
            # åˆå§‹åŒ–æ—¶é—´æ­¥å¯¹è±¡ï¼Œè®¾å®šç›¸å…³å‚æ•°
            self.time_proj = Timesteps(block_out_channels[0], flip_sin_to_cos, freq_shift)
            # è®¾ç½®æ—¶é—´æ­¥è¾“å…¥ç»´åº¦ä¸º block_out_channels[0]
            timestep_input_dim = block_out_channels[0]
        # å¦‚æœæ—¶é—´åµŒå…¥ç±»å‹ä¸åˆæ³•ï¼ŒæŠ›å‡ºé”™è¯¯
        else:
            raise ValueError(
                f"{time_embedding_type} does not exist. Please make sure to use one of `fourier` or `positional`."
            )
    
        # è¿”å›æ—¶é—´åµŒå…¥ç»´åº¦å’Œæ—¶é—´æ­¥è¾“å…¥ç»´åº¦
        return time_embed_dim, timestep_input_dim
    
    # å®šä¹‰è®¾ç½®ç¼–ç å™¨éšè—æŠ•å½±çš„æ–¹æ³•
    def _set_encoder_hid_proj(
        self,
        encoder_hid_dim_type: Optional[str],
        cross_attention_dim: Union[int, Tuple[int]],
        encoder_hid_dim: Optional[int],
    ):
        # å¦‚æœç¼–ç å™¨éšè—ç»´åº¦ç±»å‹ä¸ºç©ºä¸”éšè—ç»´åº¦å·²å®šä¹‰
        if encoder_hid_dim_type is None and encoder_hid_dim is not None:
            # é»˜è®¤å°†ç¼–ç å™¨éšè—ç»´åº¦ç±»å‹è®¾ä¸º'text_proj'
            encoder_hid_dim_type = "text_proj"
            # æ³¨å†Œç¼–ç å™¨éšè—ç»´åº¦ç±»å‹åˆ°é…ç½®ä¸­
            self.register_to_config(encoder_hid_dim_type=encoder_hid_dim_type)
            # è®°å½•ä¿¡æ¯æ—¥å¿—
            logger.info("encoder_hid_dim_type defaults to 'text_proj' as `encoder_hid_dim` is defined.")
    
        # å¦‚æœç¼–ç å™¨éšè—ç»´åº¦ä¸ºç©ºä¸”éšè—ç»´åº¦ç±»å‹å·²å®šä¹‰ï¼ŒæŠ›å‡ºé”™è¯¯
        if encoder_hid_dim is None and encoder_hid_dim_type is not None:
            raise ValueError(
                f"`encoder_hid_dim` has to be defined when `encoder_hid_dim_type` is set to {encoder_hid_dim_type}."
            )
    
        # åˆ¤æ–­ç¼–ç å™¨éšè—ç»´åº¦ç±»å‹æ˜¯å¦ä¸º'text_proj'
        if encoder_hid_dim_type == "text_proj":
            # åˆå§‹åŒ–çº¿æ€§æŠ•å½±å±‚ï¼Œè¾“å…¥ç»´åº¦ä¸ºencoder_hid_dimï¼Œè¾“å‡ºç»´åº¦ä¸ºcross_attention_dim
            self.encoder_hid_proj = nn.Linear(encoder_hid_dim, cross_attention_dim)
        # åˆ¤æ–­ç¼–ç å™¨éšè—ç»´åº¦ç±»å‹æ˜¯å¦ä¸º'text_image_proj'
        elif encoder_hid_dim_type == "text_image_proj":
            # åˆå§‹åŒ–æ–‡æœ¬-å›¾åƒæŠ•å½±å¯¹è±¡ï¼Œè®¾å®šç›¸å…³å‚æ•°
            self.encoder_hid_proj = TextImageProjection(
                text_embed_dim=encoder_hid_dim,
                image_embed_dim=cross_attention_dim,
                cross_attention_dim=cross_attention_dim,
            )
        # åˆ¤æ–­ç¼–ç å™¨éšè—ç»´åº¦ç±»å‹æ˜¯å¦ä¸º'image_proj'
        elif encoder_hid_dim_type == "image_proj":
            # åˆå§‹åŒ–å›¾åƒæŠ•å½±å¯¹è±¡ï¼Œè®¾å®šç›¸å…³å‚æ•°
            self.encoder_hid_proj = ImageProjection(
                image_embed_dim=encoder_hid_dim,
                cross_attention_dim=cross_attention_dim,
            )
        # å¦‚æœç¼–ç å™¨éšè—ç»´åº¦ç±»å‹ä¸åˆæ³•ï¼ŒæŠ›å‡ºé”™è¯¯
        elif encoder_hid_dim_type is not None:
            raise ValueError(
                f"encoder_hid_dim_type: {encoder_hid_dim_type} must be None, 'text_proj' or 'text_image_proj'."
            )
        # å¦‚æœéƒ½ä¸ç¬¦åˆï¼Œå°†ç¼–ç å™¨éšè—æŠ•å½±è®¾ä¸ºNone
        else:
            self.encoder_hid_proj = None
    # è®¾ç½®ç±»åµŒå…¥çš„ç§æœ‰æ–¹æ³•
        def _set_class_embedding(
            self,
            class_embed_type: Optional[str],  # åµŒå…¥ç±»å‹ï¼Œå¯èƒ½ä¸º None æˆ–ç‰¹å®šå­—ç¬¦ä¸²
            act_fn: str,  # æ¿€æ´»å‡½æ•°çš„åç§°
            num_class_embeds: Optional[int],  # ç±»åµŒå…¥æ•°é‡ï¼Œå¯èƒ½ä¸º None
            projection_class_embeddings_input_dim: Optional[int],  # æŠ•å½±ç±»åµŒå…¥è¾“å…¥ç»´åº¦ï¼Œå¯èƒ½ä¸º None
            time_embed_dim: int,  # æ—¶é—´åµŒå…¥çš„ç»´åº¦
            timestep_input_dim: int,  # æ—¶é—´æ­¥è¾“å…¥çš„ç»´åº¦
        ):
            # å¦‚æœåµŒå…¥ç±»å‹ä¸º None ä¸”ç±»åµŒå…¥æ•°é‡ä¸ä¸º None
            if class_embed_type is None and num_class_embeds is not None:
                # åˆ›å»ºåµŒå…¥å±‚ï¼Œå¤§å°ä¸ºç±»åµŒå…¥æ•°é‡å’Œæ—¶é—´åµŒå…¥ç»´åº¦
                self.class_embedding = nn.Embedding(num_class_embeds, time_embed_dim)
            # å¦‚æœåµŒå…¥ç±»å‹ä¸º "timestep"
            elif class_embed_type == "timestep":
                # åˆ›å»ºæ—¶é—´æ­¥åµŒå…¥å¯¹è±¡
                self.class_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim, act_fn=act_fn)
            # å¦‚æœåµŒå…¥ç±»å‹ä¸º "identity"
            elif class_embed_type == "identity":
                # åˆ›å»ºæ’ç­‰å±‚ï¼Œè¾“å…¥å’Œè¾“å‡ºç»´åº¦ç›¸åŒ
                self.class_embedding = nn.Identity(time_embed_dim, time_embed_dim)
            # å¦‚æœåµŒå…¥ç±»å‹ä¸º "projection"
            elif class_embed_type == "projection":
                # å¦‚æœæŠ•å½±ç±»åµŒå…¥è¾“å…¥ç»´åº¦ä¸º Noneï¼ŒæŠ›å‡ºé”™è¯¯
                if projection_class_embeddings_input_dim is None:
                    raise ValueError(
                        "`class_embed_type`: 'projection' requires `projection_class_embeddings_input_dim` be set"
                    )
                # åˆ›å»ºæŠ•å½±æ—¶é—´æ­¥åµŒå…¥å¯¹è±¡
                self.class_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)
            # å¦‚æœåµŒå…¥ç±»å‹ä¸º "simple_projection"
            elif class_embed_type == "simple_projection":
                # å¦‚æœæŠ•å½±ç±»åµŒå…¥è¾“å…¥ç»´åº¦ä¸º Noneï¼ŒæŠ›å‡ºé”™è¯¯
                if projection_class_embeddings_input_dim is None:
                    raise ValueError(
                        "`class_embed_type`: 'simple_projection' requires `projection_class_embeddings_input_dim` be set"
                    )
                # åˆ›å»ºçº¿æ€§å±‚ä½œä¸ºç®€å•æŠ•å½±
                self.class_embedding = nn.Linear(projection_class_embeddings_input_dim, time_embed_dim)
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„åµŒå…¥ç±»å‹
            else:
                # å°†ç±»åµŒå…¥è®¾ç½®ä¸º None
                self.class_embedding = None
    
        # è®¾ç½®é™„åŠ åµŒå…¥çš„ç§æœ‰æ–¹æ³•
        def _set_add_embedding(
            self,
            addition_embed_type: str,  # é™„åŠ åµŒå…¥ç±»å‹
            addition_embed_type_num_heads: int,  # é™„åŠ åµŒå…¥ç±»å‹çš„å¤´æ•°
            addition_time_embed_dim: Optional[int],  # é™„åŠ æ—¶é—´åµŒå…¥ç»´åº¦ï¼Œå¯èƒ½ä¸º None
            flip_sin_to_cos: bool,  # æ˜¯å¦ç¿»è½¬æ­£å¼¦åˆ°ä½™å¼¦
            freq_shift: float,  # é¢‘ç‡åç§»é‡
            cross_attention_dim: Optional[int],  # äº¤å‰æ³¨æ„åŠ›ç»´åº¦ï¼Œå¯èƒ½ä¸º None
            encoder_hid_dim: Optional[int],  # ç¼–ç å™¨éšè—ç»´åº¦ï¼Œå¯èƒ½ä¸º None
            projection_class_embeddings_input_dim: Optional[int],  # æŠ•å½±ç±»åµŒå…¥è¾“å…¥ç»´åº¦ï¼Œå¯èƒ½ä¸º None
            time_embed_dim: int,  # æ—¶é—´åµŒå…¥ç»´åº¦
    ):
        # æ£€æŸ¥é™„åŠ åµŒå…¥ç±»å‹æ˜¯å¦ä¸º "text"
        if addition_embed_type == "text":
            # å¦‚æœç¼–ç å™¨éšè—ç»´åº¦ä¸ä¸º Noneï¼Œåˆ™ä½¿ç”¨è¯¥ç»´åº¦
            if encoder_hid_dim is not None:
                text_time_embedding_from_dim = encoder_hid_dim
            # å¦åˆ™ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ç»´åº¦
            else:
                text_time_embedding_from_dim = cross_attention_dim

            # åˆ›å»ºæ–‡æœ¬æ—¶é—´åµŒå…¥å¯¹è±¡
            self.add_embedding = TextTimeEmbedding(
                text_time_embedding_from_dim, time_embed_dim, num_heads=addition_embed_type_num_heads
            )
        # æ£€æŸ¥é™„åŠ åµŒå…¥ç±»å‹æ˜¯å¦ä¸º "text_image"
        elif addition_embed_type == "text_image":
            # text_embed_dim å’Œ image_embed_dim ä¸å¿…æ˜¯ `cross_attention_dim`ï¼Œä¸ºäº†é¿å… __init__ è¿‡äºç¹æ‚
            # åœ¨è¿™é‡Œè®¾ç½®ä¸º `cross_attention_dim`ï¼Œå› ä¸ºè¿™æ˜¯å½“å‰å”¯ä¸€ä½¿ç”¨æƒ…å†µçš„æ‰€éœ€ç»´åº¦ (Kandinsky 2.1)
            self.add_embedding = TextImageTimeEmbedding(
                text_embed_dim=cross_attention_dim, image_embed_dim=cross_attention_dim, time_embed_dim=time_embed_dim
            )
        # æ£€æŸ¥é™„åŠ åµŒå…¥ç±»å‹æ˜¯å¦ä¸º "text_time"
        elif addition_embed_type == "text_time":
            # åˆ›å»ºæ—¶é—´æŠ•å½±å¯¹è±¡
            self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos, freq_shift)
            # åˆ›å»ºæ—¶é—´åµŒå…¥å¯¹è±¡
            self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)
        # æ£€æŸ¥é™„åŠ åµŒå…¥ç±»å‹æ˜¯å¦ä¸º "image"
        elif addition_embed_type == "image":
            # Kandinsky 2.2
            # åˆ›å»ºå›¾åƒæ—¶é—´åµŒå…¥å¯¹è±¡
            self.add_embedding = ImageTimeEmbedding(image_embed_dim=encoder_hid_dim, time_embed_dim=time_embed_dim)
        # æ£€æŸ¥é™„åŠ åµŒå…¥ç±»å‹æ˜¯å¦ä¸º "image_hint"
        elif addition_embed_type == "image_hint":
            # Kandinsky 2.2 ControlNet
            # åˆ›å»ºå›¾åƒæç¤ºæ—¶é—´åµŒå…¥å¯¹è±¡
            self.add_embedding = ImageHintTimeEmbedding(image_embed_dim=encoder_hid_dim, time_embed_dim=time_embed_dim)
        # æ£€æŸ¥é™„åŠ åµŒå…¥ç±»å‹æ˜¯å¦ä¸º None ä»¥å¤–çš„å€¼
        elif addition_embed_type is not None:
            # æŠ›å‡ºå€¼é”™è¯¯ï¼Œæç¤ºæ— æ•ˆçš„é™„åŠ åµŒå…¥ç±»å‹
            raise ValueError(f"addition_embed_type: {addition_embed_type} must be None, 'text' or 'text_image'.")

    # å®šä¹‰ä¸€ä¸ªå±æ€§æ–¹æ³•ï¼Œç”¨äºè®¾ç½®ä½ç½®ç½‘ç»œ
    def _set_pos_net_if_use_gligen(self, attention_type: str, cross_attention_dim: int):
        # æ£€æŸ¥æ³¨æ„åŠ›ç±»å‹æ˜¯å¦ä¸º "gated" æˆ– "gated-text-image"
        if attention_type in ["gated", "gated-text-image"]:
            positive_len = 768  # é»˜è®¤çš„æ­£å‘é•¿åº¦
            # å¦‚æœäº¤å‰æ³¨æ„åŠ›ç»´åº¦æ˜¯æ•´æ•°ï¼Œåˆ™ä½¿ç”¨è¯¥å€¼
            if isinstance(cross_attention_dim, int):
                positive_len = cross_attention_dim
            # å¦‚æœäº¤å‰æ³¨æ„åŠ›ç»´åº¦æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼
            elif isinstance(cross_attention_dim, (list, tuple)):
                positive_len = cross_attention_dim[0]

            # æ ¹æ®æ³¨æ„åŠ›ç±»å‹ç¡®å®šç‰¹å¾ç±»å‹
            feature_type = "text-only" if attention_type == "gated" else "text-image"
            # åˆ›å»º GLIGEN æ–‡æœ¬è¾¹ç•Œæ¡†æŠ•å½±å¯¹è±¡
            self.position_net = GLIGENTextBoundingboxProjection(
                positive_len=positive_len, out_dim=cross_attention_dim, feature_type=feature_type
            )

    # å®šä¹‰ä¸€ä¸ªå±æ€§
    @property
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¨¡å‹ä¸­æ‰€æœ‰çš„æ³¨æ„åŠ›å¤„ç†å™¨
    def attn_processors(self) -> Dict[str, AttentionProcessor]:
        r"""
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        """
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨æ³¨æ„åŠ›å¤„ç†å™¨
        processors = {}

        # å®šä¹‰ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œç”¨äºæ·»åŠ å¤„ç†å™¨åˆ°å­—å…¸
        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
            # æ£€æŸ¥æ¨¡å—æ˜¯å¦æœ‰è·å–å¤„ç†å™¨çš„æ–¹æ³•
            if hasattr(module, "get_processor"):
                # å°†å¤„ç†å™¨æ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œé”®ä¸ºåç§°ï¼Œå€¼ä¸ºå¤„ç†å™¨
                processors[f"{name}.processor"] = module.get_processor()

            # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
            for sub_name, child in module.named_children():
                # é€’å½’è°ƒç”¨ï¼Œå¤„ç†å­æ¨¡å—
                fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)

            # è¿”å›æ›´æ–°åçš„å¤„ç†å™¨å­—å…¸
            return processors

        # éå†å½“å‰æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
        for name, module in self.named_children():
            # è°ƒç”¨é€’å½’å‡½æ•°ï¼Œæ·»åŠ å¤„ç†å™¨
            fn_recursive_add_processors(name, module, processors)

        # è¿”å›åŒ…å«æ‰€æœ‰å¤„ç†å™¨çš„å­—å…¸
        return processors

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œè®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨
    def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
        r"""
        Sets the attention processor to use to compute attention.

        Parameters:
            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):
                The instantiated processor class or a dictionary of processor classes that will be set as the processor
                for **all** `Attention` layers.

                If `processor` is a dict, the key needs to define the path to the corresponding cross attention
                processor. This is strongly recommended when setting trainable attention processors.

        """
        # è·å–å½“å‰å¤„ç†å™¨çš„æ•°é‡
        count = len(self.attn_processors.keys())

        # å¦‚æœä¼ å…¥çš„æ˜¯å­—å…¸ï¼Œä¸”å­—å…¸é•¿åº¦ä¸æ³¨æ„åŠ›å±‚æ•°é‡ä¸åŒ¹é…ï¼ŒæŠ›å‡ºé”™è¯¯
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f"A dict of processors was passed, but the number of processors {len(processor)} does not match the"
                f" number of attention layers: {count}. Please make sure to pass {count} processor classes."
            )

        # å®šä¹‰ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œç”¨äºè®¾ç½®å¤„ç†å™¨
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            # æ£€æŸ¥æ¨¡å—æ˜¯å¦æœ‰è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
            if hasattr(module, "set_processor"):
                # å¦‚æœå¤„ç†å™¨ä¸æ˜¯å­—å…¸ï¼Œç›´æ¥è®¾ç½®
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    # ä»å­—å…¸ä¸­å¼¹å‡ºå¯¹åº”çš„å¤„ç†å™¨å¹¶è®¾ç½®
                    module.set_processor(processor.pop(f"{name}.processor"))

            # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
            for sub_name, child in module.named_children():
                # é€’å½’è°ƒç”¨ï¼Œè®¾ç½®å­æ¨¡å—çš„å¤„ç†å™¨
                fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

        # éå†å½“å‰æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
        for name, module in self.named_children():
            # è°ƒç”¨é€’å½’å‡½æ•°ï¼Œè®¾ç½®å¤„ç†å™¨
            fn_recursive_attn_processor(name, module, processor)
    # å®šä¹‰è®¾ç½®é»˜è®¤æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•
    def set_default_attn_processor(self):
        """
        ç¦ç”¨è‡ªå®šä¹‰æ³¨æ„åŠ›å¤„ç†å™¨å¹¶è®¾ç½®é»˜è®¤çš„æ³¨æ„åŠ›å®ç°ã€‚
        """
        # æ£€æŸ¥æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨æ˜¯å¦å±äºæ·»åŠ çš„é”®å€¼æ³¨æ„åŠ›å¤„ç†å™¨ç±»
        if all(proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
            # åˆ›å»ºæ·»åŠ é”®å€¼æ³¨æ„åŠ›å¤„ç†å™¨çš„å®ä¾‹
            processor = AttnAddedKVProcessor()
        # æ£€æŸ¥æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨æ˜¯å¦å±äºäº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨ç±»
        elif all(proc.__class__ in CROSS_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
            # åˆ›å»ºæ ‡å‡†æ³¨æ„åŠ›å¤„ç†å™¨çš„å®ä¾‹
            processor = AttnProcessor()
        else:
            # å¦‚æœæ³¨æ„åŠ›å¤„ç†å™¨ç±»å‹ä¸åŒ¹é…ï¼Œåˆ™æŠ›å‡ºé”™è¯¯
            raise ValueError(
                f"Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}"
            )

        # è®¾ç½®é€‰å®šçš„æ³¨æ„åŠ›å¤„ç†å™¨
        self.set_attn_processor(processor)

    # å®šä¹‰è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ–¹æ³•
    def _set_gradient_checkpointing(self, module, value=False):
        # å¦‚æœæ¨¡å—å…·æœ‰æ¢¯åº¦æ£€æŸ¥ç‚¹å±æ€§ï¼Œåˆ™è®¾ç½®å…¶å€¼
        if hasattr(module, "gradient_checkpointing"):
            module.gradient_checkpointing = value

    # å®šä¹‰å¯ç”¨ FreeU æœºåˆ¶çš„æ–¹æ³•
    def enable_freeu(self, s1: float, s2: float, b1: float, b2: float):
        r"""å¯ç”¨ FreeU æœºåˆ¶ï¼Œè¯¦ç»†ä¿¡æ¯è¯·è§ https://arxiv.org/abs/2309.11497ã€‚

        åœ¨ç¼©æ”¾å› å­åé¢çš„åç¼€è¡¨ç¤ºå®ƒä»¬è¢«åº”ç”¨çš„é˜¶æ®µå—ã€‚

        è¯·å‚è€ƒ [å®˜æ–¹ä»“åº“](https://github.com/ChenyangSi/FreeU) ä»¥è·å–å·²çŸ¥åœ¨ä¸åŒç®¡é“ï¼ˆå¦‚ Stable Diffusion v1ã€v2 å’Œ Stable Diffusion XLï¼‰ä¸­æ•ˆæœè‰¯å¥½çš„å€¼ç»„åˆã€‚

        å‚æ•°ï¼š
            s1 (`float`):
                é˜¶æ®µ 1 çš„ç¼©æ”¾å› å­ï¼Œç”¨äºå‡å¼±è·³è·ƒç‰¹å¾çš„è´¡çŒ®ï¼Œä»¥å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡å¹³æ»‘æ•ˆåº”â€ã€‚
            s2 (`float`):
                é˜¶æ®µ 2 çš„ç¼©æ”¾å› å­ï¼Œç”¨äºå‡å¼±è·³è·ƒç‰¹å¾çš„è´¡çŒ®ï¼Œä»¥å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡å¹³æ»‘æ•ˆåº”â€ã€‚
            b1 (`float`): é˜¶æ®µ 1 çš„ç¼©æ”¾å› å­ï¼Œç”¨äºå¢å¼ºéª¨å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚
            b2 (`float`): é˜¶æ®µ 2 çš„ç¼©æ”¾å› å­ï¼Œç”¨äºå¢å¼ºéª¨å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚
        """
        # éå†ä¸Šé‡‡æ ·å—å¹¶è®¾ç½®ç›¸åº”çš„ç¼©æ”¾å› å­
        for i, upsample_block in enumerate(self.up_blocks):
            setattr(upsample_block, "s1", s1)  # è®¾ç½®é˜¶æ®µ 1 çš„ç¼©æ”¾å› å­
            setattr(upsample_block, "s2", s2)  # è®¾ç½®é˜¶æ®µ 2 çš„ç¼©æ”¾å› å­
            setattr(upsample_block, "b1", b1)  # è®¾ç½®é˜¶æ®µ 1 çš„éª¨å¹²ç¼©æ”¾å› å­
            setattr(upsample_block, "b2", b2)  # è®¾ç½®é˜¶æ®µ 2 çš„éª¨å¹²ç¼©æ”¾å› å­

    # å®šä¹‰ç¦ç”¨ FreeU æœºåˆ¶çš„æ–¹æ³•
    def disable_freeu(self):
        """ç¦ç”¨ FreeU æœºåˆ¶ã€‚"""
        freeu_keys = {"s1", "s2", "b1", "b2"}  # å®šä¹‰ FreeU ç›¸å…³çš„é”®
        # éå†ä¸Šé‡‡æ ·å—
        for i, upsample_block in enumerate(self.up_blocks):
            # éå†æ¯ä¸ª FreeU é”®
            for k in freeu_keys:
                # å¦‚æœä¸Šé‡‡æ ·å—å…·æœ‰è¯¥é”®çš„å±æ€§æˆ–å…¶å€¼ä¸ä¸º Noneï¼Œåˆ™å°†å…¶å€¼è®¾ç½®ä¸º None
                if hasattr(upsample_block, k) or getattr(upsample_block, k, None) is not None:
                    setattr(upsample_block, k, None)
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå¯ç”¨èåˆçš„ QKV æŠ•å½±
    def fuse_qkv_projections(self):
        """
        å¯ç”¨èåˆçš„ QKV æŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚
        å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼çš„æŠ•å½±çŸ©é˜µè¢«èåˆã€‚

        <Tip warning={true}>

        æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚

        </Tip>
        """
        # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None
        self.original_attn_processors = None

        # éå†æ³¨æ„åŠ›å¤„ç†å™¨ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«â€œAddedâ€å­—æ ·
        for _, attn_processor in self.attn_processors.items():
            # å¦‚æœå‘ç°æ·»åŠ çš„ KV æŠ•å½±ï¼ŒæŠ›å‡ºé”™è¯¯
            if "Added" in str(attn_processor.__class__.__name__):
                raise ValueError("`fuse_qkv_projections()` is not supported for models having added KV projections.")

        # ä¿å­˜å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨
        self.original_attn_processors = self.attn_processors

        # éå†æ¨¡å—ï¼ŒæŸ¥æ‰¾ç±»å‹ä¸º Attention çš„æ¨¡å—
        for module in self.modules():
            if isinstance(module, Attention):
                # å¯ç”¨æŠ•å½±èåˆ
                module.fuse_projections(fuse=True)

        # è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä¸ºèåˆçš„å¤„ç†å™¨
        self.set_attn_processor(FusedAttnProcessor2_0())

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºç¦ç”¨å·²å¯ç”¨çš„èåˆ QKV æŠ•å½±
    def unfuse_qkv_projections(self):
        """ç¦ç”¨å·²å¯ç”¨çš„èåˆ QKV æŠ•å½±ã€‚

        <Tip warning={true}>

        æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚

        </Tip>

        """
        # å¦‚æœåŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸ä¸º Noneï¼Œåˆ™æ¢å¤åˆ°åŸå§‹å¤„ç†å™¨
        if self.original_attn_processors is not None:
            self.set_attn_processor(self.original_attn_processors)

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºè·å–æ—¶é—´åµŒå…¥
    def get_time_embed(
        self, sample: torch.Tensor, timestep: Union[torch.Tensor, float, int]
    ) -> Optional[torch.Tensor]:
        # å°†æ—¶é—´æ­¥é•¿èµ‹å€¼ç»™ timesteps
        timesteps = timestep
        # å¦‚æœ timesteps ä¸æ˜¯å¼ é‡
        if not torch.is_tensor(timesteps):
            # TODO: è¿™éœ€è¦åœ¨ CPU å’Œ GPU ä¹‹é—´åŒæ­¥ã€‚å› æ­¤ï¼Œå¦‚æœå¯ä»¥çš„è¯ï¼Œå°½é‡å°† timesteps ä½œä¸ºå¼ é‡ä¼ é€’
            # è¿™å°†æ˜¯ä½¿ç”¨ `match` è¯­å¥çš„å¥½ä¾‹å­ï¼ˆPython 3.10+ï¼‰
            is_mps = sample.device.type == "mps"  # æ£€æŸ¥è®¾å¤‡ç±»å‹æ˜¯å¦ä¸º MPS
            # æ ¹æ®æ—¶é—´æ­¥é•¿ç±»å‹è®¾ç½®æ•°æ®ç±»å‹
            if isinstance(timestep, float):
                dtype = torch.float32 if is_mps else torch.float64  # æµ®ç‚¹æ•°ç±»å‹
            else:
                dtype = torch.int32 if is_mps else torch.int64  # æ•´æ•°ç±»å‹
            # å°† timesteps è½¬æ¢ä¸ºå¼ é‡
            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)
        # å¦‚æœ timesteps æ˜¯æ ‡é‡ï¼ˆé›¶ç»´å¼ é‡ï¼‰ï¼Œåˆ™æ‰©å±•ç»´åº¦
        elif len(timesteps.shape) == 0:
            timesteps = timesteps[None].to(sample.device)  # å¢åŠ ä¸€ä¸ªç»´åº¦å¹¶è½¬ç§»åˆ°æ ·æœ¬è®¾å¤‡

        # å°† timesteps å¹¿æ’­åˆ°ä¸æ ·æœ¬æ‰¹æ¬¡ç»´åº¦å…¼å®¹çš„æ–¹å¼
        timesteps = timesteps.expand(sample.shape[0])  # æ‰©å±•åˆ°æ‰¹æ¬¡å¤§å°

        # é€šè¿‡æ—¶é—´æŠ•å½±è·å¾—æ—¶é—´åµŒå…¥
        t_emb = self.time_proj(timesteps)
        # `Timesteps` ä¸åŒ…å«ä»»ä½•æƒé‡ï¼Œæ€»æ˜¯è¿”å› f32 å¼ é‡
        # ä½†æ—¶é—´åµŒå…¥å¯èƒ½å®é™…åœ¨ fp16 ä¸­è¿è¡Œï¼Œå› æ­¤éœ€è¦è¿›è¡Œç±»å‹è½¬æ¢ã€‚
        # å¯èƒ½æœ‰æ›´å¥½çš„æ–¹æ³•æ¥å°è£…è¿™ä¸€ç‚¹ã€‚
        t_emb = t_emb.to(dtype=sample.dtype)  # è½¬æ¢ t_emb çš„æ•°æ®ç±»å‹
        # è¿”å›æ—¶é—´åµŒå…¥
        return t_emb
    # è·å–ç±»åµŒå…¥çš„æ–¹æ³•ï¼Œæ¥å—æ ·æœ¬å¼ é‡å’Œå¯é€‰çš„ç±»æ ‡ç­¾
        def get_class_embed(self, sample: torch.Tensor, class_labels: Optional[torch.Tensor]) -> Optional[torch.Tensor]:
            # åˆå§‹åŒ–ç±»åµŒå…¥ä¸º None
            class_emb = None
            # æ£€æŸ¥ç±»åµŒå…¥æ˜¯å¦å­˜åœ¨
            if self.class_embedding is not None:
                # å¦‚æœç±»æ ‡ç­¾ä¸º Noneï¼ŒæŠ›å‡ºé”™è¯¯
                if class_labels is None:
                    raise ValueError("class_labels should be provided when num_class_embeds > 0")
    
                # æ£€æŸ¥ç±»åµŒå…¥ç±»å‹æ˜¯å¦ä¸ºæ—¶é—´æ­¥
                if self.config.class_embed_type == "timestep":
                    # å°†ç±»æ ‡ç­¾é€šè¿‡æ—¶é—´æŠ•å½±å¤„ç†
                    class_labels = self.time_proj(class_labels)
    
                    # `Timesteps` ä¸åŒ…å«æƒé‡ï¼Œæ€»æ˜¯è¿”å› f32 å¼ é‡
                    # å¯èƒ½æœ‰æ›´å¥½çš„æ–¹å¼æ¥å°è£…è¿™ä¸€ç‚¹
                    class_labels = class_labels.to(dtype=sample.dtype)
    
                # è·å–ç±»åµŒå…¥å¹¶è½¬æ¢ä¸ºä¸æ ·æœ¬ç›¸åŒçš„æ•°æ®ç±»å‹
                class_emb = self.class_embedding(class_labels).to(dtype=sample.dtype)
            # è¿”å›ç±»åµŒå…¥
            return class_emb
    
        # è·å–å¢å¼ºåµŒå…¥çš„æ–¹æ³•ï¼Œæ¥å—åµŒå…¥å¼ é‡ã€ç¼–ç å™¨éšè—çŠ¶æ€å’Œé¢å¤–æ¡ä»¶å‚æ•°
        def get_aug_embed(
            self, emb: torch.Tensor, encoder_hidden_states: torch.Tensor, added_cond_kwargs: Dict[str, Any]
        # å¤„ç†ç¼–ç å™¨éšè—çŠ¶æ€çš„æ–¹æ³•ï¼Œæ¥å—ç¼–ç å™¨éšè—çŠ¶æ€å’Œé¢å¤–æ¡ä»¶å‚æ•°
        def process_encoder_hidden_states(
            self, encoder_hidden_states: torch.Tensor, added_cond_kwargs: Dict[str, Any]
    # å®šä¹‰è¿”å›ç±»å‹ä¸º torch.Tensor
        ) -> torch.Tensor:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨éšè—å±‚æŠ•å½±ï¼Œå¹¶ä¸”é…ç½®ä¸º "text_proj"
            if self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_proj":
                # ä½¿ç”¨æ–‡æœ¬æŠ•å½±å¯¹ç¼–ç éšè—çŠ¶æ€è¿›è¡Œè½¬æ¢
                encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states)
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨éšè—å±‚æŠ•å½±ï¼Œå¹¶ä¸”é…ç½®ä¸º "text_image_proj"
            elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_image_proj":
                # æ£€æŸ¥æ¡ä»¶ä¸­æ˜¯å¦åŒ…å« "image_embeds"
                if "image_embeds" not in added_cond_kwargs:
                    # æŠ›å‡ºé”™è¯¯æç¤ºç¼ºå°‘å¿…è¦å‚æ•°
                    raise ValueError(
                        f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'text_image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`"
                    )
    
                # è·å–ä¼ å…¥çš„å›¾åƒåµŒå…¥
                image_embeds = added_cond_kwargs.get("image_embeds")
                # å¯¹ç¼–ç éšè—çŠ¶æ€å’Œå›¾åƒåµŒå…¥è¿›è¡ŒæŠ•å½±è½¬æ¢
                encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states, image_embeds)
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨éšè—å±‚æŠ•å½±ï¼Œå¹¶ä¸”é…ç½®ä¸º "image_proj"
            elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "image_proj":
                # æ£€æŸ¥æ¡ä»¶ä¸­æ˜¯å¦åŒ…å« "image_embeds"
                if "image_embeds" not in added_cond_kwargs:
                    # æŠ›å‡ºé”™è¯¯æç¤ºç¼ºå°‘å¿…è¦å‚æ•°
                    raise ValueError(
                        f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`"
                    )
                # è·å–ä¼ å…¥çš„å›¾åƒåµŒå…¥
                image_embeds = added_cond_kwargs.get("image_embeds")
                # ä½¿ç”¨å›¾åƒåµŒå…¥å¯¹ç¼–ç éšè—çŠ¶æ€è¿›è¡ŒæŠ•å½±è½¬æ¢
                encoder_hidden_states = self.encoder_hid_proj(image_embeds)
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨éšè—å±‚æŠ•å½±ï¼Œå¹¶ä¸”é…ç½®ä¸º "ip_image_proj"
            elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "ip_image_proj":
                # æ£€æŸ¥æ¡ä»¶ä¸­æ˜¯å¦åŒ…å« "image_embeds"
                if "image_embeds" not in added_cond_kwargs:
                    # æŠ›å‡ºé”™è¯¯æç¤ºç¼ºå°‘å¿…è¦å‚æ•°
                    raise ValueError(
                        f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'ip_image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`"
                    )
    
                # å¦‚æœå­˜åœ¨æ–‡æœ¬ç¼–ç å™¨çš„éšè—å±‚æŠ•å½±ï¼Œåˆ™å¯¹ç¼–ç éšè—çŠ¶æ€è¿›è¡ŒæŠ•å½±è½¬æ¢
                if hasattr(self, "text_encoder_hid_proj") and self.text_encoder_hid_proj is not None:
                    encoder_hidden_states = self.text_encoder_hid_proj(encoder_hidden_states)
    
                # è·å–ä¼ å…¥çš„å›¾åƒåµŒå…¥
                image_embeds = added_cond_kwargs.get("image_embeds")
                # å¯¹å›¾åƒåµŒå…¥è¿›è¡ŒæŠ•å½±è½¬æ¢
                image_embeds = self.encoder_hid_proj(image_embeds)
                # å°†ç¼–ç éšè—çŠ¶æ€å’Œå›¾åƒåµŒå…¥æ‰“åŒ…æˆå…ƒç»„
                encoder_hidden_states = (encoder_hidden_states, image_embeds)
            # è¿”å›æœ€ç»ˆçš„ç¼–ç éšè—çŠ¶æ€
            return encoder_hidden_states
    # å®šä¹‰å‰å‘ä¼ æ’­å‡½æ•°
    def forward(
            # è¾“å…¥çš„æ ·æœ¬æ•°æ®ï¼Œç±»å‹ä¸º PyTorch å¼ é‡
            sample: torch.Tensor,
            # å½“å‰æ—¶é—´æ­¥ï¼Œç±»å‹å¯ä»¥æ˜¯å¼ é‡ã€æµ®ç‚¹æ•°æˆ–æ•´æ•°
            timestep: Union[torch.Tensor, float, int],
            # ç¼–ç å™¨çš„éšè—çŠ¶æ€ï¼Œç±»å‹ä¸º PyTorch å¼ é‡
            encoder_hidden_states: torch.Tensor,
            # å¯é€‰çš„ç±»åˆ«æ ‡ç­¾ï¼Œç±»å‹ä¸º PyTorch å¼ é‡
            class_labels: Optional[torch.Tensor] = None,
            # å¯é€‰çš„æ—¶é—´æ­¥æ¡ä»¶ï¼Œç±»å‹ä¸º PyTorch å¼ é‡
            timestep_cond: Optional[torch.Tensor] = None,
            # å¯é€‰çš„æ³¨æ„åŠ›æ©ç ï¼Œç±»å‹ä¸º PyTorch å¼ é‡
            attention_mask: Optional[torch.Tensor] = None,
            # å¯é€‰çš„äº¤å‰æ³¨æ„åŠ›å‚æ•°ï¼Œç±»å‹ä¸ºå­—å…¸ï¼ŒåŒ…å«é¢å¤–çš„å…³é”®å­—å‚æ•°
            cross_attention_kwargs: Optional[Dict[str, Any]] = None,
            # å¯é€‰çš„é™„åŠ æ¡ä»¶å‚æ•°ï¼Œç±»å‹ä¸ºå­—å…¸ï¼Œé”®ä¸ºå­—ç¬¦ä¸²ï¼Œå€¼ä¸º PyTorch å¼ é‡
            added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,
            # å¯é€‰çš„ä¸‹å±‚å—é™„åŠ æ®‹å·®ï¼Œç±»å‹ä¸ºå…ƒç»„ï¼ŒåŒ…å« PyTorch å¼ é‡
            down_block_additional_residuals: Optional[Tuple[torch.Tensor]] = None,
            # å¯é€‰çš„ä¸­é—´å—é™„åŠ æ®‹å·®ï¼Œç±»å‹ä¸º PyTorch å¼ é‡
            mid_block_additional_residual: Optional[torch.Tensor] = None,
            # å¯é€‰çš„ä¸‹å±‚å†…éƒ¨å—é™„åŠ æ®‹å·®ï¼Œç±»å‹ä¸ºå…ƒç»„ï¼ŒåŒ…å« PyTorch å¼ é‡
            down_intrablock_additional_residuals: Optional[Tuple[torch.Tensor]] = None,
            # å¯é€‰çš„ç¼–ç å™¨æ³¨æ„åŠ›æ©ç ï¼Œç±»å‹ä¸º PyTorch å¼ é‡
            encoder_attention_mask: Optional[torch.Tensor] = None,
            # è¿”å›ç»“æœçš„æ ‡å¿—ï¼Œå¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸º True
            return_dict: bool = True,
```