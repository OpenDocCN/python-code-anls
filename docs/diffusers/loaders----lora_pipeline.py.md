# `.\diffusers\loaders\lora_pipeline.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œè¡¨æ˜è¯¥ä»£ç çš„ç‰ˆæƒæ‰€æœ‰è€…åŠå…¶æƒåˆ©
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# æ ¹æ® Apache 2.0 è®¸å¯è¯çš„è§„å®šä½¿ç”¨è¯¥æ–‡ä»¶
# Licensed under the Apache License, Version 2.0 (the "License");
# åªæœ‰åœ¨éµå¾ªè®¸å¯è¯çš„æƒ…å†µä¸‹ï¼Œæ‰èƒ½ä½¿ç”¨æ­¤æ–‡ä»¶
# you may not use this file except in compliance with the License.
# å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åè®®å¦æœ‰çº¦å®šï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# ä¸æä¾›ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºä¿è¯æˆ–æ¡ä»¶
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# å‚è§è®¸å¯è¯ä»¥è·å–æœ‰å…³æƒé™å’Œé™åˆ¶çš„å…·ä½“æ¡æ¬¾
# See the License for the specific language governing permissions and
# limitations under the License.

# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¨¡å—
import os
# ä» typing æ¨¡å—å¯¼å…¥ç±»å‹æç¤ºç›¸å…³çš„å·¥å…·
from typing import Callable, Dict, List, Optional, Union

# å¯¼å…¥ PyTorch åº“
import torch
# ä» huggingface_hub.utils å¯¼å…¥éªŒè¯ Hugging Face Hub å‚æ•°çš„å‡½æ•°
from huggingface_hub.utils import validate_hf_hub_args

# ä» utils æ¨¡å—ä¸­å¯¼å…¥å¤šä¸ªå·¥å…·å‡½æ•°å’Œå¸¸é‡
from ..utils import (
    USE_PEFT_BACKEND,  # ç”¨äºæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ PEFT åç«¯çš„å¸¸é‡
    convert_state_dict_to_diffusers,  # è½¬æ¢çŠ¶æ€å­—å…¸åˆ° Diffusers æ ¼å¼çš„å‡½æ•°
    convert_state_dict_to_peft,  # è½¬æ¢çŠ¶æ€å­—å…¸åˆ° PEFT æ ¼å¼çš„å‡½æ•°
    convert_unet_state_dict_to_peft,  # å°† UNet çŠ¶æ€å­—å…¸è½¬æ¢ä¸º PEFT æ ¼å¼çš„å‡½æ•°
    deprecate,  # ç”¨äºæ ‡è®°è¿‡æ—¶å‡½æ•°çš„è£…é¥°å™¨
    get_adapter_name,  # è·å–é€‚é…å™¨åç§°çš„å‡½æ•°
    get_peft_kwargs,  # è·å– PEFT å…³é”®å­—å‚æ•°çš„å‡½æ•°
    is_peft_version,  # æ£€æŸ¥æ˜¯å¦ä¸º PEFT ç‰ˆæœ¬çš„å‡½æ•°
    is_transformers_available,  # æ£€æŸ¥ Transformers åº“æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    logging,  # æ—¥å¿—è®°å½•å·¥å…·
    scale_lora_layers,  # è°ƒæ•´ LoRA å±‚è§„æ¨¡çš„å‡½æ•°
)
# ä» lora_base æ¨¡å—å¯¼å…¥ LoraBaseMixin ç±»
from .lora_base import LoraBaseMixin
# ä» lora_conversion_utils æ¨¡å—å¯¼å…¥ä¸¤ä¸ªç”¨äºè½¬æ¢çš„å‡½æ•°
from .lora_conversion_utils import _convert_non_diffusers_lora_to_diffusers, _maybe_map_sgm_blocks_to_diffusers

# å¦‚æœ Transformers åº“å¯ç”¨ï¼Œåˆ™å¯¼å…¥ç›¸å…³çš„æ¨¡å—
if is_transformers_available():
    from ..models.lora import text_encoder_attn_modules, text_encoder_mlp_modules

# åˆ›å»ºä¸€ä¸ªæ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•æœ¬æ¨¡å—çš„æ—¥å¿—ä¿¡æ¯
logger = logging.get_logger(__name__)

# å®šä¹‰ä¸€äº›å¸¸é‡ï¼Œè¡¨ç¤ºä¸åŒç»„ä»¶çš„åç§°
TEXT_ENCODER_NAME = "text_encoder"  # æ–‡æœ¬ç¼–ç å™¨çš„åç§°
UNET_NAME = "unet"  # UNet æ¨¡å‹çš„åç§°
TRANSFORMER_NAME = "transformer"  # Transformer æ¨¡å‹çš„åç§°

# å®šä¹‰ LoRA æƒé‡æ–‡ä»¶çš„åç§°
LORA_WEIGHT_NAME = "pytorch_lora_weights.bin"  # äºŒè¿›åˆ¶æ ¼å¼çš„æƒé‡æ–‡ä»¶å
LORA_WEIGHT_NAME_SAFE = "pytorch_lora_weights.safetensors"  # å®‰å…¨æ ¼å¼çš„æƒé‡æ–‡ä»¶å


# å®šä¹‰ä¸€ä¸ªç±»ï¼Œç”¨äºåŠ è½½ LoRA å±‚åˆ°ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­
class StableDiffusionLoraLoaderMixin(LoraBaseMixin):
    r"""
    å°† LoRA å±‚åŠ è½½åˆ°ç¨³å®šæ‰©æ•£æ¨¡å‹ [`UNet2DConditionModel`] å’Œ
    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel) ä¸­ã€‚
    """

    # å¯åŠ è½½çš„ LoRA æ¨¡å—åˆ—è¡¨
    _lora_loadable_modules = ["unet", "text_encoder"]
    unet_name = UNET_NAME  # UNet æ¨¡å‹çš„åç§°
    text_encoder_name = TEXT_ENCODER_NAME  # æ–‡æœ¬ç¼–ç å™¨çš„åç§°

    # å®šä¹‰åŠ è½½ LoRA æƒé‡çš„æ–¹æ³•
    def load_lora_weights(
        self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], adapter_name=None, **kwargs
    ):
        """
        åŠ è½½æŒ‡å®šçš„ LoRA æƒé‡åˆ° `self.unet` å’Œ `self.text_encoder` ä¸­ã€‚

        æ‰€æœ‰å…³é”®å­—å‚æ•°å°†è½¬å‘ç»™ `self.lora_state_dict`ã€‚

        è¯¦æƒ…è¯·å‚é˜… [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`]ï¼Œäº†è§£å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸ã€‚

        è¯¦æƒ…è¯·å‚é˜… [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`]ï¼Œäº†è§£å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ° `self.unet` ä¸­ã€‚

        è¯¦æƒ…è¯·å‚é˜… [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`]ï¼Œäº†è§£å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ° `self.text_encoder` ä¸­ã€‚

        å‚æ•°:
            pretrained_model_name_or_path_or_dict (`str` æˆ– `os.PathLike` æˆ– `dict`):
                è¯¦æƒ…è¯·å‚é˜… [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`]ã€‚
            kwargs (`dict`, *å¯é€‰*):
                è¯¦æƒ…è¯·å‚é˜… [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`]ã€‚
            adapter_name (`str`, *å¯é€‰*):
                ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨ `default_{i}`ï¼Œå…¶ä¸­ i æ˜¯åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚
        """
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ PEFT åç«¯ï¼Œå¦‚æœæœªä½¿ç”¨åˆ™å¼•å‘é”™è¯¯
        if not USE_PEFT_BACKEND:
            raise ValueError("PEFT backend is required for this method.")

        # å¦‚æœä¼ å…¥çš„æ˜¯å­—å…¸ï¼Œåˆ™å¤åˆ¶ä¸€ä»½è€Œä¸æ˜¯å°±åœ°ä¿®æ”¹
        if isinstance(pretrained_model_name_or_path_or_dict, dict):
            pretrained_model_name_or_path_or_dict = pretrained_model_name_or_path_or_dict.copy()

        # é¦–å…ˆï¼Œç¡®ä¿æ£€æŸ¥ç‚¹æ˜¯å…¼å®¹çš„ï¼Œå¹¶ä¸”å¯ä»¥æˆåŠŸåŠ è½½
        state_dict, network_alphas = self.lora_state_dict(pretrained_model_name_or_path_or_dict, **kwargs)

        # æ£€æŸ¥çŠ¶æ€å­—å…¸ä¸­çš„æ‰€æœ‰é”®æ˜¯å¦åŒ…å« "lora" æˆ– "dora_scale"
        is_correct_format = all("lora" in key or "dora_scale" in key for key in state_dict.keys())
        # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ™å¼•å‘é”™è¯¯
        if not is_correct_format:
            raise ValueError("Invalid LoRA checkpoint.")

        # å°† LoRA æƒé‡åŠ è½½åˆ° UNet ä¸­
        self.load_lora_into_unet(
            state_dict,
            network_alphas=network_alphas,
            unet=getattr(self, self.unet_name) if not hasattr(self, "unet") else self.unet,
            adapter_name=adapter_name,
            _pipeline=self,
        )
        # å°† LoRA æƒé‡åŠ è½½åˆ°æ–‡æœ¬ç¼–ç å™¨ä¸­
        self.load_lora_into_text_encoder(
            state_dict,
            network_alphas=network_alphas,
            text_encoder=getattr(self, self.text_encoder_name)
            if not hasattr(self, "text_encoder")
            else self.text_encoder,
            lora_scale=self.lora_scale,
            adapter_name=adapter_name,
            _pipeline=self,
        )

    # ç±»æ–¹æ³•ï¼Œç”¨äºéªŒè¯ HF Hub å‚æ•°
    @classmethod
    @validate_hf_hub_args
    def lora_state_dict(
        cls,
        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],
        **kwargs,
    @classmethod
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºå°† LoRA å±‚åŠ è½½åˆ° UNet æ¨¡å‹ä¸­
    def load_lora_into_unet(cls, state_dict, network_alphas, unet, adapter_name=None, _pipeline=None):
        """
        å°† `state_dict` ä¸­æŒ‡å®šçš„ LoRA å±‚åŠ è½½åˆ° `unet` ä¸­ã€‚
    
        å‚æ•°ï¼š
            state_dict (`dict`):
                åŒ…å« LoRA å±‚å‚æ•°çš„æ ‡å‡†çŠ¶æ€å­—å…¸ã€‚é”®å¯ä»¥ç›´æ¥ç´¢å¼•åˆ° unetï¼Œæˆ–è€…ä»¥é¢å¤–çš„ `unet` å‰ç¼€æ ‡è¯†ï¼Œä»¥åŒºåˆ†æ–‡æœ¬ç¼–ç å™¨çš„ LoRA å±‚ã€‚
            network_alphas (`Dict[str, float]`):
                ç”¨äºç¨³å®šå­¦ä¹ å’Œé˜²æ­¢ä¸‹æº¢çš„ç½‘ç»œ alpha å€¼ã€‚æ­¤å€¼ä¸ kohya-ss è®­ç»ƒè„šæœ¬ä¸­çš„ `--network_alpha` é€‰é¡¹å«ä¹‰ç›¸åŒã€‚å‚è€ƒ[æ­¤é“¾æ¥](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning)ã€‚
            unet (`UNet2DConditionModel`):
                ç”¨äºåŠ è½½ LoRA å±‚çš„ UNet æ¨¡å‹ã€‚
            adapter_name (`str`, *å¯é€‰*):
                ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨ `default_{i}`ï¼Œå…¶ä¸­ i æ˜¯åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚
        """
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ PEFT åç«¯ï¼Œå¦‚æœæœªä½¿ç”¨åˆ™å¼•å‘é”™è¯¯
        if not USE_PEFT_BACKEND:
            raise ValueError("PEFT backend is required for this method.")
    
        # æ£€æŸ¥åºåˆ—åŒ–æ ¼å¼æ˜¯å¦ä¸ºæ–°æ ¼å¼ï¼Œ`state_dict` çš„é”®æ˜¯å¦ä»¥ `cls.unet_name` å’Œ/æˆ– `cls.text_encoder_name` ä¸ºå‰ç¼€
        keys = list(state_dict.keys())
        only_text_encoder = all(key.startswith(cls.text_encoder_name) for key in keys)
        if not only_text_encoder:
            # åŠ è½½ä¸ UNet å¯¹åº”çš„å±‚
            logger.info(f"Loading {cls.unet_name}.")
            # è°ƒç”¨ UNet çš„åŠ è½½æ–¹æ³•ï¼Œä¼ å…¥çŠ¶æ€å­—å…¸å’Œå…¶ä»–å‚æ•°
            unet.load_attn_procs(
                state_dict, network_alphas=network_alphas, adapter_name=adapter_name, _pipeline=_pipeline
            )
    
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºå°† LoRA å±‚åŠ è½½åˆ°æ–‡æœ¬ç¼–ç å™¨ä¸­
    @classmethod
    def load_lora_into_text_encoder(
        cls,
        state_dict,
        network_alphas,
        text_encoder,
        prefix=None,
        lora_scale=1.0,
        adapter_name=None,
        _pipeline=None,
    ):
        # æ–¹æ³•å®šä¹‰ï¼Œå…·ä½“å®ç°æœªæä¾›
        pass
    
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºä¿å­˜ LoRA æƒé‡
    @classmethod
    def save_lora_weights(
        cls,
        save_directory: Union[str, os.PathLike],
        unet_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,
        text_encoder_lora_layers: Dict[str, torch.nn.Module] = None,
        is_main_process: bool = True,
        weight_name: str = None,
        save_function: Callable = None,
        safe_serialization: bool = True,
    ):
        # æ–¹æ³•å®šä¹‰ï¼Œå…·ä½“å®ç°æœªæä¾›
        pass
    ):
        r"""  # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°å‡½æ•°çš„ä½œç”¨å’Œå‚æ•°
        Save the LoRA parameters corresponding to the UNet and text encoder.  # ä¿å­˜ä¸ UNet å’Œæ–‡æœ¬ç¼–ç å™¨ç›¸å¯¹åº”çš„ LoRA å‚æ•°

        Arguments:  # å‚æ•°è¯´æ˜
            save_directory (`str` or `os.PathLike`):  # ä¿å­˜ç›®å½•çš„ç±»å‹è¯´æ˜
                Directory to save LoRA parameters to. Will be created if it doesn't exist.  # ä¿å­˜ LoRA å‚æ•°çš„ç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            unet_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):  # UNet çš„ LoRA å±‚çŠ¶æ€å­—å…¸
                State dict of the LoRA layers corresponding to the `unet`.  # ä¸ `unet` ç›¸å¯¹åº”çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸
            text_encoder_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):  # æ–‡æœ¬ç¼–ç å™¨çš„ LoRA å±‚çŠ¶æ€å­—å…¸
                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text  # ä¸ `text_encoder` ç›¸å¯¹åº”çš„ LoRA å±‚çŠ¶æ€å­—å…¸ï¼Œå¿…é¡»æ˜¾å¼ä¼ é€’
                encoder LoRA state dict because it comes from ğŸ¤— Transformers.  # å› ä¸ºå®ƒæ¥è‡ª ğŸ¤— Transformers
            is_main_process (`bool`, *optional*, defaults to `True`):  # ä¸»è¦è¿›ç¨‹çš„å¸ƒå°”å€¼ï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸º True
                Whether the process calling this is the main process or not. Useful during distributed training and you  # è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼Œåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å¾ˆæœ‰ç”¨
                need to call this function on all processes. In this case, set `is_main_process=True` only on the main  # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªåœ¨ä¸»è¿›ç¨‹ä¸Šè®¾ç½® `is_main_process=True` ä»¥é¿å…ç«äº‰æ¡ä»¶
                process to avoid race conditions.  # é¿å…ç«äº‰æ¡ä»¶
            save_function (`Callable`):  # ä¿å­˜å‡½æ•°çš„ç±»å‹è¯´æ˜
                The function to use to save the state dictionary. Useful during distributed training when you need to  # ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ï¼Œåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å¾ˆæœ‰ç”¨
                replace `torch.save` with another method. Can be configured with the environment variable  # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
                `DIFFUSERS_SAVE_MODE`.  # `DIFFUSERS_SAVE_MODE`
            safe_serialization (`bool`, *optional*, defaults to `True`):  # å®‰å…¨åºåˆ—åŒ–çš„å¸ƒå°”å€¼ï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸º True
                Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.  # æ˜¯å¦ä½¿ç”¨ `safetensors` æˆ–ä¼ ç»Ÿçš„ PyTorch æ–¹æ³• `pickle` ä¿å­˜æ¨¡å‹
        """  # æ–‡æ¡£å­—ç¬¦ä¸²ç»“æŸ
        state_dict = {}  # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„çŠ¶æ€å­—å…¸

        if not (unet_lora_layers or text_encoder_lora_layers):  # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ª LoRA å±‚
            raise ValueError("You must pass at least one of `unet_lora_layers` and `text_encoder_lora_layers`.")  # å¦‚æœæ²¡æœ‰ï¼ŒæŠ›å‡ºé”™è¯¯

        if unet_lora_layers:  # å¦‚æœå­˜åœ¨ UNet çš„ LoRA å±‚
            state_dict.update(cls.pack_weights(unet_lora_layers, cls.unet_name))  # æ›´æ–°çŠ¶æ€å­—å…¸ï¼Œæ‰“åŒ… UNet æƒé‡

        if text_encoder_lora_layers:  # å¦‚æœå­˜åœ¨æ–‡æœ¬ç¼–ç å™¨çš„ LoRA å±‚
            state_dict.update(cls.pack_weights(text_encoder_lora_layers, cls.text_encoder_name))  # æ›´æ–°çŠ¶æ€å­—å…¸ï¼Œæ‰“åŒ…æ–‡æœ¬ç¼–ç å™¨æƒé‡

        # Save the model  # ä¿å­˜æ¨¡å‹çš„æ³¨é‡Š
        cls.write_lora_layers(  # è°ƒç”¨ç±»æ–¹æ³•ä¿å­˜ LoRA å±‚
            state_dict=state_dict,  # çŠ¶æ€å­—å…¸å‚æ•°
            save_directory=save_directory,  # ä¿å­˜ç›®å½•å‚æ•°
            is_main_process=is_main_process,  # ä¸»è¦è¿›ç¨‹å‚æ•°
            weight_name=weight_name,  # æƒé‡åç§°å‚æ•°
            save_function=save_function,  # ä¿å­˜å‡½æ•°å‚æ•°
            safe_serialization=safe_serialization,  # å®‰å…¨åºåˆ—åŒ–å‚æ•°
        )  # æ–¹æ³•è°ƒç”¨ç»“æŸ

    def fuse_lora(  # å®šä¹‰ fuse_lora æ–¹æ³•
        self,  # å®ä¾‹æ–¹æ³•çš„ self å‚æ•°
        components: List[str] = ["unet", "text_encoder"],  # ç»„ä»¶åˆ—è¡¨ï¼Œé»˜è®¤åŒ…å« UNet å’Œæ–‡æœ¬ç¼–ç å™¨
        lora_scale: float = 1.0,  # LoRA ç¼©æ”¾å› å­ï¼Œé»˜è®¤å€¼ä¸º 1.0
        safe_fusing: bool = False,  # å®‰å…¨èåˆçš„å¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸º False
        adapter_names: Optional[List[str]] = None,  # é€‚é…å™¨åç§°çš„å¯é€‰åˆ—è¡¨ï¼Œé»˜è®¤å€¼ä¸º None
        **kwargs,  # æ¥æ”¶é¢å¤–çš„å…³é”®å­—å‚æ•°
    ):
        r"""  # å¼€å§‹æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°è¯¥æ–¹æ³•çš„åŠŸèƒ½å’Œç”¨æ³•
        Fuses the LoRA parameters into the original parameters of the corresponding blocks.  # å°† LoRA å‚æ•°èåˆåˆ°å¯¹åº”å—çš„åŸå§‹å‚æ•°ä¸­

        <Tip warning={true}>  # å¼€å§‹è­¦å‘Šæç¤ºæ¡†
        This is an experimental API.  # è¯´æ˜è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§ API
        </Tip>  # ç»“æŸè­¦å‘Šæç¤ºæ¡†

        Args:  # å¼€å§‹å‚æ•°è¯´æ˜
            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.  # å¯æ³¨å…¥ LoRA çš„ç»„ä»¶åˆ—è¡¨
            lora_scale (`float`, defaults to 1.0):  # LoRA å‚æ•°å¯¹è¾“å‡ºå½±å“çš„æ¯”ä¾‹
                Controls how much to influence the outputs with the LoRA parameters.  # æ§åˆ¶ LoRA å‚æ•°å¯¹è¾“å‡ºçš„å½±å“ç¨‹åº¦
            safe_fusing (`bool`, defaults to `False`):  # æ˜¯å¦åœ¨èåˆå‰æ£€æŸ¥æƒé‡æ˜¯å¦ä¸º NaN
                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.  # å¦‚æœå€¼ä¸º NaN åˆ™ä¸è¿›è¡Œèåˆ
            adapter_names (`List[str]`, *optional*):  # å¯é€‰çš„é€‚é…å™¨åç§°
                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.  # å¦‚æœæœªä¼ å…¥ï¼Œé»˜è®¤èåˆæ‰€æœ‰æ´»åŠ¨é€‚é…å™¨

        Example:  # ç¤ºä¾‹éƒ¨åˆ†çš„å¼€å§‹
        ```py  # Python ä»£ç å—å¼€å§‹
        from diffusers import DiffusionPipeline  # å¯¼å…¥ DiffusionPipeline æ¨¡å—
        import torch  # å¯¼å…¥ PyTorch åº“

        pipeline = DiffusionPipeline.from_pretrained(  # ä»é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºç®¡é“
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16  # ä½¿ç”¨ float16 ç±»å‹çš„æ¨¡å‹
        ).to("cuda")  # å°†ç®¡é“ç§»åŠ¨åˆ° GPU
        pipeline.load_lora_weights("nerijs/pixel-art-xl", weight_name="pixel-art-xl.safetensors", adapter_name="pixel")  # åŠ è½½ LoRA æƒé‡
        pipeline.fuse_lora(lora_scale=0.7)  # èåˆ LoRAï¼Œå½±å“æ¯”ä¾‹ä¸º 0.7
        ```  # Python ä»£ç å—ç»“æŸ
        """
        super().fuse_lora(  # è°ƒç”¨çˆ¶ç±»çš„ fuse_lora æ–¹æ³•
            components=components, lora_scale=lora_scale, safe_fusing=safe_fusing, adapter_names=adapter_names  # å°†å‚æ•°ä¼ é€’ç»™çˆ¶ç±»æ–¹æ³•
        )

    def unfuse_lora(self, components: List[str] = ["unet", "text_encoder"], **kwargs):  # å®šä¹‰ unfuse_lora æ–¹æ³•ï¼Œå¸¦æœ‰é»˜è®¤ç»„ä»¶
        r"""  # å¼€å§‹æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°è¯¥æ–¹æ³•çš„åŠŸèƒ½å’Œç”¨æ³•
        Reverses the effect of  # åè½¬ fuse_lora æ–¹æ³•çš„æ•ˆæœ
        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).  # æä¾› fuse_lora çš„é“¾æ¥

        <Tip warning={true}>  # å¼€å§‹è­¦å‘Šæç¤ºæ¡†
        This is an experimental API.  # è¯´æ˜è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§ API
        </Tip>  # ç»“æŸè­¦å‘Šæç¤ºæ¡†

        Args:  # å¼€å§‹å‚æ•°è¯´æ˜
            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.  # å¯æ³¨å…¥ LoRA çš„ç»„ä»¶åˆ—è¡¨ï¼Œç”¨äºåèåˆ
            unfuse_unet (`bool`, defaults to `True`):  # æ˜¯å¦åèåˆ UNet çš„ LoRA å‚æ•°
                Whether to unfuse the UNet LoRA parameters.  # åèåˆ UNet LoRA å‚æ•°çš„é€‰é¡¹
            unfuse_text_encoder (`bool`, defaults to `True`):  # æ˜¯å¦åèåˆæ–‡æœ¬ç¼–ç å™¨çš„ LoRA å‚æ•°
                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the  # åèåˆæ–‡æœ¬ç¼–ç å™¨çš„ LoRA å‚æ•°çš„é€‰é¡¹
                LoRA parameters then it won't have any effect.  # å¦‚æœæ–‡æœ¬ç¼–ç å™¨æœªè¢«ä¿®æ”¹ï¼Œåˆ™ä¸ä¼šæœ‰ä»»ä½•æ•ˆæœ
        """  # ç»“æŸæ–‡æ¡£å­—ç¬¦ä¸²
        super().unfuse_lora(components=components)  # è°ƒç”¨çˆ¶ç±»çš„ unfuse_lora æ–¹æ³•ï¼Œå¹¶ä¼ é€’ç»„ä»¶å‚æ•°
# å®šä¹‰ä¸€ä¸ªç±»ï¼Œæ··åˆè‡ª LoraBaseMixinï¼Œç”¨äºåŠ è½½ LoRA å±‚åˆ° Stable Diffusion XL
class StableDiffusionXLLoraLoaderMixin(LoraBaseMixin):
    r"""
    å°† LoRA å±‚åŠ è½½åˆ° Stable Diffusion XL çš„ [`UNet2DConditionModel`]ã€
    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel) å’Œ
    [`CLIPTextModelWithProjection`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection) ä¸­ã€‚
    """

    # å®šä¹‰å¯ä»¥åŠ è½½ LoRA çš„æ¨¡å—ååˆ—è¡¨
    _lora_loadable_modules = ["unet", "text_encoder", "text_encoder_2"]
    # æŒ‡å®š UNET çš„åç§°
    unet_name = UNET_NAME
    # æŒ‡å®šæ–‡æœ¬ç¼–ç å™¨çš„åç§°
    text_encoder_name = TEXT_ENCODER_NAME

    # å®šä¹‰ä¸€ä¸ªåŠ è½½ LoRA æƒé‡çš„æ–¹æ³•
    def load_lora_weights(
        self,
        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],
        adapter_name: Optional[str] = None,
        **kwargs,
    ):
    @classmethod
    # éªŒè¯ä¼ å…¥çš„ Hugging Face Hub å‚æ•°
    @validate_hf_hub_args
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œè·å– LoRA çŠ¶æ€å­—å…¸
    def lora_state_dict(
        cls,
        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],
        **kwargs,
    ):
    @classmethod
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºå°† LoRA åŠ è½½åˆ° UNET ä¸­
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºå°† LoRA å±‚åŠ è½½åˆ° UNet æ¨¡å‹ä¸­
    def load_lora_into_unet(cls, state_dict, network_alphas, unet, adapter_name=None, _pipeline=None):
        # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°æ–¹æ³•çš„ä½œç”¨å’Œå‚æ•°
        """
        This will load the LoRA layers specified in `state_dict` into `unet`.
    
        Parameters:
            state_dict (`dict`):
                A standard state dict containing the lora layer parameters. The keys can either be indexed directly
                into the unet or prefixed with an additional `unet` which can be used to distinguish between text
                encoder lora layers.
            network_alphas (`Dict[str, float]`):
                The value of the network alpha used for stable learning and preventing underflow. This value has the
                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this
                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).
            unet (`UNet2DConditionModel`):
                The UNet model to load the LoRA layers into.
            adapter_name (`str`, *optional*):
                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
                `default_{i}` where i is the total number of adapters being loaded.
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ PEFT åç«¯ï¼Œè‹¥æœªå¯ç”¨åˆ™æŠ›å‡ºå¼‚å¸¸
        if not USE_PEFT_BACKEND:
            raise ValueError("PEFT backend is required for this method.")
    
        # è·å– state_dict ä¸­çš„æ‰€æœ‰é”®
        keys = list(state_dict.keys())
        # æ£€æŸ¥æ‰€æœ‰é”®æ˜¯å¦éƒ½ä»¥ text_encoder_name å¼€å¤´
        only_text_encoder = all(key.startswith(cls.text_encoder_name) for key in keys)
        # å¦‚æœä¸æ˜¯ä»…æœ‰æ–‡æœ¬ç¼–ç å™¨
        if not only_text_encoder:
            # è®°å½•æ­£åœ¨åŠ è½½çš„ UNet åç§°
            logger.info(f"Loading {cls.unet_name}.")
            # åŠ è½½ä¸ UNet å¯¹åº”çš„å±‚
            unet.load_attn_procs(
                state_dict, network_alphas=network_alphas, adapter_name=adapter_name, _pipeline=_pipeline
            )
    
        @classmethod
        # ä» diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder å¤åˆ¶çš„æ–¹æ³•
        def load_lora_into_text_encoder(
            cls,
            state_dict,
            network_alphas,
            text_encoder,
            prefix=None,
            lora_scale=1.0,
            adapter_name=None,
            _pipeline=None,
        @classmethod
        # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºä¿å­˜ LoRA æƒé‡
        def save_lora_weights(
            cls,
            save_directory: Union[str, os.PathLike],
            unet_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,
            text_encoder_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,
            text_encoder_2_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,
            is_main_process: bool = True,
            weight_name: str = None,
            save_function: Callable = None,
            safe_serialization: bool = True,
    ):
        r"""
        # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°ä¿å­˜ UNet å’Œæ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„ LoRA å‚æ•°çš„åŠŸèƒ½

        Arguments:
            # ä¿å­˜ LoRA å‚æ•°çš„ç›®å½•ï¼Œè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»º
            save_directory (`str` or `os.PathLike`):
                Directory to save LoRA parameters to. Will be created if it doesn't exist.
            # UNet å¯¹åº”çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸
            unet_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):
                State dict of the LoRA layers corresponding to the `unet`.
            # æ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ï¼Œå¿…é¡»æ˜¾å¼ä¼ å…¥
            text_encoder_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):
                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text
                encoder LoRA state dict because it comes from ğŸ¤— Transformers.
            # ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ï¼Œå¿…é¡»æ˜¾å¼ä¼ å…¥
            text_encoder_2_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):
                State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text
                encoder LoRA state dict because it comes from ğŸ¤— Transformers.
            # è¡¨ç¤ºè°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼Œä¸»è¦ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒ
            is_main_process (`bool`, *optional*, defaults to `True`):
                Whether the process calling this is the main process or not. Useful during distributed training and you
                need to call this function on all processes. In this case, set `is_main_process=True` only on the main
                process to avoid race conditions.
            # ä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ï¼Œåˆ†å¸ƒå¼è®­ç»ƒæ—¶å¯æ›¿æ¢ `torch.save`
            save_function (`Callable`):
                The function to use to save the state dictionary. Useful during distributed training when you need to
                replace `torch.save` with another method. Can be configured with the environment variable
                `DIFFUSERS_SAVE_MODE`.
            # æ˜¯å¦ä½¿ç”¨ safetensors ä¿å­˜æ¨¡å‹ï¼Œé»˜è®¤ä¸º True
            safe_serialization (`bool`, *optional*, defaults to `True`):
                Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.
        """
        # åˆå§‹åŒ–çŠ¶æ€å­—å…¸ï¼Œç”¨äºå­˜å‚¨ LoRA å‚æ•°
        state_dict = {}

        # å¦‚æœæ²¡æœ‰ä¼ å…¥ä»»ä½• LoRA å±‚ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        if not (unet_lora_layers or text_encoder_lora_layers or text_encoder_2_lora_layers):
            raise ValueError(
                # æŠ¥å‘Šè‡³å°‘éœ€è¦ä¼ å…¥ä¸€ä¸ª LoRA å±‚
                "You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers` or `text_encoder_2_lora_layers`."
            )

        # å¦‚æœæœ‰ UNet çš„ LoRA å±‚ï¼Œåˆ™æ‰“åŒ…å¹¶æ›´æ–°çŠ¶æ€å­—å…¸
        if unet_lora_layers:
            state_dict.update(cls.pack_weights(unet_lora_layers, "unet"))

        # å¦‚æœæœ‰æ–‡æœ¬ç¼–ç å™¨çš„ LoRA å±‚ï¼Œåˆ™æ‰“åŒ…å¹¶æ›´æ–°çŠ¶æ€å­—å…¸
        if text_encoder_lora_layers:
            state_dict.update(cls.pack_weights(text_encoder_lora_layers, "text_encoder"))

        # å¦‚æœæœ‰ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨çš„ LoRA å±‚ï¼Œåˆ™æ‰“åŒ…å¹¶æ›´æ–°çŠ¶æ€å­—å…¸
        if text_encoder_2_lora_layers:
            state_dict.update(cls.pack_weights(text_encoder_2_lora_layers, "text_encoder_2"))

        # å†™å…¥ LoRA å±‚å‚æ•°ï¼Œè°ƒç”¨ä¿å­˜å‡½æ•°
        cls.write_lora_layers(
            state_dict=state_dict,
            save_directory=save_directory,
            is_main_process=is_main_process,
            weight_name=weight_name,
            save_function=save_function,
            safe_serialization=safe_serialization,
        )
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³• fuse_loraï¼Œç”¨äºå°† LoRA å‚æ•°èåˆåˆ°ç›¸åº”æ¨¡å—çš„åŸå§‹å‚æ•°ä¸­
    def fuse_lora(
        # æ–¹æ³•å‚æ•°ï¼šå¯æ³¨å…¥ LoRA çš„ç»„ä»¶åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["unet", "text_encoder", "text_encoder_2"]
        self,
        components: List[str] = ["unet", "text_encoder", "text_encoder_2"],
        # LoRA æƒé‡å½±å“è¾“å‡ºçš„ç¨‹åº¦ï¼Œé»˜è®¤ä¸º 1.0
        lora_scale: float = 1.0,
        # æ˜¯å¦åœ¨èåˆå‰æ£€æŸ¥æƒé‡æ˜¯å¦ä¸º NaNï¼Œé»˜è®¤ä¸º False
        safe_fusing: bool = False,
        # å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šç”¨äºèåˆçš„é€‚é…å™¨åç§°
        adapter_names: Optional[List[str]] = None,
        # å…è®¸ä¼ å…¥é¢å¤–çš„å…³é”®å­—å‚æ•°
        **kwargs,
    ):
        r"""
        å°† LoRA å‚æ•°èåˆåˆ°ç›¸åº”æ¨¡å—çš„åŸå§‹å‚æ•°ä¸­ã€‚

        <Tip warning={true}>

        è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§ APIã€‚

        </Tip>

        Args:
            components: (`List[str]`): éœ€è¦èåˆ LoRA çš„ç»„ä»¶åˆ—è¡¨ã€‚
            lora_scale (`float`, defaults to 1.0):
                æ§åˆ¶ LoRA å‚æ•°å¯¹è¾“å‡ºçš„å½±å“ç¨‹åº¦ã€‚
            safe_fusing (`bool`, defaults to `False`):
                åœ¨èåˆå‰æ£€æŸ¥æƒé‡æ˜¯å¦ä¸º NaN çš„å¼€å…³ã€‚
            adapter_names (`List[str]`, *optional*):
                ç”¨äºèåˆçš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªä¼ å…¥ï¼Œåˆ™å°†èåˆæ‰€æœ‰æ´»åŠ¨é€‚é…å™¨ã€‚

        Example:

        ```py
        from diffusers import DiffusionPipeline
        import torch

        pipeline = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
        ).to("cuda")
        # åŠ è½½ LoRA æƒé‡
        pipeline.load_lora_weights("nerijs/pixel-art-xl", weight_name="pixel-art-xl.safetensors", adapter_name="pixel")
        # èåˆ LoRA å‚æ•°ï¼Œå½±å“ç¨‹åº¦ä¸º 0.7
        pipeline.fuse_lora(lora_scale=0.7)
        ```
        """
        # è°ƒç”¨çˆ¶ç±»çš„ fuse_lora æ–¹æ³•ï¼Œä¼ å…¥ç»„ä»¶ã€LoRA æƒé‡ã€æ£€æŸ¥ NaN çš„é€‰é¡¹å’Œé€‚é…å™¨åç§°
        super().fuse_lora(
            components=components, lora_scale=lora_scale, safe_fusing=safe_fusing, adapter_names=adapter_names
        )

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³• unfuse_loraï¼Œç”¨äºé€†è½¬ LoRA å‚æ•°çš„èåˆæ•ˆæœ
    def unfuse_lora(self, components: List[str] = ["unet", "text_encoder", "text_encoder_2"], **kwargs):
        r"""
        é€†è½¬
        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora) çš„æ•ˆæœã€‚

        <Tip warning={true}>

        è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§ APIã€‚

        </Tip>

        Args:
            components (`List[str]`): éœ€è¦ä»ä¸­è§£èåˆ LoRA çš„ç»„ä»¶åˆ—è¡¨ã€‚
            unfuse_unet (`bool`, defaults to `True`): æ˜¯å¦è§£èåˆ UNet çš„ LoRA å‚æ•°ã€‚
            unfuse_text_encoder (`bool`, defaults to `True`):
                æ˜¯å¦è§£èåˆæ–‡æœ¬ç¼–ç å™¨çš„ LoRA å‚æ•°ã€‚å¦‚æœæ–‡æœ¬ç¼–ç å™¨æ²¡æœ‰è¢« LoRA å‚æ•°ä¿®è¡¥ï¼Œåˆ™ä¸ä¼šæœ‰ä»»ä½•æ•ˆæœã€‚
        """
        # è°ƒç”¨çˆ¶ç±»çš„ unfuse_lora æ–¹æ³•ï¼Œä¼ å…¥ç»„ä»¶å’Œå…¶ä»–å‚æ•°
        super().unfuse_lora(components=components)
# å®šä¹‰ä¸€ä¸ªæ··åˆç±» SD3LoraLoaderMixinï¼Œç»§æ‰¿è‡ª LoraBaseMixin
class SD3LoraLoaderMixin(LoraBaseMixin):
    r"""
    åŠ è½½ LoRA å±‚åˆ° [`SD3Transformer2DModel`]ã€
    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel) å’Œ
    [`CLIPTextModelWithProjection`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection)ã€‚

    ç‰¹å®šäº [`StableDiffusion3Pipeline`]ã€‚
    """

    # å¯åŠ è½½ LoRA çš„æ¨¡å—åˆ—è¡¨
    _lora_loadable_modules = ["transformer", "text_encoder", "text_encoder_2"]
    # è½¬æ¢å™¨åç§°ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„å¸¸é‡
    transformer_name = TRANSFORMER_NAME
    # æ–‡æœ¬ç¼–ç å™¨åç§°ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„å¸¸é‡
    text_encoder_name = TEXT_ENCODER_NAME

    # ç±»æ–¹æ³•ï¼ŒéªŒè¯ Hugging Face Hub å‚æ•°
    @classmethod
    @validate_hf_hub_args
    def lora_state_dict(
        cls,
        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],
        **kwargs,
    ):
        # åŠ è½½ LoRA æƒé‡çš„æ–¹æ³•ï¼Œæ¥æ”¶æ¨¡å‹åç§°æˆ–è·¯å¾„æˆ–å­—å…¸
        def load_lora_weights(
            self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], adapter_name=None, **kwargs
        ):
            # ç±»æ–¹æ³•ï¼Œä» diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin ä¸­å¤åˆ¶çš„åŠ è½½æ–‡æœ¬ç¼–ç å™¨çš„æ–¹æ³•
            @classmethod
            def load_lora_into_text_encoder(
                cls,
                state_dict,
                network_alphas,
                text_encoder,
                prefix=None,
                lora_scale=1.0,
                adapter_name=None,
                _pipeline=None,
            ):
                # ç±»æ–¹æ³•ï¼Œä¿å­˜ LoRA æƒé‡åˆ°æŒ‡å®šç›®å½•
                def save_lora_weights(
                    cls,
                    save_directory: Union[str, os.PathLike],
                    transformer_lora_layers: Dict[str, torch.nn.Module] = None,
                    text_encoder_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,
                    text_encoder_2_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,
                    is_main_process: bool = True,
                    weight_name: str = None,
                    save_function: Callable = None,
                    safe_serialization: bool = True,
                ):
    ):
        r"""
        ä¿å­˜ä¸ UNet å’Œæ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„ LoRA å‚æ•°ã€‚

        å‚æ•°ï¼š
            save_directory (`str` æˆ– `os.PathLike`):
                ä¿å­˜ LoRA å‚æ•°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºè¯¥ç›®å½•ã€‚
            transformer_lora_layers (`Dict[str, torch.nn.Module]` æˆ– `Dict[str, torch.Tensor]`):
                ä¸ `transformer` ç›¸å…³çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ã€‚
            text_encoder_lora_layers (`Dict[str, torch.nn.Module]` æˆ– `Dict[str, torch.Tensor]`):
                ä¸ `text_encoder` ç›¸å…³çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ã€‚å¿…é¡»æ˜¾å¼ä¼ é€’æ–‡æœ¬ç¼–ç å™¨çš„ LoRA çŠ¶æ€å­—å…¸ï¼Œå› ä¸ºå®ƒæ¥è‡ª ğŸ¤— Transformersã€‚
            text_encoder_2_lora_layers (`Dict[str, torch.nn.Module]` æˆ– `Dict[str, torch.Tensor]`):
                ä¸ `text_encoder_2` ç›¸å…³çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ã€‚å¿…é¡»æ˜¾å¼ä¼ é€’æ–‡æœ¬ç¼–ç å™¨çš„ LoRA çŠ¶æ€å­—å…¸ï¼Œå› ä¸ºå®ƒæ¥è‡ª ğŸ¤— Transformersã€‚
            is_main_process (`bool`, *å¯é€‰*, é»˜è®¤å€¼ä¸º `True`):
                è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´éå¸¸æœ‰ç”¨ï¼Œæ‚¨éœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨æ­¤å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªæœ‰åœ¨ä¸»è¿›ç¨‹ä¸Šè®¾ç½® `is_main_process=True` ä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚
            save_function (`Callable`):
                ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œå½“æ‚¨éœ€è¦å°† `torch.save` æ›¿æ¢ä¸ºå…¶ä»–æ–¹æ³•æ—¶éå¸¸æœ‰ç”¨ã€‚å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ `DIFFUSERS_SAVE_MODE` è¿›è¡Œé…ç½®ã€‚
            safe_serialization (`bool`, *å¯é€‰*, é»˜è®¤å€¼ä¸º `True`):
                æ˜¯å¦ä½¿ç”¨ `safetensors` ä¿å­˜æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨ä¼ ç»Ÿçš„ PyTorch æ–¹æ³• `pickle`ã€‚
        """
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨çŠ¶æ€å­—å…¸
        state_dict = {}

        # æ£€æŸ¥æ˜¯å¦è‡³å°‘ä¼ é€’äº†ä¸€ä¸ª LoRA å±‚çš„çŠ¶æ€å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰åˆ™å¼•å‘é”™è¯¯
        if not (transformer_lora_layers or text_encoder_lora_layers or text_encoder_2_lora_layers):
            raise ValueError(
                "å¿…é¡»è‡³å°‘ä¼ é€’ä¸€ä¸ª `transformer_lora_layers`ã€`text_encoder_lora_layers` æˆ– `text_encoder_2_lora_layers`ã€‚"
            )

        # å¦‚æœä¼ é€’äº† transformer_lora_layersï¼Œåˆ™å°†å…¶æ‰“åŒ…å¹¶æ›´æ–°çŠ¶æ€å­—å…¸
        if transformer_lora_layers:
            state_dict.update(cls.pack_weights(transformer_lora_layers, cls.transformer_name))

        # å¦‚æœä¼ é€’äº† text_encoder_lora_layersï¼Œåˆ™å°†å…¶æ‰“åŒ…å¹¶æ›´æ–°çŠ¶æ€å­—å…¸
        if text_encoder_lora_layers:
            state_dict.update(cls.pack_weights(text_encoder_lora_layers, "text_encoder"))

        # å¦‚æœä¼ é€’äº† text_encoder_2_lora_layersï¼Œåˆ™å°†å…¶æ‰“åŒ…å¹¶æ›´æ–°çŠ¶æ€å­—å…¸
        if text_encoder_2_lora_layers:
            state_dict.update(cls.pack_weights(text_encoder_2_lora_layers, "text_encoder_2"))

        # ä¿å­˜æ¨¡å‹
        cls.write_lora_layers(
            state_dict=state_dict,  # è¦ä¿å­˜çš„çŠ¶æ€å­—å…¸
            save_directory=save_directory,  # ä¿å­˜ç›®å½•
            is_main_process=is_main_process,  # ä¸»è¿›ç¨‹æ ‡å¿—
            weight_name=weight_name,  # æƒé‡åç§°
            save_function=save_function,  # ä¿å­˜å‡½æ•°
            safe_serialization=safe_serialization,  # å®‰å…¨åºåˆ—åŒ–æ ‡å¿—
        )
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°† LoRA å‚æ•°èåˆåˆ°åŸå§‹å‚æ•°ä¸­
    def fuse_lora(
        # æ–¹æ³•çš„å‚æ•°åˆ—è¡¨
        self,
        # å¯é€‰ç»„ä»¶åˆ—è¡¨ï¼Œé»˜è®¤åŒ…æ‹¬ "transformer"ã€"text_encoder" å’Œ "text_encoder_2"
        components: List[str] = ["transformer", "text_encoder", "text_encoder_2"],
        # LoRA å‚æ•°å½±å“è¾“å‡ºçš„æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 1.0
        lora_scale: float = 1.0,
        # å®‰å…¨èåˆæ ‡å¿—ï¼Œé»˜è®¤ä¸º False
        safe_fusing: bool = False,
        # å¯é€‰é€‚é…å™¨åç§°åˆ—è¡¨ï¼Œé»˜è®¤ä¸º None
        adapter_names: Optional[List[str]] = None,
        # å…¶ä»–å…³é”®å­—å‚æ•°
        **kwargs,
    ):
        # æ–¹æ³•æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°è¯¥æ–¹æ³•çš„åŠŸèƒ½å’Œå‚æ•°
        r"""
        Fuses the LoRA parameters into the original parameters of the corresponding blocks.

        <Tip warning={true}>

        This is an experimental API.

        </Tip>

        Args:
            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.
            lora_scale (`float`, defaults to 1.0):
                Controls how much to influence the outputs with the LoRA parameters.
            safe_fusing (`bool`, defaults to `False`):
                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.
            adapter_names (`List[str]`, *optional*):
                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.

        Example:

        ```py
        from diffusers import DiffusionPipeline
        import torch

        pipeline = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
        ).to("cuda")
        pipeline.load_lora_weights("nerijs/pixel-art-xl", weight_name="pixel-art-xl.safetensors", adapter_name="pixel")
        pipeline.fuse_lora(lora_scale=0.7)
        ```
        """
        # è°ƒç”¨çˆ¶ç±»çš„æ–¹æ³•è¿›è¡Œ LoRA å‚æ•°èåˆï¼Œä¼ é€’ç›¸å…³å‚æ•°
        super().fuse_lora(
            components=components, lora_scale=lora_scale, safe_fusing=safe_fusing, adapter_names=adapter_names
        )

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°† LoRA å‚æ•°ä»ç»„ä»¶ä¸­ç§»é™¤
    def unfuse_lora(self, components: List[str] = ["transformer", "text_encoder", "text_encoder_2"], **kwargs):
        # æ–¹æ³•æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°è¯¥æ–¹æ³•çš„åŠŸèƒ½å’Œå‚æ•°
        r"""
        Reverses the effect of
        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).

        <Tip warning={true}>

        This is an experimental API.

        </Tip>

        Args:
            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.
            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.
            unfuse_text_encoder (`bool`, defaults to `True`):
                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
                LoRA parameters then it won't have any effect.
        """
        # è°ƒç”¨çˆ¶ç±»çš„æ–¹æ³•è¿›è¡Œ LoRA å‚æ•°ç§»é™¤ï¼Œä¼ é€’ç»„ä»¶å‚æ•°
        super().unfuse_lora(components=components)
# å®šä¹‰ä¸€ä¸ªæ··åˆç±»ï¼Œç”¨äºåŠ è½½ LoRA å±‚ï¼Œç»§æ‰¿è‡ª LoraBaseMixin
class FluxLoraLoaderMixin(LoraBaseMixin):
    r"""
    åŠ è½½ LoRA å±‚åˆ° [`FluxTransformer2DModel`] å’Œ [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)ã€‚
    
    ç‰¹å®šäº [`StableDiffusion3Pipeline`]ã€‚
    """

    # å¯åŠ è½½çš„ LoRA æ¨¡å—åç§°åˆ—è¡¨
    _lora_loadable_modules = ["transformer", "text_encoder"]
    # Transformer çš„åç§°
    transformer_name = TRANSFORMER_NAME
    # æ–‡æœ¬ç¼–ç å™¨çš„åç§°
    text_encoder_name = TEXT_ENCODER_NAME

    # ç±»æ–¹æ³•ï¼ŒéªŒè¯ Hugging Face Hub å‚æ•°ï¼Œå¹¶è·å– LoRA çŠ¶æ€å­—å…¸
    @classmethod
    @validate_hf_hub_args
    def lora_state_dict(
        cls,
        # é¢„è®­ç»ƒæ¨¡å‹çš„åç§°ã€è·¯å¾„æˆ–å­—å…¸ï¼Œç±»å‹å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸
        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],
        # æ˜¯å¦è¿”å› alpha å€¼ï¼Œé»˜è®¤ä¸º False
        return_alphas: bool = False,
        # å…¶ä»–å…³é”®å­—å‚æ•°
        **kwargs,
    ):
        # æ–¹æ³•ä½“ç¼ºå¤±ï¼Œéœ€å®ç°å…·ä½“é€»è¾‘
        pass

    # å®ä¾‹æ–¹æ³•ï¼ŒåŠ è½½ LoRA æƒé‡
    def load_lora_weights(
        self, 
        # é¢„è®­ç»ƒæ¨¡å‹çš„åç§°ã€è·¯å¾„æˆ–å­—å…¸ï¼Œç±»å‹å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸
        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], 
        # å¯é€‰çš„é€‚é…å™¨åç§°
        adapter_name=None, 
        # å…¶ä»–å…³é”®å­—å‚æ•°
        **kwargs
    ):
        # æ–¹æ³•ä½“ç¼ºå¤±ï¼Œéœ€å®ç°å…·ä½“é€»è¾‘
        pass
    ):
        """
        åŠ è½½æŒ‡å®šçš„ LoRA æƒé‡åˆ° `self.transformer` å’Œ `self.text_encoder`ã€‚

        æ‰€æœ‰å…³é”®å­—å‚æ•°ä¼šè½¬å‘ç»™ `self.lora_state_dict`ã€‚

        è¯¦è§ [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸ã€‚

        è¯¦è§ [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ° `self.transformer`ã€‚

        å‚æ•°ï¼š
            pretrained_model_name_or_path_or_dict (`str` æˆ– `os.PathLike` æˆ– `dict`):
                è¯¦è§ [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`]ã€‚
            kwargs (`dict`, *å¯é€‰*):
                è¯¦è§ [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`]ã€‚
            adapter_name (`str`, *å¯é€‰*):
                ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨
                `default_{i}`ï¼Œå…¶ä¸­ i æ˜¯åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ PEFT åç«¯ï¼Œè‹¥æœªå¯ç”¨åˆ™æŠ›å‡ºé”™è¯¯
        if not USE_PEFT_BACKEND:
            raise ValueError("PEFT backend is required for this method.")

        # å¦‚æœä¼ å…¥çš„æ˜¯å­—å…¸ï¼Œåˆ™å¤åˆ¶å®ƒä»¥é¿å…å°±åœ°ä¿®æ”¹
        if isinstance(pretrained_model_name_or_path_or_dict, dict):
            pretrained_model_name_or_path_or_dict = pretrained_model_name_or_path_or_dict.copy()

        # é¦–å…ˆï¼Œç¡®ä¿æ£€æŸ¥ç‚¹æ˜¯å…¼å®¹çš„ï¼Œå¹¶å¯ä»¥æˆåŠŸåŠ è½½
        state_dict, network_alphas = self.lora_state_dict(
            pretrained_model_name_or_path_or_dict, return_alphas=True, **kwargs
        )

        # éªŒè¯çŠ¶æ€å­—å…¸çš„æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿åŒ…å« "lora" æˆ– "dora_scale"
        is_correct_format = all("lora" in key or "dora_scale" in key for key in state_dict.keys())
        if not is_correct_format:
            raise ValueError("Invalid LoRA checkpoint.")

        # å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ° transformer ä¸­
        self.load_lora_into_transformer(
            state_dict,
            network_alphas=network_alphas,
            transformer=getattr(self, self.transformer_name) if not hasattr(self, "transformer") else self.transformer,
            adapter_name=adapter_name,
            _pipeline=self,
        )

        # ä»çŠ¶æ€å­—å…¸ä¸­æå–ä¸ text_encoder ç›¸å…³çš„éƒ¨åˆ†
        text_encoder_state_dict = {k: v for k, v in state_dict.items() if "text_encoder." in k}
        # å¦‚æœæå–çš„å­—å…¸ä¸ä¸ºç©ºï¼Œåˆ™åŠ è½½åˆ° text_encoder ä¸­
        if len(text_encoder_state_dict) > 0:
            self.load_lora_into_text_encoder(
                text_encoder_state_dict,
                network_alphas=network_alphas,
                text_encoder=self.text_encoder,
                prefix="text_encoder",
                lora_scale=self.lora_scale,
                adapter_name=adapter_name,
                _pipeline=self,
            )

    @classmethod
    @classmethod
    # ä» diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder å¤åˆ¶çš„
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºå°† Lora æ¨¡å‹åŠ è½½åˆ°æ–‡æœ¬ç¼–ç å™¨ä¸­
        def load_lora_into_text_encoder(
            cls,  # ç±»æœ¬èº«
            state_dict,  # çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹æƒé‡
            network_alphas,  # ç½‘ç»œä¸­çš„ç¼©æ”¾å› å­
            text_encoder,  # æ–‡æœ¬ç¼–ç å™¨å®ä¾‹
            prefix=None,  # å¯é€‰çš„å‰ç¼€ï¼Œç”¨äºå‘½å
            lora_scale=1.0,  # Lora ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 1.0
            adapter_name=None,  # å¯é€‰çš„é€‚é…å™¨åç§°
            _pipeline=None,  # å¯é€‰çš„ç®¡é“å‚æ•°ï¼Œç”¨äºè¿›ä¸€æ­¥å¤„ç†
        @classmethod  # æŒ‡å®šè¿™æ˜¯ä¸€ä¸ªç±»æ–¹æ³•
        # ä» diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights æ‹·è´è€Œæ¥ï¼Œå°† unet æ›¿æ¢ä¸º transformer
        def save_lora_weights(
            cls,  # ç±»æœ¬èº«
            save_directory: Union[str, os.PathLike],  # ä¿å­˜æƒé‡çš„ç›®å½•
            transformer_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,  # transformer çš„ Lora å±‚
            text_encoder_lora_layers: Dict[str, torch.nn.Module] = None,  # æ–‡æœ¬ç¼–ç å™¨çš„ Lora å±‚
            is_main_process: bool = True,  # æ ‡è¯†å½“å‰æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
            weight_name: str = None,  # æƒé‡æ–‡ä»¶çš„åç§°
            save_function: Callable = None,  # è‡ªå®šä¹‰ä¿å­˜å‡½æ•°
            safe_serialization: bool = True,  # æ˜¯å¦å®‰å…¨åºåˆ—åŒ–ï¼Œé»˜è®¤ä¸º True
    ):
        r"""  # å®šä¹‰æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°æ­¤å‡½æ•°çš„åŠŸèƒ½åŠå‚æ•°
        Save the LoRA parameters corresponding to the UNet and text encoder.  # æè¿°ä¿å­˜LoRAå‚æ•°çš„åŠŸèƒ½

        Arguments:  # å¼€å§‹åˆ—å‡ºå‡½æ•°çš„å‚æ•°
            save_directory (`str` or `os.PathLike`):  # å‚æ•°ï¼šä¿å­˜LoRAå‚æ•°çš„ç›®å½•ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²æˆ–è·¯å¾„ç±»
                Directory to save LoRA parameters to. Will be created if it doesn't exist.  # æè¿°ï¼šå¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºè¯¥ç›®å½•
            transformer_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):  # å‚æ•°ï¼šä¸transformerå¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸
                State dict of the LoRA layers corresponding to the `transformer`.  # æè¿°ï¼šè¯´æ˜å‚æ•°çš„ä½œç”¨
            text_encoder_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):  # å‚æ•°ï¼šä¸text_encoderå¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸
                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text  # æè¿°ï¼šè¯´æ˜æ­¤å‚æ•°å¿…é¡»æä¾›ï¼Œæ¥è‡ªğŸ¤— Transformers
                encoder LoRA state dict because it comes from ğŸ¤— Transformers.  # ç»§ç»­æè¿°å‚æ•°çš„æ¥æº
            is_main_process (`bool`, *optional*, defaults to `True`):  # å‚æ•°ï¼šæŒ‡ç¤ºå½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼Œç±»å‹ä¸ºå¸ƒå°”å€¼
                Whether the process calling this is the main process or not. Useful during distributed training and you  # æè¿°ï¼šç”¨äºåˆ†å¸ƒå¼è®­ç»ƒæ—¶åˆ¤æ–­ä¸»è¿›ç¨‹
                need to call this function on all processes. In this case, set `is_main_process=True` only on the main  # è¿›ä¸€æ­¥è¯´æ˜å¦‚ä½•ä½¿ç”¨æ­¤å‚æ•°
                process to avoid race conditions.  # æè¿°ï¼šé¿å…ç«äº‰æ¡ä»¶
            save_function (`Callable`):  # å‚æ•°ï¼šç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ï¼Œç±»å‹ä¸ºå¯è°ƒç”¨å¯¹è±¡
                The function to use to save the state dictionary. Useful during distributed training when you need to  # æè¿°ï¼šåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œå¯èƒ½éœ€è¦æ›¿æ¢é»˜è®¤çš„ä¿å­˜æ–¹æ³•
                replace `torch.save` with another method. Can be configured with the environment variable  # è¯´æ˜å¦‚ä½•é…ç½®æ­¤å‚æ•°
                `DIFFUSERS_SAVE_MODE`.  # æä¾›ç¯å¢ƒå˜é‡åç§°
            safe_serialization (`bool`, *optional*, defaults to `True`):  # å‚æ•°ï¼šæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–ä¿å­˜æ¨¡å‹ï¼Œç±»å‹ä¸ºå¸ƒå°”å€¼
                Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.  # æè¿°ï¼šé€‰æ‹©ä¿å­˜æ¨¡å‹çš„æ–¹å¼
        """
        state_dict = {}  # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨çŠ¶æ€å­—å…¸

        if not (transformer_lora_layers or text_encoder_lora_layers):  # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªLoRAå±‚å­—å…¸ä¼ å…¥
            raise ValueError("You must pass at least one of `transformer_lora_layers` and `text_encoder_lora_layers`.")  # å¦‚æœæ²¡æœ‰ï¼ŒæŠ›å‡ºå¼‚å¸¸

        if transformer_lora_layers:  # å¦‚æœå­˜åœ¨transformerçš„LoRAå±‚å­—å…¸
            state_dict.update(cls.pack_weights(transformer_lora_layers, cls.transformer_name))  # æ‰“åŒ…LoRAæƒé‡å¹¶æ›´æ–°çŠ¶æ€å­—å…¸

        if text_encoder_lora_layers:  # å¦‚æœå­˜åœ¨text_encoderçš„LoRAå±‚å­—å…¸
            state_dict.update(cls.pack_weights(text_encoder_lora_layers, cls.text_encoder_name))  # æ‰“åŒ…LoRAæƒé‡å¹¶æ›´æ–°çŠ¶æ€å­—å…¸

        # Save the model  # ä¿å­˜æ¨¡å‹çš„æ³¨é‡Š
        cls.write_lora_layers(  # è°ƒç”¨ç±»æ–¹æ³•ä»¥å†™å…¥LoRAå±‚
            state_dict=state_dict,  # ä¼ å…¥çŠ¶æ€å­—å…¸
            save_directory=save_directory,  # ä¼ å…¥ä¿å­˜ç›®å½•
            is_main_process=is_main_process,  # ä¼ å…¥ä¸»è¿›ç¨‹æ ‡å¿—
            weight_name=weight_name,  # ä¼ å…¥æƒé‡åç§°
            save_function=save_function,  # ä¼ å…¥ä¿å­˜å‡½æ•°
            safe_serialization=safe_serialization,  # ä¼ å…¥å®‰å…¨åºåˆ—åŒ–æ ‡å¿—
        )

    # Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora with unet->transformer  # æ³¨é‡Šè¯´æ˜æ­¤æ–¹æ³•çš„æ¥æºå’Œä¿®æ”¹
    def fuse_lora(  # å®šä¹‰fuse_loraæ–¹æ³•
        self,  # æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å®ä¾‹è‡ªèº«
        components: List[str] = ["transformer", "text_encoder"],  # å‚æ•°ï¼šè¦èåˆçš„ç»„ä»¶åˆ—è¡¨ï¼Œé»˜è®¤åŒ…å«transformerå’Œtext_encoder
        lora_scale: float = 1.0,  # å‚æ•°ï¼šLoRAçš„ç¼©æ”¾å› å­ï¼Œé»˜è®¤å€¼ä¸º1.0
        safe_fusing: bool = False,  # å‚æ•°ï¼šæŒ‡ç¤ºæ˜¯å¦å®‰å…¨èåˆï¼Œé»˜è®¤å€¼ä¸ºFalse
        adapter_names: Optional[List[str]] = None,  # å‚æ•°ï¼šå¯é€‰çš„é€‚é…å™¨åç§°åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone
        **kwargs,  # å¯æ¥æ”¶å…¶ä»–å…³é”®å­—å‚æ•°
    ):
        r""" 
        # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ­¤å‡½æ•°çš„ä½œç”¨å’Œç”¨æ³•
        
        Fuses the LoRA parameters into the original parameters of the corresponding blocks.
        # å°† LoRA å‚æ•°èåˆåˆ°å¯¹åº”å—çš„åŸå§‹å‚æ•°ä¸­

        <Tip warning={true}>
        # è­¦å‘Šæç¤ºï¼Œè¯´æ˜è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§ API

        This is an experimental API.
        # è¿™æ˜¯ä¸€é¡¹å®éªŒæ€§ API

        </Tip>

        Args:
            components: (`List[str]`): 
            # å‚æ•°è¯´æ˜ï¼Œæ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¡¨ç¤ºè¦èåˆ LoRA çš„ç»„ä»¶
            
            lora_scale (`float`, defaults to 1.0):
            # å‚æ•°è¯´æ˜ï¼Œæ§åˆ¶ LoRA å‚æ•°å¯¹è¾“å‡ºçš„å½±å“ç¨‹åº¦
            
                Controls how much to influence the outputs with the LoRA parameters.
                # æ§åˆ¶ LoRA å‚æ•°å¯¹è¾“å‡ºçš„å½±å“ç¨‹åº¦
            
            safe_fusing (`bool`, defaults to `False`):
            # å‚æ•°è¯´æ˜ï¼Œæ˜¯å¦åœ¨èåˆä¹‹å‰æ£€æŸ¥æƒé‡ä¸­æ˜¯å¦æœ‰ NaN å€¼
            
                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.
                # æ˜¯å¦åœ¨èåˆä¹‹å‰æ£€æŸ¥æƒé‡çš„ NaN å€¼ï¼Œå¦‚æœå­˜åœ¨åˆ™ä¸è¿›è¡Œèåˆ
            
            adapter_names (`List[str]`, *optional*):
            # å‚æ•°è¯´æ˜ï¼Œå¯é€‰çš„é€‚é…å™¨åç§°åˆ—è¡¨ï¼Œç”¨äºèåˆ
            
                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.
                # ç”¨äºèåˆçš„é€‚é…å™¨åç§°åˆ—è¡¨ï¼Œå¦‚æœæœªä¼ å…¥ï¼Œåˆ™å°†èåˆæ‰€æœ‰æ´»åŠ¨é€‚é…å™¨

        Example:
        # ç¤ºä¾‹ä»£ç ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨è¯¥ API

        ```py
        from diffusers import DiffusionPipeline
        # å¯¼å…¥ DiffusionPipeline ç±»
        
        import torch
        # å¯¼å…¥ PyTorch åº“

        pipeline = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
        ).to("cuda")
        # ä»é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºç®¡é“ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ° CUDA è®¾å¤‡ä¸Š
        
        pipeline.load_lora_weights("nerijs/pixel-art-xl", weight_name="pixel-art-xl.safetensors", adapter_name="pixel")
        # åŠ è½½ LoRA æƒé‡åˆ°ç®¡é“ä¸­
        
        pipeline.fuse_lora(lora_scale=0.7)
        # èåˆ LoRA å‚æ•°ï¼Œè®¾ç½®å½±å“ç¨‹åº¦ä¸º 0.7
        ```
        """
        super().fuse_lora(
            # è°ƒç”¨çˆ¶ç±»çš„ fuse_lora æ–¹æ³•ï¼Œå°†ç›¸å…³å‚æ•°ä¼ é€’ç»™å®ƒ
            components=components, lora_scale=lora_scale, safe_fusing=safe_fusing, adapter_names=adapter_names
        )

    def unfuse_lora(self, components: List[str] = ["transformer", "text_encoder"], **kwargs):
        r"""
        # æ–¹æ³•æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ­¤æ–¹æ³•çš„ä½œç”¨å’Œç”¨æ³•
        
        Reverses the effect of
        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).
        # åè½¬ fuse_lora æ–¹æ³•çš„æ•ˆæœ

        <Tip warning={true}>
        # è­¦å‘Šæç¤ºï¼Œè¯´æ˜è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§ API

        This is an experimental API.
        # è¿™æ˜¯ä¸€é¡¹å®éªŒæ€§ API

        </Tip>

        Args:
            components (`List[str]`): 
            # å‚æ•°è¯´æ˜ï¼Œæ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¡¨ç¤ºè¦ä»ä¸­è§£é™¤ LoRA çš„ç»„ä»¶
            
            List of LoRA-injectable components to unfuse LoRA from.
            # è¦ä»ä¸­è§£é™¤ LoRA çš„ç»„ä»¶åˆ—è¡¨
        """
        super().unfuse_lora(components=components)
        # è°ƒç”¨çˆ¶ç±»çš„ unfuse_lora æ–¹æ³•ï¼Œå°†ç›¸å…³å‚æ•°ä¼ é€’ç»™å®ƒ
# è¿™é‡Œæˆ‘ä»¬ä» `StableDiffusionLoraLoaderMixin` å­ç±»åŒ–ï¼Œå› ä¸º Amused æœ€åˆä¾èµ–äºè¯¥ç±»æä¾› LoRA æ”¯æŒ
class AmusedLoraLoaderMixin(StableDiffusionLoraLoaderMixin):
    # å¯åŠ è½½çš„ LoRA æ¨¡å—åˆ—è¡¨
    _lora_loadable_modules = ["transformer", "text_encoder"]
    # å®šä¹‰å˜æ¢å™¨çš„åç§°
    transformer_name = TRANSFORMER_NAME
    # å®šä¹‰æ–‡æœ¬ç¼–ç å™¨çš„åç§°
    text_encoder_name = TEXT_ENCODER_NAME

    @classmethod
    @classmethod
    # ä» diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin ä¸­å¤åˆ¶çš„æ–¹æ³•ï¼Œç”¨äºå°† LoRA åŠ è½½åˆ°æ–‡æœ¬ç¼–ç å™¨ä¸­
    def load_lora_into_text_encoder(
        cls,
        state_dict,
        network_alphas,
        text_encoder,
        prefix=None,
        lora_scale=1.0,
        adapter_name=None,
        _pipeline=None,
    @classmethod
    # å®šä¹‰ä¿å­˜ LoRA æƒé‡çš„æ–¹æ³•
    def save_lora_weights(
        cls,
        save_directory: Union[str, os.PathLike],
        text_encoder_lora_layers: Dict[str, torch.nn.Module] = None,
        transformer_lora_layers: Dict[str, torch.nn.Module] = None,
        is_main_process: bool = True,
        weight_name: str = None,
        save_function: Callable = None,
        safe_serialization: bool = True,
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œä¿å­˜ä¸ UNet å’Œæ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„ LoRA å‚æ•°
        ):
            r""" 
            ä¿å­˜ä¸ UNet å’Œæ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„ LoRA å‚æ•°ã€‚
    
            å‚æ•°ï¼š
                save_directory (`str` æˆ– `os.PathLike`):
                    ä¿å­˜ LoRA å‚æ•°çš„ç›®å½•ã€‚å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå°†è¢«åˆ›å»ºã€‚
                unet_lora_layers (`Dict[str, torch.nn.Module]` æˆ– `Dict[str, torch.Tensor]`):
                    ä¸ `unet` ç›¸å…³çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ã€‚
                text_encoder_lora_layers (`Dict[str, torch.nn.Module]` æˆ– `Dict[str, torch.Tensor]`):
                    ä¸ `text_encoder` ç›¸å…³çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ã€‚å¿…é¡»æ˜ç¡®ä¼ é€’æ–‡æœ¬ç¼–ç å™¨çš„ LoRA çŠ¶æ€å­—å…¸ï¼Œå› ä¸ºå®ƒæ¥è‡ª ğŸ¤— Transformersã€‚
                is_main_process (`bool`, *å¯é€‰*, é»˜è®¤å€¼ä¸º `True`):
                    è°ƒç”¨æ­¤å‡½æ•°çš„è¿‡ç¨‹æ˜¯å¦ä¸ºä¸»è¿‡ç¨‹ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´ï¼Œæ‚¨éœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨æ­¤å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªæœ‰åœ¨ä¸»è¿‡ç¨‹ä¸­å°† `is_main_process=True`ï¼Œä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚
                save_function (`Callable`):
                    ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œéœ€è¦ç”¨å…¶ä»–æ–¹æ³•æ›¿æ¢ `torch.save`ã€‚å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ `DIFFUSERS_SAVE_MODE` è¿›è¡Œé…ç½®ã€‚
                safe_serialization (`bool`, *å¯é€‰*, é»˜è®¤å€¼ä¸º `True`):
                    æ˜¯å¦ä½¿ç”¨ `safetensors` æˆ–ä¼ ç»Ÿçš„ PyTorch æ–¹å¼é€šè¿‡ `pickle` ä¿å­˜æ¨¡å‹ã€‚
            """
            # åˆå§‹åŒ–çŠ¶æ€å­—å…¸ï¼Œç”¨äºå­˜å‚¨ LoRA å‚æ•°
            state_dict = {}
    
            # æ£€æŸ¥è‡³å°‘ä¼ é€’ä¸€ä¸ª LoRA å±‚çš„çŠ¶æ€å­—å…¸
            if not (transformer_lora_layers or text_encoder_lora_layers):
                # å¦‚æœä¸¤ä¸ªéƒ½æ²¡æœ‰ï¼ŒæŠ›å‡ºé”™è¯¯
                raise ValueError("You must pass at least one of `transformer_lora_layers` or `text_encoder_lora_layers`.")
    
            # å¦‚æœæœ‰ transformer LoRA å±‚ï¼Œæ›´æ–°çŠ¶æ€å­—å…¸
            if transformer_lora_layers:
                state_dict.update(cls.pack_weights(transformer_lora_layers, cls.transformer_name))
    
            # å¦‚æœæœ‰æ–‡æœ¬ç¼–ç å™¨ LoRA å±‚ï¼Œæ›´æ–°çŠ¶æ€å­—å…¸
            if text_encoder_lora_layers:
                state_dict.update(cls.pack_weights(text_encoder_lora_layers, cls.text_encoder_name))
    
            # ä¿å­˜æ¨¡å‹çš„è¿‡ç¨‹
            cls.write_lora_layers(
                # ä¼ å…¥çŠ¶æ€å­—å…¸
                state_dict=state_dict,
                # ä¿å­˜ç›®å½•
                save_directory=save_directory,
                # æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
                is_main_process=is_main_process,
                # æƒé‡åç§°
                weight_name=weight_name,
                # ä¿å­˜å‡½æ•°
                save_function=save_function,
                # æ˜¯å¦ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–
                safe_serialization=safe_serialization,
            )
# å®šä¹‰ä¸€ä¸ªåä¸º LoraLoaderMixin çš„ç±»ï¼Œç»§æ‰¿è‡ª StableDiffusionLoraLoaderMixin
class LoraLoaderMixin(StableDiffusionLoraLoaderMixin):
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥æ”¶å¯å˜ä½ç½®å’Œå…³é”®å­—å‚æ•°
    def __init__(self, *args, **kwargs):
        # è®¾ç½®å¼ƒç”¨è­¦å‘Šä¿¡æ¯ï¼Œæç¤ºç”¨æˆ·è¯¥ç±»å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­ç§»é™¤
        deprecation_message = "LoraLoaderMixin is deprecated and this will be removed in a future version. Please use `StableDiffusionLoraLoaderMixin`, instead."
        # è°ƒç”¨ deprecate å‡½æ•°ï¼Œè®°å½•è¯¥ç±»çš„å¼ƒç”¨ä¿¡æ¯
        deprecate("LoraLoaderMixin", "1.0.0", deprecation_message)
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œä¼ é€’ä½ç½®å’Œå…³é”®å­—å‚æ•°
        super().__init__(*args, **kwargs)
```