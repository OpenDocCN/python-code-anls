# `.\diffusers\models\transformers\cogvideox_transformer_3d.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œè¯´æ˜ä»£ç çš„ç‰ˆæƒæ‰€æœ‰è€…å’Œä½¿ç”¨è®¸å¯
# Copyright 2024 The CogVideoX team, Tsinghua University & ZhipuAI and The HuggingFace Team.
# æ‰€æœ‰æƒåˆ©ä¿ç•™ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ï¼Œç¬¬ 2.0 ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰è¿›è¡Œæˆæƒï¼›
# é™¤ééµå¾ªè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸èƒ½ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰çº¦å®šï¼Œ
# æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰â€œåŸæ ·â€æä¾›çš„ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚
# æœ‰å…³è®¸å¯è¯ä¸‹ç‰¹å®šæƒé™å’Œé™åˆ¶çš„ä¿¡æ¯ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚

# ä» typing æ¨¡å—å¯¼å…¥æ‰€éœ€çš„ç±»å‹
from typing import Any, Dict, Optional, Tuple, Union

# å¯¼å…¥ PyTorch åº“
import torch
# ä» PyTorch å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—
from torch import nn

# å¯¼å…¥å…¶ä»–æ¨¡å—ä¸­çš„å·¥å…·å’Œç±»
from ...configuration_utils import ConfigMixin, register_to_config
from ...utils import is_torch_version, logging
from ...utils.torch_utils import maybe_allow_in_graph
from ..attention import Attention, FeedForward
from ..attention_processor import AttentionProcessor, CogVideoXAttnProcessor2_0, FusedCogVideoXAttnProcessor2_0
from ..embeddings import CogVideoXPatchEmbed, TimestepEmbedding, Timesteps
from ..modeling_outputs import Transformer2DModelOutput
from ..modeling_utils import ModelMixin
from ..normalization import AdaLayerNorm, CogVideoXLayerNormZero

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨ï¼Œä»¥ä¾¿åœ¨æ¨¡å—ä¸­è®°å½•ä¿¡æ¯
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name

# ä½¿ç”¨è£…é¥°å™¨ï¼Œå…è®¸åœ¨å›¾è®¡ç®—ä¸­å¯èƒ½çš„åŠŸèƒ½
@maybe_allow_in_graph
# å®šä¹‰ä¸€ä¸ªåä¸º CogVideoXBlock çš„ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
class CogVideoXBlock(nn.Module):
    r"""
    åœ¨ [CogVideoX](https://github.com/THUDM/CogVideo) æ¨¡å‹ä¸­ä½¿ç”¨çš„ Transformer å—ã€‚
    # å®šä¹‰å‡½æ•°å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°å„ä¸ªå‚æ•°çš„ç”¨é€”
    Parameters:
        dim (`int`):  # è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°
            The number of channels in the input and output.
        num_attention_heads (`int`):  # å¤šå¤´æ³¨æ„åŠ›ä¸­ä½¿ç”¨çš„å¤´æ•°
            The number of heads to use for multi-head attention.
        attention_head_dim (`int`):  # æ¯ä¸ªå¤´çš„é€šé“æ•°
            The number of channels in each head.
        time_embed_dim (`int`):  # æ—¶é—´æ­¥åµŒå…¥çš„é€šé“æ•°
            The number of channels in timestep embedding.
        dropout (`float`, defaults to `0.0`):  # ä½¿ç”¨çš„ä¸¢å¼ƒæ¦‚ç‡
            The dropout probability to use.
        activation_fn (`str`, defaults to `"gelu-approximate"`):  # å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°
            Activation function to be used in feed-forward.
        attention_bias (`bool`, defaults to `False`):  # æ˜¯å¦åœ¨æ³¨æ„åŠ›æŠ•å½±å±‚ä½¿ç”¨åç½®
            Whether or not to use bias in attention projection layers.
        qk_norm (`bool`, defaults to `True`):  # æ˜¯å¦åœ¨æ³¨æ„åŠ›ä¸­æŸ¥è¯¢å’Œé”®çš„æŠ•å½±åä½¿ç”¨å½’ä¸€åŒ–
            Whether or not to use normalization after query and key projections in Attention.
        norm_elementwise_affine (`bool`, defaults to `True`):  # æ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ çš„é€å…ƒç´ ä»¿å°„å‚æ•°è¿›è¡Œå½’ä¸€åŒ–
            Whether to use learnable elementwise affine parameters for normalization.
        norm_eps (`float`, defaults to `1e-5`):  # å½’ä¸€åŒ–å±‚çš„ epsilon å€¼
            Epsilon value for normalization layers.
        final_dropout (`bool`, defaults to `False`):  # æ˜¯å¦åœ¨æœ€åçš„å‰é¦ˆå±‚ååº”ç”¨æœ€ç»ˆçš„ä¸¢å¼ƒ
            Whether to apply a final dropout after the last feed-forward layer.
        ff_inner_dim (`int`, *optional*, defaults to `None`):  # å‰é¦ˆå±‚çš„è‡ªå®šä¹‰éšè—ç»´åº¦
            Custom hidden dimension of Feed-forward layer. If not provided, `4 * dim` is used.
        ff_bias (`bool`, defaults to `True`):  # æ˜¯å¦åœ¨å‰é¦ˆå±‚ä¸­ä½¿ç”¨åç½®
            Whether or not to use bias in Feed-forward layer.
        attention_out_bias (`bool`, defaults to `True`):  # æ˜¯å¦åœ¨æ³¨æ„åŠ›è¾“å‡ºæŠ•å½±å±‚ä¸­ä½¿ç”¨åç½®
            Whether or not to use bias in Attention output projection layer.
    """  # ç»“æŸæ–‡æ¡£å­—ç¬¦ä¸²

    def __init__(  # å®šä¹‰æ„é€ å‡½æ•°
        self,  # å®ä¾‹è‡ªèº«
        dim: int,  # è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°
        num_attention_heads: int,  # å¤šå¤´æ³¨æ„åŠ›ä¸­å¤´æ•°
        attention_head_dim: int,  # æ¯ä¸ªå¤´çš„é€šé“æ•°
        time_embed_dim: int,  # æ—¶é—´æ­¥åµŒå…¥é€šé“æ•°
        dropout: float = 0.0,  # é»˜è®¤ä¸¢å¼ƒæ¦‚ç‡
        activation_fn: str = "gelu-approximate",  # é»˜è®¤æ¿€æ´»å‡½æ•°
        attention_bias: bool = False,  # é»˜è®¤ä¸ä½¿ç”¨æ³¨æ„åŠ›åç½®
        qk_norm: bool = True,  # é»˜è®¤ä½¿ç”¨æŸ¥è¯¢å’Œé”®çš„å½’ä¸€åŒ–
        norm_elementwise_affine: bool = True,  # é»˜è®¤ä½¿ç”¨é€å…ƒç´ ä»¿å°„å‚æ•°
        norm_eps: float = 1e-5,  # é»˜è®¤å½’ä¸€åŒ–çš„ epsilon å€¼
        final_dropout: bool = True,  # é»˜è®¤ä½¿ç”¨æœ€ç»ˆä¸¢å¼ƒ
        ff_inner_dim: Optional[int] = None,  # å‰é¦ˆå±‚çš„å¯é€‰éšè—ç»´åº¦
        ff_bias: bool = True,  # é»˜è®¤ä½¿ç”¨å‰é¦ˆå±‚çš„åç½®
        attention_out_bias: bool = True,  # é»˜è®¤ä½¿ç”¨æ³¨æ„åŠ›è¾“å‡ºå±‚çš„åç½®
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
        super().__init__()

        # 1. Self Attention
        # åˆ›å»ºå½’ä¸€åŒ–å±‚ï¼Œå¤„ç†æ—¶é—´åµŒå…¥ç»´åº¦å’Œç‰¹å¾ç»´åº¦
        self.norm1 = CogVideoXLayerNormZero(time_embed_dim, dim, norm_elementwise_affine, norm_eps, bias=True)

        # åˆ›å»ºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé…ç½®æŸ¥è¯¢ç»´åº¦å’Œå¤´æ•°ç­‰å‚æ•°
        self.attn1 = Attention(
            query_dim=dim,
            dim_head=attention_head_dim,
            heads=num_attention_heads,
            qk_norm="layer_norm" if qk_norm else None,
            eps=1e-6,
            bias=attention_bias,
            out_bias=attention_out_bias,
            processor=CogVideoXAttnProcessor2_0(),
        )

        # 2. Feed Forward
        # åˆ›å»ºå¦ä¸€ä¸ªå½’ä¸€åŒ–å±‚ï¼Œç”¨äºåç»­çš„å‰é¦ˆç½‘ç»œ
        self.norm2 = CogVideoXLayerNormZero(time_embed_dim, dim, norm_elementwise_affine, norm_eps, bias=True)

        # åˆ›å»ºå‰é¦ˆç½‘ç»œï¼Œé…ç½®éšè—å±‚ç»´åº¦åŠå…¶ä»–è¶…å‚æ•°
        self.ff = FeedForward(
            dim,
            dropout=dropout,
            activation_fn=activation_fn,
            final_dropout=final_dropout,
            inner_dim=ff_inner_dim,
            bias=ff_bias,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        temb: torch.Tensor,
        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> torch.Tensor:
        # è·å–ç¼–ç å™¨éšè—çŠ¶æ€çš„åºåˆ—é•¿åº¦
        text_seq_length = encoder_hidden_states.size(1)

        # norm & modulate
        # å¯¹è¾“å…¥çš„éšè—çŠ¶æ€å’Œç¼–ç å™¨çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–å’Œè°ƒåˆ¶
        norm_hidden_states, norm_encoder_hidden_states, gate_msa, enc_gate_msa = self.norm1(
            hidden_states, encoder_hidden_states, temb
        )

        # attention
        # æ‰§è¡Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè®¡ç®—æ–°çš„éšè—çŠ¶æ€
        attn_hidden_states, attn_encoder_hidden_states = self.attn1(
            hidden_states=norm_hidden_states,
            encoder_hidden_states=norm_encoder_hidden_states,
            image_rotary_emb=image_rotary_emb,
        )

        # æ›´æ–°éšè—çŠ¶æ€å’Œç¼–ç å™¨éšè—çŠ¶æ€ï¼Œç»“åˆæ³¨æ„åŠ›è¾“å‡º
        hidden_states = hidden_states + gate_msa * attn_hidden_states
        encoder_hidden_states = encoder_hidden_states + enc_gate_msa * attn_encoder_hidden_states

        # norm & modulate
        # å†æ¬¡è¿›è¡Œå½’ä¸€åŒ–å’Œè°ƒåˆ¶
        norm_hidden_states, norm_encoder_hidden_states, gate_ff, enc_gate_ff = self.norm2(
            hidden_states, encoder_hidden_states, temb
        )

        # feed-forward
        # å°†å½’ä¸€åŒ–åçš„éšè—çŠ¶æ€å’Œç¼–ç å™¨çŠ¶æ€è¿æ¥ï¼Œè¾“å…¥å‰é¦ˆç½‘ç»œ
        norm_hidden_states = torch.cat([norm_encoder_hidden_states, norm_hidden_states], dim=1)
        ff_output = self.ff(norm_hidden_states)

        # æ›´æ–°éšè—çŠ¶æ€å’Œç¼–ç å™¨çŠ¶æ€ï¼Œç»“åˆå‰é¦ˆç½‘ç»œè¾“å‡º
        hidden_states = hidden_states + gate_ff * ff_output[:, text_seq_length:]
        encoder_hidden_states = encoder_hidden_states + enc_gate_ff * ff_output[:, :text_seq_length]

        # è¿”å›æ›´æ–°åçš„éšè—çŠ¶æ€å’Œç¼–ç å™¨çŠ¶æ€
        return hidden_states, encoder_hidden_states
# å®šä¹‰ä¸€ä¸ªç”¨äºè§†é¢‘æ•°æ®çš„ Transformer æ¨¡å‹ï¼Œç»§æ‰¿è‡ª ModelMixin å’Œ ConfigMixin
class CogVideoXTransformer3DModel(ModelMixin, ConfigMixin):
    """
    A Transformer model for video-like data in [CogVideoX](https://github.com/THUDM/CogVideo).
    """

    # è®¾ç½®æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹
    _supports_gradient_checkpointing = True

    # æ³¨å†Œåˆ°é…ç½®ä¸­çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œå®šä¹‰å¤šä¸ªè¶…å‚æ•°
    @register_to_config
    def __init__(
        # æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º 30
        num_attention_heads: int = 30,
        # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º 64
        attention_head_dim: int = 64,
        # è¾“å…¥é€šé“çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º 16
        in_channels: int = 16,
        # è¾“å‡ºé€šé“çš„æ•°é‡ï¼Œå¯é€‰ï¼Œé»˜è®¤å€¼ä¸º 16
        out_channels: Optional[int] = 16,
        # æ˜¯å¦ç¿»è½¬æ­£å¼¦åˆ°ä½™å¼¦ï¼Œé»˜è®¤å€¼ä¸º True
        flip_sin_to_cos: bool = True,
        # é¢‘ç‡åç§»é‡ï¼Œé»˜è®¤å€¼ä¸º 0
        freq_shift: int = 0,
        # æ—¶é—´åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º 512
        time_embed_dim: int = 512,
        # æ–‡æœ¬åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º 4096
        text_embed_dim: int = 4096,
        # å±‚çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º 30
        num_layers: int = 30,
        # dropout æ¦‚ç‡ï¼Œé»˜è®¤å€¼ä¸º 0.0
        dropout: float = 0.0,
        # æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›åç½®ï¼Œé»˜è®¤å€¼ä¸º True
        attention_bias: bool = True,
        # é‡‡æ ·å®½åº¦ï¼Œé»˜è®¤å€¼ä¸º 90
        sample_width: int = 90,
        # é‡‡æ ·é«˜åº¦ï¼Œé»˜è®¤å€¼ä¸º 60
        sample_height: int = 60,
        # é‡‡æ ·å¸§æ•°ï¼Œé»˜è®¤å€¼ä¸º 49
        sample_frames: int = 49,
        # è¡¥ä¸å¤§å°ï¼Œé»˜è®¤å€¼ä¸º 2
        patch_size: int = 2,
        # æ—¶é—´å‹ç¼©æ¯”ä¾‹ï¼Œé»˜è®¤å€¼ä¸º 4
        temporal_compression_ratio: int = 4,
        # æœ€å¤§æ–‡æœ¬åºåˆ—é•¿åº¦ï¼Œé»˜è®¤å€¼ä¸º 226
        max_text_seq_length: int = 226,
        # æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º "gelu-approximate"
        activation_fn: str = "gelu-approximate",
        # æ—¶é—´æ­¥æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œé»˜è®¤å€¼ä¸º "silu"
        timestep_activation_fn: str = "silu",
        # æ˜¯å¦ä½¿ç”¨å…ƒç´ é€ä¸ªä»¿å°„çš„å½’ä¸€åŒ–ï¼Œé»˜è®¤å€¼ä¸º True
        norm_elementwise_affine: bool = True,
        # å½’ä¸€åŒ–çš„ epsilon å€¼ï¼Œé»˜è®¤å€¼ä¸º 1e-5
        norm_eps: float = 1e-5,
        # ç©ºé—´æ’å€¼ç¼©æ”¾å› å­ï¼Œé»˜è®¤å€¼ä¸º 1.875
        spatial_interpolation_scale: float = 1.875,
        # æ—¶é—´æ’å€¼ç¼©æ”¾å› å­ï¼Œé»˜è®¤å€¼ä¸º 1.0
        temporal_interpolation_scale: float = 1.0,
        # æ˜¯å¦ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œé»˜è®¤å€¼ä¸º False
        use_rotary_positional_embeddings: bool = False,
        # æ˜¯å¦ä½¿ç”¨å­¦ä¹ çš„ä½ç½®åµŒå…¥ï¼Œé»˜è®¤å€¼ä¸º False
        use_learned_positional_embeddings: bool = False,
    ):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()
        # è®¡ç®—å†…éƒ¨ç»´åº¦ï¼Œç­‰äºæ³¨æ„åŠ›å¤´æ•°ä¸æ¯ä¸ªå¤´çš„ç»´åº¦ä¹˜ç§¯
        inner_dim = num_attention_heads * attention_head_dim

        # æ£€æŸ¥ä½ç½®åµŒå…¥çš„ä½¿ç”¨æƒ…å†µï¼Œå¦‚æœä¸æ”¯æŒåˆ™æŠ›å‡ºé”™è¯¯
        if not use_rotary_positional_embeddings and use_learned_positional_embeddings:
            raise ValueError(
                "There are no CogVideoX checkpoints available with disable rotary embeddings and learned positional "
                "embeddings. If you're using a custom model and/or believe this should be supported, please open an "
                "issue at https://github.com/huggingface/diffusers/issues."
            )

        # 1. åˆ›å»ºè¡¥ä¸åµŒå…¥å±‚
        self.patch_embed = CogVideoXPatchEmbed(
            # è®¾ç½®è¡¥ä¸å¤§å°
            patch_size=patch_size,
            # è¾“å…¥é€šé“æ•°
            in_channels=in_channels,
            # åµŒå…¥ç»´åº¦
            embed_dim=inner_dim,
            # æ–‡æœ¬åµŒå…¥ç»´åº¦
            text_embed_dim=text_embed_dim,
            # æ˜¯å¦ä½¿ç”¨åç½®
            bias=True,
            # æ ·æœ¬å®½åº¦
            sample_width=sample_width,
            # æ ·æœ¬é«˜åº¦
            sample_height=sample_height,
            # æ ·æœ¬å¸§æ•°
            sample_frames=sample_frames,
            # æ—¶é—´å‹ç¼©æ¯”
            temporal_compression_ratio=temporal_compression_ratio,
            # æœ€å¤§æ–‡æœ¬åºåˆ—é•¿åº¦
            max_text_seq_length=max_text_seq_length,
            # ç©ºé—´æ’å€¼ç¼©æ”¾
            spatial_interpolation_scale=spatial_interpolation_scale,
            # æ—¶é—´æ’å€¼ç¼©æ”¾
            temporal_interpolation_scale=temporal_interpolation_scale,
            # ä½¿ç”¨ä½ç½®åµŒå…¥
            use_positional_embeddings=not use_rotary_positional_embeddings,
            # ä½¿ç”¨å­¦ä¹ çš„ä½ç½®åµŒå…¥
            use_learned_positional_embeddings=use_learned_positional_embeddings,
        )
        # åˆ›å»ºåµŒå…¥ä¸¢å¼ƒå±‚
        self.embedding_dropout = nn.Dropout(dropout)

        # 2. åˆ›å»ºæ—¶é—´åµŒå…¥
        self.time_proj = Timesteps(inner_dim, flip_sin_to_cos, freq_shift)
        # åˆ›å»ºæ—¶é—´æ­¥åµŒå…¥å±‚
        self.time_embedding = TimestepEmbedding(inner_dim, time_embed_dim, timestep_activation_fn)

        # 3. å®šä¹‰æ—¶ç©ºå˜æ¢å™¨å—
        self.transformer_blocks = nn.ModuleList(
            [
                # åˆ›å»ºå¤šä¸ªå˜æ¢å™¨å—
                CogVideoXBlock(
                    dim=inner_dim,
                    num_attention_heads=num_attention_heads,
                    attention_head_dim=attention_head_dim,
                    time_embed_dim=time_embed_dim,
                    dropout=dropout,
                    activation_fn=activation_fn,
                    attention_bias=attention_bias,
                    norm_elementwise_affine=norm_elementwise_affine,
                    norm_eps=norm_eps,
                )
                # æ ¹æ®å±‚æ•°é‡å¤åˆ›å»ºå˜æ¢å™¨å—
                for _ in range(num_layers)
            ]
        )
        # åˆ›å»ºæœ€ç»ˆçš„å±‚å½’ä¸€åŒ–
        self.norm_final = nn.LayerNorm(inner_dim, norm_eps, norm_elementwise_affine)

        # 4. è¾“å‡ºå—çš„å®šä¹‰
        self.norm_out = AdaLayerNorm(
            # åµŒå…¥ç»´åº¦
            embedding_dim=time_embed_dim,
            # è¾“å‡ºç»´åº¦
            output_dim=2 * inner_dim,
            # æ˜¯å¦ä½¿ç”¨å…ƒç´ çº§åˆ«çš„å½’ä¸€åŒ–
            norm_elementwise_affine=norm_elementwise_affine,
            # å½’ä¸€åŒ–çš„epsilonå€¼
            norm_eps=norm_eps,
            # å—çš„ç»´åº¦
            chunk_dim=1,
        )
        # åˆ›å»ºè¾“å‡ºçš„çº¿æ€§å±‚
        self.proj_out = nn.Linear(inner_dim, patch_size * patch_size * out_channels)

        # åˆå§‹åŒ–æ¢¯åº¦æ£€æŸ¥ç‚¹æ ‡å¿—ä¸º False
        self.gradient_checkpointing = False

    # è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ–¹æ³•
    def _set_gradient_checkpointing(self, module, value=False):
        # æ›´æ–°æ¢¯åº¦æ£€æŸ¥ç‚¹æ ‡å¿—
        self.gradient_checkpointing = value

    @property
    # ä» diffusers.models.unets.unet_2d_condition ä¸­å¤åˆ¶çš„å±æ€§
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œè¿”å›æ³¨æ„åŠ›å¤„ç†å™¨çš„å­—å…¸
    def attn_processors(self) -> Dict[str, AttentionProcessor]:
        r"""
        è¿”å›å€¼:
            `dict` çš„æ³¨æ„åŠ›å¤„ç†å™¨: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¨¡å‹ä¸­æ‰€æœ‰ä½¿ç”¨çš„æ³¨æ„åŠ›å¤„ç†å™¨ï¼Œä»¥æƒé‡åç§°ç´¢å¼•ã€‚
        """
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸ç”¨äºå­˜å‚¨å¤„ç†å™¨
        processors = {}

        # å®šä¹‰ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œç”¨äºæ·»åŠ æ³¨æ„åŠ›å¤„ç†å™¨
        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
            # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰è·å–å¤„ç†å™¨çš„æ–¹æ³•
            if hasattr(module, "get_processor"):
                # å°†å¤„ç†å™¨æ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œé”®ä¸ºå¤„ç†å™¨çš„åç§°
                processors[f"{name}.processor"] = module.get_processor()

            # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
            for sub_name, child in module.named_children():
                # é€’å½’è°ƒç”¨ï¼Œå¤„ç†å­æ¨¡å—
                fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)

            # è¿”å›å¤„ç†å™¨å­—å…¸
            return processors

        # éå†å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—
        for name, module in self.named_children():
            # è°ƒç”¨é€’å½’å‡½æ•°ï¼Œæ·»åŠ å¤„ç†å™¨
            fn_recursive_add_processors(name, module, processors)

        # è¿”å›æ”¶é›†åˆ°çš„å¤„ç†å™¨å­—å…¸
        return processors

    # ä» UNet2DConditionModel ä¸­å¤åˆ¶çš„æ–¹æ³•ï¼Œç”¨äºè®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨
    def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
        r"""
        è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚

        å‚æ•°:
            processor (`dict` çš„ `AttentionProcessor` æˆ–ä»… `AttentionProcessor`):
                å®ä¾‹åŒ–çš„å¤„ç†å™¨ç±»æˆ–å¤„ç†å™¨ç±»çš„å­—å…¸ï¼Œå°†ä½œä¸ºæ‰€æœ‰ `Attention` å±‚çš„å¤„ç†å™¨è®¾ç½®ã€‚

                å¦‚æœ `processor` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®éœ€è¦å®šä¹‰ç›¸åº”äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„è·¯å¾„ã€‚
                åœ¨è®¾ç½®å¯è®­ç»ƒçš„æ³¨æ„åŠ›å¤„ç†å™¨æ—¶ï¼Œå¼ºçƒˆå»ºè®®è¿™æ ·åšã€‚

        """
        # è®¡ç®—å½“å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„æ•°é‡
        count = len(self.attn_processors.keys())

        # å¦‚æœä¼ å…¥çš„å¤„ç†å™¨æ˜¯å­—å…¸ï¼Œä¸”æ•°é‡ä¸å½“å‰å¤„ç†å™¨ä¸åŒ¹é…ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f"ä¼ å…¥äº†å¤„ç†å™¨å­—å…¸ï¼Œä½†å¤„ç†å™¨æ•°é‡ {len(processor)} ä¸æ³¨æ„åŠ›å±‚æ•°é‡ {count} ä¸åŒ¹é…ã€‚è¯·ç¡®ä¿ä¼ å…¥ {count} ä¸ªå¤„ç†å™¨ç±»ã€‚"
            )

        # å®šä¹‰ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œç”¨äºè®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
            if hasattr(module, "set_processor"):
                # å¦‚æœå¤„ç†å™¨ä¸æ˜¯å­—å…¸ï¼Œåˆ™ç›´æ¥è®¾ç½®
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    # ä»å­—å…¸ä¸­å¼¹å‡ºå¯¹åº”çš„å¤„ç†å™¨å¹¶è®¾ç½®
                    module.set_processor(processor.pop(f"{name}.processor"))

            # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
            for sub_name, child in module.named_children():
                # é€’å½’è°ƒç”¨ï¼Œå¤„ç†å­æ¨¡å—
                fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

        # éå†å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—
        for name, module in self.named_children():
            # è°ƒç”¨é€’å½’å‡½æ•°ï¼Œè®¾ç½®å¤„ç†å™¨
            fn_recursive_attn_processor(name, module, processor)

    # ä» UNet2DConditionModel ä¸­å¤åˆ¶çš„æ–¹æ³•ï¼Œæ¶‰åŠèåˆ QKV æŠ•å½±
    # å®šä¹‰èåˆ QKV æŠ•å½±çš„æ–¹æ³•
        def fuse_qkv_projections(self):
            """
            å¯ç”¨èåˆçš„ QKV æŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚
            å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚
    
            <Tip warning={true}>
    
            æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚
    
            </Tip>
            """
            # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None
            self.original_attn_processors = None
    
            # éå†æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨
            for _, attn_processor in self.attn_processors.items():
                # å¦‚æœæ³¨æ„åŠ›å¤„ç†å™¨çš„ç±»ååŒ…å« "Added"
                if "Added" in str(attn_processor.__class__.__name__):
                    # æŠ›å‡ºå¼‚å¸¸ï¼Œè¡¨ç¤ºä¸æ”¯æŒæœ‰é¢å¤– KV æŠ•å½±çš„æ¨¡å‹
                    raise ValueError("`fuse_qkv_projections()` is not supported for models having added KV projections.")
    
            # ä¿å­˜åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨
            self.original_attn_processors = self.attn_processors
    
            # éå†æ‰€æœ‰æ¨¡å—
            for module in self.modules():
                # å¦‚æœæ¨¡å—æ˜¯ Attention ç±»å‹
                if isinstance(module, Attention):
                    # èåˆæŠ•å½±
                    module.fuse_projections(fuse=True)
    
            # è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä¸ºèåˆçš„å¤„ç†å™¨
            self.set_attn_processor(FusedCogVideoXAttnProcessor2_0())
    
        # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections æ‹·è´è€Œæ¥
        def unfuse_qkv_projections(self):
            """ç¦ç”¨èåˆçš„ QKV æŠ•å½±ï¼ˆå¦‚æœå·²å¯ç”¨ï¼‰ã€‚
    
            <Tip warning={true}>
    
            æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚
    
            </Tip>
    
            """
            # å¦‚æœåŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸ä¸º None
            if self.original_attn_processors is not None:
                # è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä¸ºåŸå§‹å¤„ç†å™¨
                self.set_attn_processor(self.original_attn_processors)
    
        # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•
        def forward(
            # éšè—çŠ¶æ€è¾“å…¥çš„å¼ é‡
            hidden_states: torch.Tensor,
            # ç¼–ç å™¨éšè—çŠ¶æ€çš„å¼ é‡
            encoder_hidden_states: torch.Tensor,
            # æ—¶é—´æ­¥çš„æ•´æ•°æˆ–æµ®ç‚¹æ•°
            timestep: Union[int, float, torch.LongTensor],
            # å¯é€‰çš„æ—¶é—´æ­¥æ¡ä»¶å¼ é‡
            timestep_cond: Optional[torch.Tensor] = None,
            # å¯é€‰çš„å›¾åƒæ—‹è½¬åµŒå…¥
            image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
            # è¿”å›å­—å…¸çš„å¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸º True
            return_dict: bool = True,
```