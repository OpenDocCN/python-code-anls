# `.\diffusers\models\controlnet_sd3.py`

```
# ç‰ˆæƒæ‰€æœ‰ 2024 Stability AI, HuggingFace å›¢é˜Ÿå’Œ InstantX å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è¿›è¡Œè®¸å¯ï¼›
# é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å¾—è®¸å¯è¯å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯ä»¥â€œåŸæ ·â€åŸºç¡€åˆ†å‘çš„ï¼Œ
# ä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºæˆ–æš—ç¤ºçš„ã€‚
# æœ‰å…³è®¸å¯è¯çš„ç‰¹å®šæƒé™å’Œé™åˆ¶ï¼Œè¯·å‚è§è®¸å¯è¯ã€‚

# ä» dataclasses æ¨¡å—å¯¼å…¥ dataclass è£…é¥°å™¨ï¼Œç”¨äºç®€åŒ–ç±»çš„å®šä¹‰
from dataclasses import dataclass
# ä» typing æ¨¡å—å¯¼å…¥ç±»å‹æç¤ºçš„ç›¸å…³ç±»å‹
from typing import Any, Dict, List, Optional, Tuple, Union

# å¯¼å…¥ PyTorch åº“åŠå…¶ç¥ç»ç½‘ç»œæ¨¡å—
import torch
import torch.nn as nn

# å¯¼å…¥é…ç½®å’Œæ³¨å†ŒåŠŸèƒ½ç›¸å…³çš„æ¨¡å—
from ..configuration_utils import ConfigMixin, register_to_config
# å¯¼å…¥æ¨¡å‹åŠ è½½çš„æ··åˆæ¥å£
from ..loaders import FromOriginalModelMixin, PeftAdapterMixin
# å¯¼å…¥è”åˆå˜æ¢å™¨å—çš„å®šä¹‰
from ..models.attention import JointTransformerBlock
# å¯¼å…¥æ³¨æ„åŠ›å¤„ç†ç›¸å…³çš„æ¨¡å—
from ..models.attention_processor import Attention, AttentionProcessor, FusedJointAttnProcessor2_0
# å¯¼å…¥å˜æ¢å™¨ 2D æ¨¡å‹è¾“å‡ºçš„å®šä¹‰
from ..models.modeling_outputs import Transformer2DModelOutput
# å¯¼å…¥æ¨¡å‹çš„é€šç”¨åŠŸèƒ½æ··åˆæ¥å£
from ..models.modeling_utils import ModelMixin
# å¯¼å…¥å·¥å…·å‡½æ•°å’Œå¸¸é‡
from ..utils import USE_PEFT_BACKEND, is_torch_version, logging, scale_lora_layers, unscale_lora_layers
# å¯¼å…¥æ§åˆ¶ç½‘ç»œç›¸å…³çš„åŸºç¡€è¾“å‡ºå’Œé›¶æ¨¡å—
from .controlnet import BaseOutput, zero_module
# å¯¼å…¥ç»„åˆæ—¶é—´æ­¥æ–‡æœ¬æŠ•å½±åµŒå…¥å’Œè¡¥ä¸åµŒå…¥çš„å®šä¹‰
from .embeddings import CombinedTimestepTextProjEmbeddings, PatchEmbed

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨å®ä¾‹ï¼Œç”¨äºè®°å½•ä¿¡æ¯å’Œè°ƒè¯•
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name

# å®šä¹‰æ•°æ®ç±» SD3ControlNetOutputï¼Œç”¨äºå­˜å‚¨æ§åˆ¶ç½‘ç»œå—çš„æ ·æœ¬è¾“å‡º
@dataclass
class SD3ControlNetOutput(BaseOutput):
    # æ§åˆ¶ç½‘ç»œå—çš„æ ·æœ¬ï¼Œä½¿ç”¨å…ƒç»„å­˜å‚¨å¼ é‡
    controlnet_block_samples: Tuple[torch.Tensor]

# å®šä¹‰ SD3ControlNetModel ç±»ï¼Œé›†æˆå¤šç§æ··åˆæ¥å£ä»¥å®ç°æ¨¡å‹åŠŸèƒ½
class SD3ControlNetModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin):
    # æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå…è®¸èŠ‚çœå†…å­˜
    _supports_gradient_checkpointing = True

    @register_to_config
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®æ¨¡å‹çš„å„ç§å‚æ•°ï¼Œæä¾›é»˜è®¤å€¼
    def __init__(
        self,
        sample_size: int = 128,  # è¾“å…¥æ ·æœ¬å¤§å°
        patch_size: int = 2,  # è¡¥ä¸å¤§å°
        in_channels: int = 16,  # è¾“å…¥é€šé“æ•°
        num_layers: int = 18,  # æ¨¡å‹å±‚æ•°
        attention_head_dim: int = 64,  # æ³¨æ„åŠ›å¤´çš„ç»´åº¦
        num_attention_heads: int = 18,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡
        joint_attention_dim: int = 4096,  # è”åˆæ³¨æ„åŠ›çš„ç»´åº¦
        caption_projection_dim: int = 1152,  # æ ‡é¢˜æŠ•å½±çš„ç»´åº¦
        pooled_projection_dim: int = 2048,  # æ± åŒ–æŠ•å½±çš„ç»´åº¦
        out_channels: int = 16,  # è¾“å‡ºé€šé“æ•°
        pos_embed_max_size: int = 96,  # ä½ç½®åµŒå…¥çš„æœ€å¤§å°ºå¯¸
    ):
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__()
        # é»˜è®¤è¾“å‡ºé€šé“è®¾ç½®ä¸ºè¾“å…¥é€šé“
        default_out_channels = in_channels
        # è¾“å‡ºé€šé“ä¸ºæŒ‡å®šå€¼æˆ–é»˜è®¤å€¼
        self.out_channels = out_channels if out_channels is not None else default_out_channels
        # å†…éƒ¨ç»´åº¦ç­‰äºæ³¨æ„åŠ›å¤´æ•°é‡ä¹˜ä»¥æ¯ä¸ªå¤´çš„ç»´åº¦
        self.inner_dim = num_attention_heads * attention_head_dim

        # åˆ›å»ºä½ç½®åµŒå…¥å¯¹è±¡ï¼Œç”¨äºå¤„ç†å›¾åƒè¡¥ä¸
        self.pos_embed = PatchEmbed(
            height=sample_size,  # è¾“å…¥å›¾åƒé«˜åº¦
            width=sample_size,   # è¾“å…¥å›¾åƒå®½åº¦
            patch_size=patch_size,  # å›¾åƒè¡¥ä¸å¤§å°
            in_channels=in_channels,  # è¾“å…¥é€šé“æ•°é‡
            embed_dim=self.inner_dim,  # åµŒå…¥ç»´åº¦
            pos_embed_max_size=pos_embed_max_size,  # æœ€å¤§ä½ç½®åµŒå…¥å¤§å°
        )
        # åˆ›å»ºæ—¶é—´å’Œæ–‡æœ¬çš„è”åˆåµŒå…¥
        self.time_text_embed = CombinedTimestepTextProjEmbeddings(
            embedding_dim=self.inner_dim,  # åµŒå…¥ç»´åº¦
            pooled_projection_dim=pooled_projection_dim  # èšåˆæŠ•å½±ç»´åº¦
        )
        # å®šä¹‰ä¸Šä¸‹æ–‡åµŒå…¥çš„çº¿æ€§å±‚
        self.context_embedder = nn.Linear(joint_attention_dim, caption_projection_dim)

        # æ³¨æ„åŠ›å¤´ç»´åº¦åŠ å€ä»¥é€‚åº”æ··åˆ
        # éœ€è¦åœ¨å®é™…æ£€æŸ¥ç‚¹ä¸­å¤„ç†
        self.transformer_blocks = nn.ModuleList(
            [
                # åˆ›å»ºå¤šä¸ªè”åˆå˜æ¢å—
                JointTransformerBlock(
                    dim=self.inner_dim,  # å—çš„ç»´åº¦
                    num_attention_heads=num_attention_heads,  # æ³¨æ„åŠ›å¤´æ•°é‡
                    attention_head_dim=self.config.attention_head_dim,  # æ¯ä¸ªå¤´çš„ç»´åº¦
                    context_pre_only=False,  # æ˜¯å¦ä»…ä¸Šä¸‹æ–‡å…ˆè¡Œ
                )
                for i in range(num_layers)  # æ ¹æ®å±‚æ•°ç”Ÿæˆå—
            ]
        )

        # æ§åˆ¶ç½‘ç»œå—
        self.controlnet_blocks = nn.ModuleList([])  # åˆå§‹åŒ–ç©ºçš„æ§åˆ¶ç½‘ç»œå—åˆ—è¡¨
        for _ in range(len(self.transformer_blocks)):  # æ ¹æ®å˜æ¢å—æ•°é‡åˆ›å»ºæ§åˆ¶ç½‘ç»œå—
            controlnet_block = nn.Linear(self.inner_dim, self.inner_dim)  # åˆ›å»ºçº¿æ€§å±‚
            controlnet_block = zero_module(controlnet_block)  # é›¶åŒ–æ¨¡å—ä»¥åˆå§‹åŒ–
            self.controlnet_blocks.append(controlnet_block)  # æ·»åŠ åˆ°æ§åˆ¶ç½‘ç»œå—åˆ—è¡¨
        # åˆ›å»ºä½ç½®åµŒå…¥è¾“å…¥å¯¹è±¡
        pos_embed_input = PatchEmbed(
            height=sample_size,  # è¾“å…¥å›¾åƒé«˜åº¦
            width=sample_size,   # è¾“å…¥å›¾åƒå®½åº¦
            patch_size=patch_size,  # å›¾åƒè¡¥ä¸å¤§å°
            in_channels=in_channels,  # è¾“å…¥é€šé“æ•°é‡
            embed_dim=self.inner_dim,  # åµŒå…¥ç»´åº¦
            pos_embed_type=None,  # ä¸ä½¿ç”¨ä½ç½®åµŒå…¥ç±»å‹
        )
        # é›¶åŒ–ä½ç½®åµŒå…¥è¾“å…¥
        self.pos_embed_input = zero_module(pos_embed_input)

        # å…³é—­æ¢¯åº¦æ£€æŸ¥ç‚¹
        self.gradient_checkpointing = False

    # ä» diffusers.models.unets.unet_3d_condition.UNet3DConditionModel å¤åˆ¶çš„å¯ç”¨å‰å‘åˆ†å—æ–¹æ³•
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œå¯ç”¨å‰é¦ˆå±‚çš„åˆ†å—å¤„ç†
        def enable_forward_chunking(self, chunk_size: Optional[int] = None, dim: int = 0) -> None:
            """
            è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä½¿ç”¨å‰é¦ˆåˆ†å—ã€‚
            
            å‚æ•°ï¼š
                chunk_size (`int`, *optional*):
                    å‰é¦ˆå±‚çš„åˆ†å—å¤§å°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†å¯¹æ¯ä¸ªç»´åº¦ä¸º`dim`çš„å¼ é‡å•ç‹¬è¿è¡Œå‰é¦ˆå±‚ã€‚
                dim (`int`, *optional*, defaults to `0`):
                    åº”è¯¥è¿›è¡Œå‰é¦ˆè®¡ç®—çš„ç»´åº¦ã€‚å¯ä»¥é€‰æ‹©dim=0ï¼ˆæ‰¹æ¬¡ï¼‰æˆ–dim=1ï¼ˆåºåˆ—é•¿åº¦ï¼‰ã€‚
            """
            # æ£€æŸ¥dimæ˜¯å¦åœ¨å…è®¸çš„èŒƒå›´å†…
            if dim not in [0, 1]:
                raise ValueError(f"Make sure to set `dim` to either 0 or 1, not {dim}")
    
            # é»˜è®¤çš„åˆ†å—å¤§å°ä¸º1
            chunk_size = chunk_size or 1
    
            # å®šä¹‰ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œå¤„ç†æ¯ä¸ªæ¨¡å—çš„å‰é¦ˆåˆ†å—è®¾ç½®
            def fn_recursive_feed_forward(module: torch.nn.Module, chunk_size: int, dim: int):
                # å¦‚æœæ¨¡å—æœ‰è®¾ç½®å‰é¦ˆåˆ†å—çš„æ–¹æ³•ï¼Œåˆ™è°ƒç”¨å®ƒ
                if hasattr(module, "set_chunk_feed_forward"):
                    module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
    
                # é€’å½’å¤„ç†å­æ¨¡å—
                for child in module.children():
                    fn_recursive_feed_forward(child, chunk_size, dim)
    
            # å¯¹å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—åº”ç”¨åˆ†å—è®¾ç½®
            for module in self.children():
                fn_recursive_feed_forward(module, chunk_size, dim)
    
        @property
        # ä»å…¶ä»–æ¨¡å‹å¤åˆ¶çš„å±æ€§ï¼Œè¿”å›æ³¨æ„åŠ›å¤„ç†å™¨
        def attn_processors(self) -> Dict[str, AttentionProcessor]:
            r"""
            è¿”å›ï¼š
                `dict` æ³¨æ„åŠ›å¤„ç†å™¨ï¼šåŒ…å«æ¨¡å‹ä¸­æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨çš„å­—å…¸ï¼ŒæŒ‰æƒé‡åç§°ç´¢å¼•ã€‚
            """
            # å®šä¹‰ä¸€ä¸ªç©ºå­—å…¸æ¥å­˜å‚¨å¤„ç†å™¨
            processors = {}
    
            # å®šä¹‰é€’å½’å‡½æ•°ï¼Œæ·»åŠ å¤„ç†å™¨åˆ°å­—å…¸ä¸­
            def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
                # å¦‚æœæ¨¡å—æœ‰è·å–å¤„ç†å™¨çš„æ–¹æ³•ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°å­—å…¸ä¸­
                if hasattr(module, "get_processor"):
                    processors[f"{name}.processor"] = module.get_processor()
    
                # é€’å½’å¤„ç†å­æ¨¡å—
                for sub_name, child in module.named_children():
                    fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)
    
                return processors
    
            # å¯¹å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—æ·»åŠ å¤„ç†å™¨
            for name, module in self.named_children():
                fn_recursive_add_processors(name, module, processors)
    
            # è¿”å›æ‰€æœ‰å¤„ç†å™¨çš„å­—å…¸
            return processors
    
        # ä»å…¶ä»–æ¨¡å‹å¤åˆ¶çš„è®¾ç½®æ–¹æ³•
    # å®šä¹‰è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•ï¼Œæ¥æ”¶ä¸€ä¸ªæ³¨æ„åŠ›å¤„ç†å™¨æˆ–å¤„ç†å™¨å­—å…¸
    def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
        r"""
        è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚

        å‚æ•°ï¼š
            processor (`dict` of `AttentionProcessor` æˆ– `AttentionProcessor`):
                å®ä¾‹åŒ–çš„å¤„ç†å™¨ç±»æˆ–å°†ä½œä¸ºå¤„ç†å™¨è®¾ç½®åˆ°**æ‰€æœ‰** `Attention` å±‚çš„å¤„ç†å™¨ç±»å­—å…¸ã€‚

                å¦‚æœ `processor` æ˜¯å­—å…¸ï¼Œé”®éœ€è¦å®šä¹‰å¯¹åº”çš„äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„è·¯å¾„ã€‚å½“è®¾ç½®å¯è®­ç»ƒçš„æ³¨æ„åŠ›å¤„ç†å™¨æ—¶ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨å­—å…¸ã€‚

        """
        # è·å–å½“å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„æ•°é‡
        count = len(self.attn_processors.keys())

        # å¦‚æœä¼ å…¥çš„æ˜¯å­—å…¸ä¸”å­—å…¸çš„é•¿åº¦ä¸å½“å‰å¤„ç†å™¨æ•°é‡ä¸åŒ¹é…ï¼Œåˆ™æŠ›å‡ºé”™è¯¯
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f"ä¼ å…¥äº†å¤„ç†å™¨å­—å…¸ï¼Œä½†å¤„ç†å™¨æ•°é‡ {len(processor)} ä¸æ³¨æ„åŠ›å±‚æ•°é‡ {count} ä¸åŒ¹é…ã€‚è¯·ç¡®ä¿ä¼ å…¥ {count} ä¸ªå¤„ç†å™¨ç±»ã€‚"
            )

        # å®šä¹‰é€’å½’å¤„ç†æ³¨æ„åŠ›å¤„ç†å™¨çš„å‡½æ•°
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            # å¦‚æœæ¨¡å—æœ‰è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
            if hasattr(module, "set_processor"):
                # å¦‚æœä¼ å…¥çš„ä¸æ˜¯å­—å…¸ï¼Œåˆ™ç›´æ¥è®¾ç½®å¤„ç†å™¨
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    # ä»å­—å…¸ä¸­è·å–å¯¹åº”çš„å¤„ç†å™¨å¹¶è®¾ç½®
                    module.set_processor(processor.pop(f"{name}.processor"))

            # éå†å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨è‡ªèº«
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

        # éå†å½“å‰å¯¹è±¡çš„å­æ¨¡å—
        for name, module in self.named_children():
            # å¯¹æ¯ä¸ªå­æ¨¡å—è°ƒç”¨é€’å½’å¤„ç†å™¨è®¾ç½®å‡½æ•°
            fn_recursive_attn_processor(name, module, processor)

    # ä» diffusers.models.transformers.transformer_sd3.SD3Transformer2DModel.fuse_qkv_projections å¤åˆ¶çš„æ–¹æ³•
    def fuse_qkv_projections(self):
        """
        å¯ç”¨èåˆçš„ QKV æŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰è¢«èåˆã€‚
        å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼çš„æŠ•å½±çŸ©é˜µè¢«èåˆã€‚

        <æç¤º è­¦å‘Š={true}>

        æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚

        </æç¤º>
        """
        # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None
        self.original_attn_processors = None

        # æ£€æŸ¥æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨ï¼Œç¡®ä¿æ²¡æœ‰æ·»åŠ çš„ KV æŠ•å½±
        for _, attn_processor in self.attn_processors.items():
            if "Added" in str(attn_processor.__class__.__name__):
                raise ValueError("`fuse_qkv_projections()` ä¸æ”¯æŒå…·æœ‰æ·»åŠ çš„ KV æŠ•å½±çš„æ¨¡å‹ã€‚")

        # ä¿å­˜å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨ä»¥å¤‡åç”¨
        self.original_attn_processors = self.attn_processors

        # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨¡å—
        for module in self.modules():
            # å¦‚æœæ¨¡å—æ˜¯ Attention ç±»å‹
            if isinstance(module, Attention):
                # èåˆæŠ•å½±çŸ©é˜µ
                module.fuse_projections(fuse=True)

        # è®¾ç½®æ–°çš„èåˆæ³¨æ„åŠ›å¤„ç†å™¨
        self.set_attn_processor(FusedJointAttnProcessor2_0())

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections å¤åˆ¶çš„æ–¹æ³•
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•æ¥ç¦ç”¨å·²å¯ç”¨çš„èåˆ QKV æŠ•å½±
    def unfuse_qkv_projections(self):
        """å¦‚æœå¯ç”¨äº†èåˆçš„ QKV æŠ•å½±ï¼Œåˆ™ç¦ç”¨å®ƒã€‚

        <Tip warning={true}>

        æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚

        </Tip>

        """
        # å¦‚æœåŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸ä¸ºç©ºï¼Œåˆ™æ¢å¤åˆ°åŸå§‹è®¾ç½®
        if self.original_attn_processors is not None:
            self.set_attn_processor(self.original_attn_processors)

    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•æ¥è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹
    def _set_gradient_checkpointing(self, module, value=False):
        # å¦‚æœæ¨¡å—æœ‰æ¢¯åº¦æ£€æŸ¥ç‚¹å±æ€§ï¼Œåˆ™è®¾ç½®å…¶å€¼
        if hasattr(module, "gradient_checkpointing"):
            module.gradient_checkpointing = value

    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ä» Transformer åˆ›å»º ControlNet å®ä¾‹
    @classmethod
    def from_transformer(cls, transformer, num_layers=12, load_weights_from_transformer=True):
        # è·å– Transformer çš„é…ç½®
        config = transformer.config
        # è®¾ç½®å±‚æ•°ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å±‚æ•°
        config["num_layers"] = num_layers or config.num_layers
        # åˆ›å»º ControlNet å®ä¾‹ï¼Œä¼ å…¥é…ç½®å‚æ•°
        controlnet = cls(**config)

        # å¦‚æœéœ€è¦ä» Transformer åŠ è½½æƒé‡
        if load_weights_from_transformer:
            # åŠ è½½ä½ç½®åµŒå…¥çš„æƒé‡
            controlnet.pos_embed.load_state_dict(transformer.pos_embed.state_dict())
            # åŠ è½½æ—¶é—´æ–‡æœ¬åµŒå…¥çš„æƒé‡
            controlnet.time_text_embed.load_state_dict(transformer.time_text_embed.state_dict())
            # åŠ è½½ä¸Šä¸‹æ–‡åµŒå…¥å™¨çš„æƒé‡
            controlnet.context_embedder.load_state_dict(transformer.context_embedder.state_dict())
            # åŠ è½½å˜æ¢å™¨å—çš„æƒé‡ï¼Œä¸¥æ ¼æ¨¡å¼ä¸º False
            controlnet.transformer_blocks.load_state_dict(transformer.transformer_blocks.state_dict(), strict=False)

            # å°†ä½ç½®åµŒå…¥è¾“å…¥åˆå§‹åŒ–ä¸ºé›¶æ¨¡å—
            controlnet.pos_embed_input = zero_module(controlnet.pos_embed_input)

        # è¿”å›åˆ›å»ºçš„ ControlNet å®ä¾‹
        return controlnet

    # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•
    def forward(
        # è¾“å…¥çš„éšè—çŠ¶æ€å¼ é‡
        hidden_states: torch.FloatTensor,
        # æ§åˆ¶ç½‘æ¡ä»¶å¼ é‡
        controlnet_cond: torch.Tensor,
        # æ¡ä»¶ç¼©æ”¾å› å­ï¼Œé»˜è®¤å€¼ä¸º 1.0
        conditioning_scale: float = 1.0,
        # ç¼–ç å™¨éšè—çŠ¶æ€å¼ é‡ï¼Œé»˜è®¤ä¸º None
        encoder_hidden_states: torch.FloatTensor = None,
        # æ± åŒ–æŠ•å½±å¼ é‡ï¼Œé»˜è®¤ä¸º None
        pooled_projections: torch.FloatTensor = None,
        # æ—¶é—´æ­¥é•¿å¼ é‡ï¼Œé»˜è®¤ä¸º None
        timestep: torch.LongTensor = None,
        # è”åˆæ³¨æ„åŠ›å‚æ•°ï¼Œé»˜è®¤ä¸º None
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
        # æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„è¾“å‡ºï¼Œé»˜è®¤ä¸º True
        return_dict: bool = True,
# SD3MultiControlNetModel ç±»ï¼Œç»§æ‰¿è‡ª ModelMixin
class SD3MultiControlNetModel(ModelMixin):
    r"""
    `SD3ControlNetModel` çš„åŒ…è£…ç±»ï¼Œç”¨äº Multi-SD3ControlNet

    è¯¥æ¨¡å—æ˜¯å¤šä¸ª `SD3ControlNetModel` å®ä¾‹çš„åŒ…è£…ã€‚`forward()` API è®¾è®¡ä¸ `SD3ControlNetModel` å…¼å®¹ã€‚

    å‚æ•°:
        controlnets (`List[SD3ControlNetModel]`):
            åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¸º unet æä¾›é¢å¤–çš„æ¡ä»¶ã€‚å¿…é¡»å°†å¤šä¸ª `SD3ControlNetModel` ä½œä¸ºåˆ—è¡¨è®¾ç½®ã€‚
    """

    # åˆå§‹åŒ–å‡½æ•°ï¼Œæ¥æ”¶æ§åˆ¶ç½‘åˆ—è¡¨å¹¶è°ƒç”¨çˆ¶ç±»æ„é€ 
    def __init__(self, controlnets):
        super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        self.nets = nn.ModuleList(controlnets)  # å°†æ§åˆ¶ç½‘åˆ—è¡¨å­˜å‚¨ä¸ºæ¨¡å—åˆ—è¡¨

    # å‰å‘ä¼ æ’­å‡½æ•°ï¼Œæ¥æ”¶å¤šä¸ªè¾“å…¥å‚æ•°ä»¥å¤„ç†æ•°æ®
    def forward(
        self,
        hidden_states: torch.FloatTensor,  # éšè—çŠ¶æ€å¼ é‡
        controlnet_cond: List[torch.tensor],  # æ§åˆ¶ç½‘æ¡ä»¶åˆ—è¡¨
        conditioning_scale: List[float],  # æ¡ä»¶ç¼©æ”¾å› å­åˆ—è¡¨
        pooled_projections: torch.FloatTensor,  # æ± åŒ–çš„æŠ•å½±å¼ é‡
        encoder_hidden_states: torch.FloatTensor = None,  # å¯é€‰ç¼–ç å™¨éšè—çŠ¶æ€
        timestep: torch.LongTensor = None,  # å¯é€‰æ—¶é—´æ­¥é•¿
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,  # å¯é€‰çš„è”åˆæ³¨æ„åŠ›å‚æ•°
        return_dict: bool = True,  # è¿”å›æ ¼å¼ï¼Œé»˜è®¤ä¸ºå­—å…¸
    ) -> Union[SD3ControlNetOutput, Tuple]:  # è¿”å›ç±»å‹å¯ä»¥æ˜¯è¾“å‡ºå¯¹è±¡æˆ–å…ƒç»„
        # éå†æ§åˆ¶ç½‘æ¡ä»¶ã€ç¼©æ”¾å› å­å’Œæ§åˆ¶ç½‘
        for i, (image, scale, controlnet) in enumerate(zip(controlnet_cond, conditioning_scale, self.nets)):
            # è°ƒç”¨æ§åˆ¶ç½‘çš„å‰å‘ä¼ æ’­ä»¥è·å–å—æ ·æœ¬
            block_samples = controlnet(
                hidden_states=hidden_states,  # ä¼ é€’éšè—çŠ¶æ€
                timestep=timestep,  # ä¼ é€’æ—¶é—´æ­¥é•¿
                encoder_hidden_states=encoder_hidden_states,  # ä¼ é€’ç¼–ç å™¨éšè—çŠ¶æ€
                pooled_projections=pooled_projections,  # ä¼ é€’æ± åŒ–æŠ•å½±
                controlnet_cond=image,  # ä¼ é€’æ§åˆ¶ç½‘æ¡ä»¶
                conditioning_scale=scale,  # ä¼ é€’æ¡ä»¶ç¼©æ”¾å› å­
                joint_attention_kwargs=joint_attention_kwargs,  # ä¼ é€’è”åˆæ³¨æ„åŠ›å‚æ•°
                return_dict=return_dict,  # ä¼ é€’è¿”å›æ ¼å¼
            )

            # åˆå¹¶æ ·æœ¬
            if i == 0:  # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ§åˆ¶ç½‘
                control_block_samples = block_samples  # ç›´æ¥ä½¿ç”¨å—æ ·æœ¬
            else:  # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªæ§åˆ¶ç½‘
                # å°†å½“å‰å—æ ·æœ¬ä¸ä¹‹å‰çš„æ ·æœ¬é€å…ƒç´ ç›¸åŠ 
                control_block_samples = [
                    control_block_sample + block_sample
                    for control_block_sample, block_sample in zip(control_block_samples[0], block_samples[0])
                ]
                control_block_samples = (tuple(control_block_samples),)  # å°†åˆå¹¶ç»“æœè½¬ä¸ºå…ƒç»„

        # è¿”å›åˆå¹¶åçš„æ§åˆ¶å—æ ·æœ¬
        return control_block_samples
```