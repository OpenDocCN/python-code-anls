# `.\diffusers\pipelines\wuerstchen\pipeline_wuerstchen_combined.py`

```py
# ç‰ˆæƒä¿¡æ¯ï¼Œè¡¨æ˜ç‰ˆæƒæ‰€æœ‰è€…å’Œè®¸å¯ä¿¡æ¯
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# æ ¹æ® Apache è®¸å¯è¯ç‰ˆæœ¬ 2.0 è¿›è¡Œè®¸å¯
# Licensed under the Apache License, Version 2.0 (the "License");
# æœ¬æ–‡ä»¶åªèƒ½åœ¨éµå¾ªè®¸å¯è¯çš„æƒ…å†µä¸‹ä½¿ç”¨
# you may not use this file except in compliance with the License.
# å¯ä»¥åœ¨ä»¥ä¸‹åœ°å€è·å–è®¸å¯è¯å‰¯æœ¬
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰â€œåŸæ ·â€åŸºç¡€åˆ†å‘
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# ä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºæˆ–æš—ç¤º
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# è¯·å‚è§è®¸å¯è¯ä»¥äº†è§£ç®¡ç†æƒé™å’Œé™åˆ¶çš„å…·ä½“è¯­è¨€
# See the License for the specific language governing permissions and
# limitations under the License.
# å¯¼å…¥æ‰€éœ€çš„ç±»å‹æç¤º
from typing import Callable, Dict, List, Optional, Union

# å¯¼å…¥ PyTorch åº“
import torch
# ä» transformers åº“å¯¼å…¥ CLIP æ–‡æœ¬æ¨¡å‹å’Œåˆ†è¯å™¨
from transformers import CLIPTextModel, CLIPTokenizer

# ä»è‡ªå®šä¹‰è°ƒåº¦å™¨å¯¼å…¥ DDPMWuerstchenScheduler
from ...schedulers import DDPMWuerstchenScheduler
# ä»è‡ªå®šä¹‰å·¥å…·å¯¼å…¥å»é™¤è¿‡æ—¶å‡½æ•°å’Œæ›¿æ¢ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²çš„å‡½æ•°
from ...utils import deprecate, replace_example_docstring
# ä»ç®¡é“å·¥å…·å¯¼å…¥ DiffusionPipeline åŸºç±»
from ..pipeline_utils import DiffusionPipeline
# ä»æ¨¡å‹æ¨¡å—å¯¼å…¥ PaellaVQModel
from .modeling_paella_vq_model import PaellaVQModel
# ä»æ¨¡å‹æ¨¡å—å¯¼å…¥ WuerstchenDiffNeXt
from .modeling_wuerstchen_diffnext import WuerstchenDiffNeXt
# ä»æ¨¡å‹æ¨¡å—å¯¼å…¥ WuerstchenPrior
from .modeling_wuerstchen_prior import WuerstchenPrior
# ä»ç®¡é“æ¨¡å—å¯¼å…¥ WuerstchenDecoderPipeline
from .pipeline_wuerstchen import WuerstchenDecoderPipeline
# ä»ç®¡é“æ¨¡å—å¯¼å…¥ WuerstchenPriorPipeline
from .pipeline_wuerstchen_prior import WuerstchenPriorPipeline

# æ–‡æ¡£å­—ç¬¦ä¸²ç¤ºä¾‹ï¼Œç”¨äºå±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–‡æœ¬è½¬å›¾åƒçš„ç®¡é“
TEXT2IMAGE_EXAMPLE_DOC_STRING = """
    Examples:
        ```py
        >>> from diffusions import WuerstchenCombinedPipeline

        >>> pipe = WuerstchenCombinedPipeline.from_pretrained("warp-ai/Wuerstchen", torch_dtype=torch.float16).to(
        ...     "cuda"
        ... )
        >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
        >>> images = pipe(prompt=prompt)
        ```
"""

# å®šä¹‰ä¸€ä¸ªç»“åˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ç±»
class WuerstchenCombinedPipeline(DiffusionPipeline):
    """
    ä½¿ç”¨ Wuerstchen è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“

    è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [`DiffusionPipeline`]ã€‚æŸ¥çœ‹çˆ¶ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•
    (å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè¿è¡Œåœ¨ç‰¹å®šè®¾å¤‡ç­‰)ã€‚

    å‚æ•°:
        tokenizer (`CLIPTokenizer`):
            ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨åˆ†è¯å™¨ã€‚
        text_encoder (`CLIPTextModel`):
            ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨æ–‡æœ¬ç¼–ç å™¨ã€‚
        decoder (`WuerstchenDiffNeXt`):
            ç”¨äºå›¾åƒç”Ÿæˆç®¡é“çš„è§£ç å™¨æ¨¡å‹ã€‚
        scheduler (`DDPMWuerstchenScheduler`):
            ç”¨äºå›¾åƒç”Ÿæˆç®¡é“çš„è°ƒåº¦å™¨ã€‚
        vqgan (`PaellaVQModel`):
            ç”¨äºå›¾åƒç”Ÿæˆç®¡é“çš„ VQGAN æ¨¡å‹ã€‚
        prior_tokenizer (`CLIPTokenizer`):
            ç”¨äºæ–‡æœ¬è¾“å…¥çš„å…ˆå‰åˆ†è¯å™¨ã€‚
        prior_text_encoder (`CLIPTextModel`):
            ç”¨äºæ–‡æœ¬è¾“å…¥çš„å…ˆå‰æ–‡æœ¬ç¼–ç å™¨ã€‚
        prior_prior (`WuerstchenPrior`):
            ç”¨äºå…ˆå‰ç®¡é“çš„å…ˆå‰æ¨¡å‹ã€‚
        prior_scheduler (`DDPMWuerstchenScheduler`):
            ç”¨äºå…ˆå‰ç®¡é“çš„è°ƒåº¦å™¨ã€‚
    """

    # æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦åŠ è½½è¿æ¥çš„ç®¡é“
    _load_connected_pipes = True
    # åˆå§‹åŒ–ç±»çš„æ„é€ å‡½æ•°ï¼Œæ¥æ”¶å¤šä¸ªæ¨¡å‹å’Œè°ƒåº¦å™¨ä½œä¸ºå‚æ•°
        def __init__(
            self,
            tokenizer: CLIPTokenizer,  # è¯æ±‡å¤„ç†å™¨
            text_encoder: CLIPTextModel,  # æ–‡æœ¬ç¼–ç å™¨
            decoder: WuerstchenDiffNeXt,  # è§£ç å™¨æ¨¡å‹
            scheduler: DDPMWuerstchenScheduler,  # è°ƒåº¦å™¨
            vqgan: PaellaVQModel,  # VQGANæ¨¡å‹
            prior_tokenizer: CLIPTokenizer,  # å…ˆéªŒè¯æ±‡å¤„ç†å™¨
            prior_text_encoder: CLIPTextModel,  # å…ˆéªŒæ–‡æœ¬ç¼–ç å™¨
            prior_prior: WuerstchenPrior,  # å…ˆéªŒæ¨¡å‹
            prior_scheduler: DDPMWuerstchenScheduler,  # å…ˆéªŒè°ƒåº¦å™¨
        ):
            super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°
    
            # æ³¨å†Œå„ä¸ªæ¨¡å‹å’Œè°ƒåº¦å™¨åˆ°å½“å‰å®ä¾‹
            self.register_modules(
                text_encoder=text_encoder,
                tokenizer=tokenizer,
                decoder=decoder,
                scheduler=scheduler,
                vqgan=vqgan,
                prior_prior=prior_prior,
                prior_text_encoder=prior_text_encoder,
                prior_tokenizer=prior_tokenizer,
                prior_scheduler=prior_scheduler,
            )
            # åˆå§‹åŒ–å…ˆéªŒç®¡é“ï¼Œç”¨äºå¤„ç†å…ˆéªŒç›¸å…³æ“ä½œ
            self.prior_pipe = WuerstchenPriorPipeline(
                prior=prior_prior,  # å…ˆéªŒæ¨¡å‹
                text_encoder=prior_text_encoder,  # å…ˆéªŒæ–‡æœ¬ç¼–ç å™¨
                tokenizer=prior_tokenizer,  # å…ˆéªŒè¯æ±‡å¤„ç†å™¨
                scheduler=prior_scheduler,  # å…ˆéªŒè°ƒåº¦å™¨
            )
            # åˆå§‹åŒ–è§£ç å™¨ç®¡é“ï¼Œç”¨äºå¤„ç†è§£ç ç›¸å…³æ“ä½œ
            self.decoder_pipe = WuerstchenDecoderPipeline(
                text_encoder=text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
                tokenizer=tokenizer,  # è¯æ±‡å¤„ç†å™¨
                decoder=decoder,  # è§£ç å™¨
                scheduler=scheduler,  # è°ƒåº¦å™¨
                vqgan=vqgan,  # VQGANæ¨¡å‹
            )
    
        # å¯ç”¨èŠ‚çœå†…å­˜çš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
        def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):
            # åœ¨è§£ç å™¨ç®¡é“ä¸­å¯ç”¨é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
            self.decoder_pipe.enable_xformers_memory_efficient_attention(attention_op)
    
        # å¯ç”¨æ¨¡å‹çš„CPUå¸è½½ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
        def enable_model_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = "cuda"):
            r"""
            ä½¿ç”¨accelerateå°†æ‰€æœ‰æ¨¡å‹å¸è½½åˆ°CPUï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ä¸”å¯¹æ€§èƒ½å½±å“è¾ƒå°ã€‚
            æ­¤æ–¹æ³•åœ¨è°ƒç”¨æ¨¡å‹çš„`forward`æ–¹æ³•æ—¶å°†æ•´ä¸ªæ¨¡å‹ç§»åˆ°GPUï¼Œæ¨¡å‹å°†åœ¨ä¸‹ä¸€ä¸ªæ¨¡å‹è¿è¡Œä¹‹å‰ä¿æŒåœ¨GPUä¸Šã€‚
            ç›¸æ¯”äº`enable_sequential_cpu_offload`ï¼Œå†…å­˜èŠ‚çœè¾ƒå°‘ï¼Œä½†æ€§èƒ½æ›´ä½³ã€‚
            """
            # åœ¨å…ˆéªŒç®¡é“ä¸­å¯ç”¨æ¨¡å‹çš„CPUå¸è½½
            self.prior_pipe.enable_model_cpu_offload(gpu_id=gpu_id, device=device)
            # åœ¨è§£ç å™¨ç®¡é“ä¸­å¯ç”¨æ¨¡å‹çš„CPUå¸è½½
            self.decoder_pipe.enable_model_cpu_offload(gpu_id=gpu_id, device=device)
    
        # å¯ç”¨é¡ºåºCPUå¸è½½ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨
        def enable_sequential_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = "cuda"):
            r"""
            ä½¿ç”¨ğŸ¤—Accelerateå°†æ‰€æœ‰æ¨¡å‹å¸è½½åˆ°CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
            æ¨¡å‹è¢«ç§»åŠ¨åˆ°`torch.device('meta')`ï¼Œå¹¶ä»…åœ¨è°ƒç”¨ç‰¹å®šå­æ¨¡å—çš„`forward`æ–¹æ³•æ—¶åŠ è½½åˆ°GPUã€‚
            å¸è½½æ˜¯åŸºäºå­æ¨¡å—è¿›è¡Œçš„ï¼Œå†…å­˜èŠ‚çœæ¯”ä½¿ç”¨`enable_model_cpu_offload`é«˜ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
            """
            # åœ¨å…ˆéªŒç®¡é“ä¸­å¯ç”¨é¡ºåºCPUå¸è½½
            self.prior_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id, device=device)
            # åœ¨è§£ç å™¨ç®¡é“ä¸­å¯ç”¨é¡ºåºCPUå¸è½½
            self.decoder_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id, device=device)
    # å®šä¹‰è¿›åº¦æ¡æ–¹æ³•ï¼Œæ¥å—å¯è¿­ä»£å¯¹è±¡å’Œæ€»è®¡æ•°ä½œä¸ºå‚æ•°
    def progress_bar(self, iterable=None, total=None):
        # åœ¨ prior_pipe ä¸Šæ›´æ–°è¿›åº¦æ¡ï¼Œä¼ å…¥å¯è¿­ä»£å¯¹è±¡å’Œæ€»è®¡æ•°
        self.prior_pipe.progress_bar(iterable=iterable, total=total)
        # åœ¨ decoder_pipe ä¸Šæ›´æ–°è¿›åº¦æ¡ï¼Œä¼ å…¥å¯è¿­ä»£å¯¹è±¡å’Œæ€»è®¡æ•°
        self.decoder_pipe.progress_bar(iterable=iterable, total=total)

    # å®šä¹‰è®¾ç½®è¿›åº¦æ¡é…ç½®çš„æ–¹æ³•ï¼Œæ¥æ”¶ä»»æ„å…³é”®å­—å‚æ•°
    def set_progress_bar_config(self, **kwargs):
        # åœ¨ prior_pipe ä¸Šè®¾ç½®è¿›åº¦æ¡é…ç½®ï¼Œä¼ å…¥å…³é”®å­—å‚æ•°
        self.prior_pipe.set_progress_bar_config(**kwargs)
        # åœ¨ decoder_pipe ä¸Šè®¾ç½®è¿›åº¦æ¡é…ç½®ï¼Œä¼ å…¥å…³é”®å­—å‚æ•°
        self.decoder_pipe.set_progress_bar_config(**kwargs)

    # ä½¿ç”¨ torch.no_grad() è£…é¥°å™¨ï¼Œè¡¨ç¤ºåœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ä¸è®¡ç®—æ¢¯åº¦
    @torch.no_grad()
    # æ›¿æ¢ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²çš„è£…é¥°å™¨
    @replace_example_docstring(TEXT2IMAGE_EXAMPLE_DOC_STRING)
    # å®šä¹‰è°ƒç”¨æ–¹æ³•ï¼Œå¤„ç†æ–‡æœ¬åˆ°å›¾åƒçš„è½¬æ¢
    def __call__(
        # æ¥å—æç¤ºæ–‡æœ¬ï¼Œæ”¯æŒå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œé»˜è®¤ä¸º None
        prompt: Optional[Union[str, List[str]]] = None,
        # å›¾åƒé«˜åº¦ï¼Œé»˜è®¤ä¸º 512
        height: int = 512,
        # å›¾åƒå®½åº¦ï¼Œé»˜è®¤ä¸º 512
        width: int = 512,
        # prior é˜¶æ®µæ¨ç†æ­¥éª¤æ•°ï¼Œé»˜è®¤ä¸º 60
        prior_num_inference_steps: int = 60,
        # prior é˜¶æ®µæ—¶é—´æ­¥ï¼Œé»˜è®¤ä¸º None
        prior_timesteps: Optional[List[float]] = None,
        # prior é˜¶æ®µå¼•å¯¼æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 4.0
        prior_guidance_scale: float = 4.0,
        # decoder é˜¶æ®µæ¨ç†æ­¥éª¤æ•°ï¼Œé»˜è®¤ä¸º 12
        num_inference_steps: int = 12,
        # decoder é˜¶æ®µæ—¶é—´æ­¥ï¼Œé»˜è®¤ä¸º None
        decoder_timesteps: Optional[List[float]] = None,
        # decoder é˜¶æ®µå¼•å¯¼æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 0.0
        decoder_guidance_scale: float = 0.0,
        # è´Ÿæç¤ºæ–‡æœ¬ï¼Œæ”¯æŒå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œé»˜è®¤ä¸º None
        negative_prompt: Optional[Union[str, List[str]]] = None,
        # æç¤ºåµŒå…¥ï¼Œé»˜è®¤ä¸º None
        prompt_embeds: Optional[torch.Tensor] = None,
        # è´Ÿæç¤ºåµŒå…¥ï¼Œé»˜è®¤ä¸º None
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        # æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ï¼Œé»˜è®¤ä¸º 1
        num_images_per_prompt: int = 1,
        # éšæœºæ•°ç”Ÿæˆå™¨ï¼Œé»˜è®¤ä¸º None
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        # æ½œåœ¨è¡¨ç¤ºï¼Œé»˜è®¤ä¸º None
        latents: Optional[torch.Tensor] = None,
        # è¾“å‡ºç±»å‹ï¼Œé»˜è®¤ä¸º "pil"
        output_type: Optional[str] = "pil",
        # æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼ï¼Œé»˜è®¤ä¸º True
        return_dict: bool = True,
        # prior é˜¶æ®µçš„å›è°ƒå‡½æ•°ï¼Œé»˜è®¤ä¸º None
        prior_callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
        # prior é˜¶æ®µå›è°ƒå‡½æ•°è¾“å…¥çš„å¼ é‡åç§°åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["latents"]
        prior_callback_on_step_end_tensor_inputs: List[str] = ["latents"],
        # decoder é˜¶æ®µçš„å›è°ƒå‡½æ•°ï¼Œé»˜è®¤ä¸º None
        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
        # decoder é˜¶æ®µå›è°ƒå‡½æ•°è¾“å…¥çš„å¼ é‡åç§°åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["latents"]
        callback_on_step_end_tensor_inputs: List[str] = ["latents"],
        # æ¥å—å…¶ä»–ä»»æ„å…³é”®å­—å‚æ•°
        **kwargs,
```