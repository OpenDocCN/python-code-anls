# `.\diffusers\pipelines\paint_by_example\pipeline_paint_by_example.py`

```py
# ç‰ˆæƒå£°æ˜ï¼Œè¡¨æ˜è¯¥æ–‡ä»¶çš„æ‰€æœ‰æƒå’Œä½¿ç”¨è®¸å¯
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# æ ¹æ® Apache 2.0 è®¸å¯è¯ï¼Œä½¿ç”¨è¯¥æ–‡ä»¶çš„æ¡ä»¶
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# å¯ä»¥åœ¨æ­¤è·å–è®¸å¯è¯çš„å‰¯æœ¬
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰çº¦å®šï¼Œå¦åˆ™è½¯ä»¶æŒ‰â€œåŸæ ·â€åˆ†å‘ï¼Œä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# æŸ¥çœ‹è®¸å¯è¯ä»¥è·å–æœ‰å…³æƒé™å’Œé™åˆ¶çš„å…·ä½“ä¿¡æ¯
# See the License for the specific language governing permissions and
# limitations under the License.

import inspect  # å¯¼å…¥inspectæ¨¡å—ï¼Œç”¨äºè·å–å¯¹è±¡çš„ä¿¡æ¯
from typing import Callable, List, Optional, Union  # å¯¼å…¥ç±»å‹æç¤ºåŠŸèƒ½

import numpy as np  # å¯¼å…¥numpyåº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—
import PIL.Image  # å¯¼å…¥PILåº“ï¼Œç”¨äºå›¾åƒå¤„ç†
import torch  # å¯¼å…¥PyTorchåº“ï¼Œç”¨äºæ·±åº¦å­¦ä¹ 
from transformers import CLIPImageProcessor  # å¯¼å…¥CLIPå›¾åƒå¤„ç†å™¨

from ...image_processor import VaeImageProcessor  # ä»ä¸Šçº§æ¨¡å—å¯¼å…¥VaeImageProcessor
from ...models import AutoencoderKL, UNet2DConditionModel  # å¯¼å…¥æ¨¡å‹ç›¸å…³ç±»
from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler  # å¯¼å…¥è°ƒåº¦å™¨
from ...utils import deprecate, logging  # å¯¼å…¥å·¥å…·å‡½æ•°å’Œæ—¥å¿—è®°å½•
from ...utils.torch_utils import randn_tensor  # ä»å·¥å…·æ¨¡å—å¯¼å…¥éšæœºå¼ é‡ç”Ÿæˆå‡½æ•°
from ..pipeline_utils import DiffusionPipeline, StableDiffusionMixin  # å¯¼å…¥æ‰©æ•£ç®¡é“å’Œæ··åˆç±»
from ..stable_diffusion import StableDiffusionPipelineOutput  # å¯¼å…¥ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»
from ..stable_diffusion.safety_checker import StableDiffusionSafetyChecker  # å¯¼å…¥å®‰å…¨æ£€æŸ¥å™¨
from .image_encoder import PaintByExampleImageEncoder  # ä»å½“å‰æ¨¡å—å¯¼å…¥å›¾åƒç¼–ç å™¨

logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨ï¼Œä¾¿äºè°ƒè¯•
# pylint: disable=invalid-name  # ç¦ç”¨pylintå…³äºåç§°çš„æ— æ•ˆè­¦å‘Š

# ä»diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2imgå¤åˆ¶çš„å‡½æ•°
def retrieve_latents(
    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = "sample"
):
    # æ£€æŸ¥encoder_outputæ˜¯å¦å…·æœ‰latent_distå±æ€§ä¸”é‡‡æ ·æ¨¡å¼ä¸º'sample'
    if hasattr(encoder_output, "latent_dist") and sample_mode == "sample":
        # è¿”å›latentåˆ†å¸ƒçš„æ ·æœ¬
        return encoder_output.latent_dist.sample(generator)
    # æ£€æŸ¥encoder_outputæ˜¯å¦å…·æœ‰latent_distå±æ€§ä¸”é‡‡æ ·æ¨¡å¼ä¸º'argmax'
    elif hasattr(encoder_output, "latent_dist") and sample_mode == "argmax":
        # è¿”å›latentåˆ†å¸ƒçš„ä¼—æ•°
        return encoder_output.latent_dist.mode()
    # æ£€æŸ¥encoder_outputæ˜¯å¦å…·æœ‰latentså±æ€§
    elif hasattr(encoder_output, "latents"):
        # è¿”å›latentså±æ€§
        return encoder_output.latents
    # å¦‚æœéƒ½ä¸æ»¡è¶³ï¼ŒæŠ›å‡ºå¼‚å¸¸
    else:
        raise AttributeError("Could not access latents of provided encoder_output")

# å‡†å¤‡å›¾åƒå’Œæ©ç ä»¥ä¾›â€œæŒ‰ç¤ºä¾‹ç»˜åˆ¶â€ç®¡é“ä½¿ç”¨
def prepare_mask_and_masked_image(image, mask):
    """
    å‡†å¤‡ä¸€å¯¹ (image, mask)ï¼Œä½¿å…¶å¯ä»¥è¢« Paint by Example ç®¡é“ä½¿ç”¨ã€‚
    è¿™æ„å‘³ç€è¿™äº›è¾“å…¥å°†è½¬æ¢ä¸º``torch.Tensor``ï¼Œå½¢çŠ¶ä¸º``batch x channels x height x width``ï¼Œ
    å…¶ä¸­``channels``ä¸º``3``ï¼ˆå¯¹äº``image``ï¼‰å’Œ``1``ï¼ˆå¯¹äº``mask``ï¼‰ã€‚

    ``image`` å°†è½¬æ¢ä¸º ``torch.float32`` å¹¶å½’ä¸€åŒ–ä¸º ``[-1, 1]``ã€‚
    ``mask`` å°†è¢«äºŒå€¼åŒ–ï¼ˆ``mask > 0.5``ï¼‰å¹¶åŒæ ·è½¬æ¢ä¸º ``torch.float32``ã€‚
    ```
    # å‡½æ•°å‚æ•°è¯´æ˜
    Args:
        # è¾“å…¥å›¾åƒï¼Œç±»å‹å¯ä»¥æ˜¯ np.arrayã€PIL.Image æˆ– torch.Tensor
        image (Union[np.array, PIL.Image, torch.Tensor]): The image to inpaint.
            # æè¿°å›¾åƒçš„ä¸åŒå¯èƒ½æ ¼å¼ï¼ŒåŒ…æ‹¬ PIL.Imageã€np.array æˆ– torch.Tensor
            It can be a ``PIL.Image``, or a ``height x width x 3`` ``np.array`` or a ``channels x height x width``
            ``torch.Tensor`` or a ``batch x channels x height x width`` ``torch.Tensor``.
        # æ©ç ï¼Œç”¨äºæŒ‡å®šéœ€è¦ä¿®å¤çš„åŒºåŸŸ
        mask (_type_): The mask to apply to the image, i.e. regions to inpaint.
            # æè¿°æ©ç çš„ä¸åŒå¯èƒ½æ ¼å¼ï¼Œç±»ä¼¼äºå›¾åƒ
            It can be a ``PIL.Image``, or a ``height x width`` ``np.array`` or a ``1 x height x width``
            ``torch.Tensor`` or a ``batch x 1 x height x width`` ``torch.Tensor``.

    # å¼‚å¸¸è¯´æ˜
    Raises:
        # è§¦å‘æ¡ä»¶ä¸º torch.Tensor æ ¼å¼å›¾åƒæˆ–æ©ç çš„æ•°å€¼èŒƒå›´ä¸æ­£ç¡®
        ValueError: ``torch.Tensor`` images should be in the ``[-1, 1]`` range. ValueError: ``torch.Tensor`` mask
        should be in the ``[0, 1]`` range. ValueError: ``mask`` and ``image`` should have the same spatial dimensions.
        # ç±»å‹é”™è¯¯ï¼Œå½“å›¾åƒå’Œæ©ç ç±»å‹ä¸åŒ¹é…æ—¶æŠ›å‡º
        TypeError: ``mask`` is a ``torch.Tensor`` but ``image`` is not
            (ot the other way around).

    # è¿”å›å€¼è¯´æ˜
    Returns:
        # è¿”å›ä¸€ä¸ªåŒ…å«æ©ç å’Œä¿®å¤å›¾åƒçš„å…ƒç»„ï¼Œå‡ä¸º torch.Tensor æ ¼å¼ï¼Œå…·æœ‰ 4 ä¸ªç»´åº¦
        tuple[torch.Tensor]: The pair (mask, masked_image) as ``torch.Tensor`` with 4
            dimensions: ``batch x channels x height x width``.
    """
    # æ£€æŸ¥è¾“å…¥å›¾åƒæ˜¯å¦ä¸º torch.Tensor ç±»å‹
    if isinstance(image, torch.Tensor):
        # å¦‚æœæ©ç ä¸æ˜¯ torch.Tensorï¼ŒæŠ›å‡ºç±»å‹é”™è¯¯
        if not isinstance(mask, torch.Tensor):
            raise TypeError(f"`image` is a torch.Tensor but `mask` (type: {type(mask)} is not")

        # å¦‚æœå›¾åƒä¸ºå•ä¸ªå›¾åƒï¼Œå°†å…¶è½¬æ¢ä¸ºæ‰¹å¤„ç†æ ¼å¼
        # Batch single image
        if image.ndim == 3:
            # ç¡®ä¿å•ä¸ªå›¾åƒçš„å½¢çŠ¶ä¸º (3, H, W)
            assert image.shape[0] == 3, "Image outside a batch should be of shape (3, H, W)"
            # åœ¨ç¬¬ä¸€ä¸ªç»´åº¦æ·»åŠ æ‰¹å¤„ç†ç»´åº¦
            image = image.unsqueeze(0)

        # å¦‚æœæ©ç ä¸ºäºŒç»´ï¼Œæ·»åŠ æ‰¹å¤„ç†å’Œé€šé“ç»´åº¦
        # Batch and add channel dim for single mask
        if mask.ndim == 2:
            # åœ¨å‰é¢æ·»åŠ ä¸¤ä¸ªç»´åº¦
            mask = mask.unsqueeze(0).unsqueeze(0)

        # å¦‚æœæ©ç ä¸ºä¸‰ç»´ï¼Œæ£€æŸ¥å…¶ä¸å›¾åƒçš„æ‰¹æ¬¡åŒ¹é…
        # Batch single mask or add channel dim
        if mask.ndim == 3:
            # Batched mask
            if mask.shape[0] == image.shape[0]:
                # å¦‚æœæ©ç çš„æ‰¹æ¬¡ä¸å›¾åƒç›¸åŒï¼Œæ·»åŠ é€šé“ç»´åº¦
                mask = mask.unsqueeze(1)
            else:
                # å¦åˆ™ï¼Œåœ¨å‰é¢æ·»åŠ æ‰¹å¤„ç†ç»´åº¦
                mask = mask.unsqueeze(0)

        # ç¡®ä¿å›¾åƒå’Œæ©ç éƒ½æ˜¯å››ç»´
        assert image.ndim == 4 and mask.ndim == 4, "Image and Mask must have 4 dimensions"
        # ç¡®ä¿å›¾åƒå’Œæ©ç çš„ç©ºé—´ç»´åº¦ç›¸åŒ
        assert image.shape[-2:] == mask.shape[-2:], "Image and Mask must have the same spatial dimensions"
        # ç¡®ä¿å›¾åƒå’Œæ©ç çš„æ‰¹å¤„ç†å¤§å°ç›¸åŒ
        assert image.shape[0] == mask.shape[0], "Image and Mask must have the same batch size"
        # ç¡®ä¿æ©ç åªæœ‰ä¸€ä¸ªé€šé“
        assert mask.shape[1] == 1, "Mask image must have a single channel"

        # æ£€æŸ¥å›¾åƒçš„æ•°å€¼èŒƒå›´æ˜¯å¦åœ¨ [-1, 1] ä¹‹é—´
        # Check image is in [-1, 1]
        if image.min() < -1 or image.max() > 1:
            raise ValueError("Image should be in [-1, 1] range")

        # æ£€æŸ¥æ©ç çš„æ•°å€¼èŒƒå›´æ˜¯å¦åœ¨ [0, 1] ä¹‹é—´
        # Check mask is in [0, 1]
        if mask.min() < 0 or mask.max() > 1:
            raise ValueError("Mask should be in [0, 1] range")

        # å¯¹æ©ç è¿›è¡Œåè½¬ï¼Œä»¥ä¾¿äºä¿®å¤
        # paint-by-example inverses the mask
        mask = 1 - mask

        # äºŒå€¼åŒ–æ©ç 
        # Binarize mask
        mask[mask < 0.5] = 0
        mask[mask >= 0.5] = 1

        # å°†å›¾åƒè½¬æ¢ä¸º float32 ç±»å‹
        # Image as float32
        image = image.to(dtype=torch.float32)
    # å¦‚æœæ©ç æ˜¯ torch.Tensor ç±»å‹ï¼Œä½†å›¾åƒä¸æ˜¯ï¼Œåˆ™æŠ›å‡ºç±»å‹é”™è¯¯
    elif isinstance(mask, torch.Tensor):
        raise TypeError(f"`mask` is a torch.Tensor but `image` (type: {type(image)} is not")
    else:
        # å¦‚æœè¾“å…¥çš„ image æ˜¯ PIL å›¾åƒå¯¹è±¡ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(image, PIL.Image.Image):
            image = [image]

        # å°†æ¯ä¸ªå›¾åƒè½¬æ¢ä¸º RGB æ ¼å¼ï¼Œå¹¶æ‹¼æ¥æˆä¸€ä¸ªæ•°ç»„ï¼Œå¢åŠ ç»´åº¦ä»¥é€‚åº”åç»­å¤„ç†
        image = np.concatenate([np.array(i.convert("RGB"))[None, :] for i in image], axis=0)
        # å°†å›¾åƒæ•°ç»„çš„ç»´åº¦é¡ºåºè°ƒæ•´ä¸º (æ‰¹é‡, é€šé“, é«˜, å®½)
        image = image.transpose(0, 3, 1, 2)
        # å°† NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡å¹¶å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0

        # å¤„ç† mask
        # å¦‚æœè¾“å…¥çš„ mask æ˜¯ PIL å›¾åƒå¯¹è±¡ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(mask, PIL.Image.Image):
            mask = [mask]

        # å°†æ¯ä¸ªæ©è†œå›¾åƒè½¬æ¢ä¸ºç°åº¦æ ¼å¼ï¼Œå¹¶æ‹¼æ¥æˆä¸€ä¸ªæ•°ç»„ï¼Œå¢åŠ ç»´åº¦ä»¥é€‚åº”åç»­å¤„ç†
        mask = np.concatenate([np.array(m.convert("L"))[None, None, :] for m in mask], axis=0)
        # å°†æ©è†œæ•°ç»„è½¬æ¢ä¸º float32 ç±»å‹å¹¶å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´
        mask = mask.astype(np.float32) / 255.0

        # paint-by-example æ–¹æ³•åè½¬æ©è†œ
        mask = 1 - mask

        # å°†æ©è†œä¸­ä½äº 0.5 çš„å€¼è®¾ç½®ä¸º 0ï¼Œé«˜äºæˆ–ç­‰äº 0.5 çš„å€¼è®¾ç½®ä¸º 1
        mask[mask < 0.5] = 0
        mask[mask >= 0.5] = 1
        # å°† NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡
        mask = torch.from_numpy(mask)

    # å°†å›¾åƒä¸æ©è†œç›¸ä¹˜ï¼Œå¾—åˆ°è¢«æ©è†œå¤„ç†çš„å›¾åƒ
    masked_image = image * mask

    # è¿”å›æ©è†œå’Œè¢«æ©è†œå¤„ç†çš„å›¾åƒ
    return mask, masked_image
# å®šä¹‰ä¸€ä¸ªåä¸º PaintByExamplePipeline çš„ç±»ï¼Œç»§æ‰¿è‡ª DiffusionPipeline å’Œ StableDiffusionMixin
class PaintByExamplePipeline(DiffusionPipeline, StableDiffusionMixin):
    r""" 
    # è­¦å‘Šæç¤ºï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§ç‰¹æ€§
    <Tip warning={true}>
    ğŸ§ª This is an experimental feature!
    </Tip>

    # ä½¿ç”¨ Stable Diffusion è¿›è¡Œå›¾åƒå¼•å¯¼çš„å›¾åƒä¿®è¡¥çš„ç®¡é“ã€‚

    # è¯¥æ¨¡å‹ä» [`DiffusionPipeline`] ç»§æ‰¿ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰ç®¡é“çš„é€šç”¨æ–¹æ³•
    # ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

    # å‚æ•°è¯´æ˜ï¼š
        vae ([`AutoencoderKL`]):
            ç”¨äºå°†å›¾åƒç¼–ç å’Œè§£ç ä¸ºæ½œåœ¨è¡¨ç¤ºçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ã€‚
        image_encoder ([`PaintByExampleImageEncoder`]):
            ç¼–ç ç¤ºä¾‹è¾“å…¥å›¾åƒã€‚`unet` æ˜¯åŸºäºç¤ºä¾‹å›¾åƒè€Œéæ–‡æœ¬æç¤ºè¿›è¡Œæ¡ä»¶å¤„ç†ã€‚
        tokenizer ([`~transformers.CLIPTokenizer`]):
            ç”¨äºæ–‡æœ¬åˆ†è¯çš„ `CLIPTokenizer`ã€‚
        unet ([`UNet2DConditionModel`]):
            ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œåœ¨çš„ `UNet2DConditionModel`ã€‚
        scheduler ([`SchedulerMixin`]):
            ä¸ `unet` ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œåœ¨çš„è°ƒåº¦å™¨ï¼Œå¯ä»¥æ˜¯
            [`DDIMScheduler`], [`LMSDiscreteScheduler`] æˆ– [`PNDMScheduler`]ã€‚
        safety_checker ([`StableDiffusionSafetyChecker`]):
            ä¼°è®¡ç”Ÿæˆå›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå†’çŠ¯æˆ–æœ‰å®³çš„åˆ†ç±»æ¨¡å—ã€‚
            è¯·å‚è€ƒ [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) ä»¥è·å–æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šç»†èŠ‚ã€‚
        feature_extractor ([`~transformers.CLIPImageProcessor`]):
            ç”¨äºä»ç”Ÿæˆå›¾åƒä¸­æå–ç‰¹å¾çš„ `CLIPImageProcessor`ï¼›ç”¨ä½œ `safety_checker` çš„è¾“å…¥ã€‚
    """

    # TODO: å¦‚æœç®¡é“æ²¡æœ‰ feature_extractorï¼Œåˆ™éœ€è¦åœ¨åˆå§‹å›¾åƒï¼ˆå¦‚æœä¸º PIL æ ¼å¼ï¼‰ç¼–ç æ—¶ç»™å‡ºæè¿°æ€§æ¶ˆæ¯ã€‚

    # å®šä¹‰æ¨¡å‹åœ¨ CPU ä¸Šå¸è½½çš„é¡ºåºï¼ŒæŒ‡å®š 'unet' åœ¨å‰ï¼Œ'vae' åœ¨å
    model_cpu_offload_seq = "unet->vae"
    # å®šä¹‰åœ¨ CPU å¸è½½æ—¶æ’é™¤çš„ç»„ä»¶ï¼ŒæŒ‡å®š 'image_encoder' ä¸å‚ä¸å¸è½½
    _exclude_from_cpu_offload = ["image_encoder"]
    # å®šä¹‰å¯é€‰ç»„ä»¶ï¼ŒæŒ‡å®š 'safety_checker' ä¸ºå¯é€‰
    _optional_components = ["safety_checker"]

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®ç®¡é“çš„ä¸»è¦ç»„ä»¶
    def __init__(
        self,
        vae: AutoencoderKL,
        image_encoder: PaintByExampleImageEncoder,
        unet: UNet2DConditionModel,
        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],
        safety_checker: StableDiffusionSafetyChecker,
        feature_extractor: CLIPImageProcessor,
        requires_safety_checker: bool = False,
    ):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()

        # æ³¨å†Œå„ä¸ªæ¨¡å—ï¼Œè®¾ç½®ç®¡é“çš„ç»„æˆéƒ¨åˆ†
        self.register_modules(
            vae=vae,
            image_encoder=image_encoder,
            unet=unet,
            scheduler=scheduler,
            safety_checker=safety_checker,
            feature_extractor=feature_extractor,
        )
        # è®¡ç®— VAE çš„ç¼©æ”¾å› å­ï¼ŒåŸºäº VAE é…ç½®ä¸­çš„å—è¾“å‡ºé€šé“æ•°
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
        # åˆå§‹åŒ– VaeImageProcessorï¼Œä½¿ç”¨è®¡ç®—å‡ºçš„ç¼©æ”¾å› å­
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        # å°†æ˜¯å¦éœ€è¦å®‰å…¨æ£€æŸ¥å™¨çš„ä¿¡æ¯æ³¨å†Œåˆ°é…ç½®ä¸­
        self.register_to_config(requires_safety_checker=requires_safety_checker)
    # ä» diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.run_safety_checker å¤åˆ¶è€Œæ¥
        def run_safety_checker(self, image, device, dtype):
            # å¦‚æœå®‰å…¨æ£€æŸ¥å™¨ä¸å­˜åœ¨ï¼Œå°†æœ‰å®³æ¦‚å¿µæ ‡è®°ä¸º None
            if self.safety_checker is None:
                has_nsfw_concept = None
            else:
                # å¦‚æœè¾“å…¥å›¾åƒæ˜¯å¼ é‡ï¼Œä½¿ç”¨å›¾åƒå¤„ç†å™¨åå¤„ç†ä¸º PIL æ ¼å¼
                if torch.is_tensor(image):
                    feature_extractor_input = self.image_processor.postprocess(image, output_type="pil")
                else:
                    # å¦‚æœè¾“å…¥å›¾åƒä¸æ˜¯å¼ é‡ï¼Œå°†å…¶è½¬æ¢ä¸º PIL æ ¼å¼
                    feature_extractor_input = self.image_processor.numpy_to_pil(image)
                # ä½¿ç”¨ç‰¹å¾æå–å™¨å¤„ç†å›¾åƒå¹¶è½¬ç§»åˆ°æŒ‡å®šè®¾å¤‡
                safety_checker_input = self.feature_extractor(feature_extractor_input, return_tensors="pt").to(device)
                # è¿è¡Œå®‰å…¨æ£€æŸ¥å™¨ï¼Œè¿”å›å¤„ç†åçš„å›¾åƒå’Œæœ‰å®³æ¦‚å¿µçš„å­˜åœ¨æƒ…å†µ
                image, has_nsfw_concept = self.safety_checker(
                    images=image, clip_input=safety_checker_input.pixel_values.to(dtype)
                )
            # è¿”å›å¤„ç†åçš„å›¾åƒå’Œæœ‰å®³æ¦‚å¿µ
            return image, has_nsfw_concept
    
        # ä» diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs å¤åˆ¶è€Œæ¥
        def prepare_extra_step_kwargs(self, generator, eta):
            # å‡†å¤‡è°ƒåº¦å™¨æ­¥éª¤çš„é¢å¤–å‚æ•°ï¼Œå› ä¸ºå¹¶éæ‰€æœ‰è°ƒåº¦å™¨éƒ½æœ‰ç›¸åŒçš„ç­¾å
            # etaï¼ˆÎ·ï¼‰ä»…åœ¨ DDIMScheduler ä¸­ä½¿ç”¨ï¼Œå¯¹äºå…¶ä»–è°ƒåº¦å™¨å°†è¢«å¿½ç•¥
            # eta å¯¹åº”äº DDIM è®ºæ–‡ä¸­çš„ Î·ï¼šhttps://arxiv.org/abs/2010.02502
            # åº”åœ¨ [0, 1] ä¹‹é—´
    
            # æ£€æŸ¥è°ƒåº¦å™¨æ˜¯å¦æ¥å— eta å‚æ•°
            accepts_eta = "eta" in set(inspect.signature(self.scheduler.step).parameters.keys())
            extra_step_kwargs = {}
            if accepts_eta:
                # å¦‚æœæ¥å— etaï¼Œå°†å…¶æ·»åŠ åˆ°é¢å¤–å‚æ•°ä¸­
                extra_step_kwargs["eta"] = eta
    
            # æ£€æŸ¥è°ƒåº¦å™¨æ˜¯å¦æ¥å— generator å‚æ•°
            accepts_generator = "generator" in set(inspect.signature(self.scheduler.step).parameters.keys())
            if accepts_generator:
                # å¦‚æœæ¥å— generatorï¼Œå°†å…¶æ·»åŠ åˆ°é¢å¤–å‚æ•°ä¸­
                extra_step_kwargs["generator"] = generator
            # è¿”å›å‡†å¤‡å¥½çš„é¢å¤–å‚æ•°
            return extra_step_kwargs
    
        # ä» diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.decode_latents å¤åˆ¶è€Œæ¥
        def decode_latents(self, latents):
            # è­¦å‘Šä¿¡æ¯ï¼Œæç¤º decode_latents æ–¹æ³•å·²å¼ƒç”¨ï¼Œå°†åœ¨ 1.0.0 ä¸­ç§»é™¤
            deprecation_message = "The decode_latents method is deprecated and will be removed in 1.0.0. Please use VaeImageProcessor.postprocess(...) instead"
            # è®°å½•å¼ƒç”¨è­¦å‘Š
            deprecate("decode_latents", "1.0.0", deprecation_message, standard_warn=False)
    
            # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨å‘é‡
            latents = 1 / self.vae.config.scaling_factor * latents
            # è§£ç æ½œåœ¨å‘é‡ï¼Œè¿”å›å›¾åƒ
            image = self.vae.decode(latents, return_dict=False)[0]
            # å°†å›¾åƒç¼©æ”¾åˆ° [0, 1] èŒƒå›´å†…
            image = (image / 2 + 0.5).clamp(0, 1)
            # å°†å›¾åƒè½¬æ¢ä¸º float32 æ ¼å¼ä»¥å…¼å®¹ bfloat16
            image = image.cpu().permute(0, 2, 3, 1).float().numpy()
            # è¿”å›å¤„ç†åçš„å›¾åƒ
            return image
    
        # ä» diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_image_variation.StableDiffusionImageVariationPipeline.check_inputs å¤åˆ¶è€Œæ¥
    # æ£€æŸ¥è¾“å…¥å‚æ•°çš„æœ‰æ•ˆæ€§
    def check_inputs(self, image, height, width, callback_steps):
        # æ£€æŸ¥ `image` æ˜¯å¦ä¸ºæœ‰æ•ˆç±»å‹ï¼Œå¿…é¡»æ˜¯ `torch.Tensor`ã€`PIL.Image.Image` æˆ–åˆ—è¡¨
        if (
            not isinstance(image, torch.Tensor)
            and not isinstance(image, PIL.Image.Image)
            and not isinstance(image, list)
        ):
            # å¦‚æœ `image` ç±»å‹ä¸ç¬¦åˆï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(
                "`image` has to be of type `torch.Tensor` or `PIL.Image.Image` or `List[PIL.Image.Image]` but is"
                f" {type(image)}"
            )

        # æ£€æŸ¥ `height` å’Œ `width` æ˜¯å¦èƒ½è¢« 8 æ•´é™¤
        if height % 8 != 0 or width % 8 != 0:
            # å¦‚æœä¸èƒ½æ•´é™¤ï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(f"`height` and `width` have to be divisible by 8 but are {height} and {width}.")

        # æ£€æŸ¥ `callback_steps` æ˜¯å¦æœ‰æ•ˆï¼Œå¿…é¡»æ˜¯æ­£æ•´æ•°æˆ– None
        if (callback_steps is None) or (
            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)
        ):
            # å¦‚æœæ— æ•ˆï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(
                f"`callback_steps` has to be a positive integer but is {callback_steps} of type"
                f" {type(callback_steps)}."
            )

    # ä» StableDiffusionPipeline å¤åˆ¶çš„å‡†å¤‡æ½œåœ¨å˜é‡çš„æ–¹æ³•
    def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):
        # å®šä¹‰æ½œåœ¨å˜é‡çš„å½¢çŠ¶
        shape = (
            batch_size,
            num_channels_latents,
            int(height) // self.vae_scale_factor,
            int(width) // self.vae_scale_factor,
        )
        # æ£€æŸ¥ç”Ÿæˆå™¨åˆ—è¡¨çš„é•¿åº¦æ˜¯å¦ä¸æ‰¹é‡å¤§å°åŒ¹é…
        if isinstance(generator, list) and len(generator) != batch_size:
            # å¦‚æœä¸åŒ¹é…ï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(
                f"You have passed a list of generators of length {len(generator)}, but requested an effective batch"
                f" size of {batch_size}. Make sure the batch size matches the length of the generators."
            )

        # å¦‚æœæ²¡æœ‰æä¾›æ½œåœ¨å˜é‡ï¼Œåˆ™ç”Ÿæˆéšæœºæ½œåœ¨å˜é‡
        if latents is None:
            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
        else:
            # å¦‚æœæä¾›äº†æ½œåœ¨å˜é‡ï¼Œå°†å…¶è½¬ç§»åˆ°æŒ‡å®šè®¾å¤‡
            latents = latents.to(device)

        # å°†åˆå§‹å™ªå£°ç¼©æ”¾åˆ°è°ƒåº¦å™¨æ‰€éœ€çš„æ ‡å‡†å·®
        latents = latents * self.scheduler.init_noise_sigma
        # è¿”å›å‡†å¤‡å¥½çš„æ½œåœ¨å˜é‡
        return latents

    # ä» StableDiffusionInpaintPipeline å¤åˆ¶çš„å‡†å¤‡æ©è†œæ½œåœ¨å˜é‡çš„æ–¹æ³•
    def prepare_mask_latents(
        # å®šä¹‰æ–¹æ³•çš„è¾“å…¥å‚æ•°ï¼ŒåŒ…æ‹¬æ©è†œå’Œå…¶ä»–ä¿¡æ¯
        self, mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance
    ):
        # å°†æ©ç è°ƒæ•´ä¸ºä¸æ½œå˜é‡å½¢çŠ¶ç›¸åŒï¼Œä»¥ä¾¿å°†æ©ç ä¸æ½œå˜é‡æ‹¼æ¥
        # è¿™æ ·åšå¯ä»¥é¿å…åœ¨ä½¿ç”¨ cpu_offload å’ŒåŠç²¾åº¦æ—¶å‡ºç°é—®é¢˜
        mask = torch.nn.functional.interpolate(
            mask, size=(height // self.vae_scale_factor, width // self.vae_scale_factor)
        )  # é€šè¿‡æ’å€¼è°ƒæ•´æ©ç çš„å¤§å°
        mask = mask.to(device=device, dtype=dtype)  # å°†æ©ç ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è½¬æ¢æ•°æ®ç±»å‹

        masked_image = masked_image.to(device=device, dtype=dtype)  # å°†æ©ç å›¾åƒç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è½¬æ¢æ•°æ®ç±»å‹

        if masked_image.shape[1] == 4:  # æ£€æŸ¥æ©ç å›¾åƒæ˜¯å¦ä¸ºå››é€šé“
            masked_image_latents = masked_image  # å¦‚æœæ˜¯ï¼Œç›´æ¥å°†å…¶èµ‹å€¼ç»™æ½œå˜é‡
        else:
            masked_image_latents = self._encode_vae_image(masked_image, generator=generator)  # å¦åˆ™ï¼Œä½¿ç”¨ VAE ç¼–ç å›¾åƒ

        # é’ˆå¯¹æ¯ä¸ªæç¤ºé‡å¤æ©ç å’Œæ©ç å›¾åƒæ½œå˜é‡ï¼Œä½¿ç”¨é€‚åˆ MPS çš„æ–¹æ³•
        if mask.shape[0] < batch_size:  # æ£€æŸ¥æ©ç çš„æ•°é‡æ˜¯å¦å°‘äºæ‰¹å¤„ç†å¤§å°
            if not batch_size % mask.shape[0] == 0:  # æ£€æŸ¥æ©ç æ•°é‡æ˜¯å¦å¯æ•´é™¤æ‰¹å¤„ç†å¤§å°
                raise ValueError(
                    "The passed mask and the required batch size don't match. Masks are supposed to be duplicated to"
                    f" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number"
                    " of masks that you pass is divisible by the total requested batch size."
                )  # å¦‚æœä¸åŒ¹é…ï¼ŒæŠ›å‡ºå€¼é”™è¯¯
            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)  # é‡å¤æ©ç ä»¥åŒ¹é…æ‰¹å¤„ç†å¤§å°
        if masked_image_latents.shape[0] < batch_size:  # æ£€æŸ¥æ½œå˜é‡æ•°é‡æ˜¯å¦å°‘äºæ‰¹å¤„ç†å¤§å°
            if not batch_size % masked_image_latents.shape[0] == 0:  # æ£€æŸ¥æ½œå˜é‡æ•°é‡æ˜¯å¦å¯æ•´é™¤æ‰¹å¤„ç†å¤§å°
                raise ValueError(
                    "The passed images and the required batch size don't match. Images are supposed to be duplicated"
                    f" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed."
                    " Make sure the number of images that you pass is divisible by the total requested batch size."
                )  # å¦‚æœä¸åŒ¹é…ï¼ŒæŠ›å‡ºå€¼é”™è¯¯
            masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1)  # é‡å¤æ½œå˜é‡ä»¥åŒ¹é…æ‰¹å¤„ç†å¤§å°

        mask = torch.cat([mask] * 2) if do_classifier_free_guidance else mask  # æ ¹æ®æ˜¯å¦ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼é€‰æ‹©æ©ç 
        masked_image_latents = (
            torch.cat([masked_image_latents] * 2) if do_classifier_free_guidance else masked_image_latents
        )  # æ ¹æ®æ˜¯å¦ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼é€‰æ‹©æ½œå˜é‡

        # è°ƒæ•´è®¾å¤‡ä»¥é˜²æ­¢ä¸æ½œå˜é‡æ¨¡å‹è¾“å…¥æ‹¼æ¥æ—¶å‡ºç°è®¾å¤‡é”™è¯¯
        masked_image_latents = masked_image_latents.to(device=device, dtype=dtype)  # å°†æ½œå˜é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è½¬æ¢æ•°æ®ç±»å‹
        return mask, masked_image_latents  # è¿”å›å¤„ç†åçš„æ©ç å’Œæ½œå˜é‡

    # ä» diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_inpaint.StableDiffusionInpaintPipeline._encode_vae_image å¤åˆ¶
    # å®šä¹‰ä¸€ä¸ªç¼–ç å˜åˆ†è‡ªç¼–ç å™¨å›¾åƒçš„ç§æœ‰æ–¹æ³•
        def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):
            # æ£€æŸ¥ç”Ÿæˆå™¨æ˜¯å¦ä¸ºåˆ—è¡¨ç±»å‹
            if isinstance(generator, list):
                # å¯¹æ¯ä¸ªå›¾åƒç¼–ç å¹¶è·å–æ½œåœ¨è¡¨ç¤ºï¼Œä½¿ç”¨å¯¹åº”çš„ç”Ÿæˆå™¨
                image_latents = [
                    retrieve_latents(self.vae.encode(image[i : i + 1]), generator=generator[i])
                    for i in range(image.shape[0])
                ]
                # å°†æ‰€æœ‰æ½œåœ¨è¡¨ç¤ºåœ¨ç¬¬0ç»´ä¸Šæ‹¼æ¥æˆä¸€ä¸ªå¼ é‡
                image_latents = torch.cat(image_latents, dim=0)
            else:
                # å¦‚æœç”Ÿæˆå™¨ä¸æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ç¼–ç å›¾åƒå¹¶è·å–æ½œåœ¨è¡¨ç¤º
                image_latents = retrieve_latents(self.vae.encode(image), generator=generator)
    
            # å°†æ½œåœ¨è¡¨ç¤ºä¹˜ä»¥ç¼©æ”¾å› å­
            image_latents = self.vae.config.scaling_factor * image_latents
    
            # è¿”å›ç¼–ç åçš„æ½œåœ¨è¡¨ç¤º
            return image_latents
    
        # å®šä¹‰ä¸€ä¸ªç¼–ç å›¾åƒçš„ç§æœ‰æ–¹æ³•
        def _encode_image(self, image, device, num_images_per_prompt, do_classifier_free_guidance):
            # è·å–å›¾åƒç¼–ç å™¨å‚æ•°çš„æ•°æ®ç±»å‹
            dtype = next(self.image_encoder.parameters()).dtype
    
            # æ£€æŸ¥è¾“å…¥å›¾åƒæ˜¯å¦ä¸ºå¼ é‡ï¼Œå¦‚æœä¸æ˜¯åˆ™æå–ç‰¹å¾
            if not isinstance(image, torch.Tensor):
                image = self.feature_extractor(images=image, return_tensors="pt").pixel_values
    
            # å°†å›¾åƒç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼Œå¹¶è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹
            image = image.to(device=device, dtype=dtype)
            # å¯¹å›¾åƒè¿›è¡Œç¼–ç ï¼Œè·å–å›¾åƒåµŒå…¥å’Œè´Ÿæç¤ºåµŒå…¥
            image_embeddings, negative_prompt_embeds = self.image_encoder(image, return_uncond_vector=True)
    
            # å¤åˆ¶å›¾åƒåµŒå…¥ä»¥é€‚åº”æ¯ä¸ªæç¤ºçš„ç”Ÿæˆæ•°é‡
            bs_embed, seq_len, _ = image_embeddings.shape
            image_embeddings = image_embeddings.repeat(1, num_images_per_prompt, 1)
            # é‡å¡‘åµŒå…¥å¼ é‡çš„å½¢çŠ¶
            image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)
    
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼
            if do_classifier_free_guidance:
                # å¤åˆ¶è´Ÿæç¤ºåµŒå…¥ä»¥åŒ¹é…å›¾åƒåµŒå…¥çš„æ•°é‡
                negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1)
                negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1)
    
                # ä¸ºæ— åˆ†ç±»å™¨å¼•å¯¼æ‰§è¡Œä¸¤ä¸ªå‰å‘ä¼ æ’­ï¼Œé€šè¿‡æ‹¼æ¥æ— æ¡ä»¶å’Œæ–‡æœ¬åµŒå…¥æ¥é¿å…ä¸¤ä¸ªå‰å‘ä¼ æ’­
                image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])
    
            # è¿”å›ç¼–ç åçš„å›¾åƒåµŒå…¥
            return image_embeddings
    
        # å®šä¹‰ä¸€ä¸ªè°ƒç”¨æ–¹æ³•ï¼Œç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥æé«˜æ•ˆç‡
        @torch.no_grad()
        def __call__(
            # æ¥æ”¶ç¤ºä¾‹å›¾åƒå’Œå›¾åƒçš„å‚æ•°ï¼Œå…è®¸ä¸åŒç±»å‹çš„è¾“å…¥
            example_image: Union[torch.Tensor, PIL.Image.Image],
            image: Union[torch.Tensor, PIL.Image.Image],
            mask_image: Union[torch.Tensor, PIL.Image.Image],
            # å¯é€‰å‚æ•°å®šä¹‰å›¾åƒçš„é«˜åº¦å’Œå®½åº¦
            height: Optional[int] = None,
            width: Optional[int] = None,
            # å®šä¹‰æ¨ç†æ­¥éª¤çš„æ•°é‡å’Œå¼•å¯¼ç¼©æ”¾æ¯”ä¾‹
            num_inference_steps: int = 50,
            guidance_scale: float = 5.0,
            # è´Ÿæç¤ºçš„å¯é€‰è¾“å…¥
            negative_prompt: Optional[Union[str, List[str]]] = None,
            # æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡
            num_images_per_prompt: Optional[int] = 1,
            # æ§åˆ¶é‡‡æ ·å¤šæ ·æ€§çš„å‚æ•°
            eta: float = 0.0,
            # ç”Ÿæˆå™¨çš„å¯é€‰è¾“å…¥
            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
            # å¯é€‰çš„æ½œåœ¨å˜é‡è¾“å…¥
            latents: Optional[torch.Tensor] = None,
            # è¾“å‡ºç±»å‹çš„å¯é€‰å‚æ•°
            output_type: Optional[str] = "pil",
            # æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„ç»“æœ
            return_dict: bool = True,
            # å¯é€‰çš„å›è°ƒå‡½æ•°ç”¨äºå¤„ç†ä¸­é—´ç»“æœ
            callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,
            # æ¯éš”å¤šå°‘æ­¥è°ƒç”¨ä¸€æ¬¡å›è°ƒ
            callback_steps: int = 1,
```