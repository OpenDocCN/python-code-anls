# `.\diffusers\models\autoencoders\autoencoder_kl.py`

```
# ç‰ˆæƒå£°æ˜ï¼Œè¡¨æ˜æ­¤æ–‡ä»¶çš„ç‰ˆæƒæ‰€æœ‰è€…åŠå…¶æ‰€æœ‰æƒåˆ©
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬è¿›è¡Œè®¸å¯ï¼Œå£°æ˜è¯¥æ–‡ä»¶ä½¿ç”¨æ¡ä»¶
# Licensed under the Apache License, Version 2.0 (the "License");
# åªèƒ½åœ¨ç¬¦åˆè®¸å¯è¯çš„æƒ…å†µä¸‹ä½¿ç”¨è¯¥æ–‡ä»¶
# you may not use this file except in compliance with the License.
# å¯ä»¥åœ¨æ­¤ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰çº¦å®šï¼Œè½¯ä»¶æŒ‰ "åŸæ ·" æä¾›ï¼Œä¸”ä¸é™„å¸¦ä»»ä½•å½¢å¼çš„ä¿è¯
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# ä¸æ‰¿æ‹…ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# æŸ¥çœ‹è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šæƒé™å’Œé™åˆ¶
# See the License for the specific language governing permissions and
# limitations under the License.
# å¯¼å…¥æ‰€éœ€çš„ç±»å‹å®šä¹‰
from typing import Dict, Optional, Tuple, Union

# å¯¼å…¥ PyTorch åº“
import torch
import torch.nn as nn

# å¯¼å…¥å…¶ä»–æ¨¡å—ä¸­çš„æ··åˆç±»å’Œå·¥å…·å‡½æ•°
from ...configuration_utils import ConfigMixin, register_to_config
from ...loaders.single_file_model import FromOriginalModelMixin
from ...utils.accelerate_utils import apply_forward_hook
# å¯¼å…¥æ³¨æ„åŠ›å¤„ç†å™¨ç›¸å…³çš„ç±»å’Œå¸¸é‡
from ..attention_processor import (
    ADDED_KV_ATTENTION_PROCESSORS,
    CROSS_ATTENTION_PROCESSORS,
    Attention,
    AttentionProcessor,
    AttnAddedKVProcessor,
    AttnProcessor,
    FusedAttnProcessor2_0,
)
# å¯¼å…¥æ¨¡å‹è¾“å‡ºç›¸å…³çš„ç±»
from ..modeling_outputs import AutoencoderKLOutput
# å¯¼å…¥æ¨¡å‹å·¥å…·ç±»
from ..modeling_utils import ModelMixin
# å¯¼å…¥å˜åˆ†è‡ªç¼–ç å™¨ç›¸å…³çš„ç±»
from .vae import Decoder, DecoderOutput, DiagonalGaussianDistribution, Encoder

# å®šä¹‰ä¸€ä¸ªå˜åˆ†è‡ªç¼–ç å™¨æ¨¡å‹ï¼Œä½¿ç”¨ KL æŸå¤±ç¼–ç å›¾åƒåˆ°æ½œåœ¨ç©ºé—´å¹¶è§£ç 
class AutoencoderKL(ModelMixin, ConfigMixin, FromOriginalModelMixin):
    r"""
    ä¸€ä¸ªå¸¦æœ‰ KL æŸå¤±çš„ VAE æ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒã€‚

    è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [`ModelMixin`]ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£å…¶å®ç°çš„é€šç”¨æ–¹æ³•
    é€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼‰ã€‚
    # å‚æ•°è¯´æ˜
        Parameters:
            # è¾“å…¥å›¾åƒçš„é€šé“æ•°ï¼Œé»˜è®¤ä¸º 3
            in_channels (int, *optional*, defaults to 3): Number of channels in the input image.
            # è¾“å‡ºçš„é€šé“æ•°ï¼Œé»˜è®¤ä¸º 3
            out_channels (int,  *optional*, defaults to 3): Number of channels in the output.
            # ä¸‹é‡‡æ ·å—ç±»å‹çš„å…ƒç»„ï¼Œé»˜è®¤ä¸º ("DownEncoderBlock2D",)
            down_block_types (`Tuple[str]`, *optional*, defaults to `("DownEncoderBlock2D",)`):
                Tuple of downsample block types.
            # ä¸Šé‡‡æ ·å—ç±»å‹çš„å…ƒç»„ï¼Œé»˜è®¤ä¸º ("UpDecoderBlock2D",)
            up_block_types (`Tuple[str]`, *optional*, defaults to `("UpDecoderBlock2D",)`):
                Tuple of upsample block types.
            # å—è¾“å‡ºé€šé“çš„å…ƒç»„ï¼Œé»˜è®¤ä¸º (64,)
            block_out_channels (`Tuple[int]`, *optional*, defaults to `(64,)`):
                Tuple of block output channels.
            # ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸º "silu"
            act_fn (`str`, *optional*, defaults to `"silu"`): The activation function to use.
            # æ½œåœ¨ç©ºé—´çš„é€šé“æ•°ï¼Œé»˜è®¤ä¸º 4
            latent_channels (`int`, *optional*, defaults to 4): Number of channels in the latent space.
            # æ ·æœ¬è¾“å…¥å¤§å°ï¼Œé»˜è®¤ä¸º 32
            sample_size (`int`, *optional*, defaults to 32): Sample input size.
            # è®­ç»ƒæ½œåœ¨ç©ºé—´çš„åˆ†é‡æ ‡å‡†å·®ï¼Œé»˜è®¤ä¸º 0.18215
            scaling_factor (`float`, *optional*, defaults to 0.18215):
                The component-wise standard deviation of the trained latent space computed using the first batch of the
                training set. This is used to scale the latent space to have unit variance when training the diffusion
                model. The latents are scaled with the formula `z = z * scaling_factor` before being passed to the
                diffusion model. When decoding, the latents are scaled back to the original scale with the formula: `z = 1
                / scaling_factor * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution Image
                Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) paper.
            # æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨ float32ï¼Œä»¥é€‚åº”é«˜åˆ†è¾¨ç‡ç®¡é“ï¼Œé»˜è®¤ä¸º True
            force_upcast (`bool`, *optional*, default to `True`):
                If enabled it will force the VAE to run in float32 for high image resolution pipelines, such as SD-XL. VAE
                can be fine-tuned / trained to a lower range without losing too much precision in which case
                `force_upcast` can be set to `False` - see: https://huggingface.co/madebyollin/sdxl-vae-fp16-fix
            # æ˜¯å¦åœ¨ Encoder å’Œ Decoder çš„ mid_block ä¸­æ·»åŠ æ³¨æ„åŠ›å—ï¼Œé»˜è®¤ä¸º True
            mid_block_add_attention (`bool`, *optional*, default to `True`):
                If enabled, the mid_block of the Encoder and Decoder will have attention blocks. If set to false, the
                mid_block will only have resnet blocks
        """
    
        # æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹
        _supports_gradient_checkpointing = True
        # ä¸åˆ†å‰²çš„æ¨¡å—åˆ—è¡¨
        _no_split_modules = ["BasicTransformerBlock", "ResnetBlock2D"]
    
        # æ³¨å†Œåˆ°é…ç½®ä¸­
        @register_to_config
    # æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–æ¨¡å‹å‚æ•°
    def __init__(
        # è¾“å…¥é€šé“æ•°ï¼Œé»˜è®¤å€¼ä¸º3
        self,
        in_channels: int = 3,
        # è¾“å‡ºé€šé“æ•°ï¼Œé»˜è®¤å€¼ä¸º3
        out_channels: int = 3,
        # ä¸‹é‡‡æ ·å—çš„ç±»å‹ï¼Œé»˜è®¤ä¸ºåŒ…å«ä¸€ä¸ªä¸‹é‡‡æ ·ç¼–ç å—çš„å…ƒç»„
        down_block_types: Tuple[str] = ("DownEncoderBlock2D",),
        # ä¸Šé‡‡æ ·å—çš„ç±»å‹ï¼Œé»˜è®¤ä¸ºåŒ…å«ä¸€ä¸ªä¸Šé‡‡æ ·è§£ç å—çš„å…ƒç»„
        up_block_types: Tuple[str] = ("UpDecoderBlock2D",),
        # æ¯ä¸ªå—çš„è¾“å‡ºé€šé“æ•°ï¼Œé»˜è®¤ä¸ºåŒ…å«64çš„å…ƒç»„
        block_out_channels: Tuple[int] = (64,),
        # æ¯ä¸ªå—çš„å±‚æ•°ï¼Œé»˜è®¤ä¸º1
        layers_per_block: int = 1,
        # æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œé»˜è®¤ä¸º"silu"
        act_fn: str = "silu",
        # æ½œåœ¨é€šé“æ•°ï¼Œé»˜è®¤ä¸º4
        latent_channels: int = 4,
        # å½’ä¸€åŒ–çš„ç»„æ•°ï¼Œé»˜è®¤ä¸º32
        norm_num_groups: int = 32,
        # æ ·æœ¬å¤§å°ï¼Œé»˜è®¤ä¸º32
        sample_size: int = 32,
        # ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º0.18215
        scaling_factor: float = 0.18215,
        # ç§»ä½å› å­ï¼Œé»˜è®¤ä¸ºNoneï¼ˆå¯é€‰ï¼‰
        shift_factor: Optional[float] = None,
        # æ½œåœ¨å˜é‡çš„å‡å€¼ï¼Œé»˜è®¤ä¸ºNoneï¼ˆå¯é€‰ï¼‰
        latents_mean: Optional[Tuple[float]] = None,
        # æ½œåœ¨å˜é‡çš„æ ‡å‡†å·®ï¼Œé»˜è®¤ä¸ºNoneï¼ˆå¯é€‰ï¼‰
        latents_std: Optional[Tuple[float]] = None,
        # å¼ºåˆ¶ä¸Šæº¢å‡ºï¼Œé»˜è®¤ä¸ºTrue
        force_upcast: float = True,
        # ä½¿ç”¨é‡åŒ–å·ç§¯ï¼Œé»˜è®¤ä¸ºTrue
        use_quant_conv: bool = True,
        # ä½¿ç”¨åé‡åŒ–å·ç§¯ï¼Œé»˜è®¤ä¸ºTrue
        use_post_quant_conv: bool = True,
        # ä¸­é—´å—æ˜¯å¦æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶ï¼Œé»˜è®¤ä¸ºTrue
        mid_block_add_attention: bool = True,
    ):
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__()

        # å°†åˆå§‹åŒ–å‚æ•°ä¼ é€’ç»™ç¼–ç å™¨
        self.encoder = Encoder(
            # è¾“å…¥é€šé“æ•°
            in_channels=in_channels,
            # è¾“å‡ºæ½œåœ¨é€šé“æ•°
            out_channels=latent_channels,
            # ä¸‹é‡‡æ ·å—çš„ç±»å‹
            down_block_types=down_block_types,
            # æ¯ä¸ªå—çš„è¾“å‡ºé€šé“æ•°
            block_out_channels=block_out_channels,
            # æ¯ä¸ªå—çš„å±‚æ•°
            layers_per_block=layers_per_block,
            # æ¿€æ´»å‡½æ•°ç±»å‹
            act_fn=act_fn,
            # å½’ä¸€åŒ–çš„ç»„æ•°
            norm_num_groups=norm_num_groups,
            # æ˜¯å¦åŒé‡æ½œåœ¨å˜é‡
            double_z=True,
            # ä¸­é—´å—æ˜¯å¦æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
            mid_block_add_attention=mid_block_add_attention,
        )

        # å°†åˆå§‹åŒ–å‚æ•°ä¼ é€’ç»™è§£ç å™¨
        self.decoder = Decoder(
            # æ½œåœ¨é€šé“æ•°ä½œä¸ºè¾“å…¥
            in_channels=latent_channels,
            # è¾“å‡ºé€šé“æ•°
            out_channels=out_channels,
            # ä¸Šé‡‡æ ·å—çš„ç±»å‹
            up_block_types=up_block_types,
            # æ¯ä¸ªå—çš„è¾“å‡ºé€šé“æ•°
            block_out_channels=block_out_channels,
            # æ¯ä¸ªå—çš„å±‚æ•°
            layers_per_block=layers_per_block,
            # å½’ä¸€åŒ–çš„ç»„æ•°
            norm_num_groups=norm_num_groups,
            # æ¿€æ´»å‡½æ•°ç±»å‹
            act_fn=act_fn,
            # ä¸­é—´å—æ˜¯å¦æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
            mid_block_add_attention=mid_block_add_attention,
        )

        # æ ¹æ®æ˜¯å¦ä½¿ç”¨é‡åŒ–å·ç§¯åˆå§‹åŒ–å·ç§¯å±‚
        self.quant_conv = nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1) if use_quant_conv else None
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨åé‡åŒ–å·ç§¯åˆå§‹åŒ–å·ç§¯å±‚
        self.post_quant_conv = nn.Conv2d(latent_channels, latent_channels, 1) if use_post_quant_conv else None

        # æ˜¯å¦ä½¿ç”¨åˆ‡ç‰‡ï¼Œåˆå§‹å€¼ä¸ºFalse
        self.use_slicing = False
        # æ˜¯å¦ä½¿ç”¨å¹³é“ºï¼Œåˆå§‹å€¼ä¸ºFalse
        self.use_tiling = False

        # ä»…åœ¨å¯ç”¨VAEå¹³é“ºæ—¶ç›¸å…³
        # å¹³é“ºé‡‡æ ·çš„æœ€å°å¤§å°è®¾ç½®ä¸ºé…ç½®ä¸­çš„æ ·æœ¬å¤§å°
        self.tile_sample_min_size = self.config.sample_size
        # è·å–æ ·æœ¬å¤§å°ï¼Œå¦‚æœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„åˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        sample_size = (
            self.config.sample_size[0]
            if isinstance(self.config.sample_size, (list, tuple))
            else self.config.sample_size
        )
        # è®¡ç®—å¹³é“ºæ½œåœ¨å˜é‡çš„æœ€å°å¤§å°
        self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))
        # è®¾ç½®å¹³é“ºé‡å å› å­
        self.tile_overlap_factor = 0.25

    # è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹çš„å‡½æ•°
    def _set_gradient_checkpointing(self, module, value=False):
        # å¦‚æœæ¨¡å—æ˜¯ç¼–ç å™¨æˆ–è§£ç å™¨ï¼Œè®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹æ ‡å¿—
        if isinstance(module, (Encoder, Decoder)):
            module.gradient_checkpointing = value

    # å¯ç”¨å¹³é“ºçš„å‡½æ•°
    def enable_tiling(self, use_tiling: bool = True):
        r"""
        å¯ç”¨å¹³é“ºVAEè§£ç ã€‚å½“æ­¤é€‰é¡¹å¯ç”¨æ—¶ï¼ŒVAEå°†è¾“å…¥å¼ é‡æ‹†åˆ†ä¸ºå¹³é“ºå—ï¼Œä»¥åˆ†æ­¥è®¡ç®—è§£ç å’Œç¼–ç ã€‚
        è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§å›¾åƒéå¸¸æœ‰ç”¨ã€‚
        """
        # è®¾ç½®ä½¿ç”¨å¹³é“ºçš„æ ‡å¿—
        self.use_tiling = use_tiling
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºç¦ç”¨ç“·ç – VAE è§£ç 
    def disable_tiling(self):
        r""" 
        ç¦ç”¨ç“·ç – VAE è§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº† `enable_tiling`ï¼Œæ­¤æ–¹æ³•å°†æ¢å¤åˆ°ä¸€æ¬¡æ€§è§£ç è®¡ç®—ã€‚
        """
        # è°ƒç”¨è®¾ç½®æ–¹æ³•ï¼Œå°†ç“·ç –è§£ç çŠ¶æ€è®¾ç½®ä¸º False
        self.enable_tiling(False)
    
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºå¯ç”¨åˆ‡ç‰‡ VAE è§£ç 
    def enable_slicing(self):
        r""" 
        å¯ç”¨åˆ‡ç‰‡ VAE è§£ç ã€‚å½“æ­¤é€‰é¡¹å¯ç”¨æ—¶ï¼ŒVAE å°†æŠŠè¾“å…¥å¼ é‡åˆ†å‰²æˆåˆ‡ç‰‡ï¼Œä»¥
        å¤šæ¬¡è®¡ç®—è§£ç ã€‚è¿™æœ‰åŠ©äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°ã€‚
        """
        # è®¾ç½®ä½¿ç”¨åˆ‡ç‰‡çš„æ ‡å¿—ä¸º True
        self.use_slicing = True
    
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ç”¨äºç¦ç”¨åˆ‡ç‰‡ VAE è§£ç 
    def disable_slicing(self):
        r""" 
        ç¦ç”¨åˆ‡ç‰‡ VAE è§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº† `enable_slicing`ï¼Œæ­¤æ–¹æ³•å°†æ¢å¤åˆ°ä¸€æ¬¡æ€§è§£ç è®¡ç®—ã€‚
        """
        # è®¾ç½®ä½¿ç”¨åˆ‡ç‰‡çš„æ ‡å¿—ä¸º False
        self.use_slicing = False
    
    # å®šä¹‰ä¸€ä¸ªå±æ€§ï¼Œç”¨äºè¿”å›æ³¨æ„åŠ›å¤„ç†å™¨
    @property
    # å¤åˆ¶è‡ª diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors
    def attn_processors(self) -> Dict[str, AttentionProcessor]:
        r""" 
        è¿”å›ï¼š
            `dict` çš„æ³¨æ„åŠ›å¤„ç†å™¨ï¼šä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¨¡å‹ä¸­æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨ï¼ŒæŒ‰æƒé‡åç§°ç´¢å¼•ã€‚
        """
        # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸ç”¨äºå­˜å‚¨å¤„ç†å™¨
        processors = {}
    
        # å®šä¹‰é€’å½’å‡½æ•°ç”¨äºæ·»åŠ å¤„ç†å™¨
        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
            # å¦‚æœæ¨¡å—å…·æœ‰è·å–å¤„ç†å™¨çš„æ–¹æ³•ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°å­—å…¸ä¸­
            if hasattr(module, "get_processor"):
                processors[f"{name}.processor"] = module.get_processor()
    
            # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—ï¼Œå¹¶é€’å½’è°ƒç”¨å¤„ç†å™¨æ·»åŠ å‡½æ•°
            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)
    
            return processors
    
        # éå†å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—ï¼Œå¹¶è°ƒç”¨é€’å½’å‡½æ•°
        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)
    
        # è¿”å›æ‰€æœ‰æ”¶é›†åˆ°çš„å¤„ç†å™¨
        return processors
    
    # å¤åˆ¶è‡ª diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor
    # è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨
        def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
            r"""
            è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚
    
            å‚æ•°ï¼š
                processorï¼ˆ`dict` of `AttentionProcessor` or only `AttentionProcessor`ï¼‰ï¼š
                    å·²å®ä¾‹åŒ–çš„å¤„ç†å™¨ç±»æˆ–å¤„ç†å™¨ç±»çš„å­—å…¸ï¼Œå°†ä½œä¸º**æ‰€æœ‰** `Attention` å±‚çš„å¤„ç†å™¨è®¾ç½®ã€‚
    
                    å¦‚æœ `processor` æ˜¯å­—å…¸ï¼Œåˆ™é”®éœ€è¦å®šä¹‰å¯¹åº”çš„äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„è·¯å¾„ã€‚å½“è®¾ç½®å¯è®­ç»ƒçš„æ³¨æ„åŠ›å¤„ç†å™¨æ—¶ï¼Œå¼ºçƒˆæ¨èä½¿ç”¨è¿™ç§æ–¹å¼ã€‚
    
            """
            # è·å–å½“å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„æ•°é‡
            count = len(self.attn_processors.keys())
    
            # æ£€æŸ¥ä¼ å…¥çš„å¤„ç†å™¨å­—å…¸é•¿åº¦æ˜¯å¦ä¸æ³¨æ„åŠ›å±‚æ•°é‡åŒ¹é…
            if isinstance(processor, dict) and len(processor) != count:
                raise ValueError(
                    f"ä¼ å…¥çš„å¤„ç†å™¨å­—å…¸æ•°é‡ {len(processor)} ä¸æ³¨æ„åŠ›å±‚æ•°é‡ {count} ä¸åŒ¹é…ã€‚è¯·ç¡®ä¿ä¼ å…¥ {count} ä¸ªå¤„ç†å™¨ç±»ã€‚"
                )
    
            # é€’å½’è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„å‡½æ•°
            def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
                # å¦‚æœæ¨¡å—æœ‰è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
                if hasattr(module, "set_processor"):
                    # å¦‚æœå¤„ç†å™¨ä¸æ˜¯å­—å…¸ï¼Œç›´æ¥è®¾ç½®
                    if not isinstance(processor, dict):
                        module.set_processor(processor)
                    else:
                        # ä»å­—å…¸ä¸­å¼¹å‡ºå¯¹åº”çš„å¤„ç†å™¨å¹¶è®¾ç½®
                        module.set_processor(processor.pop(f"{name}.processor"))
    
                # éå†æ¨¡å—çš„å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨
                for sub_name, child in module.named_children():
                    fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)
    
            # å¯¹å½“å‰å®ä¾‹çš„æ¯ä¸ªå­æ¨¡å—è°ƒç”¨é€’å½’å‡½æ•°
            for name, module in self.named_children():
                fn_recursive_attn_processor(name, module, processor)
    
        # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_default_attn_processor å¤åˆ¶
        def set_default_attn_processor(self):
            """
            ç¦ç”¨è‡ªå®šä¹‰æ³¨æ„åŠ›å¤„ç†å™¨ï¼Œå¹¶è®¾ç½®é»˜è®¤çš„æ³¨æ„åŠ›å®ç°ã€‚
            """
            # æ£€æŸ¥æ‰€æœ‰å¤„ç†å™¨æ˜¯å¦å±äºæ·»åŠ çš„ KV æ³¨æ„åŠ›å¤„ç†å™¨
            if all(proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
                processor = AttnAddedKVProcessor()
            # æ£€æŸ¥æ‰€æœ‰å¤„ç†å™¨æ˜¯å¦å±äºäº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨
            elif all(proc.__class__ in CROSS_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
                processor = AttnProcessor()
            else:
                raise ValueError(
                    f"å½“æ³¨æ„åŠ›å¤„ç†å™¨çš„ç±»å‹ä¸º {next(iter(self.attn_processors.values()))} æ—¶ï¼Œæ— æ³•è°ƒç”¨ `set_default_attn_processor`"
                )
    
            # è°ƒç”¨è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
            self.set_attn_processor(processor)
    
        # åº”ç”¨å‰å‘é’©å­
        @apply_forward_hook
        def encode(
            self, x: torch.Tensor, return_dict: bool = True
    # å®šä¹‰è¿”å›ç±»å‹ä¸º AutoencoderKLOutput æˆ–è€… DiagonalGaussianDistribution çš„å‡½æ•°
    ) -> Union[AutoencoderKLOutput, Tuple[DiagonalGaussianDistribution]]:
            """
            ç¼–ç ä¸€æ‰¹å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºã€‚
    
            å‚æ•°ï¼š
                x (`torch.Tensor`): è¾“å…¥å›¾åƒçš„æ‰¹æ¬¡ã€‚
                return_dict (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`):
                    æ˜¯å¦è¿”å› [`~models.autoencoder_kl.AutoencoderKLOutput`] è€Œéç®€å•å…ƒç»„ã€‚
    
            è¿”å›ï¼š
                    ç¼–ç å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚å¦‚æœ `return_dict` ä¸º Trueï¼Œåˆ™è¿”å›ä¸€ä¸ª
                    [`~models.autoencoder_kl.AutoencoderKLOutput`]ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªæ™®é€šçš„ `tuple`ã€‚
            """
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¹³é“ºï¼Œå¹¶ä¸”è¾“å…¥å°ºå¯¸è¶…è¿‡æœ€å°å¹³é“ºå°ºå¯¸
            if self.use_tiling and (x.shape[-1] > self.tile_sample_min_size or x.shape[-2] > self.tile_sample_min_size):
                # ä½¿ç”¨å¹³é“ºç¼–ç æ–¹æ³•å¤„ç†è¾“å…¥
                return self.tiled_encode(x, return_dict=return_dict)
    
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ‡ç‰‡ï¼Œå¹¶ä¸”è¾“å…¥æ‰¹æ¬¡å¤§äº1
            if self.use_slicing and x.shape[0] > 1:
                # å¯¹è¾“å…¥çš„æ¯ä¸ªåˆ‡ç‰‡è¿›è¡Œç¼–ç ï¼Œå¹¶å°†ç»“æœè¿æ¥èµ·æ¥
                encoded_slices = [self.encoder(x_slice) for x_slice in x.split(1)]
                h = torch.cat(encoded_slices)
            else:
                # ç›´æ¥ç¼–ç æ•´ä¸ªè¾“å…¥
                h = self.encoder(x)
    
            # æ£€æŸ¥é‡åŒ–å·ç§¯æ˜¯å¦å­˜åœ¨
            if self.quant_conv is not None:
                # ä½¿ç”¨é‡åŒ–å·ç§¯å¤„ç†ç¼–ç åçš„ç»“æœ
                moments = self.quant_conv(h)
            else:
                # å¦‚æœä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨ç¼–ç ç»“æœ
                moments = h
    
            # åˆ›å»ºå¯¹è§’é«˜æ–¯åˆ†å¸ƒçš„åéªŒ
            posterior = DiagonalGaussianDistribution(moments)
    
            # å¦‚æœä¸è¿”å›å­—å…¸ï¼Œè¿”å›åéªŒåˆ†å¸ƒçš„å…ƒç»„
            if not return_dict:
                return (posterior,)
    
            # è¿”å› AutoencoderKLOutput å¯¹è±¡ï¼ŒåŒ…å«æ½œåœ¨åˆ†å¸ƒ
            return AutoencoderKLOutput(latent_dist=posterior)
    
        # å®šä¹‰è§£ç å‡½æ•°ï¼Œè¿”å›ç±»å‹ä¸º DecoderOutput æˆ– torch.Tensor
        def _decode(self, z: torch.Tensor, return_dict: bool = True) -> Union[DecoderOutput, torch.Tensor]:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¹³é“ºï¼Œå¹¶ä¸”æ½œåœ¨å‘é‡å°ºå¯¸è¶…è¿‡æœ€å°å¹³é“ºå°ºå¯¸
            if self.use_tiling and (z.shape[-1] > self.tile_latent_min_size or z.shape[-2] > self.tile_latent_min_size):
                # ä½¿ç”¨å¹³é“ºè§£ç æ–¹æ³•å¤„ç†æ½œåœ¨å‘é‡
                return self.tiled_decode(z, return_dict=return_dict)
    
            # æ£€æŸ¥åé‡åŒ–å·ç§¯æ˜¯å¦å­˜åœ¨
            if self.post_quant_conv is not None:
                # ä½¿ç”¨åé‡åŒ–å·ç§¯å¤„ç†æ½œåœ¨å‘é‡
                z = self.post_quant_conv(z)
    
            # é€šè¿‡è§£ç å™¨è§£ç æ½œåœ¨å‘é‡
            dec = self.decoder(z)
    
            # å¦‚æœä¸è¿”å›å­—å…¸ï¼Œè¿”å›è§£ç ç»“æœçš„å…ƒç»„
            if not return_dict:
                return (dec,)
    
            # è¿”å›è§£ç ç»“æœçš„ DecoderOutput å¯¹è±¡
            return DecoderOutput(sample=dec)
    
        # åº”ç”¨å‰å‘é’©å­çš„è§£ç å‡½æ•°
        @apply_forward_hook
        def decode(
            self, z: torch.FloatTensor, return_dict: bool = True, generator=None
        ) -> Union[DecoderOutput, torch.FloatTensor]:
            """
            è§£ç ä¸€æ‰¹å›¾åƒã€‚
    
            å‚æ•°ï¼š
                z (`torch.Tensor`): è¾“å…¥æ½œåœ¨å‘é‡çš„æ‰¹æ¬¡ã€‚
                return_dict (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`):
                    æ˜¯å¦è¿”å› [`~models.vae.DecoderOutput`] è€Œéç®€å•å…ƒç»„ã€‚
    
            è¿”å›ï¼š
                [`~models.vae.DecoderOutput`] æˆ– `tuple`:
                    å¦‚æœ return_dict ä¸º Trueï¼Œè¿”å› [`~models.vae.DecoderOutput`]ï¼Œå¦åˆ™è¿”å›æ™®é€š `tuple`ã€‚
            """
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ‡ç‰‡ï¼Œå¹¶ä¸”æ½œåœ¨å‘é‡æ‰¹æ¬¡å¤§äº1
            if self.use_slicing and z.shape[0] > 1:
                # å¯¹æ¯ä¸ªåˆ‡ç‰‡è¿›è¡Œè§£ç ï¼Œå¹¶å°†ç»“æœè¿æ¥èµ·æ¥
                decoded_slices = [self._decode(z_slice).sample for z_slice in z.split(1)]
                decoded = torch.cat(decoded_slices)
            else:
                # ç›´æ¥è§£ç æ•´ä¸ªæ½œåœ¨å‘é‡
                decoded = self._decode(z).sample
    
            # å¦‚æœä¸è¿”å›å­—å…¸ï¼Œè¿”å›è§£ç ç»“æœçš„å…ƒç»„
            if not return_dict:
                return (decoded,)
    
            # è¿”å›è§£ç ç»“æœçš„ DecoderOutput å¯¹è±¡
            return DecoderOutput(sample=decoded)
    # å®šä¹‰ä¸€ä¸ªå‚ç›´æ··åˆå‡½æ•°ï¼Œæ¥å—ä¸¤ä¸ªå¼ é‡å’Œæ··åˆèŒƒå›´ï¼Œè¿”å›æ··åˆåçš„å¼ é‡
        def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
            # è®¡ç®—å®é™…çš„æ··åˆèŒƒå›´ï¼Œç¡®ä¿ä¸è¶…è¿‡è¾“å…¥å¼ é‡çš„å°ºå¯¸
            blend_extent = min(a.shape[2], b.shape[2], blend_extent)
            # é€è¡Œè¿›è¡Œæ··åˆæ“ä½œï¼Œæ ¹æ®å½“å‰è¡Œçš„æ¯”ä¾‹è®¡ç®—æ··åˆå€¼
            for y in range(blend_extent):
                b[:, :, y, :] = a[:, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, y, :] * (y / blend_extent)
            # è¿”å›æ··åˆåçš„å¼ é‡
            return b
    
    # å®šä¹‰ä¸€ä¸ªæ°´å¹³æ··åˆå‡½æ•°ï¼Œæ¥å—ä¸¤ä¸ªå¼ é‡å’Œæ··åˆèŒƒå›´ï¼Œè¿”å›æ··åˆåçš„å¼ é‡
        def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
            # è®¡ç®—å®é™…çš„æ··åˆèŒƒå›´ï¼Œç¡®ä¿ä¸è¶…è¿‡è¾“å…¥å¼ é‡çš„å°ºå¯¸
            blend_extent = min(a.shape[3], b.shape[3], blend_extent)
            # é€åˆ—è¿›è¡Œæ··åˆæ“ä½œï¼Œæ ¹æ®å½“å‰åˆ—çš„æ¯”ä¾‹è®¡ç®—æ··åˆå€¼
            for x in range(blend_extent):
                b[:, :, :, x] = a[:, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, x] * (x / blend_extent)
            # è¿”å›æ··åˆåçš„å¼ é‡
            return b
    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºé€šè¿‡å¹³é“ºç¼–ç å™¨å¯¹å›¾åƒæ‰¹æ¬¡è¿›è¡Œç¼–ç 
    def tiled_encode(self, x: torch.Tensor, return_dict: bool = True) -> AutoencoderKLOutput:
        # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°è¯¥å‡½æ•°çš„ç”¨é€”åŠå‚æ•°
        r"""Encode a batch of images using a tiled encoder.
    
        å½“è¿™ä¸ªé€‰é¡¹å¯ç”¨æ—¶ï¼ŒVAE ä¼šå°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªå°å—ä»¥è¿›è¡Œç¼–ç 
        æ­¥éª¤ã€‚è¿™å¯¹äºä¿æŒå†…å­˜ä½¿ç”¨é‡æ’å®šéå¸¸æœ‰ç”¨ã€‚å¹³é“ºç¼–ç çš„æœ€ç»ˆç»“æœä¸éå¹³é“ºç¼–ç ä¸åŒï¼Œ
        å› ä¸ºæ¯ä¸ªå°å—ä½¿ç”¨ä¸åŒçš„ç¼–ç å™¨ã€‚ä¸ºäº†é¿å…å¹³é“ºä¼ªå½±ï¼Œå°å—ä¹‹é—´ä¼šé‡å å¹¶æ··åˆåœ¨ä¸€èµ·
        å½¢æˆå¹³æ»‘çš„è¾“å‡ºã€‚ä½ å¯èƒ½ä»ç„¶ä¼šçœ‹åˆ°ä¸å°å—å¤§å°ç›¸å…³çš„å˜åŒ–ï¼Œ
        ä½†è¿™äº›å˜åŒ–åº”è¯¥ä¸é‚£ä¹ˆæ˜æ˜¾ã€‚
    
        å‚æ•°:
            x (`torch.Tensor`): è¾“å…¥å›¾åƒæ‰¹æ¬¡ã€‚
            return_dict (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`):
                æ˜¯å¦è¿”å›ä¸€ä¸ª [`~models.autoencoder_kl.AutoencoderKLOutput`] è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚
    
        è¿”å›:
            [`~models.autoencoder_kl.AutoencoderKLOutput`] æˆ– `tuple`:
                å¦‚æœ return_dict ä¸º Trueï¼Œåˆ™è¿”å› [`~models.autoencoder_kl.AutoencoderKLOutput`]ï¼Œ
                å¦åˆ™è¿”å›æ™®é€šå…ƒç»„ã€‚
        """
        # è®¡ç®—é‡å åŒºåŸŸçš„å¤§å°
        overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor))
        # è®¡ç®—æ··åˆçš„èŒƒå›´
        blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)
        # è®¡ç®—è¡Œé™åˆ¶ï¼Œç¡®ä¿ä¸ä¼šè¶…å‡ºèŒƒå›´
        row_limit = self.tile_latent_min_size - blend_extent
    
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ä»¥å­˜å‚¨æ¯ä¸€è¡Œçš„ç¼–ç ç»“æœ
        rows = []
        # éå†è¾“å…¥å¼ é‡çš„é«˜åº¦ï¼Œä»¥é‡å çš„æ–¹å¼è¿›è¡Œåˆ‡ç‰‡
        for i in range(0, x.shape[2], overlap_size):
            # åˆå§‹åŒ–å½“å‰è¡Œçš„ç¼–ç ç»“æœåˆ—è¡¨
            row = []
            # éå†è¾“å…¥å¼ é‡çš„å®½åº¦ï¼Œä»¥é‡å çš„æ–¹å¼è¿›è¡Œåˆ‡ç‰‡
            for j in range(0, x.shape[3], overlap_size):
                # åˆ‡å‰²å½“å‰å°å—
                tile = x[:, :, i : i + self.tile_sample_min_size, j : j + self.tile_sample_min_size]
                # å¯¹å½“å‰å°å—è¿›è¡Œç¼–ç 
                tile = self.encoder(tile)
                # å¦‚æœé…ç½®ä½¿ç”¨é‡åŒ–å·ç§¯ï¼Œåˆ™å¯¹å°å—è¿›è¡Œé‡åŒ–å¤„ç†
                if self.config.use_quant_conv:
                    tile = self.quant_conv(tile)
                # å°†ç¼–ç åçš„å°å—æ·»åŠ åˆ°å½“å‰è¡Œä¸­
                row.append(tile)
            # å°†å½“å‰è¡Œçš„ç»“æœæ·»åŠ åˆ° rows åˆ—è¡¨ä¸­
            rows.append(row)
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ä»¥å­˜å‚¨æœ€ç»ˆçš„ç»“æœè¡Œ
        result_rows = []
        # éå†æ‰€æœ‰è¡Œä»¥è¿›è¡Œæ··åˆå¤„ç†
        for i, row in enumerate(rows):
            result_row = []
            # éå†å½“å‰è¡Œçš„æ¯ä¸ªå°å—
            for j, tile in enumerate(row):
                # å°†ä¸Šæ–¹å°å—ä¸å½“å‰å°å—æ··åˆ
                if i > 0:
                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)
                # å°†å·¦ä¾§å°å—ä¸å½“å‰å°å—æ··åˆ
                if j > 0:
                    tile = self.blend_h(row[j - 1], tile, blend_extent)
                # å°†æ··åˆåçš„å°å—è£å‰ªè‡³æŒ‡å®šå¤§å°å¹¶æ·»åŠ åˆ°ç»“æœè¡Œ
                result_row.append(tile[:, :, :row_limit, :row_limit])
            # å°†å½“å‰è¡Œçš„ç»“æœåˆå¹¶å¹¶æ·»åŠ åˆ°æœ€ç»ˆç»“æœä¸­
            result_rows.append(torch.cat(result_row, dim=3))
    
        # å°†æ‰€æœ‰ç»“æœè¡Œåˆå¹¶ä¸ºä¸€ä¸ªå¼ é‡
        moments = torch.cat(result_rows, dim=2)
        # åˆ›å»ºä¸€ä¸ªå¯¹è§’é«˜æ–¯åˆ†å¸ƒä»¥è¡¨ç¤ºåéªŒåˆ†å¸ƒ
        posterior = DiagonalGaussianDistribution(moments)
    
        # å¦‚æœä¸è¿”å›å­—å…¸ï¼Œåˆ™è¿”å›åéªŒåˆ†å¸ƒçš„å…ƒç»„
        if not return_dict:
            return (posterior,)
    
        # è¿”å›åŒ…å«åéªŒåˆ†å¸ƒçš„ AutoencoderKLOutput å¯¹è±¡
        return AutoencoderKLOutput(latent_dist=posterior)
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºè§£ç ä¸€æ‰¹å›¾åƒï¼Œä½¿ç”¨å¹³é“ºè§£ç å™¨
    def tiled_decode(self, z: torch.Tensor, return_dict: bool = True) -> Union[DecoderOutput, torch.Tensor]:
        r"""
        ä½¿ç”¨å¹³é“ºè§£ç å™¨è§£ç ä¸€æ‰¹å›¾åƒã€‚

        å‚æ•°ï¼š
            z (`torch.Tensor`): è¾“å…¥çš„æ½œåœ¨å‘é‡æ‰¹æ¬¡ã€‚
            return_dict (`bool`, *å¯é€‰*, é»˜è®¤å€¼ä¸º `True`):
                æ˜¯å¦è¿”å›ä¸€ä¸ª [`~models.vae.DecoderOutput`] è€Œä¸æ˜¯æ™®é€šçš„å…ƒç»„ã€‚

        è¿”å›ï¼š
            [`~models.vae.DecoderOutput`] æˆ– `tuple`:
                å¦‚æœ return_dict ä¸º Trueï¼Œåˆ™è¿”å›ä¸€ä¸ª [`~models.vae.DecoderOutput`]ï¼Œ
                å¦åˆ™è¿”å›æ™®é€šçš„ `tuple`ã€‚
        """
        # è®¡ç®—é‡å åŒºåŸŸçš„å¤§å°
        overlap_size = int(self.tile_latent_min_size * (1 - self.tile_overlap_factor))
        # è®¡ç®—æ··åˆåŒºåŸŸçš„èŒƒå›´
        blend_extent = int(self.tile_sample_min_size * self.tile_overlap_factor)
        # è®¡ç®—æ¯è¡Œçš„é™åˆ¶å¤§å°
        row_limit = self.tile_sample_min_size - blend_extent

        # å°† z åˆ†å‰²æˆé‡å çš„ 64x64 ç“¦ç‰‡ï¼Œå¹¶åˆ†åˆ«è§£ç 
        # ç“¦ç‰‡ä¹‹é—´æœ‰é‡å ï¼Œä»¥é¿å…ç“¦ç‰‡ä¹‹é—´çš„æ¥ç¼
        rows = []
        # éå†æ½œåœ¨å‘é‡ z çš„é«˜åº¦ï¼ŒæŒ‰é‡å å¤§å°æ­¥è¿›
        for i in range(0, z.shape[2], overlap_size):
            row = []  # å­˜å‚¨å½“å‰è¡Œçš„è§£ç ç»“æœ
            # éå†æ½œåœ¨å‘é‡ z çš„å®½åº¦ï¼ŒæŒ‰é‡å å¤§å°æ­¥è¿›
            for j in range(0, z.shape[3], overlap_size):
                # ä» z ä¸­æå–å½“å‰ç“¦ç‰‡
                tile = z[:, :, i : i + self.tile_latent_min_size, j : j + self.tile_latent_min_size]
                # å¦‚æœé…ç½®ä¸­å¯ç”¨äº†åé‡åŒ–å·ç§¯ï¼Œåˆ™å¯¹ç“¦ç‰‡è¿›è¡Œå¤„ç†
                if self.config.use_post_quant_conv:
                    tile = self.post_quant_conv(tile)
                # è§£ç å½“å‰ç“¦ç‰‡
                decoded = self.decoder(tile)
                # å°†è§£ç ç»“æœæ·»åŠ åˆ°å½“å‰è¡Œä¸­
                row.append(decoded)
            # å°†å½“å‰è¡Œæ·»åŠ åˆ°æ€»è¡Œåˆ—è¡¨ä¸­
            rows.append(row)
        result_rows = []  # å­˜å‚¨æœ€ç»ˆç»“æœçš„è¡Œ
        # éå†è§£ç çš„æ¯ä¸€è¡Œ
        for i, row in enumerate(rows):
            result_row = []  # å­˜å‚¨å½“å‰ç»“æœè¡Œ
            # éå†å½“å‰è¡Œçš„ç“¦ç‰‡
            for j, tile in enumerate(row):
                # å°†ä¸Šæ–¹çš„ç“¦ç‰‡ä¸å½“å‰ç“¦ç‰‡æ··åˆ
                if i > 0:
                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)
                # å°†å·¦ä¾§çš„ç“¦ç‰‡ä¸å½“å‰ç“¦ç‰‡æ··åˆ
                if j > 0:
                    tile = self.blend_h(row[j - 1], tile, blend_extent)
                # å°†å½“å‰ç“¦ç‰‡çš„ç»“æœè£å‰ªåˆ°é™åˆ¶å¤§å°å¹¶æ·»åŠ åˆ°ç»“æœè¡Œ
                result_row.append(tile[:, :, :row_limit, :row_limit])
            # å°†ç»“æœè¡Œä¸­çš„ç“¦ç‰‡æ²¿ç€å®½åº¦æ‹¼æ¥
            result_rows.append(torch.cat(result_row, dim=3))

        # å°†æ‰€æœ‰ç»“æœè¡Œæ²¿ç€é«˜åº¦æ‹¼æ¥
        dec = torch.cat(result_rows, dim=2)
        # å¦‚æœä¸è¿”å›å­—å…¸ï¼Œåˆ™è¿”å›è§£ç ç»“æœçš„å…ƒç»„
        if not return_dict:
            return (dec,)

        # è¿”å›è§£ç ç»“æœçš„ DecoderOutput å¯¹è±¡
        return DecoderOutput(sample=dec)

    # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•
    def forward(
        # è¾“å…¥æ ·æœ¬çš„å¼ é‡
        sample: torch.Tensor,
        # æ˜¯å¦å¯¹æ ·æœ¬è¿›è¡ŒåéªŒé‡‡æ ·
        sample_posterior: bool = False,
        # æ˜¯å¦è¿”å›å­—å…¸å½¢å¼çš„ç»“æœ
        return_dict: bool = True,
        # éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆå¯é€‰ï¼‰
        generator: Optional[torch.Generator] = None,
    ) -> Union[DecoderOutput, torch.Tensor]:
        r"""  # å‡½æ•°çš„è¿”å›ç±»å‹æ˜¯ DecoderOutput æˆ– torch.Tensor çš„è”åˆç±»å‹
        Args:  # å‚æ•°è¯´æ˜
            sample (`torch.Tensor`): Input sample.  # è¾“å…¥æ ·æœ¬ï¼Œç±»å‹ä¸º torch.Tensor
            sample_posterior (`bool`, *optional*, defaults to `False`):  # æ˜¯å¦ä»åéªŒåˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œé»˜è®¤ä¸º False
                Whether to sample from the posterior.  # æè¿°å‚æ•°çš„ç”¨é€”
            return_dict (`bool`, *optional*, defaults to `True`):  # æ˜¯å¦è¿”å› DecoderOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ï¼Œé»˜è®¤ä¸º True
                Whether or not to return a [`DecoderOutput`] instead of a plain tuple.  # æè¿°å‚æ•°çš„ç”¨é€”
        """
        x = sample  # å°†è¾“å…¥æ ·æœ¬èµ‹å€¼ç»™ x
        posterior = self.encode(x).latent_dist  # å¯¹è¾“å…¥æ ·æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶è·å–å…¶åéªŒåˆ†å¸ƒ
        if sample_posterior:  # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»åéªŒåˆ†å¸ƒä¸­é‡‡æ ·
            z = posterior.sample(generator=generator)  # ä»åéªŒåˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·
        else:  # å¦åˆ™
            z = posterior.mode()  # å–åéªŒåˆ†å¸ƒçš„ä¼—æ•°
        dec = self.decode(z).sample  # è§£ç  z å¹¶è·å–æ ·æœ¬

        if not return_dict:  # å¦‚æœä¸éœ€è¦è¿”å›å­—å…¸
            return (dec,)  # è¿”å›æ ·æœ¬ä½œä¸ºå…ƒç»„

        return DecoderOutput(sample=dec)  # è¿”å› DecoderOutput å¯¹è±¡ï¼ŒåŒ…å«è§£ç åçš„æ ·æœ¬

    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections
    def fuse_qkv_projections(self):  # å®šä¹‰èåˆ QKV æŠ•å½±çš„æ–¹æ³•
        """  # æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²
        Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query, key, value)  # å¯ç”¨èåˆçš„ QKV æŠ•å½±ï¼Œé€‚ç”¨äºè‡ªæ³¨æ„åŠ›æ¨¡å—
        are fused. For cross-attention modules, key and value projection matrices are fused.  # é€‚ç”¨äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—

        <Tip warning={true}>  # æç¤ºæ ‡ç­¾ï¼Œè¡¨ç¤ºæ­¤ API ä¸ºå®éªŒæ€§
        This API is ğŸ§ª experimental.  # æç¤ºå†…å®¹
        </Tip>
        """
        self.original_attn_processors = None  # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None

        for _, attn_processor in self.attn_processors.items():  # éå†å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨
            if "Added" in str(attn_processor.__class__.__name__):  # æ£€æŸ¥å¤„ç†å™¨ç±»åä¸­æ˜¯å¦åŒ…å« "Added"
                raise ValueError("`fuse_qkv_projections()` is not supported for models having added KV projections.")  # æŠ›å‡ºå¼‚å¸¸ï¼Œæç¤ºä¸æ”¯æŒèåˆæ“ä½œ

        self.original_attn_processors = self.attn_processors  # ä¿å­˜å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨

        for module in self.modules():  # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨¡å—
            if isinstance(module, Attention):  # å¦‚æœæ¨¡å—æ˜¯ Attention ç±»å‹
                module.fuse_projections(fuse=True)  # èåˆå…¶æŠ•å½±

        self.set_attn_processor(FusedAttnProcessor2_0())  # è®¾ç½®èåˆçš„æ³¨æ„åŠ›å¤„ç†å™¨

    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections
    def unfuse_qkv_projections(self):  # å®šä¹‰å–æ¶ˆèåˆ QKV æŠ•å½±çš„æ–¹æ³•
        """Disables the fused QKV projection if enabled.  # å¦‚æœå·²å¯ç”¨ï¼Œç¦ç”¨èåˆçš„ QKV æŠ•å½±

        <Tip warning={true}>  # æç¤ºæ ‡ç­¾ï¼Œè¡¨ç¤ºæ­¤ API ä¸ºå®éªŒæ€§
        This API is ğŸ§ª experimental.  # æç¤ºå†…å®¹
        </Tip>

        """
        if self.original_attn_processors is not None:  # å¦‚æœåŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸ä¸º None
            self.set_attn_processor(self.original_attn_processors)  # æ¢å¤åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨
```