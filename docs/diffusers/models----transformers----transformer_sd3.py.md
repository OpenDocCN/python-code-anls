# `.\diffusers\models\transformers\transformer_sd3.py`

```
# ç‰ˆæƒæ‰€æœ‰ 2024 Stability AI, The HuggingFace Team å’Œ The InstantX Teamã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è®¸å¯ï¼›
# é™¤ééµå¾ªè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åè®®å¦æœ‰è§„å®šï¼ŒæŒ‰ç…§è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯ä»¥â€œåŸæ ·â€åŸºç¡€æä¾›çš„ï¼Œ
# ä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºçš„è¿˜æ˜¯æš—ç¤ºçš„ã€‚
# æœ‰å…³è®¸å¯è¯ä¸‹æƒé™å’Œé™åˆ¶çš„å…·ä½“è¯­è¨€ï¼Œè¯·å‚è§è®¸å¯è¯ã€‚


from typing import Any, Dict, List, Optional, Union  # ä» typing æ¨¡å—å¯¼å…¥å„ç§ç±»å‹æ³¨é‡Š

import torch  # å¯¼å…¥ PyTorch åº“
import torch.nn as nn  # å¯¼å…¥ PyTorch çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œå¹¶å‘½åä¸º nn

from ...configuration_utils import ConfigMixin, register_to_config  # ä»é…ç½®å·¥å…·å¯¼å…¥é…ç½®æ··åˆç±»å’Œæ³¨å†Œå‡½æ•°
from ...loaders import FromOriginalModelMixin, PeftAdapterMixin  # ä»åŠ è½½å™¨å¯¼å…¥æ¨¡å‹æ··åˆç±»
from ...models.attention import JointTransformerBlock  # ä»æ³¨æ„åŠ›æ¨¡å—å¯¼å…¥è”åˆå˜æ¢å™¨å—
from ...models.attention_processor import Attention, AttentionProcessor, FusedJointAttnProcessor2_0  # å¯¼å…¥ä¸åŒçš„æ³¨æ„åŠ›å¤„ç†å™¨
from ...models.modeling_utils import ModelMixin  # å¯¼å…¥æ¨¡å‹æ··åˆç±»
from ...models.normalization import AdaLayerNormContinuous  # å¯¼å…¥è‡ªé€‚åº”å±‚å½’ä¸€åŒ–æ¨¡å—
from ...utils import USE_PEFT_BACKEND, is_torch_version, logging, scale_lora_layers, unscale_lora_layers  # å¯¼å…¥å·¥å…·å‡½æ•°å’Œå˜é‡
from ..embeddings import CombinedTimestepTextProjEmbeddings, PatchEmbed  # ä»åµŒå…¥æ¨¡å—å¯¼å…¥åµŒå…¥ç±»
from ..modeling_outputs import Transformer2DModelOutput  # å¯¼å…¥å˜æ¢å™¨ 2D æ¨¡å‹è¾“å‡ºç±»


logger = logging.get_logger(__name__)  # åˆ›å»ºä¸€ä¸ªè®°å½•å™¨å®ä¾‹ï¼Œåç§°ä¸ºå½“å‰æ¨¡å—åï¼Œç¦ç”¨ pylint å¯¹åç§°çš„è­¦å‘Š


class SD3Transformer2DModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin):  # å®šä¹‰ SD3 å˜æ¢å™¨ 2D æ¨¡å‹ç±»ï¼Œç»§æ‰¿å¤šä¸ªæ··åˆç±»
    """
    Stable Diffusion 3 ä¸­å¼•å…¥çš„å˜æ¢å™¨æ¨¡å‹ã€‚

    å‚è€ƒæ–‡çŒ®: https://arxiv.org/abs/2403.03206

    å‚æ•°ï¼š
        sample_size (`int`): æ½œåœ¨å›¾åƒçš„å®½åº¦ã€‚è®­ç»ƒæœŸé—´å›ºå®šä½¿ç”¨ï¼Œå› ä¸º
            å®ƒç”¨äºå­¦ä¹ ä¸€ç»„ä½ç½®åµŒå…¥ã€‚
        patch_size (`int`): å°†è¾“å…¥æ•°æ®è½¬åŒ–ä¸ºå°å—çš„å—å¤§å°ã€‚
        in_channels (`int`, *å¯é€‰*, é»˜è®¤ä¸º 16): è¾“å…¥çš„é€šé“æ•°é‡ã€‚
        num_layers (`int`, *å¯é€‰*, é»˜è®¤ä¸º 18): ä½¿ç”¨çš„å˜æ¢å™¨å—å±‚æ•°ã€‚
        attention_head_dim (`int`, *å¯é€‰*, é»˜è®¤ä¸º 64): æ¯ä¸ªå¤´çš„é€šé“æ•°é‡ã€‚
        num_attention_heads (`int`, *å¯é€‰*, é»˜è®¤ä¸º 18): å¤šå¤´æ³¨æ„åŠ›ä½¿ç”¨çš„å¤´æ•°ã€‚
        cross_attention_dim (`int`, *å¯é€‰*): ç”¨äº `encoder_hidden_states` ç»´åº¦çš„æ•°é‡ã€‚
        caption_projection_dim (`int`): ç”¨äºæŠ•å½± `encoder_hidden_states` çš„ç»´åº¦æ•°é‡ã€‚
        pooled_projection_dim (`int`): ç”¨äºæŠ•å½± `pooled_projections` çš„ç»´åº¦æ•°é‡ã€‚
        out_channels (`int`, é»˜è®¤ä¸º 16): è¾“å‡ºé€šé“çš„æ•°é‡ã€‚

    """

    _supports_gradient_checkpointing = True  # è¡¨ç¤ºæ¨¡å‹æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½

    @register_to_config  # ä½¿ç”¨è£…é¥°å™¨å°†æ­¤æ–¹æ³•æ³¨å†Œåˆ°é…ç½®ä¸­
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®æ¨¡å‹çš„åŸºæœ¬å‚æ•°
        def __init__(
            self,
            sample_size: int = 128,  # è¾“å…¥æ ·æœ¬çš„å¤§å°ï¼Œé»˜è®¤å€¼ä¸º128
            patch_size: int = 2,  # æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼Œé»˜è®¤å€¼ä¸º2
            in_channels: int = 16,  # è¾“å…¥é€šé“æ•°ï¼Œé»˜è®¤å€¼ä¸º16
            num_layers: int = 18,  # Transformerå±‚çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º18
            attention_head_dim: int = 64,  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º64
            num_attention_heads: int = 18,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º18
            joint_attention_dim: int = 4096,  # è”åˆæ³¨æ„åŠ›ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º4096
            caption_projection_dim: int = 1152,  # æ ‡é¢˜æŠ•å½±ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º1152
            pooled_projection_dim: int = 2048,  # æ± åŒ–æŠ•å½±ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º2048
            out_channels: int = 16,  # è¾“å‡ºé€šé“æ•°ï¼Œé»˜è®¤å€¼ä¸º16
            pos_embed_max_size: int = 96,  # ä½ç½®åµŒå…¥çš„æœ€å¤§å¤§å°ï¼Œé»˜è®¤å€¼ä¸º96
        ):
            super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
            default_out_channels = in_channels  # è®¾ç½®é»˜è®¤çš„è¾“å‡ºé€šé“ä¸ºè¾“å…¥é€šé“æ•°
            # å¦‚æœæŒ‡å®šè¾“å‡ºé€šé“ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šå€¼ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
            self.out_channels = out_channels if out_channels is not None else default_out_channels
            # è®¡ç®—å†…éƒ¨ç»´åº¦ï¼Œç­‰äºæ³¨æ„åŠ›å¤´æ•°é‡ä¹˜ä»¥æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
            self.inner_dim = self.config.num_attention_heads * self.config.attention_head_dim
    
            # åˆ›å»ºä½ç½®åµŒå…¥æ¨¡å—ï¼Œç”¨äºå°†è¾“å…¥å›¾åƒè½¬ä¸ºåµŒå…¥è¡¨ç¤º
            self.pos_embed = PatchEmbed(
                height=self.config.sample_size,  # é«˜åº¦è®¾ç½®ä¸ºæ ·æœ¬å¤§å°
                width=self.config.sample_size,  # å®½åº¦è®¾ç½®ä¸ºæ ·æœ¬å¤§å°
                patch_size=self.config.patch_size,  # è¡¥ä¸å¤§å°
                in_channels=self.config.in_channels,  # è¾“å…¥é€šé“æ•°
                embed_dim=self.inner_dim,  # åµŒå…¥ç»´åº¦
                pos_embed_max_size=pos_embed_max_size,  # å½“å‰ç¡¬ç¼–ç ä½ç½®åµŒå…¥æœ€å¤§å¤§å°
            )
            # åˆ›å»ºæ—¶é—´ä¸æ–‡æœ¬åµŒå…¥çš„ç»„åˆæ¨¡å—
            self.time_text_embed = CombinedTimestepTextProjEmbeddings(
                embedding_dim=self.inner_dim,  # åµŒå…¥ç»´åº¦
                pooled_projection_dim=self.config.pooled_projection_dim  # æ± åŒ–æŠ•å½±ç»´åº¦
            )
            # åˆ›å»ºçº¿æ€§å±‚ï¼Œç”¨äºå°†ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜ å°„åˆ°æ ‡é¢˜æŠ•å½±ç»´åº¦
            self.context_embedder = nn.Linear(self.config.joint_attention_dim, self.config.caption_projection_dim)
    
            # åˆ›å»ºTransformerå—çš„åˆ—è¡¨
            self.transformer_blocks = nn.ModuleList(
                [
                    JointTransformerBlock(
                        dim=self.inner_dim,  # è¾“å…¥ç»´åº¦ä¸ºå†…éƒ¨ç»´åº¦
                        num_attention_heads=self.config.num_attention_heads,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡
                        attention_head_dim=self.config.attention_head_dim,  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
                        context_pre_only=i == num_layers - 1,  # ä»…åœ¨æœ€åä¸€å±‚è®¾ç½®ä¸Šä¸‹æ–‡ä¼˜å…ˆ
                    )
                    for i in range(self.config.num_layers)  # éå†åˆ›å»ºæ¯ä¸€å±‚
                ]
            )
    
            # åˆ›å»ºè‡ªé€‚åº”å±‚å½’ä¸€åŒ–å±‚
            self.norm_out = AdaLayerNormContinuous(self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6)
            # åˆ›å»ºçº¿æ€§å±‚ï¼Œç”¨äºè¾“å‡ºæ˜ å°„
            self.proj_out = nn.Linear(self.inner_dim, patch_size * patch_size * self.out_channels, bias=True)
    
            # è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹å¼€å…³ï¼Œé»˜è®¤å€¼ä¸ºFalse
            self.gradient_checkpointing = False
    
        # ä»diffusers.models.unets.unet_3d_condition.UNet3DConditionModel.enable_forward_chunkingå¤åˆ¶çš„æ–¹æ³•
    # å®šä¹‰ä¸€ä¸ªå¯ç”¨å‰é¦ˆåˆ†å—çš„å‡½æ•°ï¼Œæ¥å—å¯é€‰çš„åˆ†å—å¤§å°å’Œç»´åº¦
        def enable_forward_chunking(self, chunk_size: Optional[int] = None, dim: int = 0) -> None:
            """
            è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä½¿ç”¨å‰é¦ˆåˆ†å—æœºåˆ¶ã€‚
    
            å‚æ•°ï¼š
                chunk_size (`int`, *å¯é€‰*):
                    å‰é¦ˆå±‚çš„åˆ†å—å¤§å°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†å•ç‹¬åœ¨ç»´åº¦ä¸º`dim`çš„æ¯ä¸ªå¼ é‡ä¸Šè¿è¡Œå‰é¦ˆå±‚ã€‚
                dim (`int`, *å¯é€‰*, é»˜è®¤å€¼ä¸º`0`):
                    å‰é¦ˆè®¡ç®—åº”åˆ†å—çš„ç»´åº¦ã€‚é€‰æ‹©dim=0ï¼ˆæ‰¹é‡ï¼‰æˆ–dim=1ï¼ˆåºåˆ—é•¿åº¦ï¼‰ã€‚
            """
            # å¦‚æœç»´åº¦ä¸æ˜¯0æˆ–1ï¼Œåˆ™æŠ›å‡ºå€¼é”™è¯¯
            if dim not in [0, 1]:
                raise ValueError(f"Make sure to set `dim` to either 0 or 1, not {dim}")
    
            # é»˜è®¤åˆ†å—å¤§å°ä¸º1
            chunk_size = chunk_size or 1
    
            # å®šä¹‰é€’å½’å‰é¦ˆå‡½æ•°ï¼Œæ¥å—æ¨¡å—ã€åˆ†å—å¤§å°å’Œç»´åº¦ä½œä¸ºå‚æ•°
            def fn_recursive_feed_forward(module: torch.nn.Module, chunk_size: int, dim: int):
                # å¦‚æœæ¨¡å—æœ‰è®¾ç½®åˆ†å—å‰é¦ˆçš„å±æ€§ï¼Œåˆ™è°ƒç”¨è¯¥æ–¹æ³•
                if hasattr(module, "set_chunk_feed_forward"):
                    module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
    
                # éå†æ¨¡å—çš„å­æ¨¡å—å¹¶é€’å½’è°ƒç”¨å‰é¦ˆå‡½æ•°
                for child in module.children():
                    fn_recursive_feed_forward(child, chunk_size, dim)
    
            # éå†å½“å‰å¯¹è±¡çš„å­æ¨¡å—ï¼Œåº”ç”¨é€’å½’å‰é¦ˆå‡½æ•°
            for module in self.children():
                fn_recursive_feed_forward(module, chunk_size, dim)
    
        # ä»diffusers.models.unets.unet_3d_conditionå¤åˆ¶çš„æ–¹æ³•ï¼Œç¦ç”¨å‰é¦ˆåˆ†å—
        def disable_forward_chunking(self):
            # å®šä¹‰é€’å½’å‰é¦ˆå‡½æ•°ï¼Œæ¥å—æ¨¡å—ã€åˆ†å—å¤§å°å’Œç»´åº¦ä½œä¸ºå‚æ•°
            def fn_recursive_feed_forward(module: torch.nn.Module, chunk_size: int, dim: int):
                # å¦‚æœæ¨¡å—æœ‰è®¾ç½®åˆ†å—å‰é¦ˆçš„å±æ€§ï¼Œåˆ™è°ƒç”¨è¯¥æ–¹æ³•
                if hasattr(module, "set_chunk_feed_forward"):
                    module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
    
                # éå†æ¨¡å—çš„å­æ¨¡å—å¹¶é€’å½’è°ƒç”¨å‰é¦ˆå‡½æ•°
                for child in module.children():
                    fn_recursive_feed_forward(child, chunk_size, dim)
    
            # éå†å½“å‰å¯¹è±¡çš„å­æ¨¡å—ï¼Œåº”ç”¨é€’å½’å‰é¦ˆå‡½æ•°ï¼Œåˆ†å—å¤§å°ä¸ºNoneï¼Œç»´åº¦ä¸º0
            for module in self.children():
                fn_recursive_feed_forward(module, None, 0)
    
        @property
        # ä»diffusers.models.unets.unet_2d_conditionå¤åˆ¶çš„å±æ€§ï¼Œè·å–æ³¨æ„åŠ›å¤„ç†å™¨
        def attn_processors(self) -> Dict[str, AttentionProcessor]:
            r"""
            è¿”å›ï¼š
                `dict`ç±»å‹çš„æ³¨æ„åŠ›å¤„ç†å™¨ï¼šä¸€ä¸ªåŒ…å«æ¨¡å‹ä¸­æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨çš„å­—å…¸ï¼Œä»¥å…¶æƒé‡åç§°ç´¢å¼•ã€‚
            """
            # åˆå§‹åŒ–å¤„ç†å™¨å­—å…¸
            processors = {}
    
            # å®šä¹‰é€’å½’æ·»åŠ å¤„ç†å™¨çš„å‡½æ•°
            def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
                # å¦‚æœæ¨¡å—æœ‰è·å–å¤„ç†å™¨çš„æ–¹æ³•ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°å¤„ç†å™¨å­—å…¸
                if hasattr(module, "get_processor"):
                    processors[f"{name}.processor"] = module.get_processor()
    
                # éå†å­æ¨¡å—å¹¶é€’å½’è°ƒç”¨æ·»åŠ å¤„ç†å™¨å‡½æ•°
                for sub_name, child in module.named_children():
                    fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)
    
                return processors
    
            # éå†å½“å‰å¯¹è±¡çš„å­æ¨¡å—ï¼Œåº”ç”¨é€’å½’æ·»åŠ å¤„ç†å™¨å‡½æ•°
            for name, module in self.named_children():
                fn_recursive_add_processors(name, module, processors)
    
            # è¿”å›å¤„ç†å™¨å­—å…¸
            return processors
    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor å¤åˆ¶è€Œæ¥
    def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
        r"""
        è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚

        å‚æ•°ï¼š
            processorï¼ˆ`dict` ç±»å‹çš„ `AttentionProcessor` æˆ–ä»…ä¸º `AttentionProcessor`ï¼‰ï¼š
                å®ä¾‹åŒ–çš„å¤„ç†å™¨ç±»æˆ–ä¸€ä¸ªå¤„ç†å™¨ç±»çš„å­—å…¸ï¼Œå°†è¢«è®¾ç½®ä¸º **æ‰€æœ‰** `Attention` å±‚çš„å¤„ç†å™¨ã€‚

                å¦‚æœ `processor` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®éœ€è¦å®šä¹‰ç›¸åº”çš„äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„è·¯å¾„ã€‚
                åœ¨è®¾ç½®å¯è®­ç»ƒçš„æ³¨æ„åŠ›å¤„ç†å™¨æ—¶ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨è¿™ç§æ–¹å¼ã€‚

        """
        # è·å–å½“å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„æ•°é‡
        count = len(self.attn_processors.keys())

        # å¦‚æœä¼ å…¥çš„å¤„ç†å™¨æ˜¯å­—å…¸ä¸”æ•°é‡ä¸æ³¨æ„åŠ›å±‚æ•°é‡ä¸åŒ¹é…ï¼ŒæŠ›å‡ºå¼‚å¸¸
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f"ä¼ å…¥äº†å¤„ç†å™¨å­—å…¸ï¼Œä½†å¤„ç†å™¨æ•°é‡ {len(processor)} ä¸æ³¨æ„åŠ›å±‚æ•°é‡ {count} ä¸åŒ¹é…ã€‚è¯·ç¡®ä¿ä¼ å…¥ {count} ä¸ªå¤„ç†å™¨ç±»ã€‚"
            )

        # å®šä¹‰é€’å½’å‡½æ•°ï¼Œç”¨äºè®¾ç½®æ¯ä¸ªæ¨¡å—çš„æ³¨æ„åŠ›å¤„ç†å™¨
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
            if hasattr(module, "set_processor"):
                # å¦‚æœå¤„ç†å™¨ä¸æ˜¯å­—å…¸ï¼Œç›´æ¥è®¾ç½®
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    # ä»å­—å…¸ä¸­å¼¹å‡ºç›¸åº”çš„å¤„ç†å™¨å¹¶è®¾ç½®
                    module.set_processor(processor.pop(f"{name}.processor"))

            # éå†æ¨¡å—çš„å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

        # éå†å½“å‰å®ä¾‹çš„å­æ¨¡å—ï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨¡å—è®¾ç½®å¤„ç†å™¨
        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections å¤åˆ¶è€Œæ¥
    def fuse_qkv_projections(self):
        """
        å¯ç”¨èåˆçš„ QKV æŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰è¢«èåˆã€‚
        å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚

        <Tip warning={true}>

        æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§ã€‚

        </Tip>
        """
        # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None
        self.original_attn_processors = None

        # æ£€æŸ¥æ¯ä¸ªæ³¨æ„åŠ›å¤„ç†å™¨æ˜¯å¦åŒ…å« "Added"
        for _, attn_processor in self.attn_processors.items():
            if "Added" in str(attn_processor.__class__.__name__):
                # å¦‚æœå‘ç°ä¸æ”¯æŒçš„å¤„ç†å™¨ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError("`fuse_qkv_projections()` ä¸æ”¯æŒå…·æœ‰æ·»åŠ  KV æŠ•å½±çš„æ¨¡å‹ã€‚")

        # å°†å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨ä¿å­˜ä¸ºåŸå§‹å¤„ç†å™¨
        self.original_attn_processors = self.attn_processors

        # éå†æ‰€æœ‰æ¨¡å—ï¼Œå¦‚æœæ¨¡å—æ˜¯ Attention ç±»å‹ï¼Œåˆ™è¿›è¡ŒæŠ•å½±èåˆ
        for module in self.modules():
            if isinstance(module, Attention):
                module.fuse_projections(fuse=True)

        # è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä¸º FusedJointAttnProcessor2_0 çš„å®ä¾‹
        self.set_attn_processor(FusedJointAttnProcessor2_0())

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections å¤åˆ¶è€Œæ¥
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºç¦ç”¨å·²å¯ç”¨çš„èåˆ QKV æŠ•å½±
    def unfuse_qkv_projections(self):
        """ç¦ç”¨èåˆçš„ QKV æŠ•å½±ï¼ˆå¦‚æœå·²å¯ç”¨ï¼‰ã€‚
    
        <Tip warning={true}>
    
        æ­¤ API ä¸º ğŸ§ª å®éªŒæ€§ã€‚
    
        </Tip>
    
        """
        # æ£€æŸ¥åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨æ˜¯å¦å­˜åœ¨
        if self.original_attn_processors is not None:
            # å°†å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨è®¾ç½®ä¸ºåŸå§‹çš„
            self.set_attn_processor(self.original_attn_processors)
    
    # å®šä¹‰ä¸€ä¸ªç§æœ‰æ–¹æ³•ï¼Œç”¨äºè®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹
    def _set_gradient_checkpointing(self, module, value=False):
        # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰æ¢¯åº¦æ£€æŸ¥ç‚¹å±æ€§
        if hasattr(module, "gradient_checkpointing"):
            # å°†æ¢¯åº¦æ£€æŸ¥ç‚¹å±æ€§è®¾ç½®ä¸ºæŒ‡å®šå€¼
            module.gradient_checkpointing = value
    
    # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œæ¥å—å¤šä¸ªè¾“å…¥å‚æ•°
    def forward(
        self,
        hidden_states: torch.FloatTensor,  # è¾“å…¥çš„éšè—çŠ¶æ€å¼ é‡
        encoder_hidden_states: torch.FloatTensor = None,  # ç¼–ç å™¨çš„éšè—çŠ¶æ€å¼ é‡ï¼Œå¯é€‰
        pooled_projections: torch.FloatTensor = None,  # æ± åŒ–åçš„æŠ•å½±å¼ é‡ï¼Œå¯é€‰
        timestep: torch.LongTensor = None,  # æ—¶é—´æ­¥é•¿å¼ é‡ï¼Œå¯é€‰
        block_controlnet_hidden_states: List = None,  # æ§åˆ¶ç½‘çš„éšè—çŠ¶æ€åˆ—è¡¨ï¼Œå¯é€‰
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,  # è”åˆæ³¨æ„åŠ›çš„é¢å¤–å‚æ•°ï¼Œå¯é€‰
        return_dict: bool = True,  # æŒ‡ç¤ºæ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„ç»“æœï¼Œé»˜è®¤ä¸º True
```