# `.\diffusers\models\controlnet_xs.py`

```
# ç‰ˆæƒä¿¡æ¯ï¼Œå£°æ˜è¯¥æ–‡ä»¶å½’ HuggingFace å›¢é˜Ÿæ‰€æœ‰ï¼Œæ‰€æœ‰æƒåˆ©ä¿ç•™
# 
# æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰è¿›è¡Œæˆæƒï¼›
# é™¤ééµå¾ªè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹åœ°å€è·å–è®¸å¯è¯çš„å‰¯æœ¬ï¼š
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰çº¦å®šï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰â€œåŸæ ·â€åŸºç¡€è¿›è¡Œåˆ†å‘ï¼Œ
# ä¸æä¾›ä»»ä½•å½¢å¼çš„æ‹…ä¿æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºæˆ–æš—ç¤ºçš„ã€‚
# è¯·å‚é˜…è®¸å¯è¯ä»¥è·å–æœ‰å…³æƒé™å’Œé™åˆ¶çš„ç‰¹å®šä¿¡æ¯ã€‚
from dataclasses import dataclass  # ä» dataclasses æ¨¡å—å¯¼å…¥ dataclass è£…é¥°å™¨
from math import gcd  # ä» math æ¨¡å—å¯¼å…¥ gcd å‡½æ•°ï¼Œç”¨äºè®¡ç®—æœ€å¤§å…¬çº¦æ•°
from typing import Any, Dict, List, Optional, Tuple, Union  # å¯¼å…¥ç±»å‹æç¤ºç›¸å…³çš„ç±»å‹

import torch  # å¯¼å…¥ PyTorch åº“
import torch.utils.checkpoint  # å¯¼å…¥ PyTorch çš„ checkpoint å·¥å…·ï¼Œç”¨äºä¿å­˜å†…å­˜
from torch import Tensor, nn  # ä» torch æ¨¡å—å¯¼å…¥ Tensor ç±»å’Œ nn æ¨¡å—

from ..configuration_utils import ConfigMixin, register_to_config  # ä»ä¸Šå±‚æ¨¡å—å¯¼å…¥é…ç½®ç›¸å…³çš„ç±»å’Œå‡½æ•°
from ..utils import BaseOutput, is_torch_version, logging  # ä»ä¸Šå±‚æ¨¡å—å¯¼å…¥å·¥å…·ç±»å’Œå‡½æ•°
from ..utils.torch_utils import apply_freeu  # ä»ä¸Šå±‚æ¨¡å—å¯¼å…¥ç‰¹å®šçš„ PyTorch å·¥å…·å‡½æ•°
from .attention_processor import (  # ä»å½“å‰åŒ…å¯¼å…¥æ³¨æ„åŠ›å¤„ç†å™¨ç›¸å…³çš„ç±»
    ADDED_KV_ATTENTION_PROCESSORS,
    CROSS_ATTENTION_PROCESSORS,
    Attention,
    AttentionProcessor,
    AttnAddedKVProcessor,
    AttnProcessor,
    FusedAttnProcessor2_0,
)
from .controlnet import ControlNetConditioningEmbedding  # ä»å½“å‰åŒ…å¯¼å…¥ ControlNet çš„æ¡ä»¶åµŒå…¥ç±»
from .embeddings import TimestepEmbedding, Timesteps  # ä»å½“å‰åŒ…å¯¼å…¥æ—¶é—´æ­¥åµŒå…¥ç›¸å…³çš„ç±»
from .modeling_utils import ModelMixin  # ä»å½“å‰åŒ…å¯¼å…¥æ¨¡å‹æ··åˆç±»
from .unets.unet_2d_blocks import (  # ä»å½“å‰åŒ…å¯¼å…¥ 2D U-Net æ¨¡å—ç›¸å…³çš„ç±»
    CrossAttnDownBlock2D,
    CrossAttnUpBlock2D,
    Downsample2D,
    ResnetBlock2D,
    Transformer2DModel,
    UNetMidBlock2DCrossAttn,
    Upsample2D,
)
from .unets.unet_2d_condition import UNet2DConditionModel  # ä»å½“å‰åŒ…å¯¼å…¥å¸¦æ¡ä»¶çš„ 2D U-Net æ¨¡å‹ç±»


logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


@dataclass  # å°†è¯¥ç±»å£°æ˜ä¸ºæ•°æ®ç±»
class ControlNetXSOutput(BaseOutput):  # å®šä¹‰ ControlNetXSOutput ç±»ï¼Œç»§æ‰¿è‡ª BaseOutput
    """
    [`UNetControlNetXSModel`] çš„è¾“å‡ºã€‚

    å‚æ•°ï¼š
        sample (`Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`):
            `UNetControlNetXSModel` çš„è¾“å‡ºã€‚ä¸ `ControlNetOutput` ä¸åŒï¼Œæ­¤è¾“å‡ºä¸æ˜¯è¦ä¸åŸºç¡€æ¨¡å‹è¾“å‡ºç›¸åŠ ï¼Œè€Œæ˜¯å·²ç»æ˜¯æœ€ç»ˆè¾“å‡ºã€‚
    """

    sample: Tensor = None  # å®šä¹‰ä¸€ä¸ªå¯é€‰çš„ Tensor å±æ€§ sampleï¼Œé»˜è®¤ä¸º None


class DownBlockControlNetXSAdapter(nn.Module):  # å®šä¹‰ DownBlockControlNetXSAdapter ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
    """ä¸åŸºç¡€æ¨¡å‹çš„å¯¹åº”ç»„ä»¶ä¸€èµ·å½¢æˆ `ControlNetXSCrossAttnDownBlock2D` çš„ç»„ä»¶"""

    def __init__(  # å®šä¹‰åˆå§‹åŒ–æ–¹æ³•
        self,
        resnets: nn.ModuleList,  # ä¼ å…¥ä¸€ä¸ª ResNet ç»„ä»¶çš„æ¨¡å—åˆ—è¡¨
        base_to_ctrl: nn.ModuleList,  # ä¼ å…¥åŸºç¡€æ¨¡å‹åˆ° ControlNet çš„æ¨¡å—åˆ—è¡¨
        ctrl_to_base: nn.ModuleList,  # ä¼ å…¥ ControlNet åˆ°åŸºç¡€æ¨¡å‹çš„æ¨¡å—åˆ—è¡¨
        attentions: Optional[nn.ModuleList] = None,  # å¯é€‰çš„æ³¨æ„åŠ›æ¨¡å—åˆ—è¡¨ï¼Œé»˜è®¤ä¸º None
        downsampler: Optional[nn.Conv2d] = None,  # å¯é€‰çš„ä¸‹é‡‡æ ·æ¨¡å—ï¼Œé»˜è®¤ä¸º None
    ):
        super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        self.resnets = resnets  # ä¿å­˜ ResNet ç»„ä»¶åˆ—è¡¨
        self.base_to_ctrl = base_to_ctrl  # ä¿å­˜åŸºç¡€æ¨¡å‹åˆ° ControlNet çš„æ¨¡å—åˆ—è¡¨
        self.ctrl_to_base = ctrl_to_base  # ä¿å­˜ ControlNet åˆ°åŸºç¡€æ¨¡å‹çš„æ¨¡å—åˆ—è¡¨
        self.attentions = attentions  # ä¿å­˜æ³¨æ„åŠ›æ¨¡å—åˆ—è¡¨
        self.downsamplers = downsampler  # ä¿å­˜ä¸‹é‡‡æ ·æ¨¡å—


class MidBlockControlNetXSAdapter(nn.Module):  # å®šä¹‰ MidBlockControlNetXSAdapter ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
    """ä¸åŸºç¡€æ¨¡å‹çš„å¯¹åº”ç»„ä»¶ä¸€èµ·å½¢æˆ `ControlNetXSCrossAttnMidBlock2D` çš„ç»„ä»¶"""
    # åˆå§‹åŒ–ç±»çš„æ„é€ å‡½æ•°
        def __init__(self, midblock: UNetMidBlock2DCrossAttn, base_to_ctrl: nn.ModuleList, ctrl_to_base: nn.ModuleList):
            # è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°
            super().__init__()
            # å°†ä¼ å…¥çš„ midblock å‚æ•°èµ‹å€¼ç»™å®ä¾‹å˜é‡ midblock
            self.midblock = midblock
            # å°†ä¼ å…¥çš„ base_to_ctrl å‚æ•°èµ‹å€¼ç»™å®ä¾‹å˜é‡ base_to_ctrl
            self.base_to_ctrl = base_to_ctrl
            # å°†ä¼ å…¥çš„ ctrl_to_base å‚æ•°èµ‹å€¼ç»™å®ä¾‹å˜é‡ ctrl_to_base
            self.ctrl_to_base = ctrl_to_base
# å®šä¹‰ä¸€ä¸ªåä¸º UpBlockControlNetXSAdapter çš„ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
class UpBlockControlNetXSAdapter(nn.Module):
    """ä¸åŸºç¡€æ¨¡å‹çš„ç›¸åº”ç»„ä»¶ä¸€èµ·ç»„æˆ `ControlNetXSCrossAttnUpBlock2D`"""

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—ä¸€ä¸ªæ§åˆ¶åˆ°åŸºç¡€çš„æ¨¡å—åˆ—è¡¨
    def __init__(self, ctrl_to_base: nn.ModuleList):
        super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        self.ctrl_to_base = ctrl_to_base  # å°†ä¼ å…¥çš„æ§åˆ¶åˆ°åŸºç¡€æ¨¡å—åˆ—è¡¨ä¿å­˜ä¸ºå®ä¾‹å˜é‡


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè·å–ä¸‹è¡Œå—é€‚é…å™¨
def get_down_block_adapter(
    base_in_channels: int,  # åŸºç¡€è¾“å…¥é€šé“æ•°
    base_out_channels: int,  # åŸºç¡€è¾“å‡ºé€šé“æ•°
    ctrl_in_channels: int,  # æ§åˆ¶è¾“å…¥é€šé“æ•°
    ctrl_out_channels: int,  # æ§åˆ¶è¾“å‡ºé€šé“æ•°
    temb_channels: int,  # æ—¶é—´åµŒå…¥é€šé“æ•°
    max_norm_num_groups: Optional[int] = 32,  # æœ€å¤§å½’ä¸€åŒ–ç»„æ•°
    has_crossattn=True,  # æ˜¯å¦ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
    transformer_layers_per_block: Optional[Union[int, Tuple[int]]] = 1,  # æ¯ä¸ªå—çš„å˜æ¢å™¨å±‚æ•°
    num_attention_heads: Optional[int] = 1,  # æ³¨æ„åŠ›å¤´æ•°é‡
    cross_attention_dim: Optional[int] = 1024,  # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
    add_downsample: bool = True,  # æ˜¯å¦æ·»åŠ ä¸‹é‡‡æ ·
    upcast_attention: Optional[bool] = False,  # æ˜¯å¦ä¸Šè°ƒæ³¨æ„åŠ›
    use_linear_projection: Optional[bool] = True,  # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
):
    num_layers = 2  # ä»…æ”¯æŒ sd + sdxl

    resnets = []  # å­˜å‚¨ ResNet å—çš„åˆ—è¡¨
    attentions = []  # å­˜å‚¨æ³¨æ„åŠ›æ¨¡å‹çš„åˆ—è¡¨
    ctrl_to_base = []  # å­˜å‚¨æ§åˆ¶åˆ°åŸºç¡€çš„å·ç§¯å±‚åˆ—è¡¨
    base_to_ctrl = []  # å­˜å‚¨åŸºç¡€åˆ°æ§åˆ¶çš„å·ç§¯å±‚åˆ—è¡¨

    # å¦‚æœä¼ å…¥çš„æ˜¯æ•´æ•°ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºä¸å±‚æ•°ç›¸åŒçš„åˆ—è¡¨
    if isinstance(transformer_layers_per_block, int):
        transformer_layers_per_block = [transformer_layers_per_block] * num_layers

    # éå†æ¯å±‚ä»¥æ„å»ºç½‘ç»œç»“æ„
    for i in range(num_layers):
        # ç¬¬ä¸€å±‚ä½¿ç”¨åŸºç¡€è¾“å…¥é€šé“æ•°ï¼Œåç»­å±‚ä½¿ç”¨åŸºç¡€è¾“å‡ºé€šé“æ•°
        base_in_channels = base_in_channels if i == 0 else base_out_channels
        # ç¬¬ä¸€å±‚ä½¿ç”¨æ§åˆ¶è¾“å…¥é€šé“æ•°ï¼Œåç»­å±‚ä½¿ç”¨æ§åˆ¶è¾“å‡ºé€šé“æ•°
        ctrl_in_channels = ctrl_in_channels if i == 0 else ctrl_out_channels

        # åœ¨åº”ç”¨ ResNet/æ³¨æ„åŠ›ä¹‹å‰ï¼Œä»åŸºç¡€åˆ°æ§åˆ¶çš„é€šé“ä¿¡æ¯è¿›è¡Œè¿æ¥
        # è¿æ¥ä¸éœ€è¦æ›´æ”¹é€šé“æ•°é‡
        base_to_ctrl.append(make_zero_conv(base_in_channels, base_in_channels))

        resnets.append(
            ResnetBlock2D(
                in_channels=ctrl_in_channels + base_in_channels,  # ä»åŸºç¡€è¿æ¥åˆ°æ§åˆ¶çš„ä¿¡æ¯
                out_channels=ctrl_out_channels,  # æ§åˆ¶è¾“å‡ºé€šé“æ•°
                temb_channels=temb_channels,  # æ—¶é—´åµŒå…¥é€šé“æ•°
                groups=find_largest_factor(ctrl_in_channels + base_in_channels, max_factor=max_norm_num_groups),  # è®¡ç®—ç»„æ•°
                groups_out=find_largest_factor(ctrl_out_channels, max_factor=max_norm_num_groups),  # è®¡ç®—è¾“å‡ºç»„æ•°
                eps=1e-5,  # å°å¸¸æ•°ä»¥é¿å…é™¤é›¶
            )
        )

        # å¦‚æœéœ€è¦äº¤å‰æ³¨æ„åŠ›ï¼Œåˆ™æ·»åŠ å¯¹åº”çš„æ¨¡å‹
        if has_crossattn:
            attentions.append(
                Transformer2DModel(
                    num_attention_heads,  # æ³¨æ„åŠ›å¤´æ•°é‡
                    ctrl_out_channels // num_attention_heads,  # æ¯ä¸ªå¤´çš„é€šé“æ•°
                    in_channels=ctrl_out_channels,  # è¾“å…¥é€šé“æ•°
                    num_layers=transformer_layers_per_block[i],  # å½“å‰å—çš„å˜æ¢å™¨å±‚æ•°
                    cross_attention_dim=cross_attention_dim,  # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
                    use_linear_projection=use_linear_projection,  # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
                    upcast_attention=upcast_attention,  # æ˜¯å¦ä¸Šè°ƒæ³¨æ„åŠ›
                    norm_num_groups=find_largest_factor(ctrl_out_channels, max_factor=max_norm_num_groups),  # è®¡ç®—å½’ä¸€åŒ–ç»„æ•°
                )
            )

        # åœ¨åº”ç”¨ ResNet/æ³¨æ„åŠ›ä¹‹åï¼Œä»æ§åˆ¶åˆ°åŸºç¡€çš„é€šé“ä¿¡æ¯è¿›è¡Œç›¸åŠ 
        # ç›¸åŠ éœ€è¦æ›´æ”¹é€šé“æ•°é‡
        ctrl_to_base.append(make_zero_conv(ctrl_out_channels, base_out_channels))  # æ·»åŠ æ§åˆ¶åˆ°åŸºç¡€çš„å·ç§¯å±‚
    # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œä¸‹é‡‡æ ·
    if add_downsample:
        # åœ¨åº”ç”¨ä¸‹é‡‡æ ·å™¨ä¹‹å‰ï¼Œå°† base çš„ä¿¡æ¯ä¸ control çš„ä¿¡æ¯è¿æ¥
        # è¿æ¥æ“ä½œä¸éœ€è¦æ”¹å˜é€šé“æ•°é‡
        base_to_ctrl.append(make_zero_conv(base_out_channels, base_out_channels))

        # åˆ›å»ºä¸‹é‡‡æ ·å™¨å¯¹è±¡ï¼Œè¾“å…¥é€šé“ä¸ºæ§åˆ¶é€šé“å’ŒåŸºç¡€é€šé“ä¹‹å’Œï¼Œä½¿ç”¨å·ç§¯ï¼Œè¾“å‡ºé€šé“ä¸ºæ§åˆ¶é€šé“æ•°é‡ï¼Œå‘½åä¸º "op"
        downsamplers = Downsample2D(
            ctrl_out_channels + base_out_channels, use_conv=True, out_channels=ctrl_out_channels, name="op"
        )

        # åœ¨åº”ç”¨ä¸‹é‡‡æ ·å™¨ä¹‹åï¼Œå°†æ§åˆ¶çš„æ•°æ®ä¿¡æ¯æ·»åŠ åˆ°åŸºç¡€æ•°æ®ä¸­
        # æ·»åŠ æ“ä½œéœ€è¦æ”¹å˜é€šé“æ•°é‡
        ctrl_to_base.append(make_zero_conv(ctrl_out_channels, base_out_channels))
    else:
        # å¦‚æœä¸éœ€è¦ä¸‹é‡‡æ ·ï¼Œåˆ™å°† downsamplers è®¾ç½®ä¸º None
        downsamplers = None

    # åˆ›å»ºä¸‹å—æ§åˆ¶ç½‘ç»œé€‚é…å™¨ï¼Œä¼ å…¥æ®‹å·®ç½‘ç»œå’Œè¿æ¥çš„æ§åˆ¶åŸºç¡€æ¨¡å—
    down_block_components = DownBlockControlNetXSAdapter(
        resnets=nn.ModuleList(resnets),
        base_to_ctrl=nn.ModuleList(base_to_ctrl),
        ctrl_to_base=nn.ModuleList(ctrl_to_base),
    )

    # å¦‚æœå­˜åœ¨äº¤å‰æ³¨æ„åŠ›ï¼Œåˆ™å°†æ³¨æ„åŠ›æ¨¡å—æ·»åŠ åˆ°ä¸‹å—ç»„ä»¶ä¸­
    if has_crossattn:
        down_block_components.attentions = nn.ModuleList(attentions)
    # å¦‚æœä¸‹é‡‡æ ·å™¨ä¸ä¸º Noneï¼Œåˆ™å°†ä¸‹é‡‡æ ·å™¨æ·»åŠ åˆ°ä¸‹å—ç»„ä»¶ä¸­
    if downsamplers is not None:
        down_block_components.downsamplers = downsamplers

    # è¿”å›ä¸‹å—ç»„ä»¶
    return down_block_components
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–ä¸­é—´å—é€‚é…å™¨ï¼Œæ¥å—å¤šä¸ªå‚æ•°ä»¥é…ç½®å…¶è¡Œä¸º
def get_mid_block_adapter(
    # åŸºç¡€é€šé“æ•°
    base_channels: int,
    # æ§åˆ¶é€šé“æ•°
    ctrl_channels: int,
    # å¯é€‰çš„æ—¶é—´åµŒå…¥é€šé“æ•°
    temb_channels: Optional[int] = None,
    # æœ€å¤§å½’ä¸€åŒ–ç»„æ•°é‡ï¼Œé»˜è®¤ä¸º32
    max_norm_num_groups: Optional[int] = 32,
    # æ¯ä¸ªå—çš„å˜æ¢å±‚æ•°ï¼Œé»˜è®¤ä¸º1
    transformer_layers_per_block: int = 1,
    # å¯é€‰çš„æ³¨æ„åŠ›å¤´æ•°é‡ï¼Œé»˜è®¤ä¸º1
    num_attention_heads: Optional[int] = 1,
    # å¯é€‰çš„äº¤å‰æ³¨æ„åŠ›ç»´åº¦ï¼Œé»˜è®¤ä¸º1024
    cross_attention_dim: Optional[int] = 1024,
    # æ˜¯å¦æå‡æ³¨æ„åŠ›ç²¾åº¦ï¼Œé»˜è®¤ä¸ºFalse
    upcast_attention: bool = False,
    # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±ï¼Œé»˜è®¤ä¸ºTrue
    use_linear_projection: bool = True,
):
    # åœ¨ä¸­é—´å—åº”ç”¨ä¹‹å‰ï¼Œä»åŸºç¡€é€šé“åˆ°æ§åˆ¶é€šé“çš„ä¿¡æ¯è¿›è¡Œæ‹¼æ¥
    # æ‹¼æ¥ä¸éœ€è¦æ”¹å˜é€šé“æ•°
    base_to_ctrl = make_zero_conv(base_channels, base_channels)

    # åˆ›å»ºä¸€ä¸ªä¸­é—´å—å¯¹è±¡ï¼Œä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
    midblock = UNetMidBlock2DCrossAttn(
        # è®¾ç½®æ¯ä¸ªå—çš„å˜æ¢å±‚æ•°
        transformer_layers_per_block=transformer_layers_per_block,
        # è¾“å…¥é€šé“ä¸ºæ§åˆ¶é€šé“å’ŒåŸºç¡€é€šé“çš„å’Œ
        in_channels=ctrl_channels + base_channels,
        # è¾“å‡ºé€šé“ä¸ºæ§åˆ¶é€šé“æ•°
        out_channels=ctrl_channels,
        # æ—¶é—´åµŒå…¥é€šé“æ•°
        temb_channels=temb_channels,
        # å½’ä¸€åŒ–ç»„æ•°é‡å¿…é¡»èƒ½å¤ŸåŒæ—¶æ•´é™¤è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°
        resnet_groups=find_largest_factor(gcd(ctrl_channels, ctrl_channels + base_channels), max_norm_num_groups),
        # äº¤å‰æ³¨æ„åŠ›çš„ç»´åº¦
        cross_attention_dim=cross_attention_dim,
        # æ³¨æ„åŠ›å¤´çš„æ•°é‡
        num_attention_heads=num_attention_heads,
        # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
        use_linear_projection=use_linear_projection,
        # æ˜¯å¦æå‡æ³¨æ„åŠ›ç²¾åº¦
        upcast_attention=upcast_attention,
    )

    # åœ¨ä¸­é—´å—åº”ç”¨ä¹‹åï¼Œä»æ§åˆ¶é€šé“åˆ°åŸºç¡€é€šé“çš„ä¿¡æ¯è¿›è¡Œç›¸åŠ 
    # ç›¸åŠ éœ€è¦æ”¹å˜é€šé“æ•°
    ctrl_to_base = make_zero_conv(ctrl_channels, base_channels)

    # è¿”å›ä¸€ä¸ªä¸­é—´å—æ§åˆ¶é€‚é…å™¨çš„å®ä¾‹ï¼ŒåŒ…å«æ‹¼æ¥å±‚ã€ä¸­é—´å—å’Œç›¸åŠ å±‚
    return MidBlockControlNetXSAdapter(base_to_ctrl=base_to_ctrl, midblock=midblock, ctrl_to_base=ctrl_to_base)


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–ä¸Šå—é€‚é…å™¨ï¼Œæ¥å—è¾“å‡ºé€šé“æ•°ã€å‰ä¸€å±‚è¾“å‡ºé€šé“æ•°å’Œæ§åˆ¶è·³è·ƒé€šé“
def get_up_block_adapter(
    # è¾“å‡ºé€šé“æ•°
    out_channels: int,
    # å‰ä¸€å±‚çš„è¾“å‡ºé€šé“æ•°
    prev_output_channel: int,
    # æ§åˆ¶è·³è·ƒé€šé“åˆ—è¡¨
    ctrl_skip_channels: List[int],
):
    # åˆå§‹åŒ–æ§åˆ¶åˆ°åŸºç¡€çš„å·ç§¯å±‚åˆ—è¡¨
    ctrl_to_base = []
    # è®¾ç½®å±‚æ•°ä¸º3ï¼Œä»…æ”¯æŒ sd å’Œ sdxl
    num_layers = 3  
    # å¾ªç¯æ„å»ºæ¯ä¸€å±‚çš„æ§åˆ¶åˆ°åŸºç¡€å·ç§¯å±‚
    for i in range(num_layers):
        # ç¬¬ä¸€å±‚ä½¿ç”¨å‰ä¸€å±‚è¾“å‡ºé€šé“ï¼Œå…¶ä»–å±‚ä½¿ç”¨è¾“å‡ºé€šé“
        resnet_in_channels = prev_output_channel if i == 0 else out_channels
        # å°†æ§åˆ¶è·³è·ƒé€šé“ä¸å½“å‰è¾“å…¥é€šé“è¿æ¥
        ctrl_to_base.append(make_zero_conv(ctrl_skip_channels[i], resnet_in_channels))

    # è¿”å›ä¸€ä¸ªä¸Šå—æ§åˆ¶é€‚é…å™¨çš„å®ä¾‹ï¼Œä½¿ç”¨nn.ModuleListç®¡ç†æ§åˆ¶åˆ°åŸºç¡€å·ç§¯å±‚
    return UpBlockControlNetXSAdapter(ctrl_to_base=nn.ModuleList(ctrl_to_base))


# å®šä¹‰ä¸€ä¸ªæ§åˆ¶ç½‘ç»œé€‚é…å™¨ç±»ï¼Œç»§æ‰¿è‡ªModelMixinå’ŒConfigMixin
class ControlNetXSAdapter(ModelMixin, ConfigMixin):
    r"""
    æ§åˆ¶ç½‘ç»œé€‚é…å™¨æ¨¡å‹ã€‚ä½¿ç”¨æ—¶ï¼Œå°†å…¶ä¼ é€’ç»™ `UNetControlNetXSModel`ï¼ˆä»¥åŠä¸€ä¸ª
    `UNet2DConditionModel` åŸºç¡€æ¨¡å‹ï¼‰ã€‚

    è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[`ModelMixin`]å’Œ[`ConfigMixin`]ã€‚è¯·æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£å…¶é€šç”¨
    æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼‰ã€‚

    ä¸`UNetControlNetXSModel`ä¸€æ ·ï¼Œ`ControlNetXSAdapter`ä¸StableDiffusionå’ŒStableDiffusion-XLå…¼å®¹ã€‚å…¶
    é»˜è®¤å‚æ•°ä¸StableDiffusionå…¼å®¹ã€‚
    # å‚æ•°éƒ¨åˆ†è¯´æ˜
    Parameters:
        # conditioning_channels: æ¡ä»¶è¾“å…¥çš„é€šé“æ•°ï¼ˆä¾‹å¦‚ï¼šä¸€å¼ å›¾åƒï¼‰ï¼Œé»˜è®¤å€¼ä¸º3
        conditioning_channels (`int`, defaults to 3):
            # æ¡ä»¶å›¾åƒçš„é€šé“é¡ºåºã€‚è‹¥ä¸º `bgr`ï¼Œåˆ™è½¬æ¢ä¸º `rgb`
            conditioning_channel_order (`str`, defaults to `"rgb"`):
            # `controlnet_cond_embedding` å±‚ä¸­æ¯ä¸ªå—çš„è¾“å‡ºé€šé“çš„å…ƒç»„ï¼Œé»˜è®¤å€¼ä¸º (16, 32, 96, 256)
            conditioning_embedding_out_channels (`tuple[int]`, defaults to `(16, 32, 96, 256)`):
            # time_embedding_mix: å¦‚æœä¸º0ï¼Œåˆ™ä»…ä½¿ç”¨æ§åˆ¶é€‚é…å™¨çš„æ—¶é—´åµŒå…¥ï¼›å¦‚æœä¸º1ï¼Œåˆ™ä»…ä½¿ç”¨åŸºç¡€ UNet çš„æ—¶é—´åµŒå…¥ï¼›å¦åˆ™ï¼Œä¸¤è€…ç»“åˆ
            time_embedding_mix (`float`, defaults to 1.0):
            # learn_time_embedding: æ˜¯å¦åº”å­¦ä¹ æ—¶é—´åµŒå…¥ï¼Œè‹¥æ˜¯åˆ™ `UNetControlNetXSModel` ä¼šç»“åˆåŸºç¡€æ¨¡å‹å’Œæ§åˆ¶é€‚é…å™¨çš„æ—¶é—´åµŒå…¥ï¼Œè‹¥å¦åˆ™åªä½¿ç”¨åŸºç¡€æ¨¡å‹çš„æ—¶é—´åµŒå…¥
            learn_time_embedding (`bool`, defaults to `False`):
            # num_attention_heads: æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º [4]
            num_attention_heads (`list[int]`, defaults to `[4]`):
            # block_out_channels: æ¯ä¸ªå—çš„è¾“å‡ºé€šé“çš„å…ƒç»„ï¼Œé»˜è®¤å€¼ä¸º [4, 8, 16, 16]
            block_out_channels (`list[int]`, defaults to `[4, 8, 16, 16]`):
            # base_block_out_channels: åŸºç¡€ UNet ä¸­æ¯ä¸ªå—çš„è¾“å‡ºé€šé“çš„å…ƒç»„ï¼Œé»˜è®¤å€¼ä¸º [320, 640, 1280, 1280]
            base_block_out_channels (`list[int]`, defaults to `[320, 640, 1280, 1280]`):
            # cross_attention_dim: è·¨æ³¨æ„åŠ›ç‰¹å¾çš„ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º 1024
            cross_attention_dim (`int`, defaults to 1024):
            # down_block_types: è¦ä½¿ç”¨çš„ä¸‹é‡‡æ ·å—çš„å…ƒç»„ï¼Œé»˜è®¤å€¼ä¸º ["CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D"]
            down_block_types (`list[str]`, defaults to `["CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D"]`):
            # sample_size: è¾“å…¥/è¾“å‡ºæ ·æœ¬çš„é«˜åº¦å’Œå®½åº¦ï¼Œé»˜è®¤å€¼ä¸º 96
            sample_size (`int`, defaults to 96):
            # transformer_layers_per_block: æ¯ä¸ªå—çš„å˜æ¢å™¨å—æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º 1ï¼Œä»…ä¸æŸäº›å—ç›¸å…³
            transformer_layers_per_block (`Union[int, Tuple[int]]`, defaults to 1):
            # upcast_attention: æ˜¯å¦åº”å§‹ç»ˆæå‡æ³¨æ„åŠ›è®¡ç®—çš„ç²¾åº¦ï¼Œé»˜è®¤å€¼ä¸º True
            upcast_attention (`bool`, defaults to `True`):
            # max_norm_num_groups: åˆ†ç»„å½’ä¸€åŒ–ä¸­çš„æœ€å¤§ç»„æ•°ï¼Œé»˜è®¤å€¼ä¸º 32ï¼Œå®é™…æ•°é‡ä¸ºä¸å¤§äº max_norm_num_groups çš„ç›¸åº”é€šé“çš„æœ€å¤§é™¤æ•°
            max_norm_num_groups (`int`, defaults to 32):
    # æ³¨é‡Šéƒ¨åˆ†ç»“æŸ
    """
    
    # æ³¨å†Œåˆ°é…ç½®ä¸­
    @register_to_config
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½® ControlNetXSAdapter çš„åŸºæœ¬å‚æ•°
        def __init__(
            # æ¡ä»¶é€šé“æ•°ï¼Œé»˜è®¤ä¸º 3
            self,
            conditioning_channels: int = 3,
            # æ¡ä»¶é€šé“çš„é¢œè‰²é¡ºåºï¼Œé»˜è®¤ä¸º RGB
            conditioning_channel_order: str = "rgb",
            # è¾“å‡ºé€šé“æ•°çš„å…ƒç»„ï¼Œå®šä¹‰å„å±‚çš„è¾“å‡ºé€šé“
            conditioning_embedding_out_channels: Tuple[int] = (16, 32, 96, 256),
            # æ—¶é—´åµŒå…¥æ··åˆå› å­ï¼Œé»˜è®¤ä¸º 1.0
            time_embedding_mix: float = 1.0,
            # æ˜¯å¦å­¦ä¹ æ—¶é—´åµŒå…¥ï¼Œé»˜è®¤ä¸º False
            learn_time_embedding: bool = False,
            # æ³¨æ„åŠ›å¤´æ•°ï¼Œé»˜è®¤ä¸º 4ï¼Œå¯ä»¥æ˜¯æ•´æ•°æˆ–æ•´æ•°å…ƒç»„
            num_attention_heads: Union[int, Tuple[int]] = 4,
            # å—è¾“å‡ºé€šé“çš„å…ƒç»„ï¼Œå®šä¹‰æ¯ä¸ªå—çš„è¾“å‡ºé€šé“
            block_out_channels: Tuple[int] = (4, 8, 16, 16),
            # åŸºç¡€å—è¾“å‡ºé€šé“çš„å…ƒç»„
            base_block_out_channels: Tuple[int] = (320, 640, 1280, 1280),
            # äº¤å‰æ³¨æ„åŠ›ç»´åº¦ï¼Œé»˜è®¤ä¸º 1024
            cross_attention_dim: int = 1024,
            # å„å±‚çš„å—ç±»å‹å…ƒç»„
            down_block_types: Tuple[str] = (
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            # é‡‡æ ·å¤§å°ï¼Œé»˜è®¤ä¸º 96
            sample_size: Optional[int] = 96,
            # æ¯ä¸ªå—çš„å˜æ¢å™¨å±‚æ•°ï¼Œå¯ä»¥æ˜¯æ•´æ•°æˆ–æ•´æ•°å…ƒç»„
            transformer_layers_per_block: Union[int, Tuple[int]] = 1,
            # æ˜¯å¦ä¸Šæº¢æ³¨æ„åŠ›ï¼Œé»˜è®¤ä¸º True
            upcast_attention: bool = True,
            # æœ€å¤§å½’ä¸€åŒ–ç»„æ•°ï¼Œé»˜è®¤ä¸º 32
            max_norm_num_groups: int = 32,
            # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±ï¼Œé»˜è®¤ä¸º True
            use_linear_projection: bool = True,
        # ç±»æ–¹æ³•ï¼Œä» UNet åˆ›å»º ControlNetXSAdapter
        @classmethod
        def from_unet(
            cls,
            # ä¼ å…¥çš„ UNet2DConditionModel å¯¹è±¡
            unet: UNet2DConditionModel,
            # å°ºå¯¸æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º None
            size_ratio: Optional[float] = None,
            # å¯é€‰çš„å—è¾“å‡ºé€šé“åˆ—è¡¨
            block_out_channels: Optional[List[int]] = None,
            # å¯é€‰çš„æ³¨æ„åŠ›å¤´æ•°åˆ—è¡¨
            num_attention_heads: Optional[List[int]] = None,
            # æ˜¯å¦å­¦ä¹ æ—¶é—´åµŒå…¥ï¼Œé»˜è®¤ä¸º False
            learn_time_embedding: bool = False,
            # æ—¶é—´åµŒå…¥æ··åˆå› å­ï¼Œé»˜è®¤ä¸º 1.0
            time_embedding_mix: int = 1.0,
            # æ¡ä»¶é€šé“æ•°ï¼Œé»˜è®¤ä¸º 3
            conditioning_channels: int = 3,
            # æ¡ä»¶é€šé“çš„é¢œè‰²é¡ºåºï¼Œé»˜è®¤ä¸º RGB
            conditioning_channel_order: str = "rgb",
            # è¾“å‡ºé€šé“æ•°çš„å…ƒç»„ï¼Œé»˜è®¤ä¸º (16, 32, 96, 256)
            conditioning_embedding_out_channels: Tuple[int] = (16, 32, 96, 256),
        # å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œå¤„ç†è¾“å…¥å‚æ•°
        def forward(self, *args, **kwargs):
            # æŠ›å‡ºé”™è¯¯ï¼ŒæŒ‡ç¤ºä¸èƒ½å•ç‹¬è¿è¡Œ ControlNetXSAdapter
            raise ValueError(
                "A ControlNetXSAdapter cannot be run by itself. Use it together with a UNet2DConditionModel to instantiate a UNetControlNetXSModel."
            )
# å®šä¹‰ä¸€ä¸ª UNet èåˆ ControlNet-XS é€‚é…å™¨çš„æ¨¡å‹ç±»
class UNetControlNetXSModel(ModelMixin, ConfigMixin):
    r"""
    A UNet fused with a ControlNet-XS adapter model

    æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [`ModelMixin`] å’Œ [`ConfigMixin`]ã€‚æœ‰å…³æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼‰ï¼Œè¯·æ£€æŸ¥è¶…ç±»æ–‡æ¡£ã€‚

    `UNetControlNetXSModel` ä¸ StableDiffusion å’Œ StableDiffusion-XL å…¼å®¹ã€‚å…¶é»˜è®¤å‚æ•°ä¸ StableDiffusion å…¼å®¹ã€‚

    å®ƒçš„å‚æ•°è¦ä¹ˆä¼ é€’ç»™åº•å±‚çš„ `UNet2DConditionModel`ï¼Œè¦ä¹ˆä¸ `ControlNetXSAdapter` å®Œå…¨ç›¸åŒã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…å®ƒä»¬çš„æ–‡æ¡£ã€‚
    """

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ”¯æŒ
    _supports_gradient_checkpointing = True

    # æ³¨å†Œåˆ°é…ç½®çš„æ–¹æ³•
    @register_to_config
    def __init__(
        self,
        # unet é…ç½®
        # æ ·æœ¬å°ºå¯¸ï¼Œé»˜è®¤å€¼ä¸º 96
        sample_size: Optional[int] = 96,
        # ä¸‹é‡‡æ ·å—ç±»å‹çš„å…ƒç»„
        down_block_types: Tuple[str] = (
            "CrossAttnDownBlock2D",
            "CrossAttnDownBlock2D",
            "CrossAttnDownBlock2D",
            "DownBlock2D",
        ),
        # ä¸Šé‡‡æ ·å—ç±»å‹çš„å…ƒç»„
        up_block_types: Tuple[str] = ("UpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D"),
        # æ¯ä¸ªå—çš„è¾“å‡ºé€šé“æ•°
        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),
        # å½’ä¸€åŒ–çš„ç»„æ•°ï¼Œé»˜è®¤ä¸º 32
        norm_num_groups: Optional[int] = 32,
        # äº¤å‰æ³¨æ„åŠ›ç»´åº¦ï¼Œé»˜è®¤ä¸º 1024
        cross_attention_dim: Union[int, Tuple[int]] = 1024,
        # æ¯ä¸ªå—çš„å˜æ¢å™¨å±‚æ•°ï¼Œé»˜è®¤ä¸º 1
        transformer_layers_per_block: Union[int, Tuple[int]] = 1,
        # æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 8
        num_attention_heads: Union[int, Tuple[int]] = 8,
        # é™„åŠ åµŒå…¥ç±»å‹ï¼Œé»˜è®¤ä¸º None
        addition_embed_type: Optional[str] = None,
        # é™„åŠ æ—¶é—´åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º None
        addition_time_embed_dim: Optional[int] = None,
        # æ˜¯å¦ä¸Šæº¯æ³¨æ„åŠ›ï¼Œé»˜è®¤ä¸º True
        upcast_attention: bool = True,
        # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±ï¼Œé»˜è®¤ä¸º True
        use_linear_projection: bool = True,
        # æ—¶é—´æ¡ä»¶æŠ•å½±ç»´åº¦ï¼Œé»˜è®¤ä¸º None
        time_cond_proj_dim: Optional[int] = None,
        # ç±»åˆ«åµŒå…¥è¾“å…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º None
        projection_class_embeddings_input_dim: Optional[int] = None,
        # é™„åŠ æ§åˆ¶ç½‘é…ç½®
        # æ—¶é—´åµŒå…¥æ··åˆç³»æ•°ï¼Œé»˜è®¤ä¸º 1.0
        time_embedding_mix: float = 1.0,
        # æ§åˆ¶æ¡ä»¶é€šé“æ•°ï¼Œé»˜è®¤ä¸º 3
        ctrl_conditioning_channels: int = 3,
        # æ§åˆ¶æ¡ä»¶åµŒå…¥è¾“å‡ºé€šé“çš„å…ƒç»„
        ctrl_conditioning_embedding_out_channels: Tuple[int] = (16, 32, 96, 256),
        # æ§åˆ¶æ¡ä»¶é€šé“é¡ºåºï¼Œé»˜è®¤ä¸º "rgb"
        ctrl_conditioning_channel_order: str = "rgb",
        # æ˜¯å¦å­¦ä¹ æ—¶é—´åµŒå…¥ï¼Œé»˜è®¤ä¸º False
        ctrl_learn_time_embedding: bool = False,
        # æ§åˆ¶å—è¾“å‡ºé€šé“çš„å…ƒç»„
        ctrl_block_out_channels: Tuple[int] = (4, 8, 16, 16),
        # æ§åˆ¶æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 4
        ctrl_num_attention_heads: Union[int, Tuple[int]] = 4,
        # æ§åˆ¶æœ€å¤§å½’ä¸€åŒ–ç»„æ•°ï¼Œé»˜è®¤ä¸º 32
        ctrl_max_norm_num_groups: int = 32,
    # å®šä¹‰ç±»æ–¹æ³•ï¼Œä» UNet åˆ›å»ºæ¨¡å‹
    @classmethod
    def from_unet(
        cls,
        # UNet2DConditionModel å®ä¾‹
        unet: UNet2DConditionModel,
        # å¯é€‰çš„ ControlNetXSAdapter å®ä¾‹
        controlnet: Optional[ControlNetXSAdapter] = None,
        # å¯é€‰çš„å¤§å°æ¯”ä¾‹
        size_ratio: Optional[float] = None,
        # å¯é€‰çš„æ§åˆ¶å—è¾“å‡ºé€šé“åˆ—è¡¨
        ctrl_block_out_channels: Optional[List[float]] = None,
        # å¯é€‰çš„æ—¶é—´åµŒå…¥æ··åˆç³»æ•°
        time_embedding_mix: Optional[float] = None,
        # å¯é€‰çš„æ§åˆ¶é¢å¤–å‚æ•°å­—å…¸
        ctrl_optional_kwargs: Optional[Dict] = None,
    # å†»ç»“ UNet2DConditionModel åŸºæœ¬éƒ¨åˆ†çš„æƒé‡ï¼Œå…¶ä»–éƒ¨åˆ†å¯ç”¨äºå¾®è°ƒ
    def freeze_unet_params(self) -> None:
        """Freeze the weights of the parts belonging to the base UNet2DConditionModel, and leave everything else unfrozen for fine
        tuning."""
        # å°†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦è®¡ç®—è®¾ç½®ä¸ºå¯ç”¨
        for param in self.parameters():
            param.requires_grad = True
    
        # è§£å†» ControlNetXSAdapter ç›¸å…³éƒ¨åˆ†
        base_parts = [
            "base_time_proj",
            "base_time_embedding",
            "base_add_time_proj",
            "base_add_embedding",
            "base_conv_in",
            "base_conv_norm_out",
            "base_conv_act",
            "base_conv_out",
        ]
        # è·å–å­˜åœ¨çš„åŸºæœ¬éƒ¨åˆ†çš„å±æ€§ï¼Œè¿‡æ»¤æ‰ None
        base_parts = [getattr(self, part) for part in base_parts if getattr(self, part) is not None]
        # å†»ç»“åŸºæœ¬éƒ¨åˆ†çš„æ‰€æœ‰å‚æ•°
        for part in base_parts:
            for param in part.parameters():
                param.requires_grad = False
    
        # å†»ç»“æ¯ä¸ªä¸‹é‡‡æ ·å—çš„åŸºæœ¬å‚æ•°
        for d in self.down_blocks:
            d.freeze_base_params()
        # å†»ç»“ä¸­é—´å—çš„åŸºæœ¬å‚æ•°
        self.mid_block.freeze_base_params()
        # å†»ç»“æ¯ä¸ªä¸Šé‡‡æ ·å—çš„åŸºæœ¬å‚æ•°
        for u in self.up_blocks:
            u.freeze_base_params()
    
    # è®¾ç½®æ¨¡å—çš„æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½
    def _set_gradient_checkpointing(self, module, value=False):
        # å¦‚æœæ¨¡å—å…·æœ‰æ¢¯åº¦æ£€æŸ¥ç‚¹å±æ€§ï¼Œåˆ™è®¾ç½®å…¶å€¼
        if hasattr(module, "gradient_checkpointing"):
            module.gradient_checkpointing = value
    
    @property
    # ä» UNet2DConditionModel ä¸­å¤åˆ¶çš„å±æ€§ï¼Œç”¨äºè·å–æ³¨æ„åŠ›å¤„ç†å™¨
    def attn_processors(self) -> Dict[str, AttentionProcessor]:
        r"""
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        """
        # ç”¨äºé€’å½’è®¾ç½®å¤„ç†å™¨çš„å­—å…¸
        processors = {}
    
        # é€’å½’æ·»åŠ å¤„ç†å™¨çš„è¾…åŠ©å‡½æ•°
        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
            # å¦‚æœæ¨¡å—å…·æœ‰è·å–å¤„ç†å™¨çš„æ–¹æ³•ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°å­—å…¸ä¸­
            if hasattr(module, "get_processor"):
                processors[f"{name}.processor"] = module.get_processor()
    
            # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨å¤„ç†å™¨æ·»åŠ å‡½æ•°
            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)
    
            return processors
    
        # éå†å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—ï¼Œå¹¶è°ƒç”¨å¤„ç†å™¨æ·»åŠ å‡½æ•°
        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)
    
        # è¿”å›æ‰€æœ‰å¤„ç†å™¨çš„å­—å…¸
        return processors
    
    # ä» UNet2DConditionModel ä¸­å¤åˆ¶çš„è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•
    # å®šä¹‰è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•ï¼Œå‚æ•°ä¸ºå•ä¸ªå¤„ç†å™¨æˆ–å¤„ç†å™¨å­—å…¸
    def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
        r"""
        è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨ã€‚
    
        å‚æ•°ï¼š
            processorï¼ˆ`dict` æˆ– `AttentionProcessor`ï¼‰ï¼š 
                å®ä¾‹åŒ–çš„å¤„ç†å™¨ç±»æˆ–å°†ä½œä¸ºå¤„ç†å™¨è®¾ç½®çš„å¤„ç†å™¨ç±»å­—å…¸
                å¯¹äº**æ‰€æœ‰** `Attention` å±‚ã€‚
    
                å¦‚æœ `processor` æ˜¯å­—å…¸ï¼Œé”®éœ€è¦å®šä¹‰å¯¹åº”äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„è·¯å¾„ã€‚
                å½“è®¾ç½®å¯è®­ç»ƒçš„æ³¨æ„åŠ›å¤„ç†å™¨æ—¶ï¼Œå¼ºçƒˆæ¨èè¿™æ ·åšã€‚
    
        """
        # è·å–å½“å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„æ•°é‡
        count = len(self.attn_processors.keys())
    
        # å¦‚æœä¼ å…¥çš„å¤„ç†å™¨ä¸ºå­—å…¸ä¸”æ•°é‡ä¸åŒ¹é…ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f"ä¼ å…¥äº†å¤„ç†å™¨å­—å…¸ï¼Œä½†å¤„ç†å™¨æ•°é‡ {len(processor)} ä¸"
                f" æ³¨æ„åŠ›å±‚æ•°é‡ {count} ä¸åŒ¹é…ã€‚è¯·ç¡®ä¿ä¼ å…¥ {count} ä¸ªå¤„ç†å™¨ç±»ã€‚"
            )
    
        # å®šä¹‰é€’å½’è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„å‡½æ•°
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            # å¦‚æœæ¨¡å—å…·æœ‰ set_processor æ–¹æ³•ï¼Œåˆ™è®¾ç½®å¤„ç†å™¨
            if hasattr(module, "set_processor"):
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    module.set_processor(processor.pop(f"{name}.processor"))
    
            # éå†æ¨¡å—çš„å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨å¤„ç†å™¨è®¾ç½®å‡½æ•°
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)
    
        # éå†å½“å‰å¯¹è±¡çš„å­æ¨¡å—ï¼Œè°ƒç”¨é€’å½’è®¾ç½®å‡½æ•°
        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)
    
    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_default_attn_processor å¤åˆ¶çš„
    def set_default_attn_processor(self):
        """
        ç¦ç”¨è‡ªå®šä¹‰æ³¨æ„åŠ›å¤„ç†å™¨å¹¶è®¾ç½®é»˜è®¤çš„æ³¨æ„åŠ›å®ç°ã€‚
        """
        # å¦‚æœæ‰€æœ‰å¤„ç†å™¨éƒ½æ˜¯æ·»åŠ çš„ KV æ³¨æ„åŠ›å¤„ç†å™¨ï¼Œåˆ™è®¾ç½®å¤„ç†å™¨ä¸º AttnAddedKVProcessor
        if all(proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
            processor = AttnAddedKVProcessor()
        # å¦‚æœæ‰€æœ‰å¤„ç†å™¨éƒ½æ˜¯äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨ï¼Œåˆ™è®¾ç½®å¤„ç†å™¨ä¸º AttnProcessor
        elif all(proc.__class__ in CROSS_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
            processor = AttnProcessor()
        else:
            # å¦åˆ™æŠ›å‡ºå¼‚å¸¸ï¼Œæç¤ºæ— æ³•è®¾ç½®é»˜è®¤å¤„ç†å™¨
            raise ValueError(
                f"å½“æ³¨æ„åŠ›å¤„ç†å™¨ç±»å‹ä¸º {next(iter(self.attn_processors.values()))} æ—¶ï¼Œæ— æ³•è°ƒç”¨ `set_default_attn_processor`"
            )
    
        # è°ƒç”¨è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•
        self.set_attn_processor(processor)
    
    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.enable_freeu å¤åˆ¶çš„
    # å®šä¹‰å¯ç”¨ FreeU æœºåˆ¶çš„æ–¹æ³•ï¼Œæ¥æ”¶å››ä¸ªæµ®ç‚¹æ•°å‚æ•°
        def enable_freeu(self, s1: float, s2: float, b1: float, b2: float):
            # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°è¯¥æ–¹æ³•çš„ç”¨é€”å’Œå‚æ•°å«ä¹‰
            r"""Enables the FreeU mechanism from https://arxiv.org/abs/2309.11497.
    
            The suffixes after the scaling factors represent the stage blocks where they are being applied.
    
            Please refer to the [official repository](https://github.com/ChenyangSi/FreeU) for combinations of values that
            are known to work well for different pipelines such as Stable Diffusion v1, v2, and Stable Diffusion XL.
    
            Args:
                s1 (`float`):
                    Scaling factor for stage 1 to attenuate the contributions of the skip features. This is done to
                    mitigate the "oversmoothing effect" in the enhanced denoising process.
                s2 (`float`):
                    Scaling factor for stage 2 to attenuate the contributions of the skip features. This is done to
                    mitigate the "oversmoothing effect" in the enhanced denoising process.
                b1 (`float`): Scaling factor for stage 1 to amplify the contributions of backbone features.
                b2 (`float`): Scaling factor for stage 2 to amplify the contributions of backbone features.
            """
            # éå†ä¸Šé‡‡æ ·æ¨¡å—å¹¶ä¸ºæ¯ä¸ªæ¨¡å—è®¾ç½®å¯¹åº”çš„ scaling å› å­
            for i, upsample_block in enumerate(self.up_blocks):
                # è®¾ç½®ä¸Šé‡‡æ ·å—çš„ s1 å±æ€§ä¸ºä¼ å…¥çš„ s1 å€¼
                setattr(upsample_block, "s1", s1)
                # è®¾ç½®ä¸Šé‡‡æ ·å—çš„ s2 å±æ€§ä¸ºä¼ å…¥çš„ s2 å€¼
                setattr(upsample_block, "s2", s2)
                # è®¾ç½®ä¸Šé‡‡æ ·å—çš„ b1 å±æ€§ä¸ºä¼ å…¥çš„ b1 å€¼
                setattr(upsample_block, "b1", b1)
                # è®¾ç½®ä¸Šé‡‡æ ·å—çš„ b2 å±æ€§ä¸ºä¼ å…¥çš„ b2 å€¼
                setattr(upsample_block, "b2", b2)
    
        # å®šä¹‰ç¦ç”¨ FreeU æœºåˆ¶çš„æ–¹æ³•
        def disable_freeu(self):
            """Disables the FreeU mechanism."""
            # å®šä¹‰ FreeU æœºåˆ¶ä¸­éœ€è¦æ¸…é™¤çš„é”®é›†åˆ
            freeu_keys = {"s1", "s2", "b1", "b2"}
            # éå†ä¸Šé‡‡æ ·æ¨¡å—
            for i, upsample_block in enumerate(self.up_blocks):
                # éå†éœ€è¦æ¸…é™¤çš„é”®
                for k in freeu_keys:
                    # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰è¯¥å±æ€§æˆ–å±æ€§å€¼é None
                    if hasattr(upsample_block, k) or getattr(upsample_block, k, None) is not None:
                        # å°†å±æ€§å€¼è®¾ç½®ä¸º Noneï¼Œç¦ç”¨ FreeU
                        setattr(upsample_block, k, None)
    
        # å®šä¹‰èåˆ QKV æŠ•å½±çš„æ–¹æ³•
        def fuse_qkv_projections(self):
            """
            Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query, key, value)
            are fused. For cross-attention modules, key and value projection matrices are fused.
    
            <Tip warning={true}>
    
            This API is ğŸ§ª experimental.
    
            </Tip>
            """
            # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None
            self.original_attn_processors = None
    
            # éå†æ³¨æ„åŠ›å¤„ç†å™¨
            for _, attn_processor in self.attn_processors.items():
                # æ£€æŸ¥æ˜¯å¦æœ‰æ·»åŠ çš„ KV æŠ•å½±ï¼Œä¸æ”¯æŒèåˆ
                if "Added" in str(attn_processor.__class__.__name__):
                    raise ValueError("`fuse_qkv_projections()` is not supported for models having added KV projections.")
    
            # è®°å½•åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨
            self.original_attn_processors = self.attn_processors
    
            # éå†æ‰€æœ‰æ¨¡å—
            for module in self.modules():
                # æ£€æŸ¥æ¨¡å—æ˜¯å¦ä¸ºæ³¨æ„åŠ›æ¨¡å—
                if isinstance(module, Attention):
                    # æ‰§è¡ŒæŠ•å½±èåˆ
                    module.fuse_projections(fuse=True)
    
            # è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä¸ºèåˆåçš„å¤„ç†å™¨
            self.set_attn_processor(FusedAttnProcessor2_0())
    
        # æ­¤éƒ¨åˆ†ä»£ç æœªæä¾›ï¼Œå¯èƒ½æ˜¯ç¦ç”¨ QKV æŠ•å½±çš„æ–¹æ³•
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºç¦ç”¨å·²å¯ç”¨çš„èåˆ QKV æŠ•å½±
    def unfuse_qkv_projections(self):
        """Disables the fused QKV projection if enabled.

        <Tip warning={true}>

        This API is ğŸ§ª experimental.

        </Tip>

        """
        # å¦‚æœåŸå§‹çš„æ³¨æ„åŠ›å¤„ç†å™¨ä¸ä¸º Noneï¼Œåˆ™è®¾ç½®ä¸ºåŸå§‹å¤„ç†å™¨
        if self.original_attn_processors is not None:
            self.set_attn_processor(self.original_attn_processors)

    # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œæ¥æ”¶å¤šä¸ªè¾“å…¥å‚æ•°
    def forward(
        self,
        sample: Tensor,  # è¾“å…¥æ ·æœ¬ï¼Œç±»å‹ä¸º Tensor
        timestep: Union[torch.Tensor, float, int],  # æ—¶é—´æ­¥ï¼Œæ”¯æŒå¤šç§ç±»å‹
        encoder_hidden_states: torch.Tensor,  # ç¼–ç å™¨çš„éšè—çŠ¶æ€ï¼Œç±»å‹ä¸º Tensor
        controlnet_cond: Optional[torch.Tensor] = None,  # å¯é€‰çš„æ§åˆ¶ç½‘ç»œæ¡ä»¶ï¼Œç±»å‹ä¸º Tensor
        conditioning_scale: Optional[float] = 1.0,  # æ¡ä»¶ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 1.0
        class_labels: Optional[torch.Tensor] = None,  # å¯é€‰çš„ç±»æ ‡ç­¾ï¼Œç±»å‹ä¸º Tensor
        timestep_cond: Optional[torch.Tensor] = None,  # å¯é€‰çš„æ—¶é—´æ­¥æ¡ä»¶ï¼Œç±»å‹ä¸º Tensor
        attention_mask: Optional[torch.Tensor] = None,  # å¯é€‰çš„æ³¨æ„åŠ›æ©ç ï¼Œç±»å‹ä¸º Tensor
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,  # å¯é€‰çš„äº¤å‰æ³¨æ„åŠ›å‚æ•°ï¼Œç±»å‹ä¸ºå­—å…¸
        added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,  # å¯é€‰çš„é™„åŠ æ¡ä»¶å‚æ•°ï¼Œç±»å‹ä¸ºå­—å…¸
        return_dict: bool = True,  # æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„ç»“æœï¼Œé»˜è®¤ä¸º True
        apply_control: bool = True,  # æ˜¯å¦åº”ç”¨æ§åˆ¶é€»è¾‘ï¼Œé»˜è®¤ä¸º True
# å®šä¹‰ä¸€ä¸ªåä¸º ControlNetXSCrossAttnDownBlock2D çš„ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
class ControlNetXSCrossAttnDownBlock2D(nn.Module):
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œå®šä¹‰ç±»çš„å±æ€§å’Œå‚æ•°
    def __init__(
        self,
        base_in_channels: int,  # åŸºç¡€è¾“å…¥é€šé“æ•°
        base_out_channels: int,  # åŸºç¡€è¾“å‡ºé€šé“æ•°
        ctrl_in_channels: int,  # æ§åˆ¶è¾“å…¥é€šé“æ•°
        ctrl_out_channels: int,  # æ§åˆ¶è¾“å‡ºé€šé“æ•°
        temb_channels: int,  # æ—¶é—´åµŒå…¥é€šé“æ•°
        norm_num_groups: int = 32,  # è§„èŒƒåŒ–ç»„æ•°
        ctrl_max_norm_num_groups: int = 32,  # æ§åˆ¶æœ€å¤§è§„èŒƒåŒ–ç»„æ•°
        has_crossattn=True,  # æ˜¯å¦åŒ…å«äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        transformer_layers_per_block: Optional[Union[int, Tuple[int]]] = 1,  # æ¯ä¸ªå—çš„å˜æ¢å™¨å±‚æ•°
        base_num_attention_heads: Optional[int] = 1,  # åŸºç¡€æ³¨æ„åŠ›å¤´æ•°
        ctrl_num_attention_heads: Optional[int] = 1,  # æ§åˆ¶æ³¨æ„åŠ›å¤´æ•°
        cross_attention_dim: Optional[int] = 1024,  # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
        add_downsample: bool = True,  # æ˜¯å¦æ·»åŠ ä¸‹é‡‡æ ·
        upcast_attention: Optional[bool] = False,  # æ˜¯å¦ä¸Šå‡æ³¨æ„åŠ›
        use_linear_projection: Optional[bool] = True,  # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
    @classmethod
    # å®šä¹‰ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºå†»ç»“åŸºç¡€æ¨¡å‹çš„å‚æ•°
    def freeze_base_params(self) -> None:
        """å†»ç»“åŸºç¡€ UNet2DConditionModel çš„æƒé‡ï¼Œä¿æŒå…¶ä»–éƒ¨åˆ†å¯è°ƒï¼Œä»¥ä¾¿å¾®è°ƒã€‚"""
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in self.parameters():
            param.requires_grad = True

        # å†»ç»“åŸºç¡€éƒ¨åˆ†çš„å‚æ•°
        base_parts = [self.base_resnets]  # åŒ…å«åŸºç¡€æ®‹å·®ç½‘ç»œéƒ¨åˆ†
        if isinstance(self.base_attentions, nn.ModuleList):  # å¦‚æœæ³¨æ„åŠ›éƒ¨åˆ†æ˜¯ä¸€ä¸ªæ¨¡å—åˆ—è¡¨
            base_parts.append(self.base_attentions)  # æ·»åŠ åŸºç¡€æ³¨æ„åŠ›éƒ¨åˆ†
        if self.base_downsamplers is not None:  # å¦‚æœå­˜åœ¨åŸºç¡€ä¸‹é‡‡æ ·éƒ¨åˆ†
            base_parts.append(self.base_downsamplers)  # æ·»åŠ åŸºç¡€ä¸‹é‡‡æ ·éƒ¨åˆ†
        for part in base_parts:  # éå†åŸºç¡€éƒ¨åˆ†
            for param in part.parameters():  # éå†å‚æ•°
                param.requires_grad = False  # å†»ç»“å‚æ•°ä»¥é˜²æ­¢æ›´æ–°

    # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•
    def forward(
        self,
        hidden_states_base: Tensor,  # åŸºç¡€éšè—çŠ¶æ€
        temb: Tensor,  # æ—¶é—´åµŒå…¥
        encoder_hidden_states: Optional[Tensor] = None,  # ç¼–ç å™¨éšè—çŠ¶æ€
        hidden_states_ctrl: Optional[Tensor] = None,  # æ§åˆ¶éšè—çŠ¶æ€
        conditioning_scale: Optional[float] = 1.0,  # æ¡ä»¶ç¼©æ”¾å› å­
        attention_mask: Optional[Tensor] = None,  # æ³¨æ„åŠ›æ©ç 
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,  # äº¤å‰æ³¨æ„åŠ›çš„å…³é”®å­—å‚æ•°
        encoder_attention_mask: Optional[Tensor] = None,  # ç¼–ç å™¨æ³¨æ„åŠ›æ©ç 
        apply_control: bool = True,  # æ˜¯å¦åº”ç”¨æ§åˆ¶
class ControlNetXSCrossAttnMidBlock2D(nn.Module):
    # å®šä¹‰ä¸€ä¸ªåä¸º ControlNetXSCrossAttnMidBlock2D çš„ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
    def __init__(
        self,
        base_channels: int,  # åŸºç¡€é€šé“æ•°
        ctrl_channels: int,  # æ§åˆ¶é€šé“æ•°
        temb_channels: Optional[int] = None,  # æ—¶é—´åµŒå…¥é€šé“æ•°ï¼ˆå¯é€‰ï¼‰
        norm_num_groups: int = 32,  # è§„èŒƒåŒ–ç»„æ•°
        ctrl_max_norm_num_groups: int = 32,  # æ§åˆ¶æœ€å¤§è§„èŒƒåŒ–ç»„æ•°
        transformer_layers_per_block: int = 1,  # æ¯ä¸ªå—çš„å˜æ¢å™¨å±‚æ•°
        base_num_attention_heads: Optional[int] = 1,  # åŸºç¡€æ³¨æ„åŠ›å¤´æ•°
        ctrl_num_attention_heads: Optional[int] = 1,  # æ§åˆ¶æ³¨æ„åŠ›å¤´æ•°
        cross_attention_dim: Optional[int] = 1024,  # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
        upcast_attention: bool = False,  # æ˜¯å¦ä¸Šå‡æ³¨æ„åŠ›
        use_linear_projection: Optional[bool] = True,  # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
    ):
        # è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°ä»¥åˆå§‹åŒ–ç»§æ‰¿çš„å±æ€§å’Œæ–¹æ³•
        super().__init__()

        # åœ¨ä¸­é—´å—åº”ç”¨ä¹‹å‰ï¼Œä»åŸºç¡€ä¿¡æ¯åˆ°æ§åˆ¶ä¿¡æ¯çš„è¿æ¥ã€‚
        # è¿æ¥ä¸éœ€è¦æ”¹å˜é€šé“æ•°é‡
        self.base_to_ctrl = make_zero_conv(base_channels, base_channels)

        # åˆ›å»ºåŸºç¡€ä¸­é—´å—ï¼Œä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        self.base_midblock = UNetMidBlock2DCrossAttn(
            # æ¯ä¸ªå—ä¸­çš„å˜æ¢å™¨å±‚æ•°é‡
            transformer_layers_per_block=transformer_layers_per_block,
            # è¾“å…¥é€šé“æ•°ä¸ºåŸºç¡€é€šé“æ•°
            in_channels=base_channels,
            # åµŒå…¥é€šé“æ•°
            temb_channels=temb_channels,
            # ResNet ç»„çš„æ•°é‡
            resnet_groups=norm_num_groups,
            # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
            cross_attention_dim=cross_attention_dim,
            # æ³¨æ„åŠ›å¤´çš„æ•°é‡
            num_attention_heads=base_num_attention_heads,
            # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
            use_linear_projection=use_linear_projection,
            # æ˜¯å¦ä¸Šæº¯æ³¨æ„åŠ›
            upcast_attention=upcast_attention,
        )

        # åˆ›å»ºæ§åˆ¶ä¸­é—´å—ï¼Œä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        self.ctrl_midblock = UNetMidBlock2DCrossAttn(
            # æ¯ä¸ªå—ä¸­çš„å˜æ¢å™¨å±‚æ•°é‡
            transformer_layers_per_block=transformer_layers_per_block,
            # è¾“å…¥é€šé“æ•°ä¸ºæ§åˆ¶é€šé“æ•°åŠ åŸºç¡€é€šé“æ•°
            in_channels=ctrl_channels + base_channels,
            # è¾“å‡ºé€šé“æ•°ä¸ºæ§åˆ¶é€šé“æ•°
            out_channels=ctrl_channels,
            # åµŒå…¥é€šé“æ•°
            temb_channels=temb_channels,
            # norm ç»„æ•°é‡å¿…é¡»åŒæ—¶èƒ½è¢«è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°æ•´é™¤
            resnet_groups=find_largest_factor(
                # è®¡ç®—æ§åˆ¶é€šé“ä¸æ§åˆ¶é€šé“åŠ åŸºç¡€é€šé“çš„æœ€å¤§å…¬çº¦æ•°
                gcd(ctrl_channels, ctrl_channels + base_channels), ctrl_max_norm_num_groups
            ),
            # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
            cross_attention_dim=cross_attention_dim,
            # æ³¨æ„åŠ›å¤´çš„æ•°é‡
            num_attention_heads=ctrl_num_attention_heads,
            # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
            use_linear_projection=use_linear_projection,
            # æ˜¯å¦ä¸Šæº¯æ³¨æ„åŠ›
            upcast_attention=upcast_attention,
        )

        # åœ¨ä¸­é—´å—åº”ç”¨ä¹‹åï¼Œä»æ§åˆ¶ä¿¡æ¯åˆ°åŸºç¡€ä¿¡æ¯çš„ç›¸åŠ 
        # ç›¸åŠ éœ€è¦æ”¹å˜é€šé“æ•°é‡
        self.ctrl_to_base = make_zero_conv(ctrl_channels, base_channels)

        # åˆå§‹åŒ–æ¢¯åº¦æ£€æŸ¥ç‚¹æ ‡å¿—ä¸ºå‡
        self.gradient_checkpointing = False

    @classmethod
    def from_modules(
        # ç±»æ–¹æ³•ï¼Œæ¥å—åŸºç¡€ä¸­é—´å—å’Œæ§åˆ¶ä¸­é—´å—ä½œä¸ºå‚æ•°
        cls,
        base_midblock: UNetMidBlock2DCrossAttn,
        ctrl_midblock: MidBlockControlNetXSAdapter,
    ):
        # è·å–ä¸­é—´å—çš„åŸºå‡†åˆ°æ§åˆ¶çš„æ˜ å°„
        base_to_ctrl = ctrl_midblock.base_to_ctrl
        # è·å–ä¸­é—´å—çš„æ§åˆ¶åˆ°åŸºå‡†çš„æ˜ å°„
        ctrl_to_base = ctrl_midblock.ctrl_to_base
        # è·å–ä¸­é—´å—çš„å®ä¾‹
        ctrl_midblock = ctrl_midblock.midblock

        # è·å–ç¬¬ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å—
        def get_first_cross_attention(midblock):
            # è¿”å›ä¸­é—´å—çš„ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„äº¤å‰æ³¨æ„åŠ›å±‚
            return midblock.attentions[0].transformer_blocks[0].attn2

        # è·å–æ§åˆ¶åˆ°åŸºå‡†çš„è¾“å‡ºé€šé“æ•°
        base_channels = ctrl_to_base.out_channels
        # è·å–æ§åˆ¶åˆ°åŸºå‡†çš„è¾“å…¥é€šé“æ•°
        ctrl_channels = ctrl_to_base.in_channels
        # è·å–åŸºå‡†ä¸­é—´å—çš„æ¯ä¸ªå—çš„è½¬æ¢å±‚æ•°
        transformer_layers_per_block = len(base_midblock.attentions[0].transformer_blocks)
        # è·å–åŸºå‡†ä¸­é—´å—æ—¶é—´åµŒå…¥çš„è¾“å…¥ç‰¹å¾æ•°
        temb_channels = base_midblock.resnets[0].time_emb_proj.in_features
        # è·å–åŸºå‡†ä¸­é—´å—çš„å½’ä¸€åŒ–ç»„æ•°
        num_groups = base_midblock.resnets[0].norm1.num_groups
        # è·å–æ§åˆ¶ä¸­é—´å—çš„å½’ä¸€åŒ–ç»„æ•°
        ctrl_num_groups = ctrl_midblock.resnets[0].norm1.num_groups
        # è·å–åŸºå‡†ä¸­é—´å—ç¬¬ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„æ³¨æ„åŠ›å¤´æ•°
        base_num_attention_heads = get_first_cross_attention(base_midblock).heads
        # è·å–æ§åˆ¶ä¸­é—´å—ç¬¬ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„æ³¨æ„åŠ›å¤´æ•°
        ctrl_num_attention_heads = get_first_cross_attention(ctrl_midblock).heads
        # è·å–åŸºå‡†ä¸­é—´å—ç¬¬ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„äº¤å‰æ³¨æ„åŠ›ç»´åº¦
        cross_attention_dim = get_first_cross_attention(base_midblock).cross_attention_dim
        # è·å–åŸºå‡†ä¸­é—´å—ç¬¬ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„ä¸Šé‡‡æ ·æ³¨æ„åŠ›è®¾ç½®
        upcast_attention = get_first_cross_attention(base_midblock).upcast_attention
        # è·å–åŸºå‡†ä¸­é—´å—ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å—çš„çº¿æ€§æŠ•å½±ä½¿ç”¨æƒ…å†µ
        use_linear_projection = base_midblock.attentions[0].use_linear_projection

        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = cls(
            # ä¼ å…¥åŸºå‡†é€šé“æ•°
            base_channels=base_channels,
            # ä¼ å…¥æ§åˆ¶é€šé“æ•°
            ctrl_channels=ctrl_channels,
            # ä¼ å…¥æ—¶é—´åµŒå…¥é€šé“æ•°
            temb_channels=temb_channels,
            # ä¼ å…¥å½’ä¸€åŒ–ç»„æ•°
            norm_num_groups=num_groups,
            # ä¼ å…¥æ§åˆ¶æœ€å¤§å½’ä¸€åŒ–ç»„æ•°
            ctrl_max_norm_num_groups=ctrl_num_groups,
            # ä¼ å…¥æ¯å—çš„è½¬æ¢å±‚æ•°
            transformer_layers_per_block=transformer_layers_per_block,
            # ä¼ å…¥åŸºå‡†æ³¨æ„åŠ›å¤´æ•°
            base_num_attention_heads=base_num_attention_heads,
            # ä¼ å…¥æ§åˆ¶æ³¨æ„åŠ›å¤´æ•°
            ctrl_num_attention_heads=ctrl_num_attention_heads,
            # ä¼ å…¥äº¤å‰æ³¨æ„åŠ›ç»´åº¦
            cross_attention_dim=cross_attention_dim,
            # ä¼ å…¥ä¸Šé‡‡æ ·æ³¨æ„åŠ›è®¾ç½®
            upcast_attention=upcast_attention,
            # ä¼ å…¥çº¿æ€§æŠ•å½±ä½¿ç”¨æƒ…å†µ
            use_linear_projection=use_linear_projection,
        )

        # åŠ è½½æ¨¡å‹æƒé‡
        model.base_to_ctrl.load_state_dict(base_to_ctrl.state_dict())
        # åŠ è½½åŸºå‡†ä¸­é—´å—çš„æƒé‡
        model.base_midblock.load_state_dict(base_midblock.state_dict())
        # åŠ è½½æ§åˆ¶ä¸­é—´å—çš„æƒé‡
        model.ctrl_midblock.load_state_dict(ctrl_midblock.state_dict())
        # åŠ è½½æ§åˆ¶åˆ°åŸºå‡†çš„æƒé‡
        model.ctrl_to_base.load_state_dict(ctrl_to_base.state_dict())

        # è¿”å›æ„å»ºå¥½çš„æ¨¡å‹
        return model

    def freeze_base_params(self) -> None:
        """å†»ç»“å±äºåŸºå‡† UNet2DConditionModel çš„æƒé‡ï¼Œä¿ç•™å…¶ä»–éƒ¨åˆ†ä»¥ä¾¿è¿›è¡Œå¾®è°ƒã€‚"""
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in self.parameters():
            param.requires_grad = True

        # å†»ç»“åŸºå‡†éƒ¨åˆ†çš„å‚æ•°
        for param in self.base_midblock.parameters():
            param.requires_grad = False

    def forward(
        self,
        # åŸºå‡†çš„éšè—çŠ¶æ€
        hidden_states_base: Tensor,
        # æ—¶é—´åµŒå…¥
        temb: Tensor,
        # ç¼–ç å™¨çš„éšè—çŠ¶æ€
        encoder_hidden_states: Tensor,
        # æ§åˆ¶çš„éšè—çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
        hidden_states_ctrl: Optional[Tensor] = None,
        # æ¡ä»¶ç¼©æ”¾å› å­ï¼ˆå¯é€‰ï¼‰ï¼Œé»˜è®¤ä¸º1.0
        conditioning_scale: Optional[float] = 1.0,
        # äº¤å‰æ³¨æ„åŠ›çš„é¢å¤–å‚æ•°ï¼ˆå¯é€‰ï¼‰
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        # æ³¨æ„åŠ›æ©ç ï¼ˆå¯é€‰ï¼‰
        attention_mask: Optional[Tensor] = None,
        # ç¼–ç å™¨çš„æ³¨æ„åŠ›æ©ç ï¼ˆå¯é€‰ï¼‰
        encoder_attention_mask: Optional[Tensor] = None,
        # æ˜¯å¦åº”ç”¨æ§åˆ¶ï¼ˆé»˜è®¤ä¸ºTrueï¼‰
        apply_control: bool = True,
    # è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„
    ) -> Tuple[Tensor, Tensor]:
        # å¦‚æœæä¾›äº†äº¤å‰æ³¨æ„åŠ›çš„å‚æ•°
        if cross_attention_kwargs is not None:
            # æ£€æŸ¥æ˜¯å¦æœ‰ scale å‚æ•°ï¼Œå¹¶å‘å‡ºè­¦å‘Š
            if cross_attention_kwargs.get("scale", None) is not None:
                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
    
        # è®¾ç½®åŸºç¡€éšè—çŠ¶æ€
        h_base = hidden_states_base
        # è®¾ç½®æ§åˆ¶éšè—çŠ¶æ€
        h_ctrl = hidden_states_ctrl
    
        # åˆ›å»ºä¸€ä¸ªåŒ…å«å¤šä¸ªå‚æ•°çš„å­—å…¸
        joint_args = {
            "temb": temb,
            "encoder_hidden_states": encoder_hidden_states,
            "attention_mask": attention_mask,
            "cross_attention_kwargs": cross_attention_kwargs,
            "encoder_attention_mask": encoder_attention_mask,
        }
    
        # å¦‚æœåº”ç”¨æ§åˆ¶ï¼Œåˆ™è¿æ¥åŸºç¡€å’Œæ§åˆ¶éšè—çŠ¶æ€
        if apply_control:
            h_ctrl = torch.cat([h_ctrl, self.base_to_ctrl(h_base)], dim=1)  # concat base -> ctrl
        # åº”ç”¨åŸºç¡€ä¸­é—´å—åˆ°åŸºç¡€éšè—çŠ¶æ€
        h_base = self.base_midblock(h_base, **joint_args)  # apply base mid block
        # å¦‚æœåº”ç”¨æ§åˆ¶ï¼Œåˆ™åº”ç”¨æ§åˆ¶ä¸­é—´å—
        if apply_control:
            h_ctrl = self.ctrl_midblock(h_ctrl, **joint_args)  # apply ctrl mid block
            # å°†æ§åˆ¶ç»“æœåŠ åˆ°åŸºç¡€éšè—çŠ¶æ€ä¸Šï¼Œä¹˜ä»¥æ¡ä»¶ç¼©æ”¾å› å­
            h_base = h_base + self.ctrl_to_base(h_ctrl) * conditioning_scale  # add ctrl -> base
    
        # è¿”å›åŸºç¡€å’Œæ§åˆ¶çš„éšè—çŠ¶æ€
        return h_base, h_ctrl
# å®šä¹‰ä¸€ä¸ªåä¸º ControlNetXSCrossAttnUpBlock2D çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œç»§æ‰¿è‡ª nn.Module
class ControlNetXSCrossAttnUpBlock2D(nn.Module):
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œå®šä¹‰è¯¥æ¨¡å—çš„å‚æ•°
    def __init__(
        self,
        in_channels: int,  # è¾“å…¥é€šé“æ•°
        out_channels: int,  # è¾“å‡ºé€šé“æ•°
        prev_output_channel: int,  # å‰ä¸€å±‚çš„è¾“å‡ºé€šé“æ•°
        ctrl_skip_channels: List[int],  # æ§åˆ¶è·³è·ƒè¿æ¥çš„é€šé“æ•°åˆ—è¡¨
        temb_channels: int,  # æ—¶é—´åµŒå…¥é€šé“æ•°
        norm_num_groups: int = 32,  # å½’ä¸€åŒ–çš„ç»„æ•°ï¼Œé»˜è®¤å€¼ä¸º32
        resolution_idx: Optional[int] = None,  # åˆ†è¾¨ç‡ç´¢å¼•ï¼Œå¯é€‰
        has_crossattn=True,  # æ˜¯å¦åŒ…å«äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œé»˜è®¤å€¼ä¸ºTrue
        transformer_layers_per_block: int = 1,  # æ¯ä¸ªæ¨¡å—çš„å˜æ¢å™¨å±‚æ•°ï¼Œé»˜è®¤å€¼ä¸º1
        num_attention_heads: int = 1,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º1
        cross_attention_dim: int = 1024,  # äº¤å‰æ³¨æ„åŠ›çš„ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º1024
        add_upsample: bool = True,  # æ˜¯å¦æ·»åŠ ä¸Šé‡‡æ ·å±‚ï¼Œé»˜è®¤å€¼ä¸ºTrue
        upcast_attention: bool = False,  # æ˜¯å¦æå‡æ³¨æ„åŠ›è®¡ç®—ç²¾åº¦ï¼Œé»˜è®¤å€¼ä¸ºFalse
        use_linear_projection: Optional[bool] = True,  # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±ï¼Œé»˜è®¤å€¼ä¸ºTrue
    ):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()
        resnets = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾ ResNet æ¨¡å—
        attentions = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾æ³¨æ„åŠ›æ¨¡å—
        ctrl_to_base = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾æ§åˆ¶åˆ°åŸºç¡€çš„å·ç§¯æ¨¡å—

        num_layers = 3  # ä»…æ”¯æŒ3å±‚ï¼Œé€‚ç”¨äº sd å’Œ sdxl

        # è®°å½•æ˜¯å¦åŒ…å«äº¤å‰æ³¨æ„åŠ›å’Œæ³¨æ„åŠ›å¤´çš„æ•°é‡
        self.has_cross_attention = has_crossattn
        self.num_attention_heads = num_attention_heads

        # å¦‚æœ transformer_layers_per_block æ˜¯æ•´æ•°ï¼Œåˆ™å°†å…¶æ‰©å±•ä¸ºåŒ…å« num_layers ä¸ªç›¸åŒå€¼çš„åˆ—è¡¨
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers

        # éå†æ¯ä¸€å±‚
        for i in range(num_layers):
            # ç¡®å®šå½“å‰å±‚çš„è·³è·ƒè¿æ¥é€šé“æ•°
            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels
            # ç¡®å®šå½“å‰å±‚çš„è¾“å…¥é€šé“æ•°
            resnet_in_channels = prev_output_channel if i == 0 else out_channels

            # åˆ›å»ºä»æ§åˆ¶é€šé“åˆ°åŸºç¡€é€šé“çš„é›¶å·ç§¯ï¼Œå¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            ctrl_to_base.append(make_zero_conv(ctrl_skip_channels[i], resnet_in_channels))

            # æ·»åŠ  ResNet æ¨¡å—åˆ° resnets åˆ—è¡¨
            resnets.append(
                ResnetBlock2D(
                    in_channels=resnet_in_channels + res_skip_channels,  # è¾“å…¥é€šé“æ•°
                    out_channels=out_channels,  # è¾“å‡ºé€šé“æ•°
                    temb_channels=temb_channels,  # æ—¶é—´åµŒå…¥é€šé“æ•°
                    groups=norm_num_groups,  # å½’ä¸€åŒ–ç»„æ•°
                )
            )

            # å¦‚æœåŒ…å«äº¤å‰æ³¨æ„åŠ›ï¼Œåˆ™æ·»åŠ  Transformer æ¨¡å—åˆ° attentions åˆ—è¡¨
            if has_crossattn:
                attentions.append(
                    Transformer2DModel(
                        num_attention_heads,  # æ³¨æ„åŠ›å¤´æ•°é‡
                        out_channels // num_attention_heads,  # æ¯ä¸ªå¤´çš„è¾“å‡ºé€šé“æ•°
                        in_channels=out_channels,  # è¾“å…¥é€šé“æ•°
                        num_layers=transformer_layers_per_block[i],  # å½“å‰å±‚çš„å˜æ¢å™¨å±‚æ•°
                        cross_attention_dim=cross_attention_dim,  # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
                        use_linear_projection=use_linear_projection,  # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
                        upcast_attention=upcast_attention,  # æ˜¯å¦æå‡æ³¨æ„åŠ›è®¡ç®—ç²¾åº¦
                        norm_num_groups=norm_num_groups,  # å½’ä¸€åŒ–ç»„æ•°
                    )
                )

        # å°† ResNet æ¨¡å—åˆ—è¡¨è½¬æ¢ä¸º nn.ModuleListï¼Œä»¥ä¾¿åœ¨æ¨¡å‹ä¸­ç®¡ç†
        self.resnets = nn.ModuleList(resnets)
        # å¦‚æœæœ‰äº¤å‰æ³¨æ„åŠ›ï¼Œè½¬æ¢ attentions åˆ—è¡¨ä¸º nn.ModuleListï¼Œå¦åˆ™å¡«å…… None
        self.attentions = nn.ModuleList(attentions) if has_crossattn else [None] * num_layers
        # å°†æ§åˆ¶åˆ°åŸºç¡€çš„å·ç§¯æ¨¡å—åˆ—è¡¨è½¬æ¢ä¸º nn.ModuleList
        self.ctrl_to_base = nn.ModuleList(ctrl_to_base)

        # å¦‚æœéœ€è¦æ·»åŠ ä¸Šé‡‡æ ·å±‚ï¼Œåˆå§‹åŒ– Upsample2D æ¨¡å—
        if add_upsample:
            self.upsamplers = Upsample2D(out_channels, use_conv=True, out_channels=out_channels)
        else:
            self.upsamplers = None  # å¦‚æœä¸éœ€è¦ä¸Šé‡‡æ ·ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º None

        self.gradient_checkpointing = False  # åˆå§‹åŒ–æ—¶ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        self.resolution_idx = resolution_idx  # è®¾ç½®åˆ†è¾¨ç‡ç´¢å¼•
    # ä»æ¨¡å—åˆ›å»ºæ¨¡å‹çš„ç±»æ–¹æ³•
        def from_modules(cls, base_upblock: CrossAttnUpBlock2D, ctrl_upblock: UpBlockControlNetXSAdapter):
            # è·å–æ§åˆ¶åˆ°åŸºç¡€çš„è·³è·ƒè¿æ¥
            ctrl_to_base_skip_connections = ctrl_upblock.ctrl_to_base
    
            # è·å–å‚æ•°
            # è·å–ç¬¬ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å—
            def get_first_cross_attention(block):
                return block.attentions[0].transformer_blocks[0].attn2
    
            # è·å–åŸºç¡€ä¸Šé‡‡æ ·å—çš„è¾“å‡ºé€šé“æ•°
            out_channels = base_upblock.resnets[0].out_channels
            # è®¡ç®—è¾“å…¥é€šé“æ•°
            in_channels = base_upblock.resnets[-1].in_channels - out_channels
            # è®¡ç®—å‰ä¸€ä¸ªè¾“å‡ºé€šé“æ•°
            prev_output_channels = base_upblock.resnets[0].in_channels - out_channels
            # è·å–æ§åˆ¶è·³è·ƒè¿æ¥çš„è¾“å…¥é€šé“æ•°
            ctrl_skip_channelss = [c.in_channels for c in ctrl_to_base_skip_connections]
            # è·å–æ—¶é—´åµŒå…¥çš„è¾“å…¥ç‰¹å¾æ•°
            temb_channels = base_upblock.resnets[0].time_emb_proj.in_features
            # è·å–å½’ä¸€åŒ–ç»„æ•°
            num_groups = base_upblock.resnets[0].norm1.num_groups
            # è·å–åˆ†è¾¨ç‡ç´¢å¼•
            resolution_idx = base_upblock.resolution_idx
            # æ£€æŸ¥åŸºç¡€ä¸Šé‡‡æ ·å—æ˜¯å¦æœ‰æ³¨æ„åŠ›æ¨¡å—
            if hasattr(base_upblock, "attentions"):
                has_crossattn = True
                # è·å–æ¯ä¸ªå—çš„å˜æ¢å±‚æ•°
                transformer_layers_per_block = len(base_upblock.attentions[0].transformer_blocks)
                # è·å–æ³¨æ„åŠ›å¤´æ•°
                num_attention_heads = get_first_cross_attention(base_upblock).heads
                # è·å–äº¤å‰æ³¨æ„åŠ›ç»´åº¦
                cross_attention_dim = get_first_cross_attention(base_upblock).cross_attention_dim
                # è·å–ä¸Šå‡æ³¨æ„åŠ›æ ‡å¿—
                upcast_attention = get_first_cross_attention(base_upblock).upcast_attention
                # è·å–æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
                use_linear_projection = base_upblock.attentions[0].use_linear_projection
            else:
                has_crossattn = False
                transformer_layers_per_block = None
                num_attention_heads = None
                cross_attention_dim = None
                upcast_attention = None
                use_linear_projection = None
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ä¸Šé‡‡æ ·
            add_upsample = base_upblock.upsamplers is not None
    
            # åˆ›å»ºæ¨¡å‹
            model = cls(
                # è¾“å…¥é€šé“æ•°
                in_channels=in_channels,
                # è¾“å‡ºé€šé“æ•°
                out_channels=out_channels,
                # å‰ä¸€ä¸ªè¾“å‡ºé€šé“
                prev_output_channel=prev_output_channels,
                # æ§åˆ¶è·³è·ƒè¿æ¥çš„è¾“å…¥é€šé“æ•°
                ctrl_skip_channels=ctrl_skip_channelss,
                # æ—¶é—´åµŒå…¥çš„é€šé“æ•°
                temb_channels=temb_channels,
                # å½’ä¸€åŒ–çš„ç»„æ•°
                norm_num_groups=num_groups,
                # åˆ†è¾¨ç‡ç´¢å¼•
                resolution_idx=resolution_idx,
                # æ˜¯å¦æœ‰äº¤å‰æ³¨æ„åŠ›
                has_crossattn=has_crossattn,
                # æ¯ä¸ªå—çš„å˜æ¢å±‚æ•°
                transformer_layers_per_block=transformer_layers_per_block,
                # æ³¨æ„åŠ›å¤´æ•°
                num_attention_heads=num_attention_heads,
                # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
                cross_attention_dim=cross_attention_dim,
                # æ˜¯å¦æ·»åŠ ä¸Šé‡‡æ ·
                add_upsample=add_upsample,
                # ä¸Šå‡æ³¨æ„åŠ›æ ‡å¿—
                upcast_attention=upcast_attention,
                # æ˜¯å¦ä½¿ç”¨çº¿æ€§æŠ•å½±
                use_linear_projection=use_linear_projection,
            )
    
            # åŠ è½½æƒé‡
            model.resnets.load_state_dict(base_upblock.resnets.state_dict())
            # å¦‚æœæœ‰äº¤å‰æ³¨æ„åŠ›ï¼ŒåŠ è½½å…¶æƒé‡
            if has_crossattn:
                model.attentions.load_state_dict(base_upblock.attentions.state_dict())
            # å¦‚æœéœ€è¦æ·»åŠ ä¸Šé‡‡æ ·ï¼ŒåŠ è½½å…¶æƒé‡
            if add_upsample:
                model.upsamplers.load_state_dict(base_upblock.upsamplers[0].state_dict())
            # åŠ è½½æ§åˆ¶åˆ°åŸºç¡€çš„è·³è·ƒè¿æ¥æƒé‡
            model.ctrl_to_base.load_state_dict(ctrl_to_base_skip_connections.state_dict())
    
            # è¿”å›åˆ›å»ºçš„æ¨¡å‹
            return model
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå†»ç»“åŸºç¡€ UNet2DConditionModel çš„å‚æ•°
    def freeze_base_params(self) -> None:
        """å†»ç»“å±äºåŸºç¡€ UNet2DConditionModel çš„æƒé‡ï¼Œå…¶ä»–éƒ¨åˆ†ä¿æŒè§£å†»ä»¥ä¾¿å¾®è°ƒã€‚"""
        # è§£å†»æ‰€æœ‰å‚æ•°ï¼Œå…è®¸è®­ç»ƒ
        for param in self.parameters():
            param.requires_grad = True
    
        # å†»ç»“åŸºç¡€éƒ¨åˆ†çš„å‚æ•°
        base_parts = [self.resnets]  # å°†åŸºç¡€éƒ¨åˆ†ï¼ˆresnetsï¼‰æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        # æ£€æŸ¥ attentions æ˜¯å¦æ˜¯ ModuleList ç±»å‹ï¼ˆå¯èƒ½åŒ…å« Noneï¼‰
        if isinstance(self.attentions, nn.ModuleList):
            base_parts.append(self.attentions)  # å¦‚æœæ˜¯ï¼Œåˆ™æ·»åŠ  attentions
        # æ£€æŸ¥ upsamplers æ˜¯å¦ä¸ä¸º None
        if self.upsamplers is not None:
            base_parts.append(self.upsamplers)  # å¦‚æœå­˜åœ¨ï¼Œæ·»åŠ  upsamplers
        # å†»ç»“åŸºç¡€éƒ¨åˆ†çš„å‚æ•°
        for part in base_parts:
            for param in part.parameters():
                param.requires_grad = False  # è®¾ç½®å‚æ•°ä¸ºä¸å¯è®­ç»ƒ
    
    # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•
    def forward(
        self,
        hidden_states: Tensor,  # è¾“å…¥çš„éšè—çŠ¶æ€
        res_hidden_states_tuple_base: Tuple[Tensor, ...],  # åŸºç¡€æ®‹å·®éšè—çŠ¶æ€å…ƒç»„
        res_hidden_states_tuple_ctrl: Tuple[Tensor, ...],  # æ§åˆ¶æ®‹å·®éšè—çŠ¶æ€å…ƒç»„
        temb: Tensor,  # æ—¶é—´åµŒå…¥
        encoder_hidden_states: Optional[Tensor] = None,  # å¯é€‰çš„ç¼–ç å™¨éšè—çŠ¶æ€
        conditioning_scale: Optional[float] = 1.0,  # å¯é€‰çš„æ¡ä»¶ç¼©æ”¾å› å­ï¼Œé»˜è®¤å€¼ä¸º 1.0
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,  # å¯é€‰çš„äº¤å‰æ³¨æ„åŠ›å‚æ•°
        attention_mask: Optional[Tensor] = None,  # å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
        upsample_size: Optional[int] = None,  # å¯é€‰çš„ä¸Šé‡‡æ ·å¤§å°
        encoder_attention_mask: Optional[Tensor] = None,  # å¯é€‰çš„ç¼–ç å™¨æ³¨æ„åŠ›æ©ç 
        apply_control: bool = True,  # æ˜¯å¦åº”ç”¨æ§åˆ¶ï¼Œé»˜è®¤å€¼ä¸º True
    # å‡½æ•°è¿”å›ä¸€ä¸ª Tensor å¯¹è±¡
    ) -> Tensor:
        # æ£€æŸ¥äº¤å‰æ³¨æ„åŠ›å‚æ•°æ˜¯å¦å­˜åœ¨
        if cross_attention_kwargs is not None:
            # æ£€æŸ¥å‚æ•°ä¸­æ˜¯å¦åŒ…å« "scale"
            if cross_attention_kwargs.get("scale", None) is not None:
                # è®°å½•è­¦å‘Šä¿¡æ¯ï¼Œè¡¨ç¤º "scale" å‚æ•°å·²å¼ƒç”¨
                logger.warning("Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.")
    
        # åˆ¤æ–­ FreeU æ˜¯å¦å¯ç”¨ï¼Œæ£€æŸ¥ç›¸å…³å±æ€§æ˜¯å¦å­˜åœ¨
        is_freeu_enabled = (
            getattr(self, "s1", None)
            and getattr(self, "s2", None)
            and getattr(self, "b1", None)
            and getattr(self, "b2", None)
        )
    
        # å®šä¹‰åˆ›å»ºè‡ªå®šä¹‰å‰å‘ä¼ æ’­çš„æ–¹æ³•
        def create_custom_forward(module, return_dict=None):
            # å®šä¹‰è‡ªå®šä¹‰å‰å‘ä¼ æ’­å‡½æ•°
            def custom_forward(*inputs):
                # æ ¹æ®æ˜¯å¦è¿”å›å­—å…¸é€‰æ‹©è°ƒç”¨æ–¹å¼
                if return_dict is not None:
                    return module(*inputs, return_dict=return_dict)
                else:
                    return module(*inputs)
    
            return custom_forward
    
        # å®šä¹‰æ¡ä»¶åº”ç”¨ FreeU çš„æ–¹æ³•
        def maybe_apply_freeu_to_subblock(hidden_states, res_h_base):
            # FreeU: ä»…åœ¨å‰ä¸¤ä¸ªé˜¶æ®µæ“ä½œ
            if is_freeu_enabled:
                # åº”ç”¨ FreeU æ“ä½œ
                return apply_freeu(
                    self.resolution_idx,
                    hidden_states,
                    res_h_base,
                    s1=self.s1,
                    s2=self.s2,
                    b1=self.b1,
                    b2=self.b2,
                )
            else:
                # å¦‚æœæœªå¯ç”¨ FreeUï¼Œç›´æ¥è¿”å›è¾“å…¥çŠ¶æ€
                return hidden_states, res_h_base
    
        # åŒæ—¶éå†å¤šä¸ªåˆ—è¡¨
        for resnet, attn, c2b, res_h_base, res_h_ctrl in zip(
            self.resnets,
            self.attentions,
            self.ctrl_to_base,
            reversed(res_hidden_states_tuple_base),
            reversed(res_hidden_states_tuple_ctrl),
        ):
            # å¦‚æœåº”ç”¨æ§åˆ¶ï¼Œåˆ™è°ƒæ•´éšè—çŠ¶æ€
            if apply_control:
                hidden_states += c2b(res_h_ctrl) * conditioning_scale
    
            # å¯èƒ½åº”ç”¨ FreeU æ“ä½œ
            hidden_states, res_h_base = maybe_apply_freeu_to_subblock(hidden_states, res_h_base)
            # å°†éšè—çŠ¶æ€å’ŒåŸºç¡€çŠ¶æ€æ²¿ç»´åº¦ 1 æ‹¼æ¥
            hidden_states = torch.cat([hidden_states, res_h_base], dim=1)
    
            # å¦‚æœåœ¨è®­ç»ƒå¹¶å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if self.training and self.gradient_checkpointing:
                # æ ¹æ® PyTorch ç‰ˆæœ¬è®¾ç½®æ£€æŸ¥ç‚¹å‚æ•°
                ckpt_kwargs: Dict[str, Any] = {"use_reentrant": False} if is_torch_version(">=", "1.11.0") else {}
                # åº”ç”¨æ£€æŸ¥ç‚¹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
                hidden_states = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(resnet),
                    hidden_states,
                    temb,
                    **ckpt_kwargs,
                )
            else:
                # ç›´æ¥ä½¿ç”¨æ®‹å·®ç½‘ç»œå¤„ç†éšè—çŠ¶æ€
                hidden_states = resnet(hidden_states, temb)
    
            # å¦‚æœæ³¨æ„åŠ›æ¨¡å—ä¸ä¸º Noneï¼Œåˆ™è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
            if attn is not None:
                hidden_states = attn(
                    hidden_states,
                    encoder_hidden_states=encoder_hidden_states,
                    cross_attention_kwargs=cross_attention_kwargs,
                    attention_mask=attention_mask,
                    encoder_attention_mask=encoder_attention_mask,
                    return_dict=False,
                )[0]
    
        # å¦‚æœä¸Šé‡‡æ ·å™¨å­˜åœ¨ï¼Œåº”ç”¨ä¸Šé‡‡æ ·æ“ä½œ
        if self.upsamplers is not None:
            hidden_states = self.upsamplers(hidden_states, upsample_size)
    
        # è¿”å›æœ€ç»ˆçš„éšè—çŠ¶æ€
        return hidden_states
# åˆ›å»ºä¸€ä¸ªé›¶å·ç§¯å±‚çš„å‡½æ•°ï¼Œæ¥æ”¶è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°
def make_zero_conv(in_channels, out_channels=None):
    # ä½¿ç”¨ zero_module å‡½æ•°åˆå§‹åŒ–ä¸€ä¸ªå·ç§¯å±‚ï¼Œå¹¶è®¾ç½®å·ç§¯æ ¸å¤§å°ä¸º1ï¼Œå¡«å……ä¸º0
    return zero_module(nn.Conv2d(in_channels, out_channels, 1, padding=0))


# åˆå§‹åŒ–ä¼ å…¥æ¨¡å—çš„å‚æ•°ä¸ºé›¶çš„å‡½æ•°
def zero_module(module):
    # éå†æ¨¡å—çš„æ‰€æœ‰å‚æ•°
    for p in module.parameters():
        # å°†æ¯ä¸ªå‚æ•°åˆå§‹åŒ–ä¸ºé›¶
        nn.init.zeros_(p)
    # è¿”å›å·²åˆå§‹åŒ–çš„æ¨¡å—
    return module


# æŸ¥æ‰¾ç»™å®šæ•°å­—çš„æœ€å¤§å› æ•°çš„å‡½æ•°ï¼Œæœ€å¤§å› æ•°ä¸è¶…è¿‡æŒ‡å®šå€¼
def find_largest_factor(number, max_factor):
    # å°†æœ€å¤§å› æ•°è®¾ç½®ä¸ºåˆå§‹å› æ•°
    factor = max_factor
    # å¦‚æœæœ€å¤§å› æ•°å¤§äºæˆ–ç­‰äºæ•°å­—ï¼Œç›´æ¥è¿”å›æ•°å­—
    if factor >= number:
        return number
    # å¾ªç¯ç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªå› æ•°
    while factor != 0:
        # è®¡ç®—æ•°å­—ä¸å› æ•°çš„ä½™æ•°
        residual = number % factor
        # å¦‚æœä½™æ•°ä¸ºé›¶ï¼Œåˆ™å› æ•°æ˜¯æœ‰æ•ˆçš„
        if residual == 0:
            return factor
        # å‡å°å› æ•°ï¼Œç»§ç»­æŸ¥æ‰¾
        factor -= 1
```