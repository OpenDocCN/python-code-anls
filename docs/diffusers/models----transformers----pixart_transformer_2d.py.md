# `.\diffusers\models\transformers\pixart_transformer_2d.py`

```
# ç‰ˆæƒæ‰€æœ‰ 2024 HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆâ€œè®¸å¯è¯â€ï¼‰è¿›è¡Œè®¸å¯ï¼›
# é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰è§„å®šï¼Œè½¯ä»¶
# æŒ‰â€œåŸæ ·â€æä¾›ï¼Œæ²¡æœ‰ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚
# è¯·å‚é˜…è®¸å¯è¯ä»¥äº†è§£æœ‰å…³æƒé™å’Œ
# é™åˆ¶çš„å…·ä½“æ¡æ¬¾ã€‚
from typing import Any, Dict, Optional, Union  # å¯¼å…¥ç±»å‹æç¤ºç›¸å…³çš„æ¨¡å—

import torch  # å¯¼å…¥ PyTorch åº“
from torch import nn  # ä» PyTorch å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—

from ...configuration_utils import ConfigMixin, register_to_config  # å¯¼å…¥é…ç½®ç›¸å…³çš„æ··åˆç±»å’Œæ³¨å†Œå‡½æ•°
from ...utils import is_torch_version, logging  # å¯¼å…¥å·¥å…·å‡½æ•°ï¼šæ£€æŸ¥ PyTorch ç‰ˆæœ¬å’Œæ—¥å¿—è®°å½•
from ..attention import BasicTransformerBlock  # å¯¼å…¥åŸºç¡€ Transformer å—
from ..attention_processor import Attention, AttentionProcessor, FusedAttnProcessor2_0  # å¯¼å…¥æ³¨æ„åŠ›ç›¸å…³çš„å¤„ç†å™¨
from ..embeddings import PatchEmbed, PixArtAlphaTextProjection  # å¯¼å…¥åµŒå…¥ç›¸å…³çš„æ¨¡å—
from ..modeling_outputs import Transformer2DModelOutput  # å¯¼å…¥æ¨¡å‹è¾“å‡ºç›¸å…³çš„ç±»
from ..modeling_utils import ModelMixin  # å¯¼å…¥æ¨¡å‹æ··åˆç±»
from ..normalization import AdaLayerNormSingle  # å¯¼å…¥è‡ªé€‚åº”å±‚å½’ä¸€åŒ–ç±»

logger = logging.get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨ï¼›pylint ç¦ç”¨å‘½åæ£€æŸ¥

class PixArtTransformer2DModel(ModelMixin, ConfigMixin):  # å®šä¹‰ PixArt 2D Transformer æ¨¡å‹ç±»ï¼Œç»§æ‰¿è‡ª ModelMixin å’Œ ConfigMixin
    r"""  # æ–‡æ¡£å­—ç¬¦ä¸²ï¼šæè¿°æ¨¡å‹åŠå…¶æ¥æº
    A 2D Transformer model as introduced in PixArt family of models (https://arxiv.org/abs/2310.00426,
    https://arxiv.org/abs/2403.04692).
    """

    _supports_gradient_checkpointing = True  # è®¾ç½®æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹
    _no_split_modules = ["BasicTransformerBlock", "PatchEmbed"]  # æŒ‡å®šä¸è¿›è¡Œåˆ†å‰²çš„æ¨¡å—

    @register_to_config  # ä½¿ç”¨è£…é¥°å™¨å°†åˆå§‹åŒ–å‡½æ•°æ³¨å†Œåˆ°é…ç½®ä¸­
    def __init__(  # å®šä¹‰åˆå§‹åŒ–å‡½æ•°
        self,
        num_attention_heads: int = 16,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 16
        attention_head_dim: int = 72,  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 72
        in_channels: int = 4,  # è¾“å…¥é€šé“æ•°ï¼Œé»˜è®¤ä¸º 4
        out_channels: Optional[int] = 8,  # è¾“å‡ºé€šé“æ•°ï¼Œé»˜è®¤ä¸º 8ï¼Œå¯é€‰
        num_layers: int = 28,  # å±‚æ•°ï¼Œé»˜è®¤ä¸º 28
        dropout: float = 0.0,  # dropout æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 0.0
        norm_num_groups: int = 32,  # å½’ä¸€åŒ–çš„ç»„æ•°ï¼Œé»˜è®¤ä¸º 32
        cross_attention_dim: Optional[int] = 1152,  # äº¤å‰æ³¨æ„åŠ›çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º 1152ï¼Œå¯é€‰
        attention_bias: bool = True,  # æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›åç½®ï¼Œé»˜è®¤ä¸º True
        sample_size: int = 128,  # æ ·æœ¬å°ºå¯¸ï¼Œé»˜è®¤ä¸º 128
        patch_size: int = 2,  # æ¯ä¸ªè¡¥ä¸çš„å°ºå¯¸ï¼Œé»˜è®¤ä¸º 2
        activation_fn: str = "gelu-approximate",  # æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œé»˜è®¤ä¸ºè¿‘ä¼¼ GELU
        num_embeds_ada_norm: Optional[int] = 1000,  # è‡ªé€‚åº”å½’ä¸€åŒ–çš„åµŒå…¥æ•°é‡ï¼Œé»˜è®¤ä¸º 1000ï¼Œå¯é€‰
        upcast_attention: bool = False,  # æ˜¯å¦æé«˜æ³¨æ„åŠ›ç²¾åº¦ï¼Œé»˜è®¤ä¸º False
        norm_type: str = "ada_norm_single",  # å½’ä¸€åŒ–ç±»å‹ï¼Œé»˜è®¤ä¸ºå•ä¸€è‡ªé€‚åº”å½’ä¸€åŒ–
        norm_elementwise_affine: bool = False,  # æ˜¯å¦ä½¿ç”¨é€å…ƒç´ ä»¿å°„å˜æ¢ï¼Œé»˜è®¤ä¸º False
        norm_eps: float = 1e-6,  # å½’ä¸€åŒ–çš„ epsilon å€¼ï¼Œé»˜è®¤ä¸º 1e-6
        interpolation_scale: Optional[int] = None,  # æ’å€¼å°ºåº¦ï¼Œå¯é€‰
        use_additional_conditions: Optional[bool] = None,  # æ˜¯å¦ä½¿ç”¨é¢å¤–æ¡ä»¶ï¼Œå¯é€‰
        caption_channels: Optional[int] = None,  # è¯´æ˜é€šé“æ•°ï¼Œå¯é€‰
        attention_type: Optional[str] = "default",  # æ³¨æ„åŠ›ç±»å‹ï¼Œé»˜è®¤ä¸ºé»˜è®¤ç±»å‹
    ):
        # åˆå§‹åŒ–å‡½æ•°å‚æ•°è®¾ç½®
        ...

    def _set_gradient_checkpointing(self, module, value=False):  # å®šä¹‰è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ–¹æ³•
        if hasattr(module, "gradient_checkpointing"):  # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰æ¢¯åº¦æ£€æŸ¥ç‚¹å±æ€§
            module.gradient_checkpointing = value  # è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹çš„å€¼

    @property  # å®šä¹‰ä¸€ä¸ªå±æ€§
    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors å¤åˆ¶çš„å±æ€§
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œè¿”å›æ¨¡å‹ä¸­æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨çš„å­—å…¸ï¼Œé”®ä¸ºæƒé‡åç§°
    def attn_processors(self) -> Dict[str, AttentionProcessor]:
        r"""
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        """
        # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨æ³¨æ„åŠ›å¤„ç†å™¨
        processors = {}

        # å®šä¹‰ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œç”¨äºæ·»åŠ å¤„ç†å™¨åˆ°å­—å…¸ä¸­
        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
            # å¦‚æœæ¨¡å—å…·æœ‰è·å–å¤„ç†å™¨çš„æ–¹æ³•ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°å­—å…¸ä¸­
            if hasattr(module, "get_processor"):
                processors[f"{name}.processor"] = module.get_processor()

            # éå†å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨è¯¥å‡½æ•°ä»¥æ·»åŠ å¤„ç†å™¨
            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)

            # è¿”å›æ›´æ–°åçš„å¤„ç†å™¨å­—å…¸
            return processors

        # éå†å½“å‰æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—ï¼Œè°ƒç”¨é€’å½’å‡½æ•°ä»¥å¡«å……å¤„ç†å™¨å­—å…¸
        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)

        # è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨çš„å­—å…¸
        return processors

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor å¤åˆ¶
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºè®¾ç½®è®¡ç®—æ³¨æ„åŠ›çš„å¤„ç†å™¨
    def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
        r"""
        Sets the attention processor to use to compute attention.

        Parameters:
            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):
                The instantiated processor class or a dictionary of processor classes that will be set as the processor
                for **all** `Attention` layers.

                If `processor` is a dict, the key needs to define the path to the corresponding cross attention
                processor. This is strongly recommended when setting trainable attention processors.

        """
        # è·å–å½“å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„æ•°é‡
        count = len(self.attn_processors.keys())

        # æ£€æŸ¥ä¼ å…¥çš„å¤„ç†å™¨å­—å…¸çš„é•¿åº¦æ˜¯å¦ä¸æ³¨æ„åŠ›å±‚çš„æ•°é‡åŒ¹é…
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f"A dict of processors was passed, but the number of processors {len(processor)} does not match the"
                f" number of attention layers: {count}. Please make sure to pass {count} processor classes."
            )

        # å®šä¹‰ä¸€ä¸ªé€’å½’å‡½æ•°ï¼Œç”¨äºè®¾ç½®å¤„ç†å™¨
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            # å¦‚æœæ¨¡å—å…·æœ‰è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•ï¼Œåˆ™è¿›è¡Œè®¾ç½®
            if hasattr(module, "set_processor"):
                if not isinstance(processor, dict):
                    module.set_processor(processor)  # è®¾ç½®å•ä¸€å¤„ç†å™¨
                else:
                    module.set_processor(processor.pop(f"{name}.processor"))  # ä»å­—å…¸ä¸­ç§»é™¤å¹¶è®¾ç½®å¤„ç†å™¨

            # éå†å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨ä»¥è®¾ç½®å¤„ç†å™¨
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

        # éå†å½“å‰æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—ï¼Œè°ƒç”¨é€’å½’å‡½æ•°ä»¥è®¾ç½®å¤„ç†å™¨
        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections å¤åˆ¶
    # å®šä¹‰èåˆ QKV æŠ•å½±çš„å‡½æ•°
    def fuse_qkv_projections(self):
        # å¯ç”¨èåˆçš„ QKV æŠ•å½±ï¼Œå¯¹è‡ªæ³¨æ„åŠ›æ¨¡å—è¿›è¡ŒèåˆæŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µ
        # å¯¹äº¤å‰æ³¨æ„åŠ›æ¨¡å—åˆ™ä»…èåˆé”®å’Œå€¼æŠ•å½±çŸ©é˜µ
        """
        Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query, key, value)
        are fused. For cross-attention modules, key and value projection matrices are fused.
    
        <Tip warning={true}>
    
        This API is ğŸ§ª experimental.
    
        </Tip>
        """
        # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None
        self.original_attn_processors = None
    
        # éå†æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨
        for _, attn_processor in self.attn_processors.items():
            # æ£€æŸ¥å¤„ç†å™¨ç±»åä¸­æ˜¯å¦åŒ…å« "Added"
            if "Added" in str(attn_processor.__class__.__name__):
                # å¦‚æœå­˜åœ¨ï¼Œåˆ™æŠ›å‡ºé”™è¯¯ï¼Œè¯´æ˜ä¸æ”¯æŒæ­¤èåˆæ“ä½œ
                raise ValueError("`fuse_qkv_projections()` is not supported for models having added KV projections.")
    
        # ä¿å­˜å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨
        self.original_attn_processors = self.attn_processors
    
        # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨¡å—
        for module in self.modules():
            # å¦‚æœæ¨¡å—æ˜¯ Attention ç±»å‹
            if isinstance(module, Attention):
                # æ‰§è¡ŒæŠ•å½±èåˆ
                module.fuse_projections(fuse=True)
    
        # è®¾ç½®æ–°çš„èåˆæ³¨æ„åŠ›å¤„ç†å™¨
        self.set_attn_processor(FusedAttnProcessor2_0())
    
    # ä» UNet2DConditionModel ä¸­å¤åˆ¶çš„å‡½æ•°ï¼Œç”¨äºå–æ¶ˆèåˆ QKV æŠ•å½±
    def unfuse_qkv_projections(self):
        # ç¦ç”¨å·²å¯ç”¨çš„èåˆ QKV æŠ•å½±
        """Disables the fused QKV projection if enabled.
    
        <Tip warning={true}>
    
        This API is ğŸ§ª experimental.
    
        </Tip>
    
        """
        # æ£€æŸ¥åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨æ˜¯å¦å­˜åœ¨
        if self.original_attn_processors is not None:
            # æ¢å¤åˆ°åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨
            self.set_attn_processor(self.original_attn_processors)
    
    # å®šä¹‰å‰å‘ä¼ æ’­å‡½æ•°
    def forward(
        # è¾“å…¥éšè—çŠ¶æ€çš„å¼ é‡
        hidden_states: torch.Tensor,
        # ç¼–ç å™¨éšè—çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
        encoder_hidden_states: Optional[torch.Tensor] = None,
        # æ—¶é—´æ­¥é•¿ï¼ˆå¯é€‰ï¼‰
        timestep: Optional[torch.LongTensor] = None,
        # æ·»åŠ çš„æ¡ä»¶å…³é”®å­—å‚æ•°ï¼ˆå­—å…¸ç±»å‹ï¼Œå¯é€‰ï¼‰
        added_cond_kwargs: Dict[str, torch.Tensor] = None,
        # äº¤å‰æ³¨æ„åŠ›å…³é”®å­—å‚æ•°ï¼ˆå­—å…¸ç±»å‹ï¼Œå¯é€‰ï¼‰
        cross_attention_kwargs: Dict[str, Any] = None,
        # æ³¨æ„åŠ›æ©ç ï¼ˆå¯é€‰ï¼‰
        attention_mask: Optional[torch.Tensor] = None,
        # ç¼–ç å™¨æ³¨æ„åŠ›æ©ç ï¼ˆå¯é€‰ï¼‰
        encoder_attention_mask: Optional[torch.Tensor] = None,
        # æ˜¯å¦è¿”å›å­—å…¸ï¼ˆé»˜è®¤å€¼ä¸º Trueï¼‰
        return_dict: bool = True,
```