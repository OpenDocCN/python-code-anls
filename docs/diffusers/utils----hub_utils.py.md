# `.\diffusers\utils\hub_utils.py`

```
# coding=utf-8  # æŒ‡å®šæ–‡ä»¶çš„ç¼–ç ä¸º UTF-8
# Copyright 2024 The HuggingFace Inc. team.  # æ–‡ä»¶ç‰ˆæƒä¿¡æ¯
#
# Licensed under the Apache License, Version 2.0 (the "License");  # è®¸å¯è¯å£°æ˜
# you may not use this file except in compliance with the License.  # ä½¿ç”¨è®¸å¯è¯çš„æ¡ä»¶è¯´æ˜
# You may obtain a copy of the License at  # è®¸å¯è¯è·å–é“¾æ¥
#
#     http://www.apache.org/licenses/LICENSE-2.0  # è®¸å¯è¯é“¾æ¥
#
# Unless required by applicable law or agreed to in writing, software  # å…è´£æ¡æ¬¾
# distributed under the License is distributed on an "AS IS" BASIS,  # å…è´£æ¡æ¬¾çš„è¿›ä¸€æ­¥è¯´æ˜
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  # ä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯
# See the License for the specific language governing permissions and  # æŸ¥çœ‹è®¸å¯è¯ä»¥è·å–ç‰¹å®šæƒé™
# limitations under the License.  # ä»¥åŠåœ¨è®¸å¯è¯ä¸‹çš„é™åˆ¶


import json  # å¯¼å…¥ json æ¨¡å—ï¼Œç”¨äºå¤„ç† JSON æ•°æ®
import os  # å¯¼å…¥ os æ¨¡å—ï¼Œç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’
import re  # å¯¼å…¥ re æ¨¡å—ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œ
import sys  # å¯¼å…¥ sys æ¨¡å—ï¼Œç”¨äºè®¿é—®ä¸ Python è§£é‡Šå™¨ç›¸å…³çš„ä¿¡æ¯
import tempfile  # å¯¼å…¥ tempfile æ¨¡å—ï¼Œç”¨äºåˆ›å»ºä¸´æ—¶æ–‡ä»¶
import traceback  # å¯¼å…¥ traceback æ¨¡å—ï¼Œç”¨äºå¤„ç†å¼‚å¸¸çš„è·Ÿè¸ªä¿¡æ¯
import warnings  # å¯¼å…¥ warnings æ¨¡å—ï¼Œç”¨äºå‘å‡ºè­¦å‘Š
from pathlib import Path  # ä» pathlib å¯¼å…¥ Path ç±»ï¼Œç”¨äºè·¯å¾„æ“ä½œ
from typing import Dict, List, Optional, Union  # å¯¼å…¥ç±»å‹æç¤ºç›¸å…³çš„ç±»
from uuid import uuid4  # ä» uuid å¯¼å…¥ uuid4 å‡½æ•°ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦

# ä» huggingface_hub å¯¼å…¥æ‰€éœ€çš„æ¨¡å—å’Œå‡½æ•°
from huggingface_hub import (
    ModelCard,  # å¯¼å…¥ ModelCard ç±»ï¼Œç”¨äºæ¨¡å‹å¡ç‰‡ç®¡ç†
    ModelCardData,  # å¯¼å…¥ ModelCardData ç±»ï¼Œç”¨äºå¤„ç†æ¨¡å‹å¡ç‰‡æ•°æ®
    create_repo,  # å¯¼å…¥ create_repo å‡½æ•°ï¼Œç”¨äºåˆ›å»ºæ¨¡å‹ä»“åº“
    hf_hub_download,  # å¯¼å…¥ hf_hub_download å‡½æ•°ï¼Œç”¨äºä¸‹è½½æ¨¡å‹
    model_info,  # å¯¼å…¥ model_info å‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹ä¿¡æ¯
    snapshot_download,  # å¯¼å…¥ snapshot_download å‡½æ•°ï¼Œç”¨äºä¸‹è½½å¿«ç…§
    upload_folder,  # å¯¼å…¥ upload_folder å‡½æ•°ï¼Œç”¨äºä¸Šä¼ æ–‡ä»¶å¤¹
)
# å¯¼å…¥ huggingface_hub çš„å¸¸é‡
from huggingface_hub.constants import HF_HUB_CACHE, HF_HUB_DISABLE_TELEMETRY, HF_HUB_OFFLINE
# ä» huggingface_hub.file_download å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼ç›¸å…³å†…å®¹
from huggingface_hub.file_download import REGEX_COMMIT_HASH
# å¯¼å…¥ huggingface_hub.utils çš„å¤šä¸ªå¼‚å¸¸å¤„ç†å’Œå®ç”¨å‡½æ•°
from huggingface_hub.utils import (
    EntryNotFoundError,  # å¯¼å…¥æ‰¾ä¸åˆ°æ¡ç›®çš„å¼‚å¸¸
    RepositoryNotFoundError,  # å¯¼å…¥æ‰¾ä¸åˆ°ä»“åº“çš„å¼‚å¸¸
    RevisionNotFoundError,  # å¯¼å…¥æ‰¾ä¸åˆ°ä¿®è®¢ç‰ˆæœ¬çš„å¼‚å¸¸
    is_jinja_available,  # å¯¼å…¥æ£€æŸ¥ Jinja æ¨¡æ¿æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    validate_hf_hub_args,  # å¯¼å…¥éªŒè¯ Hugging Face Hub å‚æ•°çš„å‡½æ•°
)
from packaging import version  # å¯¼å…¥ version æ¨¡å—ï¼Œç”¨äºç‰ˆæœ¬å¤„ç†
from requests import HTTPError  # ä» requests å¯¼å…¥ HTTPError å¼‚å¸¸ï¼Œç”¨äºå¤„ç† HTTP é”™è¯¯

from .. import __version__  # å¯¼å…¥å½“å‰åŒ…çš„ç‰ˆæœ¬ä¿¡æ¯
from .constants import (
    DEPRECATED_REVISION_ARGS,  # å¯¼å…¥å·²å¼ƒç”¨çš„ä¿®è®¢å‚æ•°å¸¸é‡
    HUGGINGFACE_CO_RESOLVE_ENDPOINT,  # å¯¼å…¥ Hugging Face è§£æç«¯ç‚¹å¸¸é‡
    SAFETENSORS_WEIGHTS_NAME,  # å¯¼å…¥å®‰å…¨å¼ é‡æƒé‡åç§°å¸¸é‡
    WEIGHTS_NAME,  # å¯¼å…¥æƒé‡åç§°å¸¸é‡
)
from .import_utils import (
    ENV_VARS_TRUE_VALUES,  # å¯¼å…¥ç¯å¢ƒå˜é‡çœŸå€¼é›†åˆ
    _flax_version,  # å¯¼å…¥ Flax ç‰ˆæœ¬
    _jax_version,  # å¯¼å…¥ JAX ç‰ˆæœ¬
    _onnxruntime_version,  # å¯¼å…¥ ONNX è¿è¡Œæ—¶ç‰ˆæœ¬
    _torch_version,  # å¯¼å…¥ PyTorch ç‰ˆæœ¬
    is_flax_available,  # å¯¼å…¥æ£€æŸ¥ Flax æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_onnx_available,  # å¯¼å…¥æ£€æŸ¥ ONNX æ˜¯å¦å¯ç”¨çš„å‡½æ•°
    is_torch_available,  # å¯¼å…¥æ£€æŸ¥ PyTorch æ˜¯å¦å¯ç”¨çš„å‡½æ•°
)
from .logging import get_logger  # ä» logging æ¨¡å—å¯¼å…¥è·å–æ—¥å¿—è®°å½•å™¨çš„å‡½æ•°

logger = get_logger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨å®ä¾‹

MODEL_CARD_TEMPLATE_PATH = Path(__file__).parent / "model_card_template.md"  # è®¾ç½®æ¨¡å‹å¡ç‰‡æ¨¡æ¿æ–‡ä»¶çš„è·¯å¾„
SESSION_ID = uuid4().hex  # ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ä¼šè¯ ID

def http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:  # å®šä¹‰ä¸€ä¸ªæ ¼å¼åŒ–ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²çš„å‡½æ•°
    """
    Formats a user-agent string with basic info about a request.  # å‡½æ•°è¯´æ˜ï¼Œæ ¼å¼åŒ–ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    """
    ua = f"diffusers/{__version__}; python/{sys.version.split()[0]}; session_id/{SESSION_ID}"  # æ„å»ºåŸºæœ¬ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    if HF_HUB_DISABLE_TELEMETRY or HF_HUB_OFFLINE:  # æ£€æŸ¥æ˜¯å¦ç¦ç”¨é¥æµ‹æˆ–å¤„äºç¦»çº¿çŠ¶æ€
        return ua + "; telemetry/off"  # è¿”å›ç¦ç”¨é¥æµ‹çš„ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    if is_torch_available():  # æ£€æŸ¥ PyTorch æ˜¯å¦å¯ç”¨
        ua += f"; torch/{_torch_version}"  # å°† PyTorch ç‰ˆæœ¬ä¿¡æ¯æ·»åŠ åˆ°ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    if is_flax_available():  # æ£€æŸ¥ Flax æ˜¯å¦å¯ç”¨
        ua += f"; jax/{_jax_version}"  # å°† JAX ç‰ˆæœ¬ä¿¡æ¯æ·»åŠ åˆ°ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
        ua += f"; flax/{_flax_version}"  # å°† Flax ç‰ˆæœ¬ä¿¡æ¯æ·»åŠ åˆ°ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    if is_onnx_available():  # æ£€æŸ¥ ONNX æ˜¯å¦å¯ç”¨
        ua += f"; onnxruntime/{_onnxruntime_version}"  # å°† ONNX è¿è¡Œæ—¶ç‰ˆæœ¬ä¿¡æ¯æ·»åŠ åˆ°ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    # CI will set this value to True  # CI ä¼šå°†æ­¤å€¼è®¾ç½®ä¸º True
    if os.environ.get("DIFFUSERS_IS_CI", "").upper() in ENV_VARS_TRUE_VALUES:  # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦æŒ‡ç¤ºåœ¨ CI ä¸­è¿è¡Œ
        ua += "; is_ci/true"  # å¦‚æœæ˜¯ CIï¼Œæ·»åŠ ç›¸å…³ä¿¡æ¯åˆ°ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    if isinstance(user_agent, dict):  # æ£€æŸ¥ç”¨æˆ·ä»£ç†æ˜¯å¦ä¸ºå­—å…¸ç±»å‹
        ua += "; " + "; ".join(f"{k}/{v}" for k, v in user_agent.items())  # å°†å­—å…¸é¡¹æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²å¹¶æ·»åŠ åˆ°ç”¨æˆ·ä»£ç†
    elif isinstance(user_agent, str):  # æ£€æŸ¥ç”¨æˆ·ä»£ç†æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹
        ua += "; " + user_agent  # ç›´æ¥æ·»åŠ ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    return ua  # è¿”å›æœ€ç»ˆçš„ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²


def load_or_create_model_card(  # å®šä¹‰åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡çš„å‡½æ•°
    repo_id_or_path: str = None,  # ä»“åº“ ID æˆ–è·¯å¾„ï¼Œé»˜è®¤ä¸º None
    token: Optional[str] = None,  # è®¿é—®ä»¤ç‰Œï¼Œé»˜è®¤ä¸º None
    is_pipeline: bool = False,  # æ˜¯å¦ä¸ºç®¡é“æ¨¡å‹ï¼Œé»˜è®¤ä¸º False
    from_training: bool = False,  # æ˜¯å¦ä»è®­ç»ƒä¸­åŠ è½½ï¼Œé»˜è®¤ä¸º False
    # å®šä¹‰æ¨¡å‹æè¿°ï¼Œç±»å‹ä¸ºå¯é€‰çš„å­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸º None
        model_description: Optional[str] = None,
        # å®šä¹‰åŸºç¡€æ¨¡å‹ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸º None
        base_model: str = None,
        # å®šä¹‰æç¤ºä¿¡æ¯ï¼Œç±»å‹ä¸ºå¯é€‰çš„å­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸º None
        prompt: Optional[str] = None,
        # å®šä¹‰è®¸å¯è¯ä¿¡æ¯ï¼Œç±»å‹ä¸ºå¯é€‰çš„å­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸º None
        license: Optional[str] = None,
        # å®šä¹‰å°éƒ¨ä»¶åˆ—è¡¨ï¼Œç±»å‹ä¸ºå¯é€‰çš„å­—å…¸åˆ—è¡¨ï¼Œé»˜è®¤å€¼ä¸º None
        widget: Optional[List[dict]] = None,
        # å®šä¹‰æ¨ç†æ ‡å¿—ï¼Œç±»å‹ä¸ºå¯é€‰çš„å¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸º None
        inference: Optional[bool] = None,
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿”å›ç±»å‹ä¸º ModelCard
) -> ModelCard:
    """
    åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡ã€‚

    å‚æ•°:
        repo_id_or_path (`str`):
            ä»“åº“ IDï¼ˆä¾‹å¦‚ "runwayml/stable-diffusion-v1-5"ï¼‰æˆ–æŸ¥æ‰¾æ¨¡å‹å¡ç‰‡çš„æœ¬åœ°è·¯å¾„ã€‚
        token (`str`, *å¯é€‰*):
            è®¤è¯ä»¤ç‰Œã€‚é»˜è®¤ä¸ºå­˜å‚¨çš„ä»¤ç‰Œã€‚è¯¦ç»†ä¿¡æ¯è§ https://huggingface.co/settings/tokenã€‚
        is_pipeline (`bool`):
            å¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦ä¸º [`DiffusionPipeline`] æ·»åŠ æ ‡ç­¾ã€‚
        from_training: (`bool`): å¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ¨¡å‹å¡ç‰‡æ˜¯å¦æ˜¯ä»è®­ç»ƒè„šæœ¬åˆ›å»ºçš„ã€‚
        model_description (`str`, *å¯é€‰*): è¦æ·»åŠ åˆ°æ¨¡å‹å¡ç‰‡çš„æ¨¡å‹æè¿°ã€‚åœ¨ä»è®­ç»ƒè„šæœ¬ä½¿ç”¨ `load_or_create_model_card` æ—¶æœ‰ç”¨ã€‚
        base_model (`str`): åŸºç¡€æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆä¾‹å¦‚ "stabilityai/stable-diffusion-xl-base-1.0"ï¼‰ã€‚å¯¹ç±»ä¼¼ DreamBooth çš„è®­ç»ƒæœ‰ç”¨ã€‚
        prompt (`str`, *å¯é€‰*): ç”¨äºè®­ç»ƒçš„æç¤ºã€‚å¯¹ç±»ä¼¼ DreamBooth çš„è®­ç»ƒæœ‰ç”¨ã€‚
        license: (`str`, *å¯é€‰*): è¾“å‡ºå·¥ä»¶çš„è®¸å¯è¯ã€‚åœ¨ä»è®­ç»ƒè„šæœ¬ä½¿ç”¨ `load_or_create_model_card` æ—¶æœ‰ç”¨ã€‚
        widget (`List[dict]`, *å¯é€‰*): é™„å¸¦ç”»å»Šæ¨¡æ¿çš„éƒ¨ä»¶ã€‚
        inference: (`bool`, *å¯é€‰*): æ˜¯å¦å¼€å¯æ¨ç†éƒ¨ä»¶ã€‚åœ¨ä»è®­ç»ƒè„šæœ¬ä½¿ç”¨ `load_or_create_model_card` æ—¶æœ‰ç”¨ã€‚
    """
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† Jinja æ¨¡æ¿å¼•æ“
    if not is_jinja_available():
        # å¦‚æœæœªå®‰è£…ï¼ŒæŠ›å‡ºä¸€ä¸ªå€¼é”™è¯¯ï¼Œå¹¶æä¾›å®‰è£…å»ºè®®
        raise ValueError(
            "Modelcard æ¸²æŸ“åŸºäº Jinja æ¨¡æ¿ã€‚"
            " è¯·ç¡®ä¿åœ¨ä½¿ç”¨ `load_or_create_model_card` ä¹‹å‰å®‰è£…äº† `jinja`."
            " è¦å®‰è£…å®ƒï¼Œè¯·è¿è¡Œ `pip install Jinja2`."
        )

    try:
        # æ£€æŸ¥è¿œç¨‹ä»“åº“ä¸­æ˜¯å¦å­˜åœ¨æ¨¡å‹å¡ç‰‡
        model_card = ModelCard.load(repo_id_or_path, token=token)
    except (EntryNotFoundError, RepositoryNotFoundError):
        # å¦‚æœæ¨¡å‹å¡ç‰‡ä¸å­˜åœ¨ï¼Œåˆ™æ ¹æ®æ¨¡æ¿åˆ›å»ºä¸€ä¸ªæ¨¡å‹å¡ç‰‡
        if from_training:
            # ä»æ¨¡æ¿åˆ›å»ºæ¨¡å‹å¡ç‰‡ï¼Œå¹¶ä½¿ç”¨å¡ç‰‡æ•°æ®ä½œä¸º YAML å—
            model_card = ModelCard.from_template(
                card_data=ModelCardData(  # å¡ç‰‡å…ƒæ•°æ®å¯¹è±¡
                    license=license,
                    library_name="diffusers",  # æŒ‡å®šåº“å
                    inference=inference,  # æŒ‡å®šæ¨ç†è®¾ç½®
                    base_model=base_model,  # æŒ‡å®šåŸºç¡€æ¨¡å‹
                    instance_prompt=prompt,  # æŒ‡å®šå®ä¾‹æç¤º
                    widget=widget,  # æŒ‡å®šéƒ¨ä»¶
                ),
                template_path=MODEL_CARD_TEMPLATE_PATH,  # æ¨¡æ¿è·¯å¾„
                model_description=model_description,  # æ¨¡å‹æè¿°
            )
        else:
            # åˆ›å»ºä¸€ä¸ªç©ºçš„æ¨¡å‹å¡ç‰‡æ•°æ®å¯¹è±¡
            card_data = ModelCardData()
            # æ ¹æ® is_pipeline å˜é‡ç¡®å®šç»„ä»¶ç±»å‹
            component = "pipeline" if is_pipeline else "model"
            # å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹æè¿°ï¼Œåˆ™ç”Ÿæˆé»˜è®¤æè¿°
            if model_description is None:
                model_description = f"This is the model card of a ğŸ§¨ diffusers {component} that has been pushed on the Hub. This model card has been automatically generated."
            # ä»æ¨¡æ¿åˆ›å»ºæ¨¡å‹å¡ç‰‡
            model_card = ModelCard.from_template(card_data, model_description=model_description)
    # è¿”å›æ¨¡å‹å¡ç‰‡çš„å†…å®¹
        return model_card
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¡«å……æ¨¡å‹å¡ç‰‡çš„åº“åç§°å’Œå¯é€‰æ ‡ç­¾
def populate_model_card(model_card: ModelCard, tags: Union[str, List[str]] = None) -> ModelCard:
    # å¦‚æœæ¨¡å‹å¡ç‰‡çš„åº“åç§°ä¸ºç©ºï¼Œåˆ™è®¾ç½®ä¸º "diffusers"
    if model_card.data.library_name is None:
        model_card.data.library_name = "diffusers"

    # å¦‚æœæ ‡ç­¾ä¸ä¸ºç©º
    if tags is not None:
        # å¦‚æœæ ‡ç­¾æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(tags, str):
            tags = [tags]
        # å¦‚æœæ¨¡å‹å¡ç‰‡çš„æ ‡ç­¾ä¸ºç©ºï¼Œåˆ™åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
        if model_card.data.tags is None:
            model_card.data.tags = []
        # éå†æ‰€æœ‰æ ‡ç­¾ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ°æ¨¡å‹å¡ç‰‡çš„æ ‡ç­¾ä¸­
        for tag in tags:
            model_card.data.tags.append(tag)

    # è¿”å›æ›´æ–°åçš„æ¨¡å‹å¡ç‰‡
    return model_card


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»å·²è§£æçš„æ–‡ä»¶åä¸­æå–æäº¤å“ˆå¸Œ
def extract_commit_hash(resolved_file: Optional[str], commit_hash: Optional[str] = None):
    # æå–æäº¤å“ˆå¸Œï¼Œä¼˜å…ˆä½¿ç”¨æä¾›çš„æäº¤å“ˆå¸Œ
    if resolved_file is None or commit_hash is not None:
        return commit_hash
    # å°†è§£æåçš„æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸º POSIX æ ¼å¼
    resolved_file = str(Path(resolved_file).as_posix())
    # åœ¨æ–‡ä»¶è·¯å¾„ä¸­æœç´¢æäº¤å“ˆå¸Œçš„æ¨¡å¼
    search = re.search(r"snapshots/([^/]+)/", resolved_file)
    # å¦‚æœæœªæ‰¾åˆ°æ¨¡å¼ï¼Œåˆ™è¿”å› None
    if search is None:
        return None
    # ä»æœç´¢ç»“æœä¸­æå–æäº¤å“ˆå¸Œ
    commit_hash = search.groups()[0]
    # å¦‚æœæäº¤å“ˆå¸Œç¬¦åˆè§„å®šæ ¼å¼ï¼Œåˆ™è¿”å›å®ƒï¼Œå¦åˆ™è¿”å› None
    return commit_hash if REGEX_COMMIT_HASH.match(commit_hash) else None


# å®šä¹‰æ—§çš„é»˜è®¤ç¼“å­˜è·¯å¾„ï¼Œå¯èƒ½éœ€è¦è¿ç§»
# è¯¥é€»è¾‘å¤§ä½“æ¥æºäº `transformers`ï¼Œå¹¶æœ‰å¦‚ä¸‹ä¸åŒä¹‹å¤„ï¼š
# - Diffusers ä¸ä½¿ç”¨è‡ªå®šä¹‰ç¯å¢ƒå˜é‡æ¥æŒ‡å®šç¼“å­˜è·¯å¾„ã€‚
# - æ— éœ€è¿ç§»ç¼“å­˜æ ¼å¼ï¼Œåªéœ€å°†æ–‡ä»¶ç§»åŠ¨åˆ°æ–°ä½ç½®ã€‚
hf_cache_home = os.path.expanduser(
    # è·å–ç¯å¢ƒå˜é‡ HF_HOMEï¼Œé»˜è®¤è·¯å¾„ä¸º ~/.cache/huggingface
    os.getenv("HF_HOME", os.path.join(os.getenv("XDG_CACHE_HOME", "~/.cache"), "huggingface"))
)
# å®šä¹‰æ—§çš„ diffusers ç¼“å­˜è·¯å¾„
old_diffusers_cache = os.path.join(hf_cache_home, "diffusers")


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºç§»åŠ¨ç¼“å­˜ç›®å½•
def move_cache(old_cache_dir: Optional[str] = None, new_cache_dir: Optional[str] = None) -> None:
    # å¦‚æœæ–°ç¼“å­˜ç›®å½•ä¸ºç©ºï¼Œåˆ™è®¾ç½®ä¸º HF_HUB_CACHE
    if new_cache_dir is None:
        new_cache_dir = HF_HUB_CACHE
    # å¦‚æœæ—§ç¼“å­˜ç›®å½•ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨æ—§çš„ diffusers ç¼“å­˜è·¯å¾„
    if old_cache_dir is None:
        old_cache_dir = old_diffusers_cache

    # æ‰©å±•ç”¨æˆ·ç›®å½•è·¯å¾„
    old_cache_dir = Path(old_cache_dir).expanduser()
    new_cache_dir = Path(new_cache_dir).expanduser()
    # éå†æ—§ç¼“å­˜ç›®å½•ä¸­çš„æ‰€æœ‰ blob æ–‡ä»¶
    for old_blob_path in old_cache_dir.glob("**/blobs/*"):
        # å¦‚æœè·¯å¾„æ˜¯æ–‡ä»¶ä¸”ä¸æ˜¯ç¬¦å·é“¾æ¥
        if old_blob_path.is_file() and not old_blob_path.is_symlink():
            # è®¡ç®—æ–° blob æ–‡ä»¶çš„è·¯å¾„
            new_blob_path = new_cache_dir / old_blob_path.relative_to(old_cache_dir)
            # åˆ›å»ºæ–°è·¯å¾„çš„çˆ¶ç›®å½•
            new_blob_path.parent.mkdir(parents=True, exist_ok=True)
            # æ›¿æ¢æ—§çš„ blob æ–‡ä»¶ä¸ºæ–°çš„ blob æ–‡ä»¶
            os.replace(old_blob_path, new_blob_path)
            # å°è¯•åœ¨æ—§è·¯å¾„å’Œæ–°è·¯å¾„ä¹‹é—´åˆ›å»ºç¬¦å·é“¾æ¥
            try:
                os.symlink(new_blob_path, old_blob_path)
            except OSError:
                # å¦‚æœæ— æ³•åˆ›å»ºç¬¦å·é“¾æ¥ï¼Œå‘å‡ºè­¦å‘Š
                logger.warning(
                    "Could not create symlink between old cache and new cache. If you use an older version of diffusers again, files will be re-downloaded."
                )
    # ç°åœ¨ï¼Œold_cache_dir åŒ…å«æŒ‡å‘æ–°ç¼“å­˜çš„ç¬¦å·é“¾æ¥ï¼ˆä»ç„¶å¯ä»¥ä½¿ç”¨ï¼‰


# å®šä¹‰ç¼“å­˜ç‰ˆæœ¬æ–‡ä»¶çš„è·¯å¾„
cache_version_file = os.path.join(HF_HUB_CACHE, "version_diffusers_cache.txt")
# å¦‚æœç¼“å­˜ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è®¾ç½®ç¼“å­˜ç‰ˆæœ¬ä¸º 0
if not os.path.isfile(cache_version_file):
    cache_version = 0
else:
    # æ‰“å¼€æ–‡ä»¶ä»¥è¯»å–ç¼“å­˜ç‰ˆæœ¬
    with open(cache_version_file) as f:
        try:
            # å°è¯•å°†è¯»å–å†…å®¹è½¬æ¢ä¸ºæ•´æ•°
            cache_version = int(f.read())
        except ValueError:
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œåˆ™è®¾ç½®ç¼“å­˜ç‰ˆæœ¬ä¸º 0
            cache_version = 0

# å¦‚æœç¼“å­˜ç‰ˆæœ¬å°äº 1
if cache_version < 1:
    # æ£€æŸ¥æ—§çš„ç¼“å­˜ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”éç©º
        old_cache_is_not_empty = os.path.isdir(old_diffusers_cache) and len(os.listdir(old_diffusers_cache)) > 0
        # å¦‚æœæ—§ç¼“å­˜ä¸ä¸ºç©ºï¼Œåˆ™è®°å½•è­¦å‘Šä¿¡æ¯
        if old_cache_is_not_empty:
            logger.warning(
                "The cache for model files in Diffusers v0.14.0 has moved to a new location. Moving your "
                "existing cached models. This is a one-time operation, you can interrupt it or run it "
                "later by calling `diffusers.utils.hub_utils.move_cache()`."
            )
            # å°è¯•ç§»åŠ¨ç¼“å­˜
            try:
                move_cache()
            # æ•è·ä»»ä½•å¼‚å¸¸å¹¶å¤„ç†
            except Exception as e:
                # è·å–å¼‚å¸¸çš„è¿½è¸ªä¿¡æ¯å¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
                trace = "\n".join(traceback.format_tb(e.__traceback__))
                # è®°å½•é”™è¯¯ä¿¡æ¯ï¼Œå»ºè®®ç”¨æˆ·åœ¨ GitHub æäº¤é—®é¢˜
                logger.error(
                    f"There was a problem when trying to move your cache:\n\n{trace}\n{e.__class__.__name__}: {e}\n\nPlease "
                    "file an issue at https://github.com/huggingface/diffusers/issues/new/choose, copy paste this whole "
                    "message and we will do our best to help."
                )
# æ£€æŸ¥ç¼“å­˜ç‰ˆæœ¬æ˜¯å¦å°äº1
if cache_version < 1:
    # å°è¯•åˆ›å»ºç¼“å­˜ç›®å½•
    try:
        os.makedirs(HF_HUB_CACHE, exist_ok=True)  # åˆ›å»ºç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
        # æ‰“å¼€ç¼“å­˜ç‰ˆæœ¬æ–‡ä»¶ä»¥å†™å…¥ç‰ˆæœ¬å·
        with open(cache_version_file, "w") as f:
            f.write("1")  # å†™å…¥ç‰ˆæœ¬å·1
    except Exception:  # æ•è·å¼‚å¸¸
        # è®°å½•è­¦å‘Šä¿¡æ¯ï¼Œæç¤ºç”¨æˆ·å¯èƒ½å­˜åœ¨çš„é—®é¢˜
        logger.warning(
            f"There was a problem when trying to write in your cache folder ({HF_HUB_CACHE}). Please, ensure "
            "the directory exists and can be written to."
        )

# å®šä¹‰å‡½æ•°ä»¥æ·»åŠ å˜ä½“åˆ°æƒé‡åç§°
def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:
    # å¦‚æœå˜ä½“ä¸ä¸º None
    if variant is not None:
        # æŒ‰ '.' åˆ†å‰²æƒé‡åç§°
        splits = weights_name.split(".")
        # ç¡®å®šåˆ†å‰²ç´¢å¼•
        split_index = -2 if weights_name.endswith(".index.json") else -1
        # æ›´æ–°æƒé‡åç§°çš„åˆ†å‰²éƒ¨åˆ†ï¼Œæ’å…¥å˜ä½“
        splits = splits[:-split_index] + [variant] + splits[-split_index:]
        # é‡æ–°è¿æ¥åˆ†å‰²éƒ¨åˆ†ä¸ºå®Œæ•´çš„æƒé‡åç§°
        weights_name = ".".join(splits)

    # è¿”å›æ›´æ–°åçš„æƒé‡åç§°
    return weights_name

# è£…é¥°å™¨ç”¨äºéªŒè¯ HF Hub çš„å‚æ•°
@validate_hf_hub_args
def _get_model_file(
    pretrained_model_name_or_path: Union[str, Path],  # é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„
    *,
    weights_name: str,  # æƒé‡æ–‡ä»¶çš„åç§°
    subfolder: Optional[str] = None,  # å­æ–‡ä»¶å¤¹ï¼Œé»˜è®¤ä¸º None
    cache_dir: Optional[str] = None,  # ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º None
    force_download: bool = False,  # å¼ºåˆ¶ä¸‹è½½æ ‡å¿—ï¼Œé»˜è®¤ä¸º False
    proxies: Optional[Dict] = None,  # ä»£ç†è®¾ç½®ï¼Œé»˜è®¤ä¸º None
    local_files_only: bool = False,  # ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶çš„æ ‡å¿—ï¼Œé»˜è®¤ä¸º False
    token: Optional[str] = None,  # è®¿é—®ä»¤ç‰Œï¼Œé»˜è®¤ä¸º None
    user_agent: Optional[Union[Dict, str]] = None,  # ç”¨æˆ·ä»£ç†è®¾ç½®ï¼Œé»˜è®¤ä¸º None
    revision: Optional[str] = None,  # ä¿®è®¢ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸º None
    commit_hash: Optional[str] = None,  # æäº¤å“ˆå¸Œå€¼ï¼Œé»˜è®¤ä¸º None
):
    # å°†é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    pretrained_model_name_or_path = str(pretrained_model_name_or_path)
    # å¦‚æœè·¯å¾„æŒ‡å‘ä¸€ä¸ªæ–‡ä»¶
    if os.path.isfile(pretrained_model_name_or_path):
        return pretrained_model_name_or_path  # ç›´æ¥è¿”å›æ–‡ä»¶è·¯å¾„
    # å¦‚æœè·¯å¾„æŒ‡å‘ä¸€ä¸ªç›®å½•
    elif os.path.isdir(pretrained_model_name_or_path):
        # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æƒé‡æ–‡ä»¶
        if os.path.isfile(os.path.join(pretrained_model_name_or_path, weights_name)):
            # ä» PyTorch æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹æ–‡ä»¶
            model_file = os.path.join(pretrained_model_name_or_path, weights_name)
            return model_file  # è¿”å›æ¨¡å‹æ–‡ä»¶è·¯å¾„
        # å¦‚æœæœ‰å­æ–‡ä»¶å¤¹ä¸”å­æ–‡ä»¶å¤¹ä¸­å­˜åœ¨æƒé‡æ–‡ä»¶
        elif subfolder is not None and os.path.isfile(
            os.path.join(pretrained_model_name_or_path, subfolder, weights_name)
        ):
            model_file = os.path.join(pretrained_model_name_or_path, subfolder, weights_name)
            return model_file  # è¿”å›å­æ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
        else:
            # æŠ›å‡ºç¯å¢ƒé”™è¯¯ï¼ŒæŒ‡ç¤ºæœªæ‰¾åˆ°æƒé‡æ–‡ä»¶
            raise EnvironmentError(
                f"Error no file named {weights_name} found in directory {pretrained_model_name_or_path}."
            )

# æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨åˆ†ç‰‡æ–‡ä»¶çš„å‡½æ•°
def _check_if_shards_exist_locally(local_dir, subfolder, original_shard_filenames):
    # æ„é€ åˆ†ç‰‡æ–‡ä»¶çš„è·¯å¾„
    shards_path = os.path.join(local_dir, subfolder)
    # è·å–æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    shard_filenames = [os.path.join(shards_path, f) for f in original_shard_filenames]
    # éå†æ¯ä¸ªåˆ†ç‰‡æ–‡ä»¶
    for shard_file in shard_filenames:
        # æ£€æŸ¥åˆ†ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(shard_file):
            # å¦‚æœä¸å­˜åœ¨ï¼ŒæŠ›å‡ºé”™è¯¯æç¤º
            raise ValueError(
                f"{shards_path} does not appear to have a file named {shard_file} which is "
                "required according to the checkpoint index."
            )

# è·å–æ£€æŸ¥ç‚¹åˆ†ç‰‡æ–‡ä»¶çš„å‡½æ•°å®šä¹‰
def _get_checkpoint_shard_files(
    pretrained_model_name_or_path,  # é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„
    index_filename,  # ç´¢å¼•æ–‡ä»¶å
    cache_dir=None,  # ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º None
    proxies=None,  # ä»£ç†è®¾ç½®ï¼Œé»˜è®¤ä¸º None
    # è®¾ç½®æ˜¯å¦ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œé»˜è®¤ä¸º False
    local_files_only=False,
    # è®¾ç½®è®¿é—®ä»¤ç‰Œï¼Œé»˜è®¤ä¸º Noneï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ä»¤ç‰Œ
    token=None,
    # è®¾ç½®ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
    user_agent=None,
    # è®¾ç½®ä¿®è®¢ç‰ˆå·ï¼Œé»˜è®¤ä¸º None
    revision=None,
    # è®¾ç½®å­æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
    subfolder="",
):
    """
    å¯¹äºç»™å®šçš„æ¨¡å‹ï¼š

    - å¦‚æœ `pretrained_model_name_or_path` æ˜¯ Hub ä¸Šçš„æ¨¡å‹ IDï¼Œåˆ™ä¸‹è½½å¹¶ç¼“å­˜æ‰€æœ‰åˆ†ç‰‡çš„æ£€æŸ¥ç‚¹
    - è¿”å›æ‰€æœ‰åˆ†ç‰‡çš„è·¯å¾„åˆ—è¡¨ï¼Œä»¥åŠä¸€äº›å…ƒæ•°æ®ã€‚

    æœ‰å…³æ¯ä¸ªå‚æ•°çš„æè¿°ï¼Œè¯·å‚è§ [`PreTrainedModel.from_pretrained`]ã€‚ `index_filename` æ˜¯ç´¢å¼•çš„å®Œæ•´è·¯å¾„
    ï¼ˆå¦‚æœ `pretrained_model_name_or_path` æ˜¯ Hub ä¸Šçš„æ¨¡å‹ IDï¼Œåˆ™ä¸‹è½½å¹¶ç¼“å­˜ï¼‰ã€‚
    """
    # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æŠ›å‡ºé”™è¯¯
    if not os.path.isfile(index_filename):
        raise ValueError(f"Can't find a checkpoint index ({index_filename}) in {pretrained_model_name_or_path}.")

    # æ‰“å¼€ç´¢å¼•æ–‡ä»¶å¹¶è¯»å–å†…å®¹ï¼Œè§£æä¸º JSON æ ¼å¼
    with open(index_filename, "r") as f:
        index = json.loads(f.read())

    # è·å–æƒé‡æ˜ å°„ä¸­çš„æ‰€æœ‰åŸå§‹åˆ†ç‰‡æ–‡ä»¶åï¼Œå¹¶å»é‡åæ’åº
    original_shard_filenames = sorted(set(index["weight_map"].values()))
    # è·å–åˆ†ç‰‡å…ƒæ•°æ®
    sharded_metadata = index["metadata"]
    # å°†æ‰€æœ‰æ£€æŸ¥ç‚¹é”®çš„åˆ—è¡¨æ·»åŠ åˆ°å…ƒæ•°æ®ä¸­
    sharded_metadata["all_checkpoint_keys"] = list(index["weight_map"].keys())
    # å¤åˆ¶æƒé‡æ˜ å°„åˆ°å…ƒæ•°æ®ä¸­
    sharded_metadata["weight_map"] = index["weight_map"].copy()
    # æ„å»ºåˆ†ç‰‡çš„è·¯å¾„
    shards_path = os.path.join(pretrained_model_name_or_path, subfolder)

    # é¦–å…ˆå¤„ç†æœ¬åœ°æ–‡ä»¶å¤¹
    if os.path.isdir(pretrained_model_name_or_path):
        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ‰€éœ€çš„åˆ†ç‰‡
        _check_if_shards_exist_locally(
            pretrained_model_name_or_path, subfolder=subfolder, original_shard_filenames=original_shard_filenames
        )
        # è¿”å›åˆ†ç‰‡è·¯å¾„å’Œåˆ†ç‰‡å…ƒæ•°æ®
        return shards_path, sharded_metadata

    # æ­¤æ—¶ pretrained_model_name_or_path æ˜¯ Hub ä¸Šçš„æ¨¡å‹æ ‡è¯†ç¬¦
    # è®¾ç½®å…è®¸çš„æ–‡ä»¶æ¨¡å¼ä¸ºåŸå§‹åˆ†ç‰‡æ–‡ä»¶å
    allow_patterns = original_shard_filenames
    # å¦‚æœæä¾›äº†å­æ–‡ä»¶å¤¹ï¼Œåˆ™æ›´æ–°å…è®¸çš„æ–‡ä»¶æ¨¡å¼
    if subfolder is not None:
        allow_patterns = [os.path.join(subfolder, p) for p in allow_patterns]

    # å®šä¹‰éœ€è¦å¿½ç•¥çš„æ–‡ä»¶æ¨¡å¼
    ignore_patterns = ["*.json", "*.md"]
    # å¦‚æœä¸æ˜¯ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    if not local_files_only:
        # `model_info` è°ƒç”¨å¿…é¡»å—åˆ°ä¸Šè¿°æ¡ä»¶çš„ä¿æŠ¤
        model_files_info = model_info(pretrained_model_name_or_path, revision=revision)
        # éå†åŸå§‹åˆ†ç‰‡æ–‡ä»¶å
        for shard_file in original_shard_filenames:
            # æ£€æŸ¥å½“å‰åˆ†ç‰‡æ–‡ä»¶æ˜¯å¦åœ¨æ¨¡å‹æ–‡ä»¶ä¿¡æ¯ä¸­å­˜åœ¨
            shard_file_present = any(shard_file in k.rfilename for k in model_files_info.siblings)
            # å¦‚æœåˆ†ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡ºç¯å¢ƒé”™è¯¯
            if not shard_file_present:
                raise EnvironmentError(
                    f"{shards_path} ä¸å­˜åœ¨åä¸º {shard_file} çš„æ–‡ä»¶ï¼Œè¿™æ˜¯æ ¹æ®æ£€æŸ¥ç‚¹ç´¢å¼•æ‰€éœ€çš„ã€‚"
                )

        try:
            # ä» URL åŠ è½½
            cached_folder = snapshot_download(
                pretrained_model_name_or_path,  # è¦ä¸‹è½½çš„æ¨¡å‹è·¯å¾„
                cache_dir=cache_dir,  # ç¼“å­˜ç›®å½•
                proxies=proxies,  # ä»£ç†è®¾ç½®
                local_files_only=local_files_only,  # æ˜¯å¦ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                token=token,  # æˆæƒä»¤ç‰Œ
                revision=revision,  # ç‰ˆæœ¬ä¿¡æ¯
                allow_patterns=allow_patterns,  # å…è®¸çš„æ–‡ä»¶æ¨¡å¼
                ignore_patterns=ignore_patterns,  # å¿½ç•¥çš„æ–‡ä»¶æ¨¡å¼
                user_agent=user_agent,  # ç”¨æˆ·ä»£ç†ä¿¡æ¯
            )
            # å¦‚æœæŒ‡å®šäº†å­æ–‡ä»¶å¤¹ï¼Œåˆ™æ›´æ–°ç¼“å­˜æ–‡ä»¶å¤¹è·¯å¾„
            if subfolder is not None:
                cached_folder = os.path.join(cached_folder, subfolder)

        # å·²ç»åœ¨è·å–ç´¢å¼•æ—¶å¤„ç†äº† RepositoryNotFoundError å’Œ RevisionNotFoundErrorï¼Œ
        # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦æ•è·å®ƒä»¬ã€‚ä¹Ÿå¤„ç†äº† EntryNotFoundErrorã€‚
        except HTTPError as e:
            # å¦‚æœæ— æ³•è¿æ¥åˆ°æŒ‡å®šçš„ç«¯ç‚¹ï¼Œåˆ™æŠ›å‡ºç¯å¢ƒé”™è¯¯
            raise EnvironmentError(
                f"æˆ‘ä»¬æ— æ³•è¿æ¥åˆ° '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' æ¥åŠ è½½ {pretrained_model_name_or_path}ã€‚è¯·æ£€æŸ¥æ‚¨çš„äº’è”ç½‘è¿æ¥åé‡è¯•ã€‚"
            ) from e

    # å¦‚æœ `local_files_only=True`ï¼Œåˆ™ `cached_folder` å¯èƒ½ä¸åŒ…å«æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶
    elif local_files_only:
        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ‰€æœ‰åˆ†ç‰‡
        _check_if_shards_exist_locally(
            local_dir=cache_dir,  # æœ¬åœ°ç›®å½•
            subfolder=subfolder,  # å­æ–‡ä»¶å¤¹
            original_shard_filenames=original_shard_filenames  # åŸå§‹åˆ†ç‰‡æ–‡ä»¶ååˆ—è¡¨
        )
        # å¦‚æœæŒ‡å®šäº†å­æ–‡ä»¶å¤¹ï¼Œåˆ™æ›´æ–°ç¼“å­˜æ–‡ä»¶å¤¹è·¯å¾„
        if subfolder is not None:
            cached_folder = os.path.join(cached_folder, subfolder)

    # è¿”å›ç¼“å­˜æ–‡ä»¶å¤¹å’Œåˆ†ç‰‡å…ƒæ•°æ®
    return cached_folder, sharded_metadata
# å®šä¹‰ä¸€ä¸ªæ··åˆç±»ï¼Œç”¨äºå°†æ¨¡å‹ã€è°ƒåº¦å™¨æˆ–ç®¡é“æ¨é€åˆ° Hugging Face Hub
class PushToHubMixin:
    """
    A Mixin to push a model, scheduler, or pipeline to the Hugging Face Hub.
    """

    # å®šä¹‰ä¸€ä¸ªç§æœ‰æ–¹æ³•ï¼Œç”¨äºä¸Šä¼ æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    def _upload_folder(
        self,
        working_dir: Union[str, os.PathLike],  # å·¥ä½œç›®å½•ï¼ŒåŒ…å«å¾…ä¸Šä¼ çš„æ–‡ä»¶
        repo_id: str,                           # ç›®æ ‡ä»“åº“çš„ ID
        token: Optional[str] = None,           # å¯é€‰çš„è®¤è¯ä»¤ç‰Œ
        commit_message: Optional[str] = None,  # å¯é€‰çš„æäº¤ä¿¡æ¯
        create_pr: bool = False,                # æ˜¯å¦åˆ›å»ºæ‹‰å–è¯·æ±‚
    ):
        """
        Uploads all files in `working_dir` to `repo_id`.
        """
        # å¦‚æœæœªæä¾›æäº¤ä¿¡æ¯ï¼Œåˆ™æ ¹æ®ç±»åç”Ÿæˆé»˜è®¤æäº¤ä¿¡æ¯
        if commit_message is None:
            if "Model" in self.__class__.__name__:
                commit_message = "Upload model"  # å¦‚æœæ˜¯æ¨¡å‹ç±»ï¼Œè®¾ç½®é»˜è®¤ä¿¡æ¯
            elif "Scheduler" in self.__class__.__name__:
                commit_message = "Upload scheduler"  # å¦‚æœæ˜¯è°ƒåº¦å™¨ç±»ï¼Œè®¾ç½®é»˜è®¤ä¿¡æ¯
            else:
                commit_message = f"Upload {self.__class__.__name__}"  # å¦åˆ™ï¼Œä½¿ç”¨ç±»åä½œä¸ºæäº¤ä¿¡æ¯

        # è®°å½•ä¸Šä¼ æ–‡ä»¶çš„æ—¥å¿—ä¿¡æ¯
        logger.info(f"Uploading the files of {working_dir} to {repo_id}.")
        # è°ƒç”¨ upload_folder å‡½æ•°ä¸Šä¼ æ–‡ä»¶ï¼Œå¹¶è¿”å›å…¶ç»“æœ
        return upload_folder(
            repo_id=repo_id,                    # ç›®æ ‡ä»“åº“ ID
            folder_path=working_dir,            # å¾…ä¸Šä¼ çš„æ–‡ä»¶å¤¹è·¯å¾„
            token=token,                        # è®¤è¯ä»¤ç‰Œ
            commit_message=commit_message,      # æäº¤ä¿¡æ¯
            create_pr=create_pr                 # æ˜¯å¦åˆ›å»ºæ‹‰å–è¯·æ±‚
        )

    # å®šä¹‰ä¸€ä¸ªå…¬å…±æ–¹æ³•ï¼Œç”¨äºå°†æ–‡ä»¶æ¨é€åˆ° Hugging Face Hub
    def push_to_hub(
        self,
        repo_id: str,                           # ç›®æ ‡ä»“åº“çš„ ID
        commit_message: Optional[str] = None,  # å¯é€‰çš„æäº¤ä¿¡æ¯
        private: Optional[bool] = None,        # å¯é€‰ï¼Œæ˜¯å¦å°†ä»“åº“è®¾ç½®ä¸ºç§æœ‰
        token: Optional[str] = None,           # å¯é€‰çš„è®¤è¯ä»¤ç‰Œ
        create_pr: bool = False,                # æ˜¯å¦åˆ›å»ºæ‹‰å–è¯·æ±‚
        safe_serialization: bool = True,        # æ˜¯å¦å®‰å…¨åºåˆ—åŒ–
        variant: Optional[str] = None,          # å¯é€‰çš„å˜ä½“å‚æ•°
    ) -> str:  # å®šä¹‰å‡½æ•°è¿”å›å€¼ç±»å‹ä¸ºå­—ç¬¦ä¸²
        """
        Upload model, scheduler, or pipeline files to the ğŸ¤— Hugging Face Hub.  # å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜åŠŸèƒ½

        Parameters:  # å‚æ•°è¯´æ˜éƒ¨åˆ†
            repo_id (`str`):  # ä»“åº“ IDï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²
                The name of the repository you want to push your model, scheduler, or pipeline files to. It should
                contain your organization name when pushing to an organization. `repo_id` can also be a path to a local
                directory.  # æè¿° repo_id çš„ç”¨é€”å’Œæ ¼å¼
            commit_message (`str`, *optional*):  # å¯é€‰å‚æ•°ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²
                Message to commit while pushing. Default to `"Upload {object}".`  # æäº¤æ¶ˆæ¯çš„é»˜è®¤å€¼
            private (`bool`, *optional*):  # å¯é€‰å‚æ•°ï¼Œç±»å‹ä¸ºå¸ƒå°”å€¼
                Whether or not the repository created should be private.  # æ˜¯å¦åˆ›å»ºç§æœ‰ä»“åº“
            token (`str`, *optional*):  # å¯é€‰å‚æ•°ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²
                The token to use as HTTP bearer authorization for remote files. The token generated when running
                `huggingface-cli login` (stored in `~/.huggingface`).  # è¯´æ˜ token çš„ç”¨é€”
            create_pr (`bool`, *optional*, defaults to `False`):  # å¯é€‰å‚æ•°ï¼Œç±»å‹ä¸ºå¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸º False
                Whether or not to create a PR with the uploaded files or directly commit.  # æ˜¯å¦åˆ›å»º PR
            safe_serialization (`bool`, *optional*, defaults to `True`):  # å¯é€‰å‚æ•°ï¼Œç±»å‹ä¸ºå¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸º True
                Whether or not to convert the model weights to the `safetensors` format.  # æ˜¯å¦ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–æ ¼å¼
            variant (`str`, *optional*):  # å¯é€‰å‚æ•°ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²
                If specified, weights are saved in the format `pytorch_model.<variant>.bin`.  # æƒé‡ä¿å­˜æ ¼å¼

        Examples:  # ç¤ºä¾‹è¯´æ˜éƒ¨åˆ†

        ```py
        from diffusers import UNet2DConditionModel  # ä» diffusers å¯¼å…¥ UNet2DConditionModel

        unet = UNet2DConditionModel.from_pretrained("stabilityai/stable-diffusion-2", subfolder="unet")  # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ UNet

        # Push the `unet` to your namespace with the name "my-finetuned-unet".  # æ¨é€åˆ°ä¸ªäººå‘½åç©ºé—´
        unet.push_to_hub("my-finetuned-unet")  # å°† unet æ¨é€åˆ°æŒ‡å®šåç§°çš„ä»“åº“

        # Push the `unet` to an organization with the name "my-finetuned-unet".  # æ¨é€åˆ°ç»„ç»‡
        unet.push_to_hub("your-org/my-finetuned-unet")  # å°† unet æ¨é€åˆ°æŒ‡å®šç»„ç»‡çš„ä»“åº“
        ```
        """  # ç»“æŸæ–‡æ¡£å­—ç¬¦ä¸²
        repo_id = create_repo(repo_id, private=private, token=token, exist_ok=True).repo_id  # åˆ›å»ºä»“åº“å¹¶è·å–ä»“åº“ ID

        # Create a new empty model card and eventually tag it  # åˆ›å»ºæ–°çš„æ¨¡å‹å¡ç‰‡å¹¶å¯èƒ½æ·»åŠ æ ‡ç­¾
        model_card = load_or_create_model_card(repo_id, token=token)  # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡
        model_card = populate_model_card(model_card)  # å¡«å……æ¨¡å‹å¡ç‰‡ä¿¡æ¯

        # Save all files.  # ä¿å­˜æ‰€æœ‰æ–‡ä»¶
        save_kwargs = {"safe_serialization": safe_serialization}  # è®¾ç½®ä¿å­˜æ–‡ä»¶çš„å‚æ•°
        if "Scheduler" not in self.__class__.__name__:  # æ£€æŸ¥å½“å‰ç±»åæ˜¯å¦åŒ…å« "Scheduler"
            save_kwargs.update({"variant": variant})  # å¦‚æœä¸åŒ…å«ï¼Œåˆ™æ·»åŠ  variant å‚æ•°

        with tempfile.TemporaryDirectory() as tmpdir:  # åˆ›å»ºä¸´æ—¶ç›®å½•
            self.save_pretrained(tmpdir, **save_kwargs)  # å°†æ¨¡å‹ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•

            # Update model card if needed:  # å¦‚æœéœ€è¦ï¼Œæ›´æ–°æ¨¡å‹å¡ç‰‡
            model_card.save(os.path.join(tmpdir, "README.md"))  # å°†æ¨¡å‹å¡ç‰‡ä¿å­˜ä¸º README.md æ–‡ä»¶

            return self._upload_folder(  # ä¸Šä¼ ä¸´æ—¶ç›®å½•ä¸­çš„æ–‡ä»¶
                tmpdir,  # ä¸´æ—¶ç›®å½•è·¯å¾„
                repo_id,  # ä»“åº“ ID
                token=token,  # è®¤è¯ token
                commit_message=commit_message,  # æäº¤æ¶ˆæ¯
                create_pr=create_pr,  # æ˜¯å¦åˆ›å»º PR
            )  # è¿”å›ä¸Šä¼ ç»“æœ
```