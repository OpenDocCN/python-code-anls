# `.\diffusers\pipelines\kandinsky\pipeline_kandinsky_combined.py`

```py
# ç‰ˆæƒæ‰€æœ‰ 2024 HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼ˆ"è®¸å¯è¯"ï¼‰è®¸å¯ï¼›
# é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œè½¯ä»¶åœ¨è®¸å¯è¯ä¸‹åˆ†å‘æ—¶ä»¥â€œåŸæ ·â€åŸºç¡€æä¾›ï¼Œ
# ä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºæˆ–æš—ç¤ºçš„ã€‚
# è¯·å‚é˜…è®¸å¯è¯ä»¥è·å–ç®¡ç†æƒé™å’Œ
# é™åˆ¶çš„å…·ä½“è¯­è¨€ã€‚
from typing import Callable, List, Optional, Union  # ä» typing æ¨¡å—å¯¼å…¥ç±»å‹æ³¨è§£åŠŸèƒ½

import PIL.Image  # å¯¼å…¥ PIL.Image æ¨¡å—ä»¥å¤„ç†å›¾åƒ
import torch  # å¯¼å…¥ PyTorch åº“ç”¨äºæ·±åº¦å­¦ä¹ 
from transformers import (  # ä» transformers åº“å¯¼å…¥å¤šä¸ªæ¨¡å‹å’Œå¤„ç†å™¨
    CLIPImageProcessor,  # å¯¼å…¥ CLIP å›¾åƒå¤„ç†å™¨
    CLIPTextModelWithProjection,  # å¯¼å…¥å…·æœ‰æŠ•å½±çš„ CLIP æ–‡æœ¬æ¨¡å‹
    CLIPTokenizer,  # å¯¼å…¥ CLIP ä»¤ç‰ŒåŒ–å·¥å…·
    CLIPVisionModelWithProjection,  # å¯¼å…¥å…·æœ‰æŠ•å½±çš„ CLIP è§†è§‰æ¨¡å‹
    XLMRobertaTokenizer,  # å¯¼å…¥ XLM-Roberta ä»¤ç‰ŒåŒ–å·¥å…·
)

from ...models import PriorTransformer, UNet2DConditionModel, VQModel  # ä»ç›¸å¯¹è·¯å¾„å¯¼å…¥æ¨¡å‹
from ...schedulers import DDIMScheduler, DDPMScheduler, UnCLIPScheduler  # å¯¼å…¥ä¸åŒçš„è°ƒåº¦å™¨
from ...utils import (  # ä»å·¥å…·æ¨¡å—å¯¼å…¥ç‰¹å®šåŠŸèƒ½
    replace_example_docstring,  # å¯¼å…¥æ›¿æ¢ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²çš„åŠŸèƒ½
)
from ..pipeline_utils import DiffusionPipeline  # ä»ä¸Šçº§è·¯å¾„å¯¼å…¥æ‰©æ•£ç®¡é“å·¥å…·
from .pipeline_kandinsky import KandinskyPipeline  # å¯¼å…¥ Kandinsky ç®¡é“
from .pipeline_kandinsky_img2img import KandinskyImg2ImgPipeline  # å¯¼å…¥ Kandinsky å›¾åƒåˆ°å›¾åƒç®¡é“
from .pipeline_kandinsky_inpaint import KandinskyInpaintPipeline  # å¯¼å…¥ Kandinsky ä¿®å¤ç®¡é“
from .pipeline_kandinsky_prior import KandinskyPriorPipeline  # å¯¼å…¥ Kandinsky å…ˆéªŒç®¡é“
from .text_encoder import MultilingualCLIP  # å¯¼å…¥å¤šè¯­è¨€ CLIP æ–‡æœ¬ç¼–ç å™¨

# å®šä¹‰æ–‡æœ¬åˆ°å›¾åƒçš„ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²
TEXT2IMAGE_EXAMPLE_DOC_STRING = """
    ç¤ºä¾‹ï¼š
        ```py
        from diffusers import AutoPipelineForText2Image  # å¯¼å…¥è‡ªåŠ¨æ–‡æœ¬åˆ°å›¾åƒç®¡é“
        import torch  # å¯¼å…¥ PyTorch åº“

        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç®¡é“
        pipe = AutoPipelineForText2Image.from_pretrained(
            "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
        )
        # å¯ç”¨æ¨¡å‹çš„ CPU å¸è½½åŠŸèƒ½
        pipe.enable_model_cpu_offload()

        # å®šä¹‰ç”Ÿæˆå›¾åƒçš„æç¤ºè¯­
        prompt = "A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k"

        # ç”Ÿæˆå›¾åƒå¹¶è·å–ç¬¬ä¸€å¼ å›¾åƒ
        image = pipe(prompt=prompt, num_inference_steps=25).images[0]
        ```py
"""

# å®šä¹‰å›¾åƒåˆ°å›¾åƒçš„ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²
IMAGE2IMAGE_EXAMPLE_DOC_STRING = """
    ç¤ºä¾‹ï¼š
        ```py
        from diffusers import AutoPipelineForImage2Image  # å¯¼å…¥è‡ªåŠ¨å›¾åƒåˆ°å›¾åƒç®¡é“
        import torch  # å¯¼å…¥ PyTorch åº“
        import requests  # å¯¼å…¥è¯·æ±‚åº“ç”¨äºè·å–å›¾åƒ
        from io import BytesIO  # ä»å­—èŠ‚æµä¸­è¯»å–æ•°æ®
        from PIL import Image  # å¯¼å…¥ PIL å›¾åƒå¤„ç†åº“
        import os  # å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£åº“

        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç®¡é“
        pipe = AutoPipelineForImage2Image.from_pretrained(
            "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
        )
        # å¯ç”¨æ¨¡å‹çš„ CPU å¸è½½åŠŸèƒ½
        pipe.enable_model_cpu_offload()

        # å®šä¹‰ç”Ÿæˆå›¾åƒçš„æç¤ºè¯­å’Œè´Ÿé¢æç¤ºè¯­
        prompt = "A fantasy landscape, Cinematic lighting"
        negative_prompt = "low quality, bad quality"

        # å›¾åƒ URL
        url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

        # å‘é€è¯·æ±‚è·å–å›¾åƒ
        response = requests.get(url)
        # æ‰“å¼€å›¾åƒå¹¶è½¬æ¢ä¸º RGB æ ¼å¼
        image = Image.open(BytesIO(response.content)).convert("RGB")
        # è°ƒæ•´å›¾åƒå¤§å°
        image.thumbnail((768, 768))

        # ç”Ÿæˆæ–°å›¾åƒå¹¶è·å–ç¬¬ä¸€å¼ å›¾åƒ
        image = pipe(prompt=prompt, image=original_image, num_inference_steps=25).images[0]
        ```py
"""

# å®šä¹‰ä¿®å¤çš„ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²
INPAINT_EXAMPLE_DOC_STRING = """
``` 
```py  # ç»“æŸä¿®å¤ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²
    # ç¤ºä¾‹ä»£ç å—
    Examples:
        ```py
        # ä» diffusers åº“å¯¼å…¥ AutoPipelineForInpainting ç±»
        from diffusers import AutoPipelineForInpainting
        # ä» diffusers.utils å¯¼å…¥ load_image å‡½æ•°
        from diffusers.utils import load_image
        # å¯¼å…¥ PyTorch åº“
        import torch
        # å¯¼å…¥ NumPy åº“
        import numpy as np

        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ AutoPipelineForInpainting å¯¹è±¡ï¼ŒæŒ‡å®šæ•°æ®ç±»å‹ä¸º float16
        pipe = AutoPipelineForInpainting.from_pretrained(
            "kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16
        )
        # å¯ç”¨æ¨¡å‹çš„ CPU å¸è½½åŠŸèƒ½ï¼Œä»¥èŠ‚çœå†…å­˜
        pipe.enable_model_cpu_offload()

        # å®šä¹‰æç¤ºè¯ï¼Œæè¿°è¦ç”Ÿæˆçš„å›¾åƒå†…å®¹
        prompt = "A fantasy landscape, Cinematic lighting"
        # å®šä¹‰è´Ÿé¢æç¤ºè¯ï¼Œç”¨äºé™åˆ¶ç”Ÿæˆå†…å®¹çš„è´¨é‡
        negative_prompt = "low quality, bad quality"

        # ä»æŒ‡å®š URL åŠ è½½åŸå§‹å›¾åƒ
        original_image = load_image(
            "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main" "/kandinsky/cat.png"
        )

        # åˆ›å»ºä¸€ä¸ªå…¨ä¸ºé›¶çš„æ©ç æ•°ç»„ï¼Œå¤§å°ä¸º 768x768ï¼Œæ•°æ®ç±»å‹ä¸º float32
        mask = np.zeros((768, 768), dtype=np.float32)
        # åœ¨çŒ«çš„å¤´éƒ¨ä¸Šæ–¹é®ç½©åŒºåŸŸ
        mask[:250, 250:-250] = 1

        # ä½¿ç”¨ç®¡é“ç”Ÿæˆæ–°å›¾åƒï¼Œè¾“å…¥æç¤ºè¯ã€åŸå§‹å›¾åƒå’Œæ©ç ï¼Œè®¾å®šæ¨ç†æ­¥éª¤æ•°é‡ä¸º 25ï¼Œå¹¶æå–ç”Ÿæˆçš„ç¬¬ä¸€å¼ å›¾åƒ
        image = pipe(prompt=prompt, image=original_image, mask_image=mask, num_inference_steps=25).images[0]
        ``` 
"""
# ç±»å®šä¹‰ï¼šKandinskyCombinedPipelineï¼Œç»§æ‰¿è‡ª DiffusionPipelineï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
class KandinskyCombinedPipeline(DiffusionPipeline):
    """
    # æ–‡æ¡£å­—ç¬¦ä¸²ï¼šæè¿°ä½¿ç”¨Kandinskyè¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“

    # æ–‡æ¡£å­—ç¬¦ä¸²ï¼šè¯´æ˜æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [`DiffusionPipeline`]ï¼Œå¹¶æåˆ°å…¶é€šç”¨æ–¹æ³•çš„æ–‡æ¡£ï¼ˆå¦‚ä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰

    # æ–‡æ¡£å­—ç¬¦ä¸²ï¼šæ¨¡å‹æ„é€ å‡½æ•°çš„å‚æ•°è¯´æ˜
        # text_encoderï¼šè¢«å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç±»å‹ä¸º [`MultilingualCLIP`]ã€‚
        # tokenizerï¼šç±»çš„åˆ†è¯å™¨ï¼Œç±»å‹ä¸º [`XLMRobertaTokenizer`]ã€‚
        # schedulerï¼šç”¨äºä¸ `unet` ç»“åˆç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ï¼Œç±»å‹ä¸º `Union[`DDIMScheduler`,`DDPMScheduler`]`ã€‚
        # unetï¼šæ¡ä»¶U-Netæ¶æ„ï¼Œç”¨äºå»å™ªå›¾åƒåµŒå…¥ï¼Œç±»å‹ä¸º [`UNet2DConditionModel`]ã€‚
        # movqï¼šä»æ½œå˜é‡ç”Ÿæˆå›¾åƒçš„ MoVQ è§£ç å™¨ï¼Œç±»å‹ä¸º [`VQModel`]ã€‚
        # prior_priorï¼šç”¨äºä»æ–‡æœ¬åµŒå…¥è¿‘ä¼¼å›¾åƒåµŒå…¥çš„è§„èŒƒ unCLIP å…ˆéªŒï¼Œç±»å‹ä¸º [`PriorTransformer`]ã€‚
        # prior_image_encoderï¼šè¢«å†»ç»“çš„å›¾åƒç¼–ç å™¨ï¼Œç±»å‹ä¸º [`CLIPVisionModelWithProjection`]ã€‚
        # prior_text_encoderï¼šè¢«å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç±»å‹ä¸º [`CLIPTextModelWithProjection`]ã€‚
        # prior_tokenizerï¼šç±»çš„åˆ†è¯å™¨ï¼Œç±»å‹ä¸º [`CLIPTokenizer`]ã€‚
        # prior_schedulerï¼šä¸ `prior` ç»“åˆç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ï¼Œç±»å‹ä¸º [`UnCLIPScheduler`]ã€‚
    """

    # è®¾å®šåŠ è½½è¿æ¥ç®¡é“çš„æ ‡å¿—ä¸ºçœŸ
    _load_connected_pipes = True
    # å®šä¹‰ CPU å¸è½½çš„æ¨¡å‹åºåˆ—
    model_cpu_offload_seq = "text_encoder->unet->movq->prior_prior->prior_image_encoder->prior_text_encoder"
    # æ’é™¤ CPU å¸è½½çš„éƒ¨åˆ†
    _exclude_from_cpu_offload = ["prior_prior"]

    # æ„é€ å‡½æ•°å®šä¹‰ï¼Œæ¥æ”¶å¤šä¸ªå‚æ•°ä»¥åˆå§‹åŒ–ç±»
    def __init__(
        # æ–‡æœ¬ç¼–ç å™¨å‚æ•°ï¼Œç±»å‹ä¸º MultilingualCLIP
        self,
        text_encoder: MultilingualCLIP,
        # åˆ†è¯å™¨å‚æ•°ï¼Œç±»å‹ä¸º XLMRobertaTokenizer
        tokenizer: XLMRobertaTokenizer,
        # U-Net å‚æ•°ï¼Œç±»å‹ä¸º UNet2DConditionModel
        unet: UNet2DConditionModel,
        # è°ƒåº¦å™¨å‚æ•°ï¼Œç±»å‹ä¸º DDIMScheduler æˆ– DDPMScheduler
        scheduler: Union[DDIMScheduler, DDPMScheduler],
        # MoVQ è§£ç å™¨å‚æ•°ï¼Œç±»å‹ä¸º VQModel
        movq: VQModel,
        # å…ˆéªŒå‚æ•°ï¼Œç±»å‹ä¸º PriorTransformer
        prior_prior: PriorTransformer,
        # å›¾åƒç¼–ç å™¨å‚æ•°ï¼Œç±»å‹ä¸º CLIPVisionModelWithProjection
        prior_image_encoder: CLIPVisionModelWithProjection,
        # æ–‡æœ¬ç¼–ç å™¨å‚æ•°ï¼Œç±»å‹ä¸º CLIPTextModelWithProjection
        prior_text_encoder: CLIPTextModelWithProjection,
        # å…ˆéªŒåˆ†è¯å™¨å‚æ•°ï¼Œç±»å‹ä¸º CLIPTokenizer
        prior_tokenizer: CLIPTokenizer,
        # å…ˆéªŒè°ƒåº¦å™¨å‚æ•°ï¼Œç±»å‹ä¸º UnCLIPScheduler
        prior_scheduler: UnCLIPScheduler,
        # å›¾åƒå¤„ç†å™¨å‚æ•°ï¼Œç±»å‹ä¸º CLIPImageProcessor
        prior_image_processor: CLIPImageProcessor,
    ):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()

        # æ³¨å†Œå¤šä¸ªæ¨¡å—ï¼Œä¼ é€’å„è‡ªçš„å‚æ•°
        self.register_modules(
            # æ–‡æœ¬ç¼–ç å™¨
            text_encoder=text_encoder,
            # åˆ†è¯å™¨
            tokenizer=tokenizer,
            # U-Net æ¨¡å‹
            unet=unet,
            # è°ƒåº¦å™¨
            scheduler=scheduler,
            # MOVQ æ¨¡å—
            movq=movq,
            # å…ˆéªŒæ¨¡å‹çš„å…ˆéªŒ
            prior_prior=prior_prior,
            # å›¾åƒç¼–ç å™¨
            prior_image_encoder=prior_image_encoder,
            # å…ˆéªŒæ–‡æœ¬ç¼–ç å™¨
            prior_text_encoder=prior_text_encoder,
            # å…ˆéªŒåˆ†è¯å™¨
            prior_tokenizer=prior_tokenizer,
            # å…ˆéªŒè°ƒåº¦å™¨
            prior_scheduler=prior_scheduler,
            # å…ˆéªŒå›¾åƒå¤„ç†å™¨
            prior_image_processor=prior_image_processor,
        )
        # åˆ›å»ºå…ˆéªŒç®¡é“å¯¹è±¡ï¼Œå°è£…å…ˆéªŒç›¸å…³æ¨¡å—
        self.prior_pipe = KandinskyPriorPipeline(
            # ä¼ å…¥å…ˆéªŒæ¨¡å‹
            prior=prior_prior,
            # ä¼ å…¥å›¾åƒç¼–ç å™¨
            image_encoder=prior_image_encoder,
            # ä¼ å…¥æ–‡æœ¬ç¼–ç å™¨
            text_encoder=prior_text_encoder,
            # ä¼ å…¥åˆ†è¯å™¨
            tokenizer=prior_tokenizer,
            # ä¼ å…¥è°ƒåº¦å™¨
            scheduler=prior_scheduler,
            # ä¼ å…¥å›¾åƒå¤„ç†å™¨
            image_processor=prior_image_processor,
        )
        # åˆ›å»ºè§£ç å™¨ç®¡é“å¯¹è±¡ï¼Œå°è£…è§£ç æ‰€éœ€æ¨¡å—
        self.decoder_pipe = KandinskyPipeline(
            # ä¼ å…¥æ–‡æœ¬ç¼–ç å™¨
            text_encoder=text_encoder,
            # ä¼ å…¥åˆ†è¯å™¨
            tokenizer=tokenizer,
            # ä¼ å…¥ U-Net æ¨¡å‹
            unet=unet,
            # ä¼ å…¥è°ƒåº¦å™¨
            scheduler=scheduler,
            # ä¼ å…¥ MOVQ æ¨¡å—
            movq=movq,
        )

    # å¯ç”¨ Xformers å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•ï¼Œæ”¯æŒå¯é€‰çš„æ³¨æ„åŠ›æ“ä½œ
    def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):
        # è°ƒç”¨è§£ç å™¨ç®¡é“ä¸­çš„å¯ç”¨æ–¹æ³•
        self.decoder_pipe.enable_xformers_memory_efficient_attention(attention_op)

    # å¯ç”¨é¡ºåº CPU å¸è½½çš„æ–¹æ³•ï¼Œæ¥æ”¶ GPU ID å‚æ•°
    def enable_sequential_cpu_offload(self, gpu_id=0):
        r"""
        å¸è½½æ‰€æœ‰æ¨¡å‹ï¼ˆ`unet`ã€`text_encoder`ã€`vae` å’Œ `safety checker` çŠ¶æ€å­—å…¸ï¼‰åˆ° CPUï¼Œä½¿ç”¨ ğŸ¤—
        Accelerateï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¨¡å‹è¢«ç§»åŠ¨åˆ° `torch.device('meta')`ï¼Œä»…åœ¨å…¶ç‰¹å®šå­æ¨¡å—çš„
        `forward` æ–¹æ³•è¢«è°ƒç”¨æ—¶æ‰åœ¨ GPU ä¸ŠåŠ è½½ã€‚å¸è½½æ˜¯åŸºäºå­æ¨¡å—è¿›è¡Œçš„ã€‚
        å†…å­˜èŠ‚çœå¤§äºä½¿ç”¨ `enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
        """
        # åœ¨å…ˆéªŒç®¡é“ä¸­å¯ç”¨é¡ºåº CPU å¸è½½
        self.prior_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id)
        # åœ¨è§£ç å™¨ç®¡é“ä¸­å¯ç”¨é¡ºåº CPU å¸è½½
        self.decoder_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id)

    # è¿›åº¦æ¡æ–¹æ³•ï¼Œæ¥æ”¶å¯è¿­ä»£å¯¹è±¡å’Œæ€»æ•°ä½œä¸ºå‚æ•°
    def progress_bar(self, iterable=None, total=None):
        # åœ¨å…ˆéªŒç®¡é“ä¸­è®¾ç½®è¿›åº¦æ¡
        self.prior_pipe.progress_bar(iterable=iterable, total=total)
        # åœ¨è§£ç å™¨ç®¡é“ä¸­è®¾ç½®è¿›åº¦æ¡
        self.decoder_pipe.progress_bar(iterable=iterable, total=total)
        # å¯ç”¨è§£ç å™¨ç®¡é“ä¸­çš„æ¨¡å‹ CPU å¸è½½
        self.decoder_pipe.enable_model_cpu_offload()

    # è®¾ç½®è¿›åº¦æ¡é…ç½®çš„æ–¹æ³•ï¼Œæ¥æ”¶å…³é”®å­—å‚æ•°
    def set_progress_bar_config(self, **kwargs):
        # åœ¨å…ˆéªŒç®¡é“ä¸­è®¾ç½®è¿›åº¦æ¡é…ç½®
        self.prior_pipe.set_progress_bar_config(**kwargs)
        # åœ¨è§£ç å™¨ç®¡é“ä¸­è®¾ç½®è¿›åº¦æ¡é…ç½®
        self.decoder_pipe.set_progress_bar_config(**kwargs)

    # ä½¿ç”¨ PyTorch çš„æ— æ¢¯åº¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œé¿å…è®¡ç®—æ¢¯åº¦
    @torch.no_grad()
    # æ›¿æ¢ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²çš„æ–¹æ³•
    @replace_example_docstring(TEXT2IMAGE_EXAMPLE_DOC_STRING)
    # å®šä¹‰ä¸€ä¸ªå¯è°ƒç”¨çš„ç±»æ–¹æ³•
    def __call__(
            # è¾“å…¥æç¤ºï¼Œå¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            self,
            prompt: Union[str, List[str]],
            # å¯é€‰çš„è´Ÿé¢æç¤ºï¼Œä¹Ÿå¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            negative_prompt: Optional[Union[str, List[str]]] = None,
            # æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œé»˜è®¤ä¸º100
            num_inference_steps: int = 100,
            # æŒ‡å¯¼æ¯”ä¾‹ï¼Œæ§åˆ¶ç”Ÿæˆçš„å›¾åƒä¸æç¤ºçš„ç›¸å…³æ€§ï¼Œé»˜è®¤ä¸º4.0
            guidance_scale: float = 4.0,
            # æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ï¼Œé»˜è®¤ä¸º1
            num_images_per_prompt: int = 1,
            # è¾“å‡ºå›¾åƒçš„é«˜åº¦ï¼Œé»˜è®¤ä¸º512åƒç´ 
            height: int = 512,
            # è¾“å‡ºå›¾åƒçš„å®½åº¦ï¼Œé»˜è®¤ä¸º512åƒç´ 
            width: int = 512,
            # å…ˆéªŒæŒ‡å¯¼æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º4.0
            prior_guidance_scale: float = 4.0,
            # å…ˆéªŒæ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œé»˜è®¤ä¸º25
            prior_num_inference_steps: int = 25,
            # å¯é€‰çš„ç”Ÿæˆå™¨ï¼Œç”¨äºæ§åˆ¶éšæœºæ•°ç”Ÿæˆ
            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
            # å¯é€‰çš„æ½œåœ¨å˜é‡ï¼Œé€šå¸¸ç”¨äºç”Ÿæˆæ¨¡å‹çš„è¾“å…¥
            latents: Optional[torch.Tensor] = None,
            # è¾“å‡ºç±»å‹ï¼Œé»˜è®¤ä¸ºâ€œpilâ€ï¼ŒæŒ‡ç”ŸæˆPILå›¾åƒ
            output_type: Optional[str] = "pil",
            # å¯é€‰çš„å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶å½“å‰æ­¥éª¤å’Œè¾“å‡ºå¼ é‡
            callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,
            # å›è°ƒçš„æ‰§è¡Œæ­¥æ•°ï¼Œé»˜è®¤ä¸º1
            callback_steps: int = 1,
            # æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„ç»“æœï¼Œé»˜è®¤ä¸ºTrue
            return_dict: bool = True,
# å®šä¹‰ä¸€ä¸ªç»“åˆç®¡é“ç±»ï¼Œç”¨äºä½¿ç”¨ Kandinsky è¿›è¡Œå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆ
class KandinskyImg2ImgCombinedPipeline(DiffusionPipeline):
    """
    Combined Pipeline for image-to-image generation using Kandinsky

    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)

    Args:
        text_encoder ([`MultilingualCLIP`]):
            Frozen text-encoder.  # å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨
        tokenizer ([`XLMRobertaTokenizer`]):
            Tokenizer of class  # åˆ†è¯å™¨ç±»
        scheduler (Union[`DDIMScheduler`,`DDPMScheduler`]):
            A scheduler to be used in combination with `unet` to generate image latents.  # ç”¨äºä¸ `unet` ç»“åˆç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨
        unet ([`UNet2DConditionModel`]):
            Conditional U-Net architecture to denoise the image embedding.  # æ¡ä»¶ U-Net æ¶æ„ï¼Œç”¨äºå»å™ªå›¾åƒåµŒå…¥
        movq ([`VQModel`]):
            MoVQ Decoder to generate the image from the latents.  # MoVQ è§£ç å™¨ï¼Œä»æ½œå˜é‡ç”Ÿæˆå›¾åƒ
        prior_prior ([`PriorTransformer`]):
            The canonical unCLIP prior to approximate the image embedding from the text embedding.  # ç»å…¸çš„ unCLIP å…ˆéªŒï¼Œç”¨äºä»æ–‡æœ¬åµŒå…¥è¿‘ä¼¼å›¾åƒåµŒå…¥
        prior_image_encoder ([`CLIPVisionModelWithProjection`]):
            Frozen image-encoder.  # å†»ç»“çš„å›¾åƒç¼–ç å™¨
        prior_text_encoder ([`CLIPTextModelWithProjection`]):
            Frozen text-encoder.  # å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨
        prior_tokenizer (`CLIPTokenizer`):
             Tokenizer of class  # åˆ†è¯å™¨ç±»
             [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        prior_scheduler ([`UnCLIPScheduler`]):
            A scheduler to be used in combination with `prior` to generate image embedding.  # ç”¨äºä¸ `prior` ç»“åˆç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨
    """

    _load_connected_pipes = True  # æŒ‡å®šæ˜¯å¦åŠ è½½è¿æ¥çš„ç®¡é“
    model_cpu_offload_seq = "prior_text_encoder->prior_image_encoder->prior_prior->" "text_encoder->unet->movq"  # æŒ‡å®šæ¨¡å‹åœ¨ CPU å¸è½½æ—¶çš„é¡ºåº
    _exclude_from_cpu_offload = ["prior_prior"]  # æŒ‡å®šåœ¨ CPU å¸è½½æ—¶æ’é™¤çš„ç»„ä»¶

    def __init__(  # åˆå§‹åŒ–æ–¹æ³•
        self,
        text_encoder: MultilingualCLIP,  # æ–‡æœ¬ç¼–ç å™¨å®ä¾‹
        tokenizer: XLMRobertaTokenizer,  # åˆ†è¯å™¨å®ä¾‹
        unet: UNet2DConditionModel,  # U-Net æ¨¡å‹å®ä¾‹
        scheduler: Union[DDIMScheduler, DDPMScheduler],  # è°ƒåº¦å™¨å®ä¾‹
        movq: VQModel,  # MoVQ è§£ç å™¨å®ä¾‹
        prior_prior: PriorTransformer,  # å…ˆéªŒå˜æ¢å™¨å®ä¾‹
        prior_image_encoder: CLIPVisionModelWithProjection,  # å›¾åƒç¼–ç å™¨å®ä¾‹
        prior_text_encoder: CLIPTextModelWithProjection,  # æ–‡æœ¬ç¼–ç å™¨å®ä¾‹
        prior_tokenizer: CLIPTokenizer,  # å…ˆéªŒåˆ†è¯å™¨å®ä¾‹
        prior_scheduler: UnCLIPScheduler,  # å…ˆéªŒè°ƒåº¦å™¨å®ä¾‹
        prior_image_processor: CLIPImageProcessor,  # å›¾åƒå¤„ç†å™¨å®ä¾‹
    # å®šä¹‰æ„é€ å‡½æ•°çš„ç»“æŸéƒ¨åˆ†ï¼Œè°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°
        ):
            super().__init__()
    
            # æ³¨å†Œå¤šä¸ªæ¨¡å—åŠå…¶ç›¸åº”çš„ç»„ä»¶
            self.register_modules(
                text_encoder=text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
                tokenizer=tokenizer,        # åˆ†è¯å™¨
                unet=unet,                  # UNet æ¨¡å‹
                scheduler=scheduler,        # è°ƒåº¦å™¨
                movq=movq,                  # MOVQ ç»„ä»¶
                prior_prior=prior_prior,    # å…ˆéªŒæ¨¡å‹
                prior_image_encoder=prior_image_encoder,  # å›¾åƒç¼–ç å™¨
                prior_text_encoder=prior_text_encoder,    # æ–‡æœ¬ç¼–ç å™¨
                prior_tokenizer=prior_tokenizer,          # å…ˆéªŒåˆ†è¯å™¨
                prior_scheduler=prior_scheduler,          # å…ˆéªŒè°ƒåº¦å™¨
                prior_image_processor=prior_image_processor,  # å›¾åƒå¤„ç†å™¨
            )
            # åˆ›å»ºå…ˆéªŒç®¡é“ï¼Œä½¿ç”¨å¤šä¸ªå…ˆéªŒç»„ä»¶
            self.prior_pipe = KandinskyPriorPipeline(
                prior=prior_prior,                          # å…ˆéªŒæ¨¡å‹
                image_encoder=prior_image_encoder,          # å›¾åƒç¼–ç å™¨
                text_encoder=prior_text_encoder,            # æ–‡æœ¬ç¼–ç å™¨
                tokenizer=prior_tokenizer,                  # åˆ†è¯å™¨
                scheduler=prior_scheduler,                  # è°ƒåº¦å™¨
                image_processor=prior_image_processor,      # å›¾åƒå¤„ç†å™¨
            )
            # åˆ›å»ºå›¾åƒåˆ°å›¾åƒçš„ç®¡é“ï¼Œä½¿ç”¨å¤šä¸ªè§£ç ç»„ä»¶
            self.decoder_pipe = KandinskyImg2ImgPipeline(
                text_encoder=text_encoder,  # æ–‡æœ¬ç¼–ç å™¨
                tokenizer=tokenizer,        # åˆ†è¯å™¨
                unet=unet,                  # UNet æ¨¡å‹
                scheduler=scheduler,        # è°ƒåº¦å™¨
                movq=movq,                  # MOVQ ç»„ä»¶
            )
    
        # å¯ç”¨é«˜æ•ˆçš„ xformers å†…å­˜æ³¨æ„åŠ›æœºåˆ¶
        def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):
            self.decoder_pipe.enable_xformers_memory_efficient_attention(attention_op)  # åœ¨è§£ç ç®¡é“ä¸­å¯ç”¨è¯¥æœºåˆ¶
    
        # å¯ç”¨é¡ºåº CPU å¸è½½ï¼Œå‡å°‘ GPU å†…å­˜ä½¿ç”¨
        def enable_sequential_cpu_offload(self, gpu_id=0):
            r"""  # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°è¯¥æ–¹æ³•çš„åŠŸèƒ½
            Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,
            text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a
            `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.
            Note that offloading happens on a submodule basis. Memory savings are higher than with
            `enable_model_cpu_offload`, but performance is lower.
            """
            # å¯ç”¨å…ˆéªŒç®¡é“çš„é¡ºåº CPU å¸è½½
            self.prior_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id)
            # å¯ç”¨è§£ç ç®¡é“çš„é¡ºåº CPU å¸è½½
            self.decoder_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id)
    
        # æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œç›‘æ§å¤„ç†è¿‡ç¨‹
        def progress_bar(self, iterable=None, total=None):
            self.prior_pipe.progress_bar(iterable=iterable, total=total)  # åœ¨å…ˆéªŒç®¡é“ä¸­æ˜¾ç¤ºè¿›åº¦æ¡
            self.decoder_pipe.progress_bar(iterable=iterable, total=total)  # åœ¨è§£ç ç®¡é“ä¸­æ˜¾ç¤ºè¿›åº¦æ¡
            self.decoder_pipe.enable_model_cpu_offload()  # å¯ç”¨è§£ç ç®¡é“çš„æ¨¡å‹ CPU å¸è½½
    
        # è®¾ç½®è¿›åº¦æ¡çš„é…ç½®
        def set_progress_bar_config(self, **kwargs):
            self.prior_pipe.set_progress_bar_config(**kwargs)  # è®¾ç½®å…ˆéªŒç®¡é“çš„è¿›åº¦æ¡é…ç½®
            self.decoder_pipe.set_progress_bar_config(**kwargs)  # è®¾ç½®è§£ç ç®¡é“çš„è¿›åº¦æ¡é…ç½®
    
        @torch.no_grad()  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
        @replace_example_docstring(IMAGE2IMAGE_EXAMPLE_DOC_STRING)  # æ›¿æ¢ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²
    # å®šä¹‰å¯è°ƒç”¨æ–¹æ³•ï¼Œå…è®¸å®ä¾‹å¯¹è±¡åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨
        def __call__(
            self,
            # è¾“å…¥æç¤ºï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            prompt: Union[str, List[str]],
            # è¾“å…¥å›¾åƒï¼Œå¯ä»¥æ˜¯å¼ é‡ã€PIL å›¾åƒæˆ–å®ƒä»¬çš„åˆ—è¡¨
            image: Union[torch.Tensor, PIL.Image.Image, List[torch.Tensor], List[PIL.Image.Image]],
            # å¯é€‰çš„è´Ÿæç¤ºï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            negative_prompt: Optional[Union[str, List[str]]] = None,
            # æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 100
            num_inference_steps: int = 100,
            # æŒ‡å¯¼æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 4.0ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆå†…å®¹çš„è‡ªç”±åº¦
            guidance_scale: float = 4.0,
            # æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ï¼Œé»˜è®¤ä¸º 1
            num_images_per_prompt: int = 1,
            # å¼ºåº¦å‚æ•°ï¼Œé»˜è®¤ä¸º 0.3ï¼Œæ§åˆ¶è¾“å…¥å›¾åƒçš„å½±å“ç¨‹åº¦
            strength: float = 0.3,
            # è¾“å‡ºå›¾åƒçš„é«˜åº¦ï¼Œé»˜è®¤ä¸º 512 åƒç´ 
            height: int = 512,
            # è¾“å‡ºå›¾åƒçš„å®½åº¦ï¼Œé»˜è®¤ä¸º 512 åƒç´ 
            width: int = 512,
            # å…ˆå‰æŒ‡å¯¼æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 4.0ï¼Œç”¨äºå…ˆå‰ç”Ÿæˆæ­¥éª¤çš„æ§åˆ¶
            prior_guidance_scale: float = 4.0,
            # å…ˆå‰æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 25
            prior_num_inference_steps: int = 25,
            # éšæœºæ•°ç”Ÿæˆå™¨ï¼Œå¯ä»¥æ˜¯å•ä¸ªç”Ÿæˆå™¨æˆ–ç”Ÿæˆå™¨åˆ—è¡¨
            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
            # å¯é€‰çš„æ½œåœ¨å¼ é‡ï¼Œç”¨äºä¼ é€’é¢„å®šä¹‰çš„æ½œåœ¨ç©ºé—´
            latents: Optional[torch.Tensor] = None,
            # è¾“å‡ºç±»å‹ï¼Œé»˜è®¤ä¸º "pil"ï¼ŒæŒ‡å®šè¿”å›çš„å›¾åƒæ ¼å¼
            output_type: Optional[str] = "pil",
            # å¯é€‰çš„å›è°ƒå‡½æ•°ï¼Œåœ¨æ¯ä¸ªæ­¥éª¤è°ƒç”¨ï¼Œæ¥å—æ­¥éª¤ä¿¡æ¯å’Œå½“å‰ç”Ÿæˆçš„å¼ é‡
            callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,
            # å›è°ƒå‡½æ•°è°ƒç”¨çš„æ­¥éª¤é—´éš”ï¼Œé»˜è®¤ä¸º 1
            callback_steps: int = 1,
            # è¿”å›ç»“æœçš„ç±»å‹ï¼Œé»˜è®¤ä¸º Trueï¼Œè¡¨ç¤ºè¿”å›å­—å…¸æ ¼å¼
            return_dict: bool = True,
# å®šä¹‰ä¸€ä¸ªåä¸º KandinskyInpaintCombinedPipeline çš„ç±»ï¼Œç»§æ‰¿è‡ª DiffusionPipeline ç±»
class KandinskyInpaintCombinedPipeline(DiffusionPipeline):
    """
    Combined Pipeline for generation using Kandinsky

    è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [`DiffusionPipeline`]ã€‚è¯·æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•
    ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

    å‚æ•°ï¼š
        text_encoder ([`MultilingualCLIP`]):
            å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚
        tokenizer ([`XLMRobertaTokenizer`]):
            ä»¤ç‰ŒåŒ–å™¨ç±»ã€‚
        scheduler (Union[`DDIMScheduler`,`DDPMScheduler`]):
            ç”¨äºä¸ `unet` ç»“åˆç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚
        unet ([`UNet2DConditionModel`]):
            ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶ U-Net æ¶æ„ã€‚
        movq ([`VQModel`]):
            ç”¨äºä»æ½œå˜é‡ç”Ÿæˆå›¾åƒçš„ MoVQ è§£ç å™¨ã€‚
        prior_prior ([`PriorTransformer`]):
            è¿‘ä¼¼æ–‡æœ¬åµŒå…¥çš„å›¾åƒåµŒå…¥çš„å…¸å‹ unCLIP å…ˆéªŒã€‚
        prior_image_encoder ([`CLIPVisionModelWithProjection`]):
            å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚
        prior_text_encoder ([`CLIPTextModelWithProjection`]):
            å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚
        prior_tokenizer (`CLIPTokenizer`):
             ä»¤ç‰ŒåŒ–å™¨ç±»
             [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)ã€‚
        prior_scheduler ([`UnCLIPScheduler`]):
            ç”¨äºä¸ `prior` ç»“åˆç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚
    """

    # æŒ‡ç¤ºåŠ è½½è¿æ¥çš„ç®¡é“
    _load_connected_pipes = True
    # å®šä¹‰æ¨¡å‹ CPU å¸è½½çš„é¡ºåº
    model_cpu_offload_seq = "prior_text_encoder->prior_image_encoder->prior_prior->text_encoder->unet->movq"
    # æŒ‡å®šä» CPU å¸è½½æ—¶è¦æ’é™¤çš„éƒ¨åˆ†
    _exclude_from_cpu_offload = ["prior_prior"]

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œç”¨äºè®¾ç½®ç±»çš„å±æ€§
    def __init__(
        # æ–‡æœ¬ç¼–ç å™¨ï¼Œç±»å‹ä¸º MultilingualCLIP
        self,
        text_encoder: MultilingualCLIP,
        # ä»¤ç‰ŒåŒ–å™¨ï¼Œç±»å‹ä¸º XLMRobertaTokenizer
        tokenizer: XLMRobertaTokenizer,
        # æ¡ä»¶ U-Netï¼Œç±»å‹ä¸º UNet2DConditionModel
        unet: UNet2DConditionModel,
        # è°ƒåº¦å™¨ï¼Œç±»å‹ä¸º DDIMScheduler æˆ– DDPMScheduler
        scheduler: Union[DDIMScheduler, DDPMScheduler],
        # MoVQ è§£ç å™¨ï¼Œç±»å‹ä¸º VQModel
        movq: VQModel,
        # å…ˆéªŒè½¬æ¢å™¨ï¼Œç±»å‹ä¸º PriorTransformer
        prior_prior: PriorTransformer,
        # å†»ç»“çš„å›¾åƒç¼–ç å™¨ï¼Œç±»å‹ä¸º CLIPVisionModelWithProjection
        prior_image_encoder: CLIPVisionModelWithProjection,
        # å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç±»å‹ä¸º CLIPTextModelWithProjection
        prior_text_encoder: CLIPTextModelWithProjection,
        # å…ˆéªŒä»¤ç‰ŒåŒ–å™¨ï¼Œç±»å‹ä¸º CLIPTokenizer
        prior_tokenizer: CLIPTokenizer,
        # å…ˆéªŒè°ƒåº¦å™¨ï¼Œç±»å‹ä¸º UnCLIPScheduler
        prior_scheduler: UnCLIPScheduler,
        # å›¾åƒå¤„ç†å™¨ï¼Œç±»å‹ä¸º CLIPImageProcessor
        prior_image_processor: CLIPImageProcessor,
    ):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()

        # æ³¨å†Œå¤šä¸ªæ¨¡å—åŠå…¶å¯¹åº”çš„å‚æ•°
        self.register_modules(
            # æ–‡æœ¬ç¼–ç å™¨æ¨¡å—
            text_encoder=text_encoder,
            # åˆ†è¯å™¨æ¨¡å—
            tokenizer=tokenizer,
            # UNetæ¨¡å—
            unet=unet,
            # è°ƒåº¦å™¨æ¨¡å—
            scheduler=scheduler,
            # ç§»åŠ¨è´¨é‡æ¨¡å—
            movq=movq,
            # å…ˆéªŒæ¨¡å—
            prior_prior=prior_prior,
            # å…ˆéªŒå›¾åƒç¼–ç å™¨
            prior_image_encoder=prior_image_encoder,
            # å…ˆéªŒæ–‡æœ¬ç¼–ç å™¨
            prior_text_encoder=prior_text_encoder,
            # å…ˆéªŒåˆ†è¯å™¨
            prior_tokenizer=prior_tokenizer,
            # å…ˆéªŒè°ƒåº¦å™¨
            prior_scheduler=prior_scheduler,
            # å…ˆéªŒå›¾åƒå¤„ç†å™¨
            prior_image_processor=prior_image_processor,
        )
        # åˆå§‹åŒ–å…ˆéªŒç®¡é“ï¼Œå°è£…å¤šä¸ªæ¨¡å—
        self.prior_pipe = KandinskyPriorPipeline(
            # ä¼ å…¥å…ˆéªŒæ¨¡å—
            prior=prior_prior,
            # ä¼ å…¥å›¾åƒç¼–ç å™¨
            image_encoder=prior_image_encoder,
            # ä¼ å…¥æ–‡æœ¬ç¼–ç å™¨
            text_encoder=prior_text_encoder,
            # ä¼ å…¥åˆ†è¯å™¨
            tokenizer=prior_tokenizer,
            # ä¼ å…¥è°ƒåº¦å™¨
            scheduler=prior_scheduler,
            # ä¼ å…¥å›¾åƒå¤„ç†å™¨
            image_processor=prior_image_processor,
        )
        # åˆå§‹åŒ–è§£ç ç®¡é“ï¼Œå°è£…å¤šä¸ªæ¨¡å—
        self.decoder_pipe = KandinskyInpaintPipeline(
            # ä¼ å…¥æ–‡æœ¬ç¼–ç å™¨
            text_encoder=text_encoder,
            # ä¼ å…¥åˆ†è¯å™¨
            tokenizer=tokenizer,
            # ä¼ å…¥ UNet æ¨¡å—
            unet=unet,
            # ä¼ å…¥è°ƒåº¦å™¨
            scheduler=scheduler,
            # ä¼ å…¥ç§»åŠ¨è´¨é‡æ¨¡å—
            movq=movq,
        )

    # å¯ç”¨ xformers çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
    def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):
        # è°ƒç”¨è§£ç ç®¡é“å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
        self.decoder_pipe.enable_xformers_memory_efficient_attention(attention_op)

    # å¯ç”¨é¡ºåº CPU ç¦»çº¿å¤„ç†
    def enable_sequential_cpu_offload(self, gpu_id=0):
        r"""
        å°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ° CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è°ƒç”¨æ—¶ï¼Œunetã€
        text_encoderã€vae å’Œå®‰å…¨æ£€æŸ¥å™¨çš„çŠ¶æ€å­—å…¸ä¿å­˜åˆ° CPUï¼Œç„¶åè½¬ç§»åˆ°
        `torch.device('meta')ï¼Œä»…åœ¨å…¶ç‰¹å®šå­æ¨¡å—çš„ `forward` æ–¹æ³•è¢«è°ƒç”¨æ—¶åŠ è½½åˆ° GPUã€‚
        æ³¨æ„ï¼Œç¦»çº¿å¤„ç†æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚ç›¸æ¯”äº
        `enable_model_cpu_offload`ï¼Œå†…å­˜èŠ‚çœæ›´é«˜ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
        """
        # å¯ç”¨å…ˆéªŒç®¡é“çš„é¡ºåº CPU ç¦»çº¿å¤„ç†
        self.prior_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id)
        # å¯ç”¨è§£ç ç®¡é“çš„é¡ºåº CPU ç¦»çº¿å¤„ç†
        self.decoder_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id)

    # æ˜¾ç¤ºè¿›åº¦æ¡
    def progress_bar(self, iterable=None, total=None):
        # åœ¨å…ˆéªŒç®¡é“ä¸­æ˜¾ç¤ºè¿›åº¦æ¡
        self.prior_pipe.progress_bar(iterable=iterable, total=total)
        # åœ¨è§£ç ç®¡é“ä¸­æ˜¾ç¤ºè¿›åº¦æ¡
        self.decoder_pipe.progress_bar(iterable=iterable, total=total)
        # å¯ç”¨è§£ç ç®¡é“çš„æ¨¡å‹ CPU ç¦»çº¿å¤„ç†
        self.decoder_pipe.enable_model_cpu_offload()

    # è®¾ç½®è¿›åº¦æ¡é…ç½®
    def set_progress_bar_config(self, **kwargs):
        # åœ¨å…ˆéªŒç®¡é“ä¸­è®¾ç½®è¿›åº¦æ¡é…ç½®
        self.prior_pipe.set_progress_bar_config(**kwargs)
        # åœ¨è§£ç ç®¡é“ä¸­è®¾ç½®è¿›åº¦æ¡é…ç½®
        self.decoder_pipe.set_progress_bar_config(**kwargs)

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜
    @torch.no_grad()
    # æ›¿æ¢ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²
    @replace_example_docstring(INPAINT_EXAMPLE_DOC_STRING)
    # å®šä¹‰ä¸€ä¸ªå¯è°ƒç”¨çš„ç±»æ–¹æ³•ï¼Œæ¥å—å¤šä¸ªå‚æ•°ä»¥ç”Ÿæˆå›¾åƒ
    def __call__(
        self,
        # è¾“å…¥æç¤ºï¼Œå¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
        prompt: Union[str, List[str]],
        # è¾“å…¥å›¾åƒï¼Œå¯ä»¥æ˜¯å¼ é‡ã€PIL å›¾åƒæˆ–å®ƒä»¬çš„åˆ—è¡¨
        image: Union[torch.Tensor, PIL.Image.Image, List[torch.Tensor], List[PIL.Image.Image]],
        # é®ç½©å›¾åƒï¼Œç”¨äºæŒ‡å®šå›¾åƒçš„å“ªäº›éƒ¨åˆ†å°†è¢«å¤„ç†ï¼Œå¯ä»¥æ˜¯å¼ é‡ã€PIL å›¾åƒæˆ–å®ƒä»¬çš„åˆ—è¡¨
        mask_image: Union[torch.Tensor, PIL.Image.Image, List[torch.Tensor], List[PIL.Image.Image]],
        # å¯é€‰çš„è´Ÿå‘æç¤ºï¼ŒæŒ‡å®šä¸å¸Œæœ›ç”Ÿæˆçš„å†…å®¹ï¼Œå¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
        negative_prompt: Optional[Union[str, List[str]]] = None,
        # æ¨ç†çš„æ­¥æ•°ï¼Œæ§åˆ¶ç”Ÿæˆå›¾åƒçš„ç»†è‡´ç¨‹åº¦ï¼Œé»˜è®¤ä¸º 100
        num_inference_steps: int = 100,
        # æŒ‡å¯¼å°ºåº¦ï¼Œå½±å“ç”Ÿæˆå›¾åƒä¸æç¤ºä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œé»˜è®¤ä¸º 4.0
        guidance_scale: float = 4.0,
        # æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ï¼Œé»˜è®¤ä¸º 1
        num_images_per_prompt: int = 1,
        # ç”Ÿæˆå›¾åƒçš„é«˜åº¦ï¼Œé»˜è®¤ä¸º 512 åƒç´ 
        height: int = 512,
        # ç”Ÿæˆå›¾åƒçš„å®½åº¦ï¼Œé»˜è®¤ä¸º 512 åƒç´ 
        width: int = 512,
        # å…ˆå‰å¼•å¯¼å°ºåº¦ï¼Œç”¨äºæ§åˆ¶å…ˆå‰ä¿¡æ¯çš„å½±å“ï¼Œé»˜è®¤ä¸º 4.0
        prior_guidance_scale: float = 4.0,
        # å…ˆå‰æ¨ç†çš„æ­¥æ•°ï¼Œé»˜è®¤ä¸º 25
        prior_num_inference_steps: int = 25,
        # å¯é€‰çš„ç”Ÿæˆå™¨ï¼Œç”¨äºæ§åˆ¶éšæœºæ€§ï¼Œå¯ä»¥æ˜¯å•ä¸ªç”Ÿæˆå™¨æˆ–ç”Ÿæˆå™¨åˆ—è¡¨
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        # å¯é€‰çš„æ½œåœ¨å¼ é‡ï¼Œç”¨äºæŒ‡å®šåˆå§‹æ½œåœ¨ç©ºé—´ï¼Œé»˜è®¤ä¸º None
        latents: Optional[torch.Tensor] = None,
        # è¾“å‡ºç±»å‹ï¼ŒæŒ‡å®šç”Ÿæˆå›¾åƒçš„æ ¼å¼ï¼Œé»˜è®¤ä¸º "pil"
        output_type: Optional[str] = "pil",
        # å¯é€‰çš„å›è°ƒå‡½æ•°ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è°ƒç”¨ï¼Œæ¥æ”¶æ­¥æ•°å’Œç”Ÿæˆçš„å¼ é‡
        callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,
        # å›è°ƒå‡½æ•°è°ƒç”¨çš„æ­¥æ•°é—´éš”ï¼Œé»˜è®¤ä¸º 1
        callback_steps: int = 1,
        # æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„ç»“æœï¼Œé»˜è®¤ä¸º True
        return_dict: bool = True,
```