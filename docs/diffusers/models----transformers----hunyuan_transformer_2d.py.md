# `.\diffusers\models\transformers\hunyuan_transformer_2d.py`

```
# ç‰ˆæƒæ‰€æœ‰ 2024 HunyuanDiT ä½œè€…ï¼ŒQixun Wang å’Œ HuggingFace å›¢é˜Ÿã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚
#
# æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰è¿›è¡Œè®¸å¯ï¼›
# é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚
# æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶å‡æŒ‰ "åŸæ ·" åŸºç¡€æä¾›ï¼Œ
# ä¸æä¾›ä»»ä½•ç§ç±»çš„ä¿è¯æˆ–æ¡ä»¶ï¼Œæ— è®ºæ˜¯æ˜ç¤ºæˆ–æš—ç¤ºçš„ã€‚
# æœ‰å…³è®¸å¯è¯çš„å…·ä½“æ¡æ¬¾å’Œæ¡ä»¶ï¼Œè¯·å‚é˜…è®¸å¯è¯ã€‚
from typing import Dict, Optional, Union  # å¯¼å…¥å­—å…¸ã€å¯é€‰å’Œè”åˆç±»å‹å®šä¹‰

import torch  # å¯¼å…¥ PyTorch åº“
from torch import nn  # ä» PyTorch å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—

from ...configuration_utils import ConfigMixin, register_to_config  # ä»é…ç½®å·¥å…·å¯¼å…¥æ··åˆç±»å’Œæ³¨å†ŒåŠŸèƒ½
from ...utils import logging  # ä»å·¥å…·åŒ…å¯¼å…¥æ—¥å¿—è®°å½•åŠŸèƒ½
from ...utils.torch_utils import maybe_allow_in_graph  # å¯¼å…¥å¯èƒ½å…è®¸å›¾å½¢å†…æ“ä½œçš„åŠŸèƒ½
from ..attention import FeedForward  # ä»æ³¨æ„åŠ›æ¨¡å—å¯¼å…¥å‰é¦ˆç½‘ç»œ
from ..attention_processor import Attention, AttentionProcessor, FusedHunyuanAttnProcessor2_0, HunyuanAttnProcessor2_0  # å¯¼å…¥æ³¨æ„åŠ›å¤„ç†å™¨
from ..embeddings import (  # å¯¼å…¥åµŒå…¥æ¨¡å—
    HunyuanCombinedTimestepTextSizeStyleEmbedding,  # ç»„åˆæ—¶é—´æ­¥ã€æ–‡æœ¬ã€å¤§å°å’Œæ ·å¼çš„åµŒå…¥
    PatchEmbed,  # å›¾åƒè¡¥ä¸åµŒå…¥
    PixArtAlphaTextProjection,  # åƒç´ è‰ºæœ¯æ–‡æœ¬æŠ•å½±
)
from ..modeling_outputs import Transformer2DModelOutput  # å¯¼å…¥ 2D å˜æ¢å™¨æ¨¡å‹è¾“å‡ºç±»å‹
from ..modeling_utils import ModelMixin  # å¯¼å…¥æ¨¡å‹æ··åˆç±»
from ..normalization import AdaLayerNormContinuous, FP32LayerNorm  # å¯¼å…¥è‡ªé€‚åº”å±‚å½’ä¸€åŒ–å’Œ FP32 å±‚å½’ä¸€åŒ–

logger = logging.get_logger(__name__)  # åˆ›å»ºå½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨ï¼Œç¦ç”¨ pylint è­¦å‘Š

class AdaLayerNormShift(nn.Module):  # å®šä¹‰è‡ªé€‚åº”å±‚å½’ä¸€åŒ–åç§»ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
    r"""  # ç±»æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°ç±»çš„åŠŸèƒ½
    Norm layer modified to incorporate timestep embeddings.  # å½’ä¸€åŒ–å±‚ï¼Œä¿®æ”¹ä»¥åŒ…å«æ—¶é—´æ­¥åµŒå…¥

    Parameters:  # å‚æ•°è¯´æ˜
        embedding_dim (`int`): The size of each embedding vector.  # åµŒå…¥å‘é‡çš„å¤§å°
        num_embeddings (`int`): The size of the embeddings dictionary.  # åµŒå…¥å­—å…¸çš„å¤§å°
    """

    def __init__(self, embedding_dim: int, elementwise_affine=True, eps=1e-6):  # åˆå§‹åŒ–æ–¹æ³•
        super().__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
        self.silu = nn.SiLU()  # å®šä¹‰ SiLU æ¿€æ´»å‡½æ•°
        self.linear = nn.Linear(embedding_dim, embedding_dim)  # å®šä¹‰çº¿æ€§å±‚ï¼Œè¾“å…¥è¾“å‡ºç»´åº¦å‡ä¸ºåµŒå…¥ç»´åº¦
        self.norm = FP32LayerNorm(embedding_dim, elementwise_affine=elementwise_affine, eps=eps)  # å®šä¹‰å±‚å½’ä¸€åŒ–

    def forward(self, x: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:  # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•
        shift = self.linear(self.silu(emb.to(torch.float32)).to(emb.dtype))  # è®¡ç®—åç§»é‡
        x = self.norm(x) + shift.unsqueeze(dim=1)  # å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–å¹¶åŠ ä¸Šåç§»
        return x  # è¿”å›å¤„ç†åçš„å¼ é‡


@maybe_allow_in_graph  # è£…é¥°å™¨ï¼Œå¯èƒ½å…è®¸åœ¨è®¡ç®—å›¾ä¸­ä½¿ç”¨
class HunyuanDiTBlock(nn.Module):  # å®šä¹‰ Hunyuan-DiT æ¨¡å‹ä¸­çš„å˜æ¢å™¨å—ç±»
    r"""  # ç±»æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°ç±»çš„åŠŸèƒ½
    Transformer block used in Hunyuan-DiT model (https://github.com/Tencent/HunyuanDiT). Allow skip connection and  # Hunyuan-DiT æ¨¡å‹ä¸­çš„å˜æ¢å™¨å—ï¼Œå…è®¸è·³è¿‡è¿æ¥å’Œ
    QKNorm  # QKNorm åŠŸèƒ½
    # å‚æ•°è¯´æ˜éƒ¨åˆ†ï¼Œå®šä¹‰å„å‚æ•°çš„ç±»å‹å’Œä½œç”¨
        Parameters:
            dim (`int`):  # è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°
                The number of channels in the input and output.
            num_attention_heads (`int`):  # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨çš„å¤´æ•°
                The number of heads to use for multi-head attention.
            cross_attention_dim (`int`, *optional*):  # è·¨æ³¨æ„åŠ›çš„ç¼–ç å™¨éšè—çŠ¶æ€å‘é‡çš„å¤§å°
                The size of the encoder_hidden_states vector for cross attention.
            dropout (`float`, *optional*, defaults to 0.0):  # ç”¨äºæ­£åˆ™åŒ–çš„ä¸¢å¼ƒæ¦‚ç‡
                The dropout probability to use.
            activation_fn (`str`, *optional*, defaults to `"geglu"`):  # å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°
                Activation function to be used in feed-forward.
            norm_elementwise_affine (`bool`, *optional*, defaults to `True`):  # æ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ çš„å…ƒç´ é€ä¸ªä»¿å°„å‚æ•°è¿›è¡Œå½’ä¸€åŒ–
                Whether to use learnable elementwise affine parameters for normalization.
            norm_eps (`float`, *optional*, defaults to 1e-6):  # åŠ åˆ°å½’ä¸€åŒ–å±‚åˆ†æ¯çš„å°å¸¸æ•°ï¼Œä»¥é˜²æ­¢é™¤ä»¥é›¶
                A small constant added to the denominator in normalization layers to prevent division by zero.
            final_dropout (`bool`, *optional*, defaults to False):  # åœ¨æœ€åçš„å‰é¦ˆå±‚åæ˜¯å¦åº”ç”¨æœ€ç»ˆä¸¢å¼ƒ
                Whether to apply a final dropout after the last feed-forward layer.
            ff_inner_dim (`int`, *optional*):  # å‰é¦ˆå—ä¸­éšè—å±‚çš„å¤§å°ï¼Œé»˜è®¤ä¸º None
                The size of the hidden layer in the feed-forward block. Defaults to `None`.
            ff_bias (`bool`, *optional*, defaults to `True`):  # å‰é¦ˆå—ä¸­æ˜¯å¦ä½¿ç”¨åç½®
                Whether to use bias in the feed-forward block.
            skip (`bool`, *optional*, defaults to `False`):  # æ˜¯å¦ä½¿ç”¨è·³è¿‡è¿æ¥ï¼Œé»˜è®¤ä¸ºä¸‹å—å’Œä¸­å—çš„ False
                Whether to use skip connection. Defaults to `False` for down-blocks and mid-blocks.
            qk_norm (`bool`, *optional*, defaults to `True`):  # åœ¨ QK è®¡ç®—ä¸­æ˜¯å¦ä½¿ç”¨å½’ä¸€åŒ–ï¼Œé»˜è®¤ä¸º True
                Whether to use normalization in QK calculation. Defaults to `True`.
        """
    
        # æ„é€ å‡½æ•°çš„å®šä¹‰ï¼Œåˆå§‹åŒ–å„å‚æ•°
        def __init__(
            self,
            dim: int,  # è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°
            num_attention_heads: int,  # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨çš„å¤´æ•°
            cross_attention_dim: int = 1024,  # é»˜è®¤çš„è·¨æ³¨æ„åŠ›ç»´åº¦
            dropout=0.0,  # é»˜è®¤çš„ä¸¢å¼ƒæ¦‚ç‡
            activation_fn: str = "geglu",  # é»˜è®¤çš„æ¿€æ´»å‡½æ•°
            norm_elementwise_affine: bool = True,  # é»˜è®¤ä½¿ç”¨å¯å­¦ä¹ çš„ä»¿å°„å‚æ•°
            norm_eps: float = 1e-6,  # é»˜è®¤çš„å½’ä¸€åŒ–å°å¸¸æ•°
            final_dropout: bool = False,  # é»˜è®¤ä¸åº”ç”¨æœ€ç»ˆä¸¢å¼ƒ
            ff_inner_dim: Optional[int] = None,  # é»˜è®¤çš„å‰é¦ˆå—éšè—å±‚å¤§å°
            ff_bias: bool = True,  # é»˜è®¤ä½¿ç”¨åç½®
            skip: bool = False,  # é»˜è®¤ä¸ä½¿ç”¨è·³è¿‡è¿æ¥
            qk_norm: bool = True,  # é»˜è®¤åœ¨ QK è®¡ç®—ä¸­ä½¿ç”¨å½’ä¸€åŒ–
    ):
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__()

        # å®šä¹‰ä¸‰ä¸ªå—ï¼Œæ¯ä¸ªå—éƒ½æœ‰è‡ªå·±çš„å½’ä¸€åŒ–å±‚ã€‚
        # æ³¨æ„ï¼šæ–°ç‰ˆæœ¬å‘å¸ƒæ—¶ï¼Œæ£€æŸ¥ norm2 å’Œ norm3
        # 1. è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.norm1 = AdaLayerNormShift(dim, elementwise_affine=norm_elementwise_affine, eps=norm_eps)

        # åˆ›å»ºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å®ä¾‹
        self.attn1 = Attention(
            query_dim=dim,  # æŸ¥è¯¢å‘é‡çš„ç»´åº¦
            cross_attention_dim=None,  # äº¤å‰æ³¨æ„åŠ›çš„ç»´åº¦ï¼Œæœªä½¿ç”¨
            dim_head=dim // num_attention_heads,  # æ¯ä¸ªå¤´çš„ç»´åº¦
            heads=num_attention_heads,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡
            qk_norm="layer_norm" if qk_norm else None,  # æŸ¥è¯¢å’Œé”®çš„å½’ä¸€åŒ–æ–¹æ³•
            eps=1e-6,  # æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
            bias=True,  # æ˜¯å¦ä½¿ç”¨åç½®
            processor=HunyuanAttnProcessor2_0(),  # æ³¨æ„åŠ›å¤„ç†å™¨çš„å®ä¾‹
        )

        # 2. äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        self.norm2 = FP32LayerNorm(dim, norm_eps, norm_elementwise_affine)

        # åˆ›å»ºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„å®ä¾‹
        self.attn2 = Attention(
            query_dim=dim,  # æŸ¥è¯¢å‘é‡çš„ç»´åº¦
            cross_attention_dim=cross_attention_dim,  # äº¤å‰æ³¨æ„åŠ›çš„ç»´åº¦
            dim_head=dim // num_attention_heads,  # æ¯ä¸ªå¤´çš„ç»´åº¦
            heads=num_attention_heads,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡
            qk_norm="layer_norm" if qk_norm else None,  # æŸ¥è¯¢å’Œé”®çš„å½’ä¸€åŒ–æ–¹æ³•
            eps=1e-6,  # æ•°å€¼ç¨³å®šæ€§å¸¸æ•°
            bias=True,  # æ˜¯å¦ä½¿ç”¨åç½®
            processor=HunyuanAttnProcessor2_0(),  # æ³¨æ„åŠ›å¤„ç†å™¨çš„å®ä¾‹
        )
        # 3. å‰é¦ˆç½‘ç»œ
        self.norm3 = FP32LayerNorm(dim, norm_eps, norm_elementwise_affine)

        # åˆ›å»ºå‰é¦ˆç½‘ç»œçš„å®ä¾‹
        self.ff = FeedForward(
            dim,  # è¾“å…¥ç»´åº¦
            dropout=dropout,  # dropout æ¯”ä¾‹
            activation_fn=activation_fn,  # æ¿€æ´»å‡½æ•°
            final_dropout=final_dropout,  # æœ€ç»ˆ dropout æ¯”ä¾‹
            inner_dim=ff_inner_dim,  # å†…éƒ¨ç»´åº¦ï¼Œé€šå¸¸æ˜¯ dim çš„å€æ•°
            bias=ff_bias,  # æ˜¯å¦ä½¿ç”¨åç½®
        )

        # 4. è·³è·ƒè¿æ¥
        if skip:  # å¦‚æœå¯ç”¨è·³è·ƒè¿æ¥
            self.skip_norm = FP32LayerNorm(2 * dim, norm_eps, elementwise_affine=True)  # åˆ›å»ºå½’ä¸€åŒ–å±‚
            self.skip_linear = nn.Linear(2 * dim, dim)  # åˆ›å»ºçº¿æ€§å±‚
        else:  # å¦‚æœä¸å¯ç”¨è·³è·ƒè¿æ¥
            self.skip_linear = None  # è®¾ç½®ä¸º None

        # å°†å—å¤§å°é»˜è®¤ä¸º None
        self._chunk_size = None  # åˆå§‹åŒ–å—å¤§å°
        self._chunk_dim = 0  # åˆå§‹åŒ–å—ç»´åº¦

    # ä» diffusers.models.attention.BasicTransformerBlock å¤åˆ¶çš„è®¾ç½®å—å‰é¦ˆæ–¹æ³•
    def set_chunk_feed_forward(self, chunk_size: Optional[int], dim: int = 0):
        # è®¾ç½®å—å‰é¦ˆ
        self._chunk_size = chunk_size  # è®¾ç½®å—å¤§å°
        self._chunk_dim = dim  # è®¾ç½®å—ç»´åº¦

    def forward(
        self,
        hidden_states: torch.Tensor,  # è¾“å…¥çš„éšè—çŠ¶æ€
        encoder_hidden_states: Optional[torch.Tensor] = None,  # ç¼–ç å™¨çš„éšè—çŠ¶æ€
        temb: Optional[torch.Tensor] = None,  # é¢å¤–çš„åµŒå…¥
        image_rotary_emb=None,  # å›¾åƒæ—‹è½¬åµŒå…¥
        skip=None,  # è·³è·ƒè¿æ¥æ ‡å¿—
    ) -> torch.Tensor:
        # æ³¨æ„ï¼šä»¥ä¸‹ä»£ç å—ä¸­çš„è®¡ç®—æ€»æ˜¯åœ¨å½’ä¸€åŒ–ä¹‹åè¿›è¡Œã€‚
        # 0. é•¿è·³è·ƒè¿æ¥
        # å¦‚æœ skip_linear ä¸ä¸º Noneï¼Œæ‰§è¡Œè·³è·ƒè¿æ¥
        if self.skip_linear is not None:
            # å°†å½“å‰çš„éšè—çŠ¶æ€ä¸è·³è·ƒè¿æ¥çš„è¾“å‡ºåœ¨æœ€åä¸€ç»´ä¸Šæ‹¼æ¥
            cat = torch.cat([hidden_states, skip], dim=-1)
            # å¯¹æ‹¼æ¥åçš„ç»“æœè¿›è¡Œå½’ä¸€åŒ–å¤„ç†
            cat = self.skip_norm(cat)
            # é€šè¿‡çº¿æ€§å±‚å¤„ç†å½’ä¸€åŒ–åçš„ç»“æœï¼Œæ›´æ–°éšè—çŠ¶æ€
            hidden_states = self.skip_linear(cat)

        # 1. è‡ªæ³¨æ„åŠ›
        # å¯¹å½“å‰éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ï¼Œå‡†å¤‡è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—
        norm_hidden_states = self.norm1(hidden_states, temb)  ### checked: self.norm1 is correct
        # è®¡ç®—è‡ªæ³¨æ„åŠ›çš„è¾“å‡º
        attn_output = self.attn1(
            norm_hidden_states,
            image_rotary_emb=image_rotary_emb,
        )
        # å°†è‡ªæ³¨æ„åŠ›çš„è¾“å‡ºåŠ åˆ°éšè—çŠ¶æ€ä¸Šï¼Œå½¢æˆæ–°çš„éšè—çŠ¶æ€
        hidden_states = hidden_states + attn_output

        # 2. äº¤å‰æ³¨æ„åŠ›
        # å°†äº¤å‰æ³¨æ„åŠ›çš„è¾“å‡ºåŠ åˆ°å½“å‰çš„éšè—çŠ¶æ€ä¸Š
        hidden_states = hidden_states + self.attn2(
            self.norm2(hidden_states),  # å…ˆè¿›è¡Œå½’ä¸€åŒ–
            encoder_hidden_states=encoder_hidden_states,  # ä½¿ç”¨ç¼–ç å™¨çš„éšè—çŠ¶æ€
            image_rotary_emb=image_rotary_emb,  # ä¼ é€’æ—‹è½¬åµŒå…¥
        )

        # å‰é¦ˆç½‘ç»œå±‚ ### TODO: åœ¨çŠ¶æ€å­—å…¸ä¸­åˆ‡æ¢ norm2 å’Œ norm3
        # å¯¹å½“å‰çš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå‡†å¤‡è¿›å…¥å‰é¦ˆç½‘ç»œ
        mlp_inputs = self.norm3(hidden_states)
        # é€šè¿‡å‰é¦ˆç½‘ç»œå¤„ç†å½’ä¸€åŒ–åçš„è¾“å…¥ï¼Œæ›´æ–°éšè—çŠ¶æ€
        hidden_states = hidden_states + self.ff(mlp_inputs)

        # è¿”å›æœ€ç»ˆçš„éšè—çŠ¶æ€
        return hidden_states
# å®šä¹‰ HunyuanDiT2DModel ç±»ï¼Œç»§æ‰¿è‡ª ModelMixin å’Œ ConfigMixin
class HunyuanDiT2DModel(ModelMixin, ConfigMixin):
    """
    HunYuanDiT: åŸºäº Transformer çš„æ‰©æ•£æ¨¡å‹ã€‚

    ç»§æ‰¿ ModelMixin å’Œ ConfigMixin ä»¥ä¸ diffusers çš„é‡‡æ ·å™¨ StableDiffusionPipeline å…¼å®¹ã€‚

    å‚æ•°:
        num_attention_heads (`int`, *å¯é€‰*, é»˜è®¤ä¸º 16):
            å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°ã€‚
        attention_head_dim (`int`, *å¯é€‰*, é»˜è®¤ä¸º 88):
            æ¯ä¸ªå¤´çš„é€šé“æ•°ã€‚
        in_channels (`int`, *å¯é€‰*):
            è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°ï¼ˆå¦‚æœè¾“å…¥ä¸º **è¿ç»­**ï¼Œéœ€æŒ‡å®šï¼‰ã€‚
        patch_size (`int`, *å¯é€‰*):
            è¾“å…¥çš„è¡¥ä¸å¤§å°ã€‚
        activation_fn (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"geglu"`):
            å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚
        sample_size (`int`, *å¯é€‰*):
            æ½œåœ¨å›¾åƒçš„å®½åº¦ã€‚è®­ç»ƒæœŸé—´å›ºå®šä½¿ç”¨ï¼Œä»¥å­¦ä¹ ä½ç½®åµŒå…¥çš„æ•°é‡ã€‚
        dropout (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0):
            ä½¿ç”¨çš„ dropout æ¦‚ç‡ã€‚
        cross_attention_dim (`int`, *å¯é€‰*):
            clip æ–‡æœ¬åµŒå…¥ä¸­çš„ç»´åº¦æ•°é‡ã€‚
        hidden_size (`int`, *å¯é€‰*):
            æ¡ä»¶åµŒå…¥å±‚ä¸­éšè—å±‚çš„å¤§å°ã€‚
        num_layers (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1):
            ä½¿ç”¨çš„ Transformer å—çš„å±‚æ•°ã€‚
        mlp_ratio (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0):
            éšè—å±‚å¤§å°ä¸è¾“å…¥å¤§å°çš„æ¯”ç‡ã€‚
        learn_sigma (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`):
             æ˜¯å¦é¢„æµ‹æ–¹å·®ã€‚
        cross_attention_dim_t5 (`int`, *å¯é€‰*):
            t5 æ–‡æœ¬åµŒå…¥ä¸­çš„ç»´åº¦æ•°é‡ã€‚
        pooled_projection_dim (`int`, *å¯é€‰*):
            æ± åŒ–æŠ•å½±çš„å¤§å°ã€‚
        text_len (`int`, *å¯é€‰*):
            clip æ–‡æœ¬åµŒå…¥çš„é•¿åº¦ã€‚
        text_len_t5 (`int`, *å¯é€‰*):
            T5 æ–‡æœ¬åµŒå…¥çš„é•¿åº¦ã€‚
        use_style_cond_and_image_meta_size (`bool`,  *å¯é€‰*):
            æ˜¯å¦ä½¿ç”¨é£æ ¼æ¡ä»¶å’Œå›¾åƒå…ƒæ•°æ®å¤§å°ã€‚ç‰ˆæœ¬ <=1.1 ä¸º Trueï¼Œç‰ˆæœ¬ >= 1.2 ä¸º False
    """

    # æ³¨å†Œåˆ°é…ç½®ä¸­
    @register_to_config
    def __init__(
        # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°ï¼Œé»˜è®¤ä¸º 16
        self,
        num_attention_heads: int = 16,
        # æ¯ä¸ªå¤´çš„é€šé“æ•°ï¼Œé»˜è®¤ä¸º 88
        attention_head_dim: int = 88,
        # è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°ï¼Œé»˜è®¤ä¸º None
        in_channels: Optional[int] = None,
        # è¾“å…¥çš„è¡¥ä¸å¤§å°ï¼Œé»˜è®¤ä¸º None
        patch_size: Optional[int] = None,
        # æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸º "gelu-approximate"
        activation_fn: str = "gelu-approximate",
        # æ½œåœ¨å›¾åƒçš„å®½åº¦ï¼Œé»˜è®¤ä¸º 32
        sample_size=32,
        # æ¡ä»¶åµŒå…¥å±‚ä¸­éšè—å±‚çš„å¤§å°ï¼Œé»˜è®¤ä¸º 1152
        hidden_size=1152,
        # ä½¿ç”¨çš„ Transformer å—çš„å±‚æ•°ï¼Œé»˜è®¤ä¸º 28
        num_layers: int = 28,
        # éšè—å±‚å¤§å°ä¸è¾“å…¥å¤§å°çš„æ¯”ç‡ï¼Œé»˜è®¤ä¸º 4.0
        mlp_ratio: float = 4.0,
        # æ˜¯å¦é¢„æµ‹æ–¹å·®ï¼Œé»˜è®¤ä¸º True
        learn_sigma: bool = True,
        # clip æ–‡æœ¬åµŒå…¥ä¸­çš„ç»´åº¦æ•°é‡ï¼Œé»˜è®¤ä¸º 1024
        cross_attention_dim: int = 1024,
        # æ­£åˆ™åŒ–ç±»å‹ï¼Œé»˜è®¤ä¸º "layer_norm"
        norm_type: str = "layer_norm",
        # t5 æ–‡æœ¬åµŒå…¥ä¸­çš„ç»´åº¦æ•°é‡ï¼Œé»˜è®¤ä¸º 2048
        cross_attention_dim_t5: int = 2048,
        # æ± åŒ–æŠ•å½±çš„å¤§å°ï¼Œé»˜è®¤ä¸º 1024
        pooled_projection_dim: int = 1024,
        # clip æ–‡æœ¬åµŒå…¥çš„é•¿åº¦ï¼Œé»˜è®¤ä¸º 77
        text_len: int = 77,
        # T5 æ–‡æœ¬åµŒå…¥çš„é•¿åº¦ï¼Œé»˜è®¤ä¸º 256
        text_len_t5: int = 256,
        # æ˜¯å¦ä½¿ç”¨é£æ ¼æ¡ä»¶å’Œå›¾åƒå…ƒæ•°æ®å¤§å°ï¼Œé»˜è®¤ä¸º True
        use_style_cond_and_image_meta_size: bool = True,
    ):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super().__init__()
        # æ ¹æ®æ˜¯å¦å­¦ä¹  sigma å†³å®šè¾“å‡ºé€šé“æ•°
        self.out_channels = in_channels * 2 if learn_sigma else in_channels
        # è®¾ç½®æ³¨æ„åŠ›å¤´çš„æ•°é‡
        self.num_heads = num_attention_heads
        # è®¡ç®—å†…éƒ¨ç»´åº¦ï¼Œç­‰äºæ³¨æ„åŠ›å¤´æ•°é‡ä¹˜ä»¥æ¯ä¸ªå¤´çš„ç»´åº¦
        self.inner_dim = num_attention_heads * attention_head_dim

        # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥å™¨ï¼Œç”¨äºå°†è¾“å…¥ç‰¹å¾æŠ•å½±åˆ°æ›´é«˜ç»´ç©ºé—´
        self.text_embedder = PixArtAlphaTextProjection(
            # è¾“å…¥ç‰¹å¾ç»´åº¦
            in_features=cross_attention_dim_t5,
            # éšè—å±‚å¤§å°ä¸ºè¾“å…¥ç‰¹å¾çš„å››å€
            hidden_size=cross_attention_dim_t5 * 4,
            # è¾“å‡ºç‰¹å¾ç»´åº¦
            out_features=cross_attention_dim,
            # æ¿€æ´»å‡½æ•°è®¾ç½®ä¸º"siluf_fp32"
            act_fn="silu_fp32",
        )

        # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥çš„å¡«å……å‚æ•°ï¼Œä½¿ç”¨éšæœºæ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
        self.text_embedding_padding = nn.Parameter(
            torch.randn(text_len + text_len_t5, cross_attention_dim, dtype=torch.float32)
        )

        # åˆå§‹åŒ–ä½ç½®åµŒå…¥ï¼Œæ„å»ºå›¾åƒçš„è¡¥ä¸åµŒå…¥
        self.pos_embed = PatchEmbed(
            # è¡¥ä¸çš„é«˜åº¦
            height=sample_size,
            # è¡¥ä¸çš„å®½åº¦
            width=sample_size,
            # è¾“å…¥é€šé“æ•°
            in_channels=in_channels,
            # åµŒå…¥ç»´åº¦
            embed_dim=hidden_size,
            # è¡¥ä¸å¤§å°
            patch_size=patch_size,
            # ä½ç½®åµŒå…¥ç±»å‹è®¾ç½®ä¸º None
            pos_embed_type=None,
        )

        # åˆå§‹åŒ–æ—¶é—´å’Œé£æ ¼åµŒå…¥ï¼Œç»“åˆæ—¶é—´æ­¥å’Œæ–‡æœ¬å¤§å°
        self.time_extra_emb = HunyuanCombinedTimestepTextSizeStyleEmbedding(
            # éšè—å±‚å¤§å°
            hidden_size,
            # æ± åŒ–æŠ•å½±ç»´åº¦
            pooled_projection_dim=pooled_projection_dim,
            # è¾“å…¥åºåˆ—é•¿åº¦
            seq_len=text_len_t5,
            # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
            cross_attention_dim=cross_attention_dim_t5,
            # æ˜¯å¦ä½¿ç”¨é£æ ¼æ¡ä»¶å’Œå›¾åƒå…ƒæ•°æ®å¤§å°
            use_style_cond_and_image_meta_size=use_style_cond_and_image_meta_size,
        )

        # åˆå§‹åŒ– HunyuanDiT å—åˆ—è¡¨
        self.blocks = nn.ModuleList(
            [
                # ä¸ºæ¯ä¸€å±‚åˆ›å»º HunyuanDiTBlock
                HunyuanDiTBlock(
                    # å†…éƒ¨ç»´åº¦
                    dim=self.inner_dim,
                    # æ³¨æ„åŠ›å¤´æ•°é‡
                    num_attention_heads=self.config.num_attention_heads,
                    # æ¿€æ´»å‡½æ•°
                    activation_fn=activation_fn,
                    # å‰é¦ˆç½‘ç»œå†…éƒ¨ç»´åº¦
                    ff_inner_dim=int(self.inner_dim * mlp_ratio),
                    # äº¤å‰æ³¨æ„åŠ›ç»´åº¦
                    cross_attention_dim=cross_attention_dim,
                    # æŸ¥è¯¢-é”®å½’ä¸€åŒ–å¼€å¯
                    qk_norm=True,  # è¯¦æƒ…è§ http://arxiv.org/abs/2302.05442
                    # å¦‚æœå½“å‰å±‚æ•°å¤§äºå±‚æ•°çš„ä¸€åŠï¼Œåˆ™è·³è¿‡
                    skip=layer > num_layers // 2,
                )
                # éå†å±‚æ•°
                for layer in range(num_layers)
            ]
        )

        # åˆå§‹åŒ–è¾“å‡ºçš„è‡ªé€‚åº”å±‚å½’ä¸€åŒ–
        self.norm_out = AdaLayerNormContinuous(self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6)
        # åˆå§‹åŒ–è¾“å‡ºçš„çº¿æ€§å±‚ï¼Œå°†å†…éƒ¨ç»´åº¦æ˜ å°„åˆ°è¾“å‡ºé€šé“æ•°
        self.proj_out = nn.Linear(self.inner_dim, patch_size * patch_size * self.out_channels, bias=True)

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel ä¸­å¤åˆ¶çš„ä»£ç ï¼Œç”¨äºèåˆ QKV æŠ•å½±ï¼Œæ›´æ–°ä¸º FusedHunyuanAttnProcessor2_0
    def fuse_qkv_projections(self):
        """ 
        å¯ç”¨èåˆçš„ QKV æŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚ 
        å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚

        <Tip warning={true}>
        
        è¯¥ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚

        </Tip>
        """
        # åˆå§‹åŒ–åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨ä¸º None
        self.original_attn_processors = None

        # éå†æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨
        for _, attn_processor in self.attn_processors.items():
            # æ£€æŸ¥æ³¨æ„åŠ›å¤„ç†å™¨ç±»åä¸­æ˜¯å¦åŒ…å« "Added"
            if "Added" in str(attn_processor.__class__.__name__):
                # å¦‚æœåŒ…å«ï¼Œåˆ™æŠ›å‡ºé”™è¯¯ï¼Œè¡¨ç¤ºä¸æ”¯æŒèåˆ QKV æŠ•å½±
                raise ValueError("`fuse_qkv_projections()` is not supported for models having added KV projections.")

        # ä¿å­˜å½“å‰çš„æ³¨æ„åŠ›å¤„ç†å™¨ä»¥å¤‡åç”¨
        self.original_attn_processors = self.attn_processors

        # éå†å½“å‰æ¨¡å—ä¸­çš„æ‰€æœ‰å­æ¨¡å—
        for module in self.modules():
            # æ£€æŸ¥æ¨¡å—æ˜¯å¦ä¸º Attention ç±»å‹
            if isinstance(module, Attention):
                # å¯¹äº Attention æ¨¡å—ï¼Œå¯ç”¨æŠ•å½±èåˆ
                module.fuse_projections(fuse=True)

        # è®¾ç½®èåˆçš„æ³¨æ„åŠ›å¤„ç†å™¨
        self.set_attn_processor(FusedHunyuanAttnProcessor2_0())

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections å¤åˆ¶
    def unfuse_qkv_projections(self):
        """ 
        å¦‚æœå·²å¯ç”¨ï¼Œåˆ™ç¦ç”¨èåˆçš„ QKV æŠ•å½±ã€‚

        <Tip warning={true}>
        
        è¯¥ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚

        </Tip>

        """
        # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨
        if self.original_attn_processors is not None:
            # æ¢å¤ä¸ºåŸå§‹æ³¨æ„åŠ›å¤„ç†å™¨
            self.set_attn_processor(self.original_attn_processors)

    @property
    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors å¤åˆ¶
    def attn_processors(self) -> Dict[str, AttentionProcessor]:
        r"""
        è¿”å›:
            `dict` ç±»å‹çš„æ³¨æ„åŠ›å¤„ç†å™¨ï¼šä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¨¡å‹ä¸­ä½¿ç”¨çš„æ‰€æœ‰æ³¨æ„åŠ›å¤„ç†å™¨ï¼ŒæŒ‰æƒé‡åç§°ç´¢å¼•ã€‚
        """
        # åˆå§‹åŒ–å¤„ç†å™¨å­—å…¸
        processors = {}

        # å®šä¹‰é€’å½’æ·»åŠ å¤„ç†å™¨çš„å‡½æ•°
        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
            # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰ get_processor æ–¹æ³•
            if hasattr(module, "get_processor"):
                # å°†å¤„ç†å™¨æ·»åŠ åˆ°å­—å…¸ä¸­
                processors[f"{name}.processor"] = module.get_processor()

            # éå†å­æ¨¡å—
            for sub_name, child in module.named_children():
                # é€’å½’è°ƒç”¨ä»¥æ·»åŠ å­æ¨¡å—çš„å¤„ç†å™¨
                fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)

            return processors

        # éå†å½“å‰æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
        for name, module in self.named_children():
            # è°ƒç”¨é€’å½’å‡½æ•°æ·»åŠ å¤„ç†å™¨
            fn_recursive_add_processors(name, module, processors)

        # è¿”å›å¤„ç†å™¨å­—å…¸
        return processors

    # ä» diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor å¤åˆ¶
    # å®šä¹‰è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•ï¼Œæ¥æ”¶ä¸€ä¸ªæ³¨æ„åŠ›å¤„ç†å™¨æˆ–å¤„ç†å™¨å­—å…¸
        def set_attn_processor(self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]):
            r"""
            è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„æ³¨æ„åŠ›å¤„ç†å™¨ã€‚
    
            å‚æ•°ï¼š
                processorï¼ˆ`dict` of `AttentionProcessor` æˆ– `AttentionProcessor`ï¼‰:
                    å·²å®ä¾‹åŒ–çš„å¤„ç†å™¨ç±»æˆ–å°†ä½œä¸ºå¤„ç†å™¨è®¾ç½®çš„å¤„ç†å™¨ç±»å­—å…¸
                    ç”¨äº**æ‰€æœ‰** `Attention` å±‚ã€‚
    
                    å¦‚æœ `processor` æ˜¯å­—å…¸ï¼Œåˆ™é”®éœ€è¦å®šä¹‰ç›¸åº”çš„äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨è·¯å¾„ã€‚
                    å½“è®¾ç½®å¯è®­ç»ƒçš„æ³¨æ„åŠ›å¤„ç†å™¨æ—¶ï¼Œè¿™å¼ºçƒˆå»ºè®®ã€‚
    
            """
            # è®¡ç®—å½“å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„æ•°é‡
            count = len(self.attn_processors.keys())
    
            # æ£€æŸ¥ä¼ å…¥çš„å¤„ç†å™¨æ˜¯å¦ä¸ºå­—å…¸ï¼Œå¹¶éªŒè¯å…¶é•¿åº¦ä¸æ³¨æ„åŠ›å±‚æ•°é‡æ˜¯å¦ä¸€è‡´
            if isinstance(processor, dict) and len(processor) != count:
                raise ValueError(
                    # æŠ›å‡ºé”™è¯¯ï¼Œæç¤ºå¤„ç†å™¨æ•°é‡ä¸æ³¨æ„åŠ›å±‚æ•°é‡ä¸åŒ¹é…
                    f"A dict of processors was passed, but the number of processors {len(processor)} does not match the"
                    f" number of attention layers: {count}. Please make sure to pass {count} processor classes."
                )
    
            # å®šä¹‰é€’å½’è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„å†…éƒ¨å‡½æ•°
            def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
                # æ£€æŸ¥æ¨¡å—æ˜¯å¦å…·æœ‰è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
                if hasattr(module, "set_processor"):
                    # å¦‚æœå¤„ç†å™¨ä¸æ˜¯å­—å…¸ï¼Œç›´æ¥è®¾ç½®å¤„ç†å™¨
                    if not isinstance(processor, dict):
                        module.set_processor(processor)
                    else:
                        # ä»å­—å…¸ä¸­å–å‡ºå¤„ç†å™¨å¹¶è®¾ç½®
                        module.set_processor(processor.pop(f"{name}.processor"))
    
                # éå†æ¨¡å—çš„å­æ¨¡å—ï¼Œé€’å½’è°ƒç”¨è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
                for sub_name, child in module.named_children():
                    fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)
    
            # éå†å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—ï¼Œè°ƒç”¨é€’å½’è®¾ç½®å¤„ç†å™¨çš„æ–¹æ³•
            for name, module in self.named_children():
                fn_recursive_attn_processor(name, module, processor)
    
        # å®šä¹‰è®¾ç½®é»˜è®¤æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•
        def set_default_attn_processor(self):
            """
            ç¦ç”¨è‡ªå®šä¹‰æ³¨æ„åŠ›å¤„ç†å™¨ï¼Œå¹¶è®¾ç½®é»˜è®¤çš„æ³¨æ„åŠ›å®ç°ã€‚
            """
            # è°ƒç”¨è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨çš„æ–¹æ³•ï¼Œä½¿ç”¨é»˜è®¤çš„ HunyuanAttnProcessor2_0
            self.set_attn_processor(HunyuanAttnProcessor2_0())
    
        # å®šä¹‰å‰å‘ä¼ æ’­çš„æ–¹æ³•ï¼Œæ¥æ”¶å¤šä¸ªè¾“å…¥å‚æ•°
        def forward(
            self,
            hidden_states,
            timestep,
            encoder_hidden_states=None,
            text_embedding_mask=None,
            encoder_hidden_states_t5=None,
            text_embedding_mask_t5=None,
            image_meta_size=None,
            style=None,
            image_rotary_emb=None,
            controlnet_block_samples=None,
            return_dict=True,
        # ä» diffusers.models.unets.unet_3d_condition.UNet3DConditionModel.enable_forward_chunking å¤åˆ¶çš„ä»£ç 
    # å®šä¹‰ä¸€ä¸ªæ–¹æ³•ä»¥å¯ç”¨å‰é¦ˆå±‚çš„åˆ†å—å¤„ç†ï¼Œå‚æ•°ä¸ºåˆ†å—å¤§å°å’Œç»´åº¦
    def enable_forward_chunking(self, chunk_size: Optional[int] = None, dim: int = 0) -> None:
            """
            è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä½¿ç”¨ [å‰é¦ˆåˆ†å—å¤„ç†](https://huggingface.co/blog/reformer#2-chunked-feed-forward-layers)ã€‚
    
            å‚æ•°:
                chunk_size (`int`, *å¯é€‰*):
                    å‰é¦ˆå±‚çš„åˆ†å—å¤§å°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†å¯¹æ¯ä¸ªç»´åº¦ä¸º `dim` çš„å¼ é‡å•ç‹¬è¿è¡Œå‰é¦ˆå±‚ã€‚
                dim (`int`, *å¯é€‰*, é»˜è®¤ä¸º `0`):
                    å‰é¦ˆè®¡ç®—åº”åˆ†å—çš„ç»´åº¦ã€‚é€‰æ‹© dim=0ï¼ˆæ‰¹å¤„ç†ï¼‰æˆ– dim=1ï¼ˆåºåˆ—é•¿åº¦ï¼‰ã€‚
            """
            # æ£€æŸ¥ç»´åº¦æ˜¯å¦ä¸º 0 æˆ– 1
            if dim not in [0, 1]:
                # æŠ›å‡ºå€¼é”™è¯¯ï¼Œæç¤ºç»´åº¦è®¾ç½®ä¸å½“
                raise ValueError(f"Make sure to set `dim` to either 0 or 1, not {dim}")
    
            # é»˜è®¤åˆ†å—å¤§å°ä¸º 1
            chunk_size = chunk_size or 1
    
            # å®šä¹‰é€’å½’å‡½æ•°ä»¥è®¾ç½®å‰é¦ˆå±‚çš„åˆ†å—å¤„ç†
            def fn_recursive_feed_forward(module: torch.nn.Module, chunk_size: int, dim: int):
                # å¦‚æœæ¨¡å—å…·æœ‰è®¾ç½®åˆ†å—å‰é¦ˆçš„å±æ€§ï¼Œåˆ™è¿›è¡Œè®¾ç½®
                if hasattr(module, "set_chunk_feed_forward"):
                    module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
    
                # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—å¹¶é€’å½’è°ƒç”¨
                for child in module.children():
                    fn_recursive_feed_forward(child, chunk_size, dim)
    
            # éå†å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—å¹¶åº”ç”¨é€’å½’å‡½æ•°
            for module in self.children():
                fn_recursive_feed_forward(module, chunk_size, dim)
    
        # ä» diffusers.models.unets.unet_3d_condition.UNet3DConditionModel.disable_forward_chunking å¤åˆ¶
        def disable_forward_chunking(self):
            # å®šä¹‰é€’å½’å‡½æ•°ä»¥ç¦ç”¨å‰é¦ˆå±‚çš„åˆ†å—å¤„ç†
            def fn_recursive_feed_forward(module: torch.nn.Module, chunk_size: int, dim: int):
                # å¦‚æœæ¨¡å—å…·æœ‰è®¾ç½®åˆ†å—å‰é¦ˆçš„å±æ€§ï¼Œåˆ™è¿›è¡Œè®¾ç½®
                if hasattr(module, "set_chunk_feed_forward"):
                    module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
    
                # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—å¹¶é€’å½’è°ƒç”¨
                for child in module.children():
                    fn_recursive_feed_forward(child, chunk_size, dim)
    
            # éå†å½“å‰å¯¹è±¡çš„æ‰€æœ‰å­æ¨¡å—å¹¶åº”ç”¨é€’å½’å‡½æ•°ï¼Œç¦ç”¨åˆ†å—å¤„ç†
            for module in self.children():
                fn_recursive_feed_forward(module, None, 0)
```