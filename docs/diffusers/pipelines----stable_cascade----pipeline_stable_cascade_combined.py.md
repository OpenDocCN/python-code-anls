# `.\diffusers\pipelines\stable_cascade\pipeline_stable_cascade_combined.py`

```py
# ç‰ˆæƒå£°æ˜ï¼Œè¯´æ˜è¯¥ä»£ç çš„æ‰€æœ‰æƒ
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# ä½¿ç”¨ Apache License 2.0 è¿›è¡Œè®¸å¯
# Licensed under the Apache License, Version 2.0 (the "License");
# è¯¥æ–‡ä»¶åªèƒ½åœ¨éµå¾ªè®¸å¯è¯çš„æƒ…å†µä¸‹ä½¿ç”¨
# you may not use this file except in compliance with the License.
# å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹æˆ–ä¹¦é¢åè®®å¦æœ‰è§„å®šï¼Œè½¯ä»¶åœ¨â€œåŸæ ·â€åŸºç¡€ä¸Šåˆ†å‘
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# ä¸æä¾›ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# æŸ¥çœ‹è®¸å¯è¯ä»¥äº†è§£ç‰¹å®šè¯­è¨€çš„æƒé™å’Œé™åˆ¶
# See the License for the specific language governing permissions and
# limitations under the License.
# å¯¼å…¥æ‰€éœ€çš„ç±»å‹å®šä¹‰
from typing import Callable, Dict, List, Optional, Union

# å¯¼å…¥å›¾åƒå¤„ç†åº“
import PIL
# å¯¼å…¥ PyTorch
import torch
# ä» transformers åº“å¯¼å…¥ CLIP æ¨¡å‹åŠå¤„ç†å™¨
from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection

# ä»æœ¬åœ°æ¨¡å‹ä¸­å¯¼å…¥ StableCascadeUNet
from ...models import StableCascadeUNet
# ä»è°ƒåº¦å™¨ä¸­å¯¼å…¥ DDPMWuerstchenScheduler
from ...schedulers import DDPMWuerstchenScheduler
# å¯¼å…¥å·¥å…·å‡½æ•°
from ...utils import is_torch_version, replace_example_docstring
# ä»ç®¡é“å·¥å…·ä¸­å¯¼å…¥ DiffusionPipeline
from ..pipeline_utils import DiffusionPipeline
# ä» VQ æ¨¡å‹ä¸­å¯¼å…¥ PaellaVQModel
from ..wuerstchen.modeling_paella_vq_model import PaellaVQModel
# å¯¼å…¥ StableCascade è§£ç å™¨ç®¡é“
from .pipeline_stable_cascade import StableCascadeDecoderPipeline
# å¯¼å…¥ StableCascade ä¼˜å…ˆç®¡é“
from .pipeline_stable_cascade_prior import StableCascadePriorPipeline


# æ–‡æ¡£å­—ç¬¦ä¸²ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–‡æœ¬è½¬å›¾åƒåŠŸèƒ½
TEXT2IMAGE_EXAMPLE_DOC_STRING = """
    Examples:
        ```py
        >>> import torch
        >>> from diffusers import StableCascadeCombinedPipeline

        # ä»é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºç®¡é“å®ä¾‹
        >>> pipe = StableCascadeCombinedPipeline.from_pretrained(
        ...     "stabilityai/stable-cascade", variant="bf16", torch_dtype=torch.bfloat16
        ... )
        # å¯ç”¨æ¨¡å‹çš„ CPU ç¦»çº¿åŠ è½½
        >>> pipe.enable_model_cpu_offload()
        # å®šä¹‰å›¾åƒç”Ÿæˆçš„æç¤º
        >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
        # ç”Ÿæˆå›¾åƒ
        >>> images = pipe(prompt=prompt)
        ```
"""

# å®šä¹‰ç¨³å®šçº§è”ç»„åˆç®¡é“ç±»ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
class StableCascadeCombinedPipeline(DiffusionPipeline):
    """
    Combined Pipeline for text-to-image generation using Stable Cascade.

    è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [`DiffusionPipeline`]ã€‚æ£€æŸ¥çˆ¶ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•
    (ä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ã€‚)
    # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæè¿°åˆå§‹åŒ–æ–¹æ³•å‚æ•°çš„å«ä¹‰
        Args:
            tokenizer (`CLIPTokenizer`):
                ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨åˆ†è¯å™¨ã€‚
            text_encoder (`CLIPTextModel`):
                ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨æ–‡æœ¬ç¼–ç å™¨ã€‚
            decoder (`StableCascadeUNet`):
                ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„è§£ç æ¨¡å‹ã€‚
            scheduler (`DDPMWuerstchenScheduler`):
                ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„è°ƒåº¦å™¨ã€‚
            vqgan (`PaellaVQModel`):
                ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„ VQGAN æ¨¡å‹ã€‚
            feature_extractor ([`~transformers.CLIPImageProcessor`]):
                ä»ç”Ÿæˆå›¾åƒä¸­æå–ç‰¹å¾çš„æ¨¡å‹ï¼Œä½œä¸º `image_encoder` çš„è¾“å…¥ã€‚
            image_encoder ([`CLIPVisionModelWithProjection`]):
                å†»ç»“çš„ CLIP å›¾åƒç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚
            prior_prior (`StableCascadeUNet`):
                ç”¨äºå…ˆéªŒç®¡é“çš„å…ˆéªŒæ¨¡å‹ã€‚
            prior_scheduler (`DDPMWuerstchenScheduler`):
                ç”¨äºå…ˆéªŒç®¡é“çš„è°ƒåº¦å™¨ã€‚
        """
    
        # è®¾ç½®åŠ è½½è¿æ¥ç®¡é“çš„æ ‡å¿—ä¸º True
        _load_connected_pipes = True
        # å®šä¹‰å¯é€‰ç»„ä»¶çš„åˆ—è¡¨
        _optional_components = ["prior_feature_extractor", "prior_image_encoder"]
    
        # åˆå§‹åŒ–æ–¹æ³•
        def __init__(
            # å®šä¹‰å‚æ•°ç±»å‹åŠåç§°
            self,
            tokenizer: CLIPTokenizer,
            text_encoder: CLIPTextModel,
            decoder: StableCascadeUNet,
            scheduler: DDPMWuerstchenScheduler,
            vqgan: PaellaVQModel,
            prior_prior: StableCascadeUNet,
            prior_text_encoder: CLIPTextModel,
            prior_tokenizer: CLIPTokenizer,
            prior_scheduler: DDPMWuerstchenScheduler,
            prior_feature_extractor: Optional[CLIPImageProcessor] = None,
            prior_image_encoder: Optional[CLIPVisionModelWithProjection] = None,
        ):
            # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
            super().__init__()
    
            # æ³¨å†Œå¤šä¸ªæ¨¡å—ä»¥ä¾¿äºç®¡ç†
            self.register_modules(
                text_encoder=text_encoder,
                tokenizer=tokenizer,
                decoder=decoder,
                scheduler=scheduler,
                vqgan=vqgan,
                prior_text_encoder=prior_text_encoder,
                prior_tokenizer=prior_tokenizer,
                prior_prior=prior_prior,
                prior_scheduler=prior_scheduler,
                prior_feature_extractor=prior_feature_extractor,
                prior_image_encoder=prior_image_encoder,
            )
            # åˆå§‹åŒ–å…ˆéªŒç®¡é“
            self.prior_pipe = StableCascadePriorPipeline(
                prior=prior_prior,
                text_encoder=prior_text_encoder,
                tokenizer=prior_tokenizer,
                scheduler=prior_scheduler,
                image_encoder=prior_image_encoder,
                feature_extractor=prior_feature_extractor,
            )
            # åˆå§‹åŒ–è§£ç å™¨ç®¡é“
            self.decoder_pipe = StableCascadeDecoderPipeline(
                text_encoder=text_encoder,
                tokenizer=tokenizer,
                decoder=decoder,
                scheduler=scheduler,
                vqgan=vqgan,
            )
    # å¯ç”¨ xformers çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
        def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):
            # è°ƒç”¨è§£ç ç®¡é“ä»¥å¯ç”¨å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶
            self.decoder_pipe.enable_xformers_memory_efficient_attention(attention_op)
    
    # å¯ç”¨æ¨¡å‹çš„ CPU ç¦»çº¿åŠ è½½
        def enable_model_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = "cuda"):
            r"""
            ä½¿ç”¨ accelerate å°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ° CPUï¼Œé™ä½å†…å­˜ä½¿ç”¨ï¼Œæ€§èƒ½å½±å“è¾ƒå°ã€‚ä¸ `enable_sequential_cpu_offload` ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è°ƒç”¨æ¨¡å‹çš„ `forward` æ–¹æ³•æ—¶ä¸€æ¬¡ç§»åŠ¨æ•´ä¸ªæ¨¡å‹åˆ° GPUï¼Œå¹¶åœ¨ä¸‹ä¸€ä¸ªæ¨¡å‹è¿è¡Œä¹‹å‰ä¿æŒåœ¨ GPU ä¸­ã€‚å†…å­˜èŠ‚çœä½äº `enable_sequential_cpu_offload`ï¼Œä½†ç”±äº `unet` çš„è¿­ä»£æ‰§è¡Œï¼Œæ€§èƒ½æ›´å¥½ã€‚
            """
            # å¯ç”¨ CPU ç¦»çº¿åŠ è½½åˆ°ä¼˜å…ˆç®¡é“
            self.prior_pipe.enable_model_cpu_offload(gpu_id=gpu_id, device=device)
            # å¯ç”¨ CPU ç¦»çº¿åŠ è½½åˆ°è§£ç ç®¡é“
            self.decoder_pipe.enable_model_cpu_offload(gpu_id=gpu_id, device=device)
    
    # å¯ç”¨é¡ºåº CPU ç¦»çº¿åŠ è½½
        def enable_sequential_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = "cuda"):
            r"""
            ä½¿ç”¨ ğŸ¤— Accelerate å°†æ‰€æœ‰æ¨¡å‹ï¼ˆ`unet`ã€`text_encoder`ã€`vae` å’Œ `safety checker` çŠ¶æ€å­—å…¸ï¼‰è½¬ç§»åˆ° CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¨¡å‹è¢«ç§»åŠ¨åˆ° `torch.device('meta')`ï¼Œä»…åœ¨è°ƒç”¨å…¶ç‰¹å®šå­æ¨¡å—çš„ `forward` æ–¹æ³•æ—¶åŠ è½½åˆ° GPUã€‚ç¦»çº¿åŠ è½½æ˜¯åŸºäºå­æ¨¡å—è¿›è¡Œçš„ã€‚å†…å­˜èŠ‚çœé«˜äºä½¿ç”¨ `enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
            """
            # å¯ç”¨é¡ºåº CPU ç¦»çº¿åŠ è½½åˆ°ä¼˜å…ˆç®¡é“
            self.prior_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id, device=device)
            # å¯ç”¨é¡ºåº CPU ç¦»çº¿åŠ è½½åˆ°è§£ç ç®¡é“
            self.decoder_pipe.enable_sequential_cpu_offload(gpu_id=gpu_id, device=device)
    
    # å¤„ç†è¿›åº¦æ¡çš„æ˜¾ç¤º
        def progress_bar(self, iterable=None, total=None):
            # åœ¨ä¼˜å…ˆç®¡é“ä¸­æ˜¾ç¤ºè¿›åº¦æ¡
            self.prior_pipe.progress_bar(iterable=iterable, total=total)
            # åœ¨è§£ç ç®¡é“ä¸­æ˜¾ç¤ºè¿›åº¦æ¡
            self.decoder_pipe.progress_bar(iterable=iterable, total=total)
    
    # è®¾ç½®è¿›åº¦æ¡çš„é…ç½®
        def set_progress_bar_config(self, **kwargs):
            # è®¾ç½®ä¼˜å…ˆç®¡é“çš„è¿›åº¦æ¡é…ç½®
            self.prior_pipe.set_progress_bar_config(**kwargs)
            # è®¾ç½®è§£ç ç®¡é“çš„è¿›åº¦æ¡é…ç½®
            self.decoder_pipe.set_progress_bar_config(**kwargs)
    
    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜
        @torch.no_grad()
        # æ›¿æ¢ç¤ºä¾‹æ–‡æ¡£å­—ç¬¦ä¸²
        @replace_example_docstring(TEXT2IMAGE_EXAMPLE_DOC_STRING)
    # å®šä¹‰å¯è°ƒç”¨æ–¹æ³•ï¼Œå…è®¸ä½¿ç”¨å¤šä¸ªå‚æ•°è¿›è¡Œæ¨ç†
        def __call__(
            self,
            # è¾“å…¥çš„æç¤ºï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            prompt: Optional[Union[str, List[str]]] = None,
            # è¾“å…¥çš„å›¾åƒï¼Œå¯ä»¥æ˜¯å¼ é‡æˆ– PIL å›¾åƒï¼Œæ”¯æŒåˆ—è¡¨å½¢å¼
            images: Union[torch.Tensor, PIL.Image.Image, List[torch.Tensor], List[PIL.Image.Image]] = None,
            # ç”Ÿæˆå›¾åƒçš„é«˜åº¦ï¼Œé»˜è®¤å€¼ä¸º 512
            height: int = 512,
            # ç”Ÿæˆå›¾åƒçš„å®½åº¦ï¼Œé»˜è®¤å€¼ä¸º 512
            width: int = 512,
            # æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œç”¨äºå…ˆéªŒæ¨¡å‹ï¼Œé»˜è®¤å€¼ä¸º 60
            prior_num_inference_steps: int = 60,
            # å…ˆéªŒæŒ‡å¯¼å°ºåº¦ï¼Œæ§åˆ¶ç”Ÿæˆçš„æ ·å¼å¼ºåº¦ï¼Œé»˜è®¤å€¼ä¸º 4.0
            prior_guidance_scale: float = 4.0,
            # æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œæ§åˆ¶å›¾åƒç”Ÿæˆçš„ç»†è‡´ç¨‹åº¦ï¼Œé»˜è®¤å€¼ä¸º 12
            num_inference_steps: int = 12,
            # è§£ç å™¨æŒ‡å¯¼å°ºåº¦ï¼Œå½±å“å›¾åƒçš„å¤šæ ·æ€§ï¼Œé»˜è®¤å€¼ä¸º 0.0
            decoder_guidance_scale: float = 0.0,
            # è´Ÿé¢æç¤ºï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            negative_prompt: Optional[Union[str, List[str]]] = None,
            # æç¤ºåµŒå…¥ï¼Œä¾›æ¨¡å‹ä½¿ç”¨çš„é¢„è®¡ç®—å¼ é‡
            prompt_embeds: Optional[torch.Tensor] = None,
            # æ± åŒ–åçš„æç¤ºåµŒå…¥ï¼Œå¢å¼ºæ¨¡å‹çš„ç†è§£èƒ½åŠ›
            prompt_embeds_pooled: Optional[torch.Tensor] = None,
            # è´Ÿé¢æç¤ºåµŒå…¥ï¼Œä¾›æ¨¡å‹ä½¿ç”¨çš„é¢„è®¡ç®—å¼ é‡
            negative_prompt_embeds: Optional[torch.Tensor] = None,
            # æ± åŒ–åçš„è´Ÿé¢æç¤ºåµŒå…¥ï¼Œå¢å¼ºæ¨¡å‹çš„ç†è§£èƒ½åŠ›
            negative_prompt_embeds_pooled: Optional[torch.Tensor] = None,
            # æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ï¼Œé»˜è®¤å€¼ä¸º 1
            num_images_per_prompt: int = 1,
            # éšæœºæ•°ç”Ÿæˆå™¨ï¼Œæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹çš„éšæœºæ€§
            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
            # æ½œåœ¨å˜é‡ï¼Œç”¨äºå›¾åƒç”Ÿæˆçš„è¾“å…¥å¼ é‡
            latents: Optional[torch.Tensor] = None,
            # è¾“å‡ºç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨ PIL å›¾åƒæ ¼å¼
            output_type: Optional[str] = "pil",
            # æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼çš„ç»“æœï¼Œé»˜è®¤å€¼ä¸º True
            return_dict: bool = True,
            # å…ˆéªŒå›è°ƒå‡½æ•°ï¼Œå¤„ç†æ¯ä¸ªæ­¥éª¤ç»“æŸæ—¶çš„æ“ä½œ
            prior_callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
            # å…ˆéªŒå›è°ƒä½¿ç”¨çš„å¼ é‡è¾“å…¥ï¼Œé»˜è®¤åŒ…å« 'latents'
            prior_callback_on_step_end_tensor_inputs: List[str] = ["latents"],
            # å›è°ƒå‡½æ•°ï¼Œå¤„ç†æ¯ä¸ªæ­¥éª¤ç»“æŸæ—¶çš„æ“ä½œ
            callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
            # å›è°ƒä½¿ç”¨çš„å¼ é‡è¾“å…¥ï¼Œé»˜è®¤åŒ…å« 'latents'
            callback_on_step_end_tensor_inputs: List[str] = ["latents"],
```