# `numpy-ml\numpy_ml\preprocessing\nlp.py`

```
# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import re
import heapq
import os.path as op
from collections import Counter, OrderedDict, defaultdict
import numpy as np

# å®šä¹‰è‹±æ–‡åœç”¨è¯åˆ—è¡¨ï¼Œæ¥æºäº"Glasgow Information Retrieval Group"
_STOP_WORDS = set(
    ).split(" "),
)

# å®šä¹‰ç”¨äºåŒ¹é…å•è¯çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºåˆ†è¯
_WORD_REGEX = re.compile(r"(?u)\b\w\w+\b")  # sklearné»˜è®¤
_WORD_REGEX_W_PUNC = re.compile(r"(?u)\w+|[^a-zA-Z0-9\s]")
_WORD_REGEX_W_PUNC_AND_WHITESPACE = re.compile(r"(?u)s?\w+\s?|\s?[^a-zA-Z0-9\s]\s?")

# å®šä¹‰ç”¨äºåŒ¹é…æ ‡ç‚¹ç¬¦å·çš„æ­£åˆ™è¡¨è¾¾å¼
_PUNC_BYTE_REGEX = re.compile(
    r"(33|34|35|36|37|38|39|40|41|42|43|44|45|"
    r"46|47|58|59|60|61|62|63|64|91|92|93|94|"
    r"95|96|123|124|125|126)",
)
# å®šä¹‰æ ‡ç‚¹ç¬¦å·
_PUNCTUATION = "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
# åˆ›å»ºç”¨äºå»é™¤æ ‡ç‚¹ç¬¦å·çš„è½¬æ¢è¡¨
_PUNC_TABLE = str.maketrans("", "", _PUNCTUATION)

# å®šä¹‰å‡½æ•°ï¼Œè¿”å›æŒ‡å®šé•¿åº¦çš„n-gramåºåˆ—
def ngrams(sequence, N):
    """Return all `N`-grams of the elements in `sequence`"""
    assert N >= 1
    return list(zip(*[sequence[i:] for i in range(N)]))

# å®šä¹‰å‡½æ•°ï¼Œå°†å­—ç¬¦ä¸²æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œå¯é€‰æ‹©æ˜¯å¦è½¬ä¸ºå°å†™ã€è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
def tokenize_whitespace(
    line, lowercase=True, filter_stopwords=True, filter_punctuation=True, **kwargs,
):
    """
    Split a string at any whitespace characters, optionally removing
    punctuation and stop-words in the process.
    """
    line = line.lower() if lowercase else line
    words = line.split()
    line = [strip_punctuation(w) for w in words] if filter_punctuation else line
    return remove_stop_words(words) if filter_stopwords else words

# å®šä¹‰å‡½æ•°ï¼Œå°†å­—ç¬¦ä¸²æŒ‰å•è¯åˆ†è¯ï¼Œå¯é€‰æ‹©æ˜¯å¦è½¬ä¸ºå°å†™ã€è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
def tokenize_words(
    line, lowercase=True, filter_stopwords=True, filter_punctuation=True, **kwargs,
):
    """
    Split a string into individual words, optionally removing punctuation and
    stop-words in the process.
    """
    REGEX = _WORD_REGEX if filter_punctuation else _WORD_REGEX_W_PUNC
    words = REGEX.findall(line.lower() if lowercase else line)
    return remove_stop_words(words) if filter_stopwords else words

# å®šä¹‰å‡½æ•°ï¼Œå°†å­—ç¬¦ä¸²æŒ‰å­—èŠ‚åˆ†è¯
def tokenize_words_bytes(
    line,
    # è®¾ç½®æ˜¯å¦å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™
    lowercase=True,
    # è®¾ç½®æ˜¯å¦è¿‡æ»¤åœç”¨è¯
    filter_stopwords=True,
    # è®¾ç½®æ˜¯å¦è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
    filter_punctuation=True,
    # è®¾ç½®æ–‡æœ¬ç¼–ç æ ¼å¼ä¸º UTF-8
    encoding="utf-8",
    # **kwargs è¡¨ç¤ºæ¥å—ä»»æ„æ•°é‡çš„å…³é”®å­—å‚æ•°ï¼Œè¿™äº›å‚æ•°ä¼šè¢«ä¼ é€’ç»™å‡½æ•°çš„å…¶ä»–éƒ¨åˆ†è¿›è¡Œå¤„ç†
    **kwargs,
# å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•è¯ï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­é€‰æ‹©æ€§åœ°åˆ é™¤æ ‡ç‚¹ç¬¦å·å’Œåœç”¨è¯ã€‚å°†æ¯ä¸ªå•è¯è½¬æ¢ä¸ºå­—èŠ‚åˆ—è¡¨ã€‚
def tokenize_words(
    line,
    lowercase=lowercase,
    filter_stopwords=filter_stopwords,
    filter_punctuation=filter_punctuation,
    **kwargs,
):
    # å¯¹å•è¯è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œæ ¹æ®å‚æ•°é€‰æ‹©æ˜¯å¦è½¬æ¢ä¸ºå°å†™ã€è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
    words = tokenize_words(
        line,
        lowercase=lowercase,
        filter_stopwords=filter_stopwords,
        filter_punctuation=filter_punctuation,
        **kwargs,
    )
    # å°†å•è¯è½¬æ¢ä¸ºå­—èŠ‚åˆ—è¡¨ï¼Œæ¯ä¸ªå­—èŠ‚ç”¨ç©ºæ ¼åˆ†éš”
    words = [" ".join([str(i) for i in w.encode(encoding)]) for w in words]
    # è¿”å›å­—èŠ‚åˆ—è¡¨
    return words


# å°†å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦è½¬æ¢ä¸ºå­—èŠ‚é›†åˆã€‚æ¯ä¸ªå­—èŠ‚ç”¨0åˆ°255ä¹‹é—´çš„æ•´æ•°è¡¨ç¤ºã€‚
def tokenize_bytes_raw(line, encoding="utf-8", splitter=None, **kwargs):
    # å°†å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦ç¼–ç ä¸ºå­—èŠ‚ï¼Œæ¯ä¸ªå­—èŠ‚ç”¨ç©ºæ ¼åˆ†éš”
    byte_str = [" ".join([str(i) for i in line.encode(encoding)])
    # å¦‚æœæŒ‡å®šäº†åˆ†éš”ç¬¦ä¸ºæ ‡ç‚¹ç¬¦å·ï¼Œåˆ™åœ¨ç¼–ç ä¸ºå­—èŠ‚ä¹‹å‰åœ¨æ ‡ç‚¹ç¬¦å·å¤„è¿›è¡Œåˆ†å‰²
    if splitter == "punctuation":
        byte_str = _PUNC_BYTE_REGEX.sub(r"-\1-", byte_str[0]).split("-")
    return byte_str


# å°†å­—èŠ‚ï¼ˆè¡¨ç¤ºä¸º0åˆ°255ä¹‹é—´çš„æ•´æ•°ï¼‰è§£ç ä¸ºæŒ‡å®šç¼–ç çš„å­—ç¬¦ã€‚
def bytes_to_chars(byte_list, encoding="utf-8"):
    # å°†å­—èŠ‚åˆ—è¡¨ä¸­çš„æ•´æ•°è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
    hex_array = [hex(a).replace("0x", "") for a in byte_list]
    # å°†åå…­è¿›åˆ¶å­—ç¬¦ä¸²è¿æ¥èµ·æ¥ï¼Œå¹¶åœ¨éœ€è¦æ—¶åœ¨å‰é¢è¡¥0
    hex_array = " ".join([h if len(h) > 1 else f"0{h}" for h in hex_array])
    # å°†åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚æ•°ç»„ï¼Œå†æ ¹æ®æŒ‡å®šç¼–ç è§£ç ä¸ºå­—ç¬¦
    return bytearray.fromhex(hex_array).decode(encoding)


# å°†å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶æ ¹æ®å‚æ•°é€‰æ‹©æ˜¯å¦è¿‡æ»¤æ ‡ç‚¹ç¬¦å·ã€‚
def tokenize_chars(line, lowercase=True, filter_punctuation=True, **kwargs):
    # å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦ï¼Œå¯é€‰æ‹©åœ¨æ­¤è¿‡ç¨‹ä¸­åˆ é™¤æ ‡ç‚¹ç¬¦å·å’Œåœç”¨è¯
    """
    # å¦‚æœéœ€è¦è½¬æ¢ä¸ºå°å†™ï¼Œåˆ™å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå°å†™
    line = line.lower() if lowercase else line
    # å¦‚æœéœ€è¦è¿‡æ»¤æ ‡ç‚¹ç¬¦å·ï¼Œåˆ™è°ƒç”¨å‡½æ•°å»é™¤æ ‡ç‚¹ç¬¦å·
    line = strip_punctuation(line) if filter_punctuation else line
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†è¿ç»­å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ªç©ºæ ¼ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œç„¶åå°†ç»“æœè½¬æ¢ä¸ºå­—ç¬¦åˆ—è¡¨
    chars = list(re.sub(" {2,}", " ", line).strip())
    # è¿”å›å­—ç¬¦åˆ—è¡¨
    return chars
# ä»å•è¯å­—ç¬¦ä¸²åˆ—è¡¨ä¸­ç§»é™¤åœç”¨è¯
def remove_stop_words(words):
    """Remove stop words from a list of word strings"""
    # è¿”å›ä¸åœ¨åœç”¨è¯åˆ—è¡¨ä¸­çš„å•è¯
    return [w for w in words if w.lower() not in _STOP_WORDS]


# ä»å­—ç¬¦ä¸²ä¸­ç§»é™¤æ ‡ç‚¹ç¬¦å·
def strip_punctuation(line):
    """Remove punctuation from a string"""
    # ä½¿ç”¨_PUNC_TABLEæ¥ç§»é™¤å­—ç¬¦ä¸²ä¸­çš„æ ‡ç‚¹ç¬¦å·ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    return line.translate(_PUNC_TABLE).strip()


#######################################################################
#                          Byte-Pair Encoder                          #
#######################################################################


# å®šä¹‰ä¸€ä¸ªByte-Pairç¼–ç å™¨ç±»
class BytePairEncoder(object):
    def __init__(self, max_merges=3000, encoding="utf-8"):
        """
        A byte-pair encoder for sub-word embeddings.

        Notes
        -----
        Byte-pair encoding [1][2] is a compression algorithm that iteratively
        replaces the most frequently ocurring byte pairs in a set of documents
        with a new, single token. It has gained popularity as a preprocessing
        step for many NLP tasks due to its simplicity and expressiveness: using
        a base coebook of just 256 unique tokens (bytes), any string can be
        encoded.

        References
        ----------
        .. [1] Gage, P. (1994). A new algorithm for data compression. *C
           Users Journal, 12(2)*, 23â€“38.
        .. [2] Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine
           translation of rare words with subword units, *Proceedings of the
           54th Annual Meeting of the Association for Computational
           Linguistics,* 1715-1725.

        Parameters
        ----------
        max_merges : int
            The maximum number of byte pair merges to perform during the
            :meth:`fit` operation. Default is 3000.
        encoding : str
            The encoding scheme for the documents used to train the encoder.
            Default is `'utf-8'`.
        """
        # åˆå§‹åŒ–å‚æ•°å­—å…¸
        self.parameters = {
            "max_merges": max_merges,
            "encoding": encoding,
        }

        # åˆå§‹åŒ–å­—èŠ‚åˆ°æ ‡è®°å’Œæ ‡è®°åˆ°å­—èŠ‚çš„æœ‰åºå­—å…¸ã€‚å­—èŠ‚ä»¥åè¿›åˆ¶è¡¨ç¤ºä¸º0åˆ°255ä¹‹é—´çš„æ•´æ•°ã€‚
        # åœ¨255ä¹‹å‰ï¼Œæ ‡è®°å’Œå­—èŠ‚è¡¨ç¤ºä¹‹é—´å­˜åœ¨ä¸€å¯¹ä¸€çš„å¯¹åº”å…³ç³»ã€‚
        self.byte2token = OrderedDict({i: i for i in range(256)})
        self.token2byte = OrderedDict({v: k for k, v in self.byte2token.items()})
    # åœ¨ç»™å®šè¯­æ–™åº“ä¸Šè®­ç»ƒä¸€ä¸ªå­—èŠ‚å¯¹ç¼–ç è¡¨
    def fit(self, corpus_fps, encoding="utf-8"):
        """
        Train a byte pair codebook on a set of documents.

        Parameters
        ----------
        corpus_fps : str or list of strs
            The filepath / list of filepaths for the document(s) to be used to
            learn the byte pair codebook.
        encoding : str
            The text encoding for documents. Common entries are either 'utf-8'
            (no header byte), or 'utf-8-sig' (header byte). Default is
            'utf-8'.
        """
        # åˆ›å»ºä¸€ä¸ªè¯æ±‡è¡¨å¯¹è±¡ï¼Œç”¨äºå­˜å‚¨å­—èŠ‚å¯¹ç¼–ç è¡¨
        vocab = (
            Vocabulary(
                lowercase=False,
                min_count=None,
                max_tokens=None,
                filter_stopwords=False,
                filter_punctuation=False,
                tokenizer="bytes",
            )
            # åœ¨ç»™å®šè¯­æ–™åº“ä¸Šæ‹Ÿåˆè¯æ±‡è¡¨
            .fit(corpus_fps, encoding=encoding)
            # è·å–è¯æ±‡è¡¨ä¸­çš„è®¡æ•°ä¿¡æ¯
            .counts
        )

        # è¿­ä»£åœ°åˆå¹¶è·¨æ–‡æ¡£ä¸­æœ€å¸¸è§çš„å­—èŠ‚äºŒå…ƒç»„
        for _ in range(self.parameters["max_merges"]):
            # è·å–è¯æ±‡è¡¨ä¸­çš„å­—èŠ‚äºŒå…ƒç»„è®¡æ•°ä¿¡æ¯
            pair_counts = self._get_counts(vocab)
            # æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„å­—èŠ‚äºŒå…ƒç»„
            most_common_bigram = max(pair_counts, key=pair_counts.get)
            # åˆå¹¶æœ€å¸¸è§çš„å­—èŠ‚äºŒå…ƒç»„åˆ°è¯æ±‡è¡¨ä¸­
            vocab = self._merge(most_common_bigram, vocab)

        # åˆå§‹åŒ–ä¸€ä¸ªç©ºé›†åˆï¼Œç”¨äºå­˜å‚¨å­—èŠ‚æ ‡è®°
        token_bytes = set()
        # éå†è¯æ±‡è¡¨ä¸­çš„é”®
        for k in vocab.keys():
            # å°†é”®æŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œç­›é€‰åŒ…å«"-"çš„å­—èŠ‚æ ‡è®°
            token_bytes = token_bytes.union([w for w in k.split(" ") if "-" in w])

        # éå†å­—èŠ‚æ ‡è®°é›†åˆ
        for i, t in enumerate(token_bytes):
            # å°†å­—èŠ‚æ ‡è®°è½¬æ¢ä¸ºå…ƒç»„å½¢å¼
            byte_tuple = tuple(int(j) for j in t.split("-"))
            # å°†å­—èŠ‚æ ‡è®°æ˜ å°„åˆ°å¯¹åº”çš„æ ‡è®°ç´¢å¼•
            self.token2byte[256 + i] = byte_tuple
            # å°†å­—èŠ‚æ ‡è®°ç´¢å¼•æ˜ å°„åˆ°å¯¹åº”çš„å­—èŠ‚æ ‡è®°
            self.byte2token[byte_tuple] = 256 + i

        # è¿”å›å½“å‰å¯¹è±¡
        return self

    # è·å–è¯æ±‡è¡¨ä¸­çš„å­—èŠ‚äºŒå…ƒç»„è®¡æ•°ä¿¡æ¯
    def _get_counts(self, vocab):
        """Collect bigram counts for the tokens in vocab"""
        # åˆå§‹åŒ–ä¸€ä¸ªé»˜è®¤å­—å…¸ï¼Œç”¨äºå­˜å‚¨å­—èŠ‚äºŒå…ƒç»„è®¡æ•°
        pair_counts = defaultdict(int)
        # éå†è¯æ±‡è¡¨ä¸­çš„å•è¯å’Œè®¡æ•°ä¿¡æ¯
        for word, count in vocab.items():
            # ç”Ÿæˆå•è¯çš„äºŒå…ƒç»„
            pairs = ngrams(word.split(" "), 2)
            # éå†å•è¯çš„äºŒå…ƒç»„
            for p in pairs:
                # æ›´æ–°å­—èŠ‚äºŒå…ƒç»„è®¡æ•°ä¿¡æ¯
                pair_counts[p] += count
        # è¿”å›å­—èŠ‚äºŒå…ƒç»„è®¡æ•°ä¿¡æ¯
        return pair_counts
    # å°†ç»™å®šçš„äºŒå…ƒç»„æ›¿æ¢ä¸ºå•ä¸ªæ ‡è®°ï¼Œå¹¶ç›¸åº”æ›´æ–°è¯æ±‡è¡¨
    def _merge(self, bigram, vocab):
        v_out = {}
        # è½¬ä¹‰äºŒå…ƒç»„ä¸­çš„å•è¯ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
        bg = re.escape(" ".join(bigram))
        # åˆ›å»ºåŒ¹é…äºŒå…ƒç»„çš„æ­£åˆ™è¡¨è¾¾å¼
        bigram_regex = re.compile(r"(?<!\S)" + bg + r"(?!\S)")
        # éå†è¯æ±‡è¡¨ä¸­çš„å•è¯
        for word in vocab.keys():
            # å°†åŒ¹é…åˆ°çš„äºŒå…ƒç»„æ›¿æ¢ä¸ºè¿æ¥ç¬¦"-"
            w_out = bigram_regex.sub("-".join(bigram), word)
            v_out[w_out] = vocab[word]
        return v_out

    # å°†æ–‡æœ¬ä¸­çš„å•è¯è½¬æ¢ä¸ºå…¶å­—èŠ‚å¯¹ç¼–ç çš„æ ‡è®°ID
    def transform(self, text):
        """
        Transform the words in `text` into their byte pair encoded token IDs.

        Parameters
        ----------
        text: str or list of `N` strings
            The list of strings to encode

        Returns
        -------
        codes : list of `N` lists
            A list of byte pair token IDs for each of the `N` strings in
            `text`.

        Examples
        --------
        >>> B = BytePairEncoder(max_merges=100).fit("./example.txt")
        >>> encoded_tokens = B.transform("Hello! How are you ğŸ˜ ?")
        >>> encoded_tokens
        [[72, 879, 474, ...]]
        """
        # å¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(text, str):
            text = [text]
        # å¯¹æ–‡æœ¬ä¸­çš„æ¯ä¸ªå­—ç¬¦ä¸²è¿›è¡Œè½¬æ¢
        return [self._transform(string) for string in text]
    # å°†å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚å¯¹ ID åˆ—è¡¨
    def _transform(self, text):
        # è·å–å‚æ•°é…ç½®
        P = self.parameters
        # å°†æ–‡æœ¬å­—ç¬¦ä¸²è½¬æ¢ä¸ºåŸå§‹å­—èŠ‚æµ
        _bytes = tokenize_bytes_raw(text, encoding=P["encoding"])

        # åˆå§‹åŒ–ç¼–ç ç»“æœåˆ—è¡¨
        encoded = []
        # éå†æ¯ä¸ªå­—èŠ‚å¯¹
        for w in _bytes:
            l, r = 0, len(w)
            # å°†å­—èŠ‚å¯¹è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
            w = [int(i) for i in w.split(" ")]

            # å¾ªç¯å¤„ç†å­—èŠ‚å¯¹
            while l < len(w):
                candidate = tuple(w[l:r])

                # å¦‚æœå€™é€‰å­—èŠ‚å¯¹é•¿åº¦å¤§äº1ä¸”åœ¨è¯æ±‡è¡¨ä¸­
                if len(candidate) > 1 and candidate in self.byte2token:
                    # å°†å€™é€‰å­—èŠ‚å¯¹çš„ ID æ·»åŠ åˆ°ç¼–ç ç»“æœåˆ—è¡¨ä¸­
                    encoded.append(self.byte2token[candidate])
                    l, r = r, len(w)
                # å¦‚æœå€™é€‰å­—èŠ‚å¯¹é•¿åº¦ä¸º1
                elif len(candidate) == 1:
                    # å°†å€™é€‰å­—èŠ‚çš„ ID æ·»åŠ åˆ°ç¼–ç ç»“æœåˆ—è¡¨ä¸­
                    encoded.append(candidate[0])
                    l, r = r, len(w)
                else:
                    # å¦‚æœå€™é€‰å­—èŠ‚å¯¹ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å‡å°ä¸Šä¸‹æ–‡çª—å£å¤§å°å¹¶é‡è¯•
                    r -= 1
        # è¿”å›ç¼–ç ç»“æœåˆ—è¡¨
        return encoded
    def inverse_transform(self, codes):
        """
        Transform an encoded sequence of byte pair codeword IDs back into
        human-readable text.

        Parameters
        ----------
        codes : list of `N` lists
            A list of `N` lists. Each sublist is a collection of integer
            byte-pair token IDs representing a particular text string.

        Returns
        -------
        text: list of `N` strings
            The decoded strings corresponding to the `N` sublists in `codes`.

        Examples
        --------
        >>> B = BytePairEncoder(max_merges=100).fit("./example.txt")
        >>> encoded_tokens = B.transform("Hello! How are you ğŸ˜ ?")
        >>> encoded_tokens
        [[72, 879, 474, ...]]
        >>> B.inverse_transform(encoded_tokens)
        ["Hello! How are you ğŸ˜ ?"]
        """
        # å¦‚æœè¾“å…¥çš„codesæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå°†å…¶è½¬æ¢ä¸ºåŒ…å«ä¸€ä¸ªåˆ—è¡¨çš„å½¢å¼
        if isinstance(codes[0], int):
            codes = [codes]

        decoded = []
        P = self.parameters

        # éå†codesä¸­çš„æ¯ä¸ªåˆ—è¡¨
        for code in codes:
            # å°†æ¯ä¸ªtokenè½¬æ¢ä¸ºå¯¹åº”çš„å­—èŠ‚
            _bytes = [self.token2byte[t] if t > 255 else [t] for t in code]
            # å°†å­—èŠ‚åˆ—è¡¨å±•å¼€ä¸ºä¸€ç»´åˆ—è¡¨
            _bytes = [b for blist in _bytes for b in blist]
            # å°†å­—èŠ‚è½¬æ¢ä¸ºå­—ç¬¦å¹¶æ·»åŠ åˆ°decodedåˆ—è¡¨ä¸­
            decoded.append(bytes_to_chars(_bytes, encoding=P["encoding"]))
        return decoded

    @property
    def codebook(self):
        """
        A list of the learned byte pair codewords, decoded into human-readable
        format
        """
        # è¿”å›å­¦ä¹ åˆ°çš„å­—èŠ‚å¯¹ç¼–ç çš„äººç±»å¯è¯»å½¢å¼
        return [
            self.inverse_transform(t)[0]
            for t in self.byte2token.keys()
            if isinstance(t, tuple)
        ]

    @property
    def tokens(self):
        """A list of the byte pair codeword IDs"""
        # è¿”å›å­—èŠ‚å¯¹ç¼–ç çš„IDåˆ—è¡¨
        return list(self.token2byte.keys())
# å®šä¹‰èŠ‚ç‚¹ç±»ï¼Œç”¨äºæ„å»ºå“ˆå¤«æ›¼æ ‘
class Node(object):
    def __init__(self, key, val):
        self.key = key
        self.val = val
        self.left = None
        self.right = None

    # é‡è½½å¤§äºè¿ç®—ç¬¦
    def __gt__(self, other):
        """Greater than"""
        if not isinstance(other, Node):
            return -1
        return self.val > other.val

    # é‡è½½å¤§äºç­‰äºè¿ç®—ç¬¦
    def __ge__(self, other):
        """Greater than or equal to"""
        if not isinstance(other, Node):
            return -1
        return self.val >= other.val

    # é‡è½½å°äºè¿ç®—ç¬¦
    def __lt__(self, other):
        """Less than"""
        if not isinstance(other, Node):
            return -1
        return self.val < other.val

    # é‡è½½å°äºç­‰äºè¿ç®—ç¬¦
    def __le__(self, other):
        """Less than or equal to"""
        if not isinstance(other, Node):
            return -1
        return self.val <= other.val

# å®šä¹‰å“ˆå¤«æ›¼ç¼–ç å™¨ç±»
class HuffmanEncoder(object):
    # ä¸ºæ–‡æœ¬ä¸­çš„æ ‡è®°æ„å»ºä¸€ä¸ªå“ˆå¤«æ›¼æ ‘ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ ‡è®°çš„äºŒè¿›åˆ¶ç¼–ç ã€‚

    # åœ¨å“ˆå¤«æ›¼ç¼–ç ä¸­ï¼Œå‡ºç°é¢‘ç‡æ›´é«˜çš„æ ‡è®°é€šå¸¸ä½¿ç”¨è¾ƒå°‘çš„ä½è¡¨ç¤ºã€‚å“ˆå¤«æ›¼ç¼–ç äº§ç”Ÿäº†æ‰€æœ‰æ–¹æ³•ä¸­å¯¹å•ç‹¬ç¼–ç æ ‡è®°çš„æœ€å°æœŸæœ›ç å­—é•¿åº¦ã€‚

    # å“ˆå¤«æ›¼ç¼–ç å¯¹åº”äºé€šè¿‡äºŒå‰æ ‘çš„è·¯å¾„ï¼Œå…¶ä¸­1è¡¨ç¤ºâ€œå‘å³ç§»åŠ¨â€ï¼Œ0è¡¨ç¤ºâ€œå‘å·¦ç§»åŠ¨â€ã€‚ä¸æ ‡å‡†äºŒå‰æ ‘ç›¸åï¼Œå“ˆå¤«æ›¼æ ‘æ˜¯è‡ªåº•å‘ä¸Šæ„å»ºçš„ã€‚æ„é€ å§‹äºåˆå§‹åŒ–ä¸€ä¸ªæœ€å°å †ä¼˜å…ˆé˜Ÿåˆ—ï¼Œå…¶ä¸­åŒ…å«è¯­æ–™åº“ä¸­çš„æ¯ä¸ªæ ‡è®°ï¼Œä¼˜å…ˆçº§å¯¹åº”äºæ ‡è®°é¢‘ç‡ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œè¯­æ–™åº“ä¸­æœ€ä¸é¢‘ç¹çš„ä¸¤ä¸ªæ ‡è®°è¢«ç§»é™¤ï¼Œå¹¶æˆä¸ºä¸€ä¸ªçˆ¶ä¼ªæ ‡è®°çš„å­èŠ‚ç‚¹ï¼Œå…¶â€œé¢‘ç‡â€æ˜¯å…¶å­èŠ‚ç‚¹é¢‘ç‡çš„æ€»å’Œã€‚å°†è¿™ä¸ªæ–°çš„çˆ¶ä¼ªæ ‡è®°æ·»åŠ åˆ°ä¼˜å…ˆé˜Ÿåˆ—ä¸­ï¼Œå¹¶é€’å½’é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æ²¡æœ‰æ ‡è®°å‰©ä½™ã€‚

    # å‚æ•°
    # text: å­—ç¬¦ä¸²åˆ—è¡¨æˆ–Vocabularyç±»çš„å®ä¾‹
    #     æ ‡è®°åŒ–çš„æ–‡æœ¬æˆ–ç”¨äºæ„å»ºå“ˆå¤«æ›¼ç¼–ç çš„é¢„è®­ç»ƒVocabularyå¯¹è±¡ã€‚
    
    def fit(self, text):
        # æ„å»ºå“ˆå¤«æ›¼æ ‘
        self._build_tree(text)
        # ç”Ÿæˆç¼–ç 
        self._generate_codes()
    def transform(self, text):
        """
        Transform the words in `text` into their Huffman-code representations.

        Parameters
        ----------
        text: list of `N` strings
            The list of words to encode

        Returns
        -------
        codes : list of `N` binary strings
            The encoded words in `text`
        """
        # å¦‚æœè¾“å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è½¬æ¢ä¸ºåŒ…å«è¯¥å­—ç¬¦ä¸²çš„åˆ—è¡¨
        if isinstance(text, str):
            text = [text]
        # éå†æ–‡æœ¬ä¸­çš„æ¯ä¸ªå•è¯
        for token in set(text):
            # å¦‚æœå•è¯ä¸åœ¨ Huffman æ ‘ä¸­ï¼Œåˆ™æŠ›å‡ºè­¦å‘Šå¹¶è·³è¿‡
            if token not in self._item2code:
                raise Warning("Token '{}' not in Huffman tree. Skipping".format(token))
        # è¿”å›æ¯ä¸ªå•è¯çš„ Huffman ç¼–ç 
        return [self._item2code.get(t, None) for t in text]

    def inverse_transform(self, codes):
        """
        Transform an encoded sequence of bit-strings back into words.

        Parameters
        ----------
        codes : list of `N` binary strings
            A list of encoded bit-strings, represented as strings.

        Returns
        -------
        text: list of `N` strings
            The decoded text.
        """
        # å¦‚æœè¾“å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è½¬æ¢ä¸ºåŒ…å«è¯¥å­—ç¬¦ä¸²çš„åˆ—è¡¨
        if isinstance(codes, str):
            codes = [codes]
        # éå†ç¼–ç åºåˆ—ä¸­çš„æ¯ä¸ªç¼–ç 
        for code in set(codes):
            # å¦‚æœç¼–ç ä¸åœ¨ Huffman æ ‘ä¸­ï¼Œåˆ™æŠ›å‡ºè­¦å‘Šå¹¶è·³è¿‡
            if code not in self._code2item:
                raise Warning("Code '{}' not in Huffman tree. Skipping".format(code))
        # è¿”å›æ¯ä¸ªç¼–ç å¯¹åº”çš„å•è¯
        return [self._code2item.get(c, None) for c in codes]

    @property
    def tokens(self):
        """A list the unique tokens in `text`"""
        # è¿”å› Huffman æ ‘ä¸­çš„æ‰€æœ‰å”¯ä¸€å•è¯
        return list(self._item2code.keys())

    @property
    def codes(self):
        """A list with the Huffman code for each unique token in `text`"""
        # è¿”å› Huffman æ ‘ä¸­æ¯ä¸ªå”¯ä¸€å•è¯çš„ Huffman ç¼–ç 
        return list(self._code2item.keys())

    def _counter(self, text):
        counts = {}
        # ç»Ÿè®¡æ–‡æœ¬ä¸­æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°
        for item in text:
            counts[item] = counts.get(item, 0) + 1
        return counts
    # æ„å»ºå“ˆå¤«æ›¼æ ‘
    def _build_tree(self, text):
        """Construct Huffman Tree"""
        # åˆå§‹åŒ–ä¼˜å…ˆé˜Ÿåˆ—
        PQ = []

        # å¦‚æœè¾“å…¥æ˜¯ Vocabulary å¯¹è±¡ï¼Œåˆ™ä½¿ç”¨å…¶ counts å±æ€§
        if isinstance(text, Vocabulary):
            counts = text.counts
        else:
            # å¦åˆ™ä½¿ç”¨ _counter æ–¹æ³•è®¡ç®—é¢‘ç‡
            counts = self._counter(text)

        # å°†æ¯ä¸ªå­—ç¬¦åŠå…¶é¢‘ç‡ä½œä¸ºèŠ‚ç‚¹åŠ å…¥ä¼˜å…ˆé˜Ÿåˆ—
        for (k, c) in counts.items():
            PQ.append(Node(k, c))

        # åˆ›å»ºä¸€ä¸ªä¼˜å…ˆé˜Ÿåˆ—ï¼Œä¼˜å…ˆçº§ä¸ºé¢‘ç‡
        heapq.heapify(PQ)

        # æ„å»ºå“ˆå¤«æ›¼æ ‘
        while len(PQ) > 1:
            node1 = heapq.heappop(PQ)  # å¼¹å‡ºé¢‘ç‡æœ€å°çš„èŠ‚ç‚¹
            node2 = heapq.heappop(PQ)  # å¼¹å‡ºé¢‘ç‡ç¬¬äºŒå°çš„èŠ‚ç‚¹

            parent = Node(None, node1.val + node2.val)
            parent.left = node1
            parent.right = node2

            heapq.heappush(PQ, parent)

        self._root = heapq.heappop(PQ)

    # ç”Ÿæˆç¼–ç 
    def _generate_codes(self):
        current_code = ""
        self._item2code = {}
        self._code2item = {}
        self._build_code(self._root, current_code)

    # é€’å½’æ„å»ºç¼–ç 
    def _build_code(self, root, current_code):
        if root is None:
            return

        if root.key is not None:
            # å°†å¶å­èŠ‚ç‚¹çš„å­—ç¬¦ä¸ç¼–ç å¯¹åº”å­˜å‚¨
            self._item2code[root.key] = current_code
            self._code2item[current_code] = root.key
            return

        # 0 = å‘å·¦ç§»åŠ¨ï¼Œ1 = å‘å³ç§»åŠ¨
        self._build_code(root.left, current_code + "0")
        self._build_code(root.right, current_code + "1")
# å®šä¹‰ Token ç±»ï¼Œç”¨äºè¡¨ç¤ºä¸€ä¸ªå•è¯çš„è®¡æ•°å’Œå†…å®¹
class Token:
    def __init__(self, word):
        # åˆå§‹åŒ–å•è¯è®¡æ•°ä¸º 0
        self.count = 0
        # åˆå§‹åŒ–å•è¯å†…å®¹
        self.word = word

    def __repr__(self):
        """A string representation of the token"""
        # è¿”å› Token å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼ŒåŒ…æ‹¬å•è¯å†…å®¹å’Œè®¡æ•°
        return "Token(word='{}', count={})".format(self.word, self.count)


# å®šä¹‰ TFIDFEncoder ç±»ï¼Œç”¨äºè®¡ç®— TF-IDF ç¼–ç 
class TFIDFEncoder:
    def __init__(
        self,
        vocab=None,
        lowercase=True,
        min_count=0,
        smooth_idf=True,
        max_tokens=None,
        input_type="files",
        filter_stopwords=True,
        filter_punctuation=True,
        tokenizer="words",
    ):
        # åˆå§‹åŒ– TFIDFEncoder å¯¹è±¡çš„å„ç§å‚æ•°

    # å®šä¹‰å†…éƒ¨æ–¹æ³• _encode_documentï¼Œç”¨äºå¯¹æ–‡æ¡£è¿›è¡Œç¼–ç 
    def _encode_document(
        self, doc, word2idx, idx2word, tokens, doc_count, bol_ix, eol_ix,
    ):
        """Perform tokenization and compute token counts for a single document"""
        # è·å–è¶…å‚æ•°
        H = self.hyperparameters
        # æ˜¯å¦è½¬æ¢ä¸ºå°å†™
        lowercase = H["lowercase"]
        # æ˜¯å¦è¿‡æ»¤åœç”¨è¯
        filter_stop = H["filter_stopwords"]
        # æ˜¯å¦è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
        filter_punc = H["filter_punctuation"]

        # å¦‚æœè¾“å…¥ç±»å‹ä¸ºæ–‡ä»¶
        if H["input_type"] == "files":
            # æ‰“å¼€æ–‡ä»¶å¹¶è¯»å–å†…å®¹
            with open(doc, "r", encoding=H["encoding"]) as handle:
                doc = handle.read()

        # å®šä¹‰ä¸åŒç±»å‹çš„åˆ†è¯å™¨
        tokenizer_dict = {
            "words": tokenize_words,
            "characters": tokenize_chars,
            "whitespace": tokenize_whitespace,
            "bytes": tokenize_bytes_raw,
        }
        # æ ¹æ®è¶…å‚æ•°é€‰æ‹©ç›¸åº”çš„åˆ†è¯å™¨
        tokenizer = tokenizer_dict[H["tokenizer"]]

        # åˆå§‹åŒ–å•è¯æ•°é‡
        n_words = 0
        # å°†æ–‡æ¡£æŒ‰è¡Œåˆ†å‰²
        lines = doc.split("\n")
        # éå†æ¯ä¸€è¡Œ
        for line in lines:
            # å¯¹æ¯ä¸€è¡Œè¿›è¡Œåˆ†è¯
            words = tokenizer(
                line,
                lowercase=lowercase,
                filter_stopwords=filter_stop,
                filter_punctuation=filter_punc,
                encoding=H["encoding"],
            )
            # è¿‡æ»¤è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„è¯
            words = self._filter_vocab(words)
            # æ›´æ–°å•è¯æ•°é‡
            n_words += len(words)

            # éå†æ¯ä¸ªè¯
            for ww in words:
                # å¦‚æœè¯ä¸åœ¨ word2idx ä¸­ï¼Œåˆ™æ·»åŠ 
                if ww not in word2idx:
                    word2idx[ww] = len(tokens)
                    idx2word[len(tokens)] = ww
                    tokens.append(Token(ww))

                # è·å–è¯çš„ç´¢å¼•
                t_idx = word2idx[ww]
                # æ›´æ–°è¯é¢‘
                tokens[t_idx].count += 1
                # æ›´æ–°æ–‡æ¡£ä¸­è¯çš„å‡ºç°æ¬¡æ•°
                doc_count[t_idx] = doc_count.get(t_idx, 0) + 1

            # åœ¨æ¯è¡Œå¼€å¤´å’Œç»“å°¾æ·»åŠ  <bol> å’Œ <eol> æ ‡ç­¾
            tokens[bol_ix].count += 1
            tokens[eol_ix].count += 1

            doc_count[bol_ix] = doc_count.get(bol_ix, 0) + 1
            doc_count[eol_ix] = doc_count.get(eol_ix, 0) + 1
        # è¿”å›å•è¯åˆ°ç´¢å¼•çš„æ˜ å°„ã€ç´¢å¼•åˆ°å•è¯çš„æ˜ å°„ã€å•è¯åˆ—è¡¨ã€æ–‡æ¡£ä¸­å•è¯å‡ºç°æ¬¡æ•°
        return word2idx, idx2word, tokens, doc_count
    # ä¿ç•™å‰ N ä¸ªæœ€é¢‘ç¹å‡ºç°çš„è¯æ±‡
    def _keep_top_n_tokens(self):
        # è·å–æœ€å¤§è¯æ±‡æ•°
        N = self.hyperparameters["max_tokens"]
        # åˆå§‹åŒ–è¯æ±‡è®¡æ•°ã€è¯æ±‡åˆ°ç´¢å¼•ã€ç´¢å¼•åˆ°è¯æ±‡çš„å­—å…¸
        doc_counts, word2idx, idx2word = {}, {}, {}
        # æ ¹æ®è¯æ±‡å‡ºç°æ¬¡æ•°æ’åºè¯æ±‡åˆ—è¡¨
        tokens = sorted(self._tokens, key=lambda x: x.count, reverse=True)

        # é‡æ–°ç´¢å¼•å‰ N ä¸ªè¯æ±‡...
        unk_ix = None
        for idx, tt in enumerate(tokens[:N]):
            word2idx[tt.word] = idx
            idx2word[idx] = tt.word

            # å¦‚æœ <unk> ä¸åœ¨å‰ N ä¸ªè¯æ±‡ä¸­ï¼Œå°†å…¶æ·»åŠ è¿›å»ï¼Œæ›¿æ¢ç¬¬ N ä¸ªæœ€é¢‘ç¹å‡ºç°çš„è¯æ±‡ï¼Œå¹¶ç›¸åº”è°ƒæ•´ <unk> çš„è®¡æ•°...
            if tt.word == "<unk>":
                unk_ix = idx

        # ... æœ€åï¼Œå°†æ‰€æœ‰è¢«åˆ é™¤çš„è¯æ±‡é‡æ–°ç¼–ç ä¸º "<unk>"
        for tt in tokens[N:]:
            tokens[unk_ix].count += tt.count

        # ... æœ€åï¼Œé‡æ–°ä¸ºæ¯ä¸ªæ–‡æ¡£é‡æ–°ç´¢å¼•è¯æ±‡è®¡æ•°
        for d_ix in self.term_freq.keys():
            doc_counts[d_ix] = {}
            for old_ix, d_count in self.term_freq[d_ix].items():
                word = self.idx2token[old_ix]
                new_ix = word2idx.get(word, unk_ix)
                doc_counts[d_ix][new_ix] = doc_counts[d_ix].get(new_ix, 0) + d_count

        # æ›´æ–°è¯æ±‡åˆ—è¡¨ã€è¯æ±‡åˆ°ç´¢å¼•ã€ç´¢å¼•åˆ°è¯æ±‡çš„å­—å…¸ä»¥åŠæ–‡æ¡£è¯é¢‘
        self._tokens = tokens[:N]
        self.token2idx = word2idx
        self.idx2token = idx2word
        self.term_freq = doc_counts

        # æ–­è¨€è¯æ±‡åˆ—è¡¨é•¿åº¦ä¸è¶…è¿‡ N
        assert len(self._tokens) <= N
    def _drop_low_freq_tokens(self):
        """
        æ›¿æ¢æ‰€æœ‰å‡ºç°æ¬¡æ•°å°‘äº `min_count` çš„æ ‡è®°ä¸º `<unk>` æ ‡è®°ã€‚
        """
        H = self.hyperparameters
        # è·å– `<unk>` æ ‡è®°çš„ç´¢å¼•
        unk_token = self._tokens[self.token2idx["<unk>"]]
        # è·å– `<eol>` æ ‡è®°çš„ç´¢å¼•
        eol_token = self._tokens[self.token2idx["<eol>"]]
        # è·å– `<bol>` æ ‡è®°çš„ç´¢å¼•
        bol_token = self._tokens[self.token2idx["<bol>"]]
        # åˆå§‹åŒ–ç‰¹æ®Šæ ‡è®°åˆ—è¡¨
        tokens = [unk_token, eol_token, bol_token]

        # åˆå§‹åŒ– `<unk>` æ ‡è®°çš„ç´¢å¼•
        unk_idx = 0
        # åˆå§‹åŒ–ç‰¹æ®Šæ ‡è®°åˆ°ç´¢å¼•çš„æ˜ å°„
        word2idx = {"<unk>": 0, "<eol>": 1, "<bol>": 2}
        # åˆå§‹åŒ–ç´¢å¼•åˆ°ç‰¹æ®Šæ ‡è®°çš„æ˜ å°„
        idx2word = {0: "<unk>", 1: "<eol>", 2: "<bol>"}
        # åˆå§‹åŒ–ç‰¹æ®Šæ ‡è®°é›†åˆ
        special = {"<eol>", "<bol>", "<unk>"}

        # éå†æ‰€æœ‰æ ‡è®°
        for tt in self._tokens:
            # å¦‚æœæ ‡è®°ä¸æ˜¯ç‰¹æ®Šæ ‡è®°
            if tt.word not in special:
                # å¦‚æœæ ‡è®°å‡ºç°æ¬¡æ•°å°äº `min_count`
                if tt.count < H["min_count"]:
                    # å°†å‡ºç°æ¬¡æ•°åŠ åˆ° `<unk>` æ ‡è®°ä¸Š
                    tokens[unk_idx].count += tt.count
                else:
                    # æ›´æ–°æ ‡è®°åˆ°ç´¢å¼•çš„æ˜ å°„
                    word2idx[tt.word] = len(tokens)
                    # æ›´æ–°ç´¢å¼•åˆ°æ ‡è®°çš„æ˜ å°„
                    idx2word[len(tokens)] = tt.word
                    # æ·»åŠ æ ‡è®°åˆ°åˆ—è¡¨ä¸­
                    tokens.append(tt)

        # é‡æ–°ç´¢å¼•æ–‡æ¡£è®¡æ•°
        doc_counts = {}
        for d_idx in self.term_freq.keys():
            doc_counts[d_idx] = {}
            for old_idx, d_count in self.term_freq[d_idx].items():
                word = self.idx2token[old_idx]
                new_idx = word2idx.get(word, unk_idx)
                doc_counts[d_idx][new_idx] = doc_counts[d_idx].get(new_idx, 0) + d_count

        # æ›´æ–°æ ‡è®°åˆ—è¡¨
        self._tokens = tokens
        # æ›´æ–°æ ‡è®°åˆ°ç´¢å¼•çš„æ˜ å°„
        self.token2idx = word2idx
        # æ›´æ–°ç´¢å¼•åˆ°æ ‡è®°çš„æ˜ å°„
        self.idx2token = idx2word
        # æ›´æ–°æ–‡æ¡£è®¡æ•°
        self.term_freq = doc_counts
    # å¯¹ tokens è¿›è¡Œæ’åºï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åºå¹¶é‡æ–°ç¼–ç 
    def _sort_tokens(self):
        # åˆå§‹åŒ–ç´¢å¼•
        ix = 0
        # åˆå§‹åŒ– token åˆ°ç´¢å¼•å’Œç´¢å¼•åˆ° token çš„å­—å…¸
        token2idx, idx2token, = (
            {},
            {},
        )
        # ç‰¹æ®Š token åˆ—è¡¨
        special = ["<eol>", "<bol>", "<unk>"]
        # å¯¹ token2idx å­—å…¸ä¸­çš„é”®è¿›è¡Œæ’åº
        words = sorted(self.token2idx.keys())
        # åˆå§‹åŒ– term_freq å­—å…¸
        term_freq = {d: {} for d in self.term_freq.keys()}

        # éå†æ’åºåçš„ tokens
        for w in words:
            # å¦‚æœå½“å‰ token ä¸åœ¨ç‰¹æ®Š token åˆ—è¡¨ä¸­
            if w not in special:
                # è·å–å½“å‰ token çš„æ—§ç´¢å¼•
                old_ix = self.token2idx[w]
                # æ›´æ–° token2idx å’Œ idx2token å­—å…¸
                token2idx[w], idx2token[ix] = ix, w
                # æ›´æ–° term_freq å­—å…¸
                for d in self.term_freq.keys():
                    if old_ix in self.term_freq[d]:
                        count = self.term_freq[d][old_ix]
                        term_freq[d][ix] = count
                ix += 1

        # å¤„ç†ç‰¹æ®Š token
        for w in special:
            token2idx[w] = len(token2idx)
            idx2token[len(idx2token)] = w

        # æ›´æ–°å¯¹è±¡çš„ token2idxã€idx2tokenã€term_freq å’Œ vocab_counts å±æ€§
        self.token2idx = token2idx
        self.idx2token = idx2token
        self.term_freq = term_freq
        self.vocab_counts = Counter({t.word: t.count for t in self._tokens})
    def _calc_idf(self):
        """
        è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªæ ‡è®°çš„ï¼ˆå¹³æ»‘çš„ï¼‰é€†æ–‡æ¡£é¢‘ç‡ã€‚

        å¯¹äºä¸€ä¸ªå•è¯æ ‡è®° `w`ï¼ŒIDF ç®€å•åœ°å®šä¹‰ä¸º

            IDF(w) = log ( |D| / |{ d in D: w in d }| ) + 1

        å…¶ä¸­ D æ˜¯è¯­æ–™åº“ä¸­æ‰€æœ‰æ–‡æ¡£çš„é›†åˆï¼Œ

            D = {d1, d2, ..., dD}

        å¦‚æœ `smooth_idf` ä¸º Trueï¼Œæˆ‘ä»¬å¯¹åŒ…å«ç»™å®šå•è¯çš„æ–‡æ¡£æ•°é‡è¿›è¡ŒåŠ æ³•å¹³æ»‘å¤„ç†ï¼Œç›¸å½“äºå‡è®¾å­˜åœ¨ç¬¬ D+1 ä¸ªæ–‡æ¡£ï¼Œå…¶ä¸­åŒ…å«è¯­æ–™åº“ä¸­çš„æ¯ä¸ªå•è¯ï¼š

            SmoothedIDF(w) = log ( |D| + 1 / [1 + |{ d in D: w in d }|] ) + 1
        """
        inv_doc_freq = {}
        smooth_idf = self.hyperparameters["smooth_idf"]
        tf, doc_idxs = self.term_freq, self._idx2doc.keys()

        D = len(self._idx2doc) + int(smooth_idf)
        for word, w_ix in self.token2idx.items():
            d_count = int(smooth_idf)
            d_count += np.sum([1 if w_ix in tf[d_ix] else 0 for d_ix in doc_idxs])
            inv_doc_freq[w_ix] = 1 if d_count == 0 else np.log(D / d_count) + 1
        self.inv_doc_freq = inv_doc_freq
    def transform(self, ignore_special_chars=True):
        """
        ç”Ÿæˆæ–‡æœ¬è¯­æ–™åº“çš„è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ç¼–ç ã€‚

        Parameters
        ----------
        ignore_special_chars : bool
            æ˜¯å¦ä»æœ€ç»ˆçš„tfidfç¼–ç ä¸­åˆ é™¤ä¸"<eol>", "<bol>", "<unk>"æ ‡è®°å¯¹åº”çš„åˆ—ã€‚é»˜è®¤ä¸ºTrueã€‚

        Returns
        -------
        tfidf : numpy array of shape `(D, M [- 3])`
            ç¼–ç åçš„è¯­æ–™åº“ï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªæ–‡æ¡£ï¼Œæ¯åˆ—å¯¹åº”ä¸€ä¸ªæ ‡è®°IDã€‚å¦‚æœ`ignore_special_chars`ä¸ºFalseï¼Œåˆ™åœ¨`idx2token`å±æ€§ä¸­å­˜å‚¨åˆ—å·ä¸æ ‡è®°ä¹‹é—´çš„æ˜ å°„ã€‚å¦åˆ™ï¼Œæ˜ å°„ä¸å‡†ç¡®ã€‚
        """
        D, N = len(self._idx2doc), len(self._tokens)
        # åˆå§‹åŒ–è¯é¢‘çŸ©é˜µå’Œé€†æ–‡æ¡£é¢‘ç‡çŸ©é˜µ
        tf = np.zeros((D, N))
        idf = np.zeros((D, N))

        # éå†æ–‡æ¡£ç´¢å¼•
        for d_ix in self._idx2doc.keys():
            # è·å–æ–‡æ¡£ä¸­çš„è¯å’Œè¯é¢‘
            words, counts = zip(*self.term_freq[d_ix].items())
            # åˆ›å»ºæ–‡æ¡£ç´¢å¼•æ•°ç»„
            docs = np.ones(len(words), dtype=int) * d_ix
            # æ›´æ–°è¯é¢‘çŸ©é˜µ
            tf[docs, words] = counts

        # è·å–æ‰€æœ‰è¯çš„æ’åºåˆ—è¡¨
        words = sorted(self.idx2token.keys())
        # æ ¹æ®è¯çš„é€†æ–‡æ¡£é¢‘ç‡åˆ›å»ºçŸ©é˜µ
        idf = np.tile(np.array([self.inv_doc_freq[w] for w in words]), (D, 1))
        # è®¡ç®—tfidfçŸ©é˜µ
        tfidf = tf * idf

        # å¦‚æœå¿½ç•¥ç‰¹æ®Šå­—ç¬¦
        if ignore_special_chars:
            # è·å–ç‰¹æ®Šå­—ç¬¦çš„ç´¢å¼•
            idxs = [
                self.token2idx["<unk>"],
                self.token2idx["<eol>"],
                self.token2idx["<bol>"],
            ]
            # ä»tfidfçŸ©é˜µä¸­åˆ é™¤ç‰¹æ®Šå­—ç¬¦åˆ—
            tfidf = np.delete(tfidf, idxs, 1)

        # è¿”å›tfidfçŸ©é˜µ
        return tfidf
# å®šä¹‰ä¸€ä¸ªåä¸º Vocabulary çš„ç±»
class Vocabulary:
    # åˆå§‹åŒ–æ–¹æ³•ï¼Œè®¾ç½®ç±»çš„å±æ€§
    def __init__(
        self,
        lowercase=True,  # æ˜¯å¦å°†å•è¯è½¬æ¢ä¸ºå°å†™ï¼Œé»˜è®¤ä¸ºTrue
        min_count=None,  # å•è¯æœ€å°å‡ºç°æ¬¡æ•°ï¼Œé»˜è®¤ä¸ºNone
        max_tokens=None,  # æœ€å¤§å•è¯æ•°é‡ï¼Œé»˜è®¤ä¸ºNone
        filter_stopwords=True,  # æ˜¯å¦è¿‡æ»¤åœç”¨è¯ï¼Œé»˜è®¤ä¸ºTrue
        filter_punctuation=True,  # æ˜¯å¦è¿‡æ»¤æ ‡ç‚¹ç¬¦å·ï¼Œé»˜è®¤ä¸ºTrue
        tokenizer="words",  # åˆ†è¯å™¨ç±»å‹ï¼Œé»˜è®¤ä¸º"words"
    ):
        """
        ç”¨äºç¼–è¯‘å’Œç¼–ç æ–‡æœ¬è¯­æ–™åº“ä¸­å”¯ä¸€æ ‡è®°çš„å¯¹è±¡ã€‚

        å‚æ•°
        ----------
        lowercase : bool
            æ˜¯å¦åœ¨æ ‡è®°åŒ–ä¹‹å‰å°†æ¯ä¸ªå­—ç¬¦ä¸²è½¬æ¢ä¸ºå°å†™ã€‚
            é»˜è®¤ä¸º Trueã€‚
        min_count : int
            æ ‡è®°å¿…é¡»å‡ºç°çš„æœ€å°æ¬¡æ•°æ‰èƒ½åŒ…å«åœ¨è¯æ±‡è¡¨ä¸­ã€‚
            å¦‚æœä¸º `None`ï¼Œåˆ™åœ¨è¯æ±‡è¡¨ä¸­åŒ…å«æ¥è‡ª `corpus_fp` çš„æ‰€æœ‰æ ‡è®°ã€‚
            é»˜è®¤ä¸º Noneã€‚
        max_tokens : int
            ä»…å°†å‡ºç°æ¬¡æ•°è¶…è¿‡ `min_count` çš„å‰ `max_tokens` ä¸ªæœ€å¸¸è§æ ‡è®°æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚
            å¦‚æœä¸º Noneï¼Œåˆ™æ·»åŠ æ‰€æœ‰å‡ºç°æ¬¡æ•°è¶…è¿‡ `min_count` çš„æ ‡è®°ã€‚
            é»˜è®¤ä¸º Noneã€‚
        filter_stopwords : bool
            æ˜¯å¦åœ¨å¯¹è¯­æ–™åº“ä¸­çš„å•è¯è¿›è¡Œç¼–ç ä¹‹å‰åˆ é™¤åœç”¨è¯ã€‚
            é»˜è®¤ä¸º Trueã€‚
        filter_punctuation : bool
            æ˜¯å¦åœ¨å¯¹è¯­æ–™åº“ä¸­çš„å•è¯è¿›è¡Œç¼–ç ä¹‹å‰åˆ é™¤æ ‡ç‚¹ç¬¦å·ã€‚
            é»˜è®¤ä¸º Trueã€‚
        tokenizer : {'whitespace', 'words', 'characters', 'bytes'}
            åœ¨å°†å­—ç¬¦ä¸²æ˜ å°„åˆ°æ ‡è®°æ—¶è¦éµå¾ªçš„ç­–ç•¥ã€‚ 
            `'whitespace'` æ ‡è®°åŒ–å™¨åœ¨ç©ºæ ¼å­—ç¬¦å¤„æ‹†åˆ†å­—ç¬¦ä¸²ã€‚
            `'words'` æ ‡è®°åŒ–å™¨ä½¿ç”¨â€œå•è¯â€æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†å­—ç¬¦ä¸²ã€‚
            `'characters'` æ ‡è®°åŒ–å™¨å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦ã€‚
            `'bytes'` æ ‡è®°åŒ–å™¨å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºä¸€ç»„å•ä¸ªå­—èŠ‚ã€‚
        """
        self.hyperparameters = {
            "id": "Vocabulary",
            "encoding": None,
            "corpus_fps": None,
            "lowercase": lowercase,
            "min_count": min_count,
            "max_tokens": max_tokens,
            "filter_stopwords": filter_stopwords,
            "filter_punctuation": filter_punctuation,
            "tokenizer": tokenizer,
        }

    def __len__(self):
        """è¿”å›è¯æ±‡è¡¨ä¸­æ ‡è®°çš„æ•°é‡"""
        return len(self._tokens)
    # è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºéå†è¯æ±‡è¡¨ä¸­çš„æ ‡è®°
    def __iter__(self):
        return iter(self._tokens)

    # åˆ¤æ–­ç»™å®šçš„å•è¯æ˜¯å¦æ˜¯è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªæ ‡è®°
    def __contains__(self, word):
        return word in self.token2idx

    # æ ¹æ®é”®è¿”å›è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ï¼ˆå¦‚æœé”®æ˜¯æ•´æ•°ï¼‰æˆ–ç´¢å¼•ï¼ˆå¦‚æœé”®æ˜¯å­—ç¬¦ä¸²ï¼‰
    def __getitem__(self, key):
        if isinstance(key, str):
            return self._tokens[self.token2idx[key]]
        if isinstance(key, int):
            return self._tokens[key]

    # è¿”å›è¯æ±‡è¡¨ä¸­å”¯ä¸€å•è¯æ ‡è®°çš„æ•°é‡
    @property
    def n_tokens(self):
        return len(self.token2idx)

    # è¿”å›è¯­æ–™åº“ä¸­å•è¯çš„æ€»æ•°
    @property
    def n_words(self):
        return sum(self.counts.values())

    # è¿”å›è¯æ±‡è¡¨ä¸­å”¯ä¸€å•è¯æ ‡è®°çš„å½¢çŠ¶
    @property
    def shape(self):
        return self._tokens.shape

    # è¿”å›è¯­æ–™åº“ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„å‰nä¸ªæ ‡è®°
    def most_common(self, n=5):
        return self.counts.most_common()[:n]

    # è¿”å›åœ¨è¯­æ–™åº“ä¸­å‡ºç°kæ¬¡çš„æ‰€æœ‰æ ‡è®°
    def words_with_count(self, k):
        return [w for w, c in self.counts.items() if c == k]
    def filter(self, words, unk=True):  # noqa: A003
        """
        Filter (or replace) any word in `words` that is not present in
        `Vocabulary`.

        Parameters
        ----------
        words : list of strs
            A list of words to filter
        unk : bool
            Whether to replace any out of vocabulary words in `words` with the
            ``<unk>`` token (True) or skip them entirely (False).  Default is
            True.

        Returns
        -------
        filtered : list of strs
            The list of words filtered against the words in Vocabulary.
        """
        # å¦‚æœ unk ä¸º Trueï¼Œåˆ™å°†ä¸åœ¨ Vocabulary ä¸­çš„å•è¯æ›¿æ¢ä¸º "<unk>"ï¼Œå¦åˆ™è·³è¿‡
        if unk:
            return [w if w in self else "<unk>" for w in words]
        # å¦‚æœ unk ä¸º Falseï¼Œåˆ™åªä¿ç•™åœ¨ Vocabulary ä¸­çš„å•è¯
        return [w for w in words if w in self]

    def words_to_indices(self, words):
        """
        Convert the words in `words` to their token indices. If a word is not
        in the vocabulary, return the index for the ``<unk>`` token

        Parameters
        ----------
        words : list of strs
            A list of words to filter

        Returns
        -------
        indices : list of ints
            The token indices for each word in `words`
        """
        # è·å– "<unk>" çš„ç´¢å¼•
        unk_ix = self.token2idx["<unk>"]
        # è·å–æ˜¯å¦è½¬æ¢ä¸ºå°å†™çš„è®¾ç½®
        lowercase = self.hyperparameters["lowercase"]
        # å¦‚æœéœ€è¦è½¬æ¢ä¸ºå°å†™ï¼Œåˆ™å°†å•è¯åˆ—è¡¨ä¸­çš„å•è¯è½¬æ¢ä¸ºå°å†™
        words = [w.lower() for w in words] if lowercase else words
        # å°†å•è¯è½¬æ¢ä¸ºå®ƒä»¬åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ï¼Œå¦‚æœä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™è¿”å› "<unk>" çš„ç´¢å¼•
        return [self.token2idx[w] if w in self else unk_ix for w in words]

    def indices_to_words(self, indices):
        """
        Convert the indices in `indices` to their word values. If an index is
        not in the vocabulary, return the ``<unk>`` token.

        Parameters
        ----------
        indices : list of ints
            The token indices for each word in `words`

        Returns
        -------
        words : list of strs
            The word strings corresponding to each token index in `indices`
        """
        # è®¾ç½® "<unk>" æ ‡è®°
        unk = "<unk>"
        # å°†ç´¢å¼•è½¬æ¢ä¸ºå¯¹åº”çš„å•è¯ï¼Œå¦‚æœç´¢å¼•ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™è¿”å› "<unk>"
        return [self.idx2token[i] if i in self.idx2token else unk for i in indices]
    # ä¿ç•™è¯æ±‡è¡¨ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„å‰ N ä¸ªè¯çš„ç´¢å¼•
    def _keep_top_n_tokens(self):
        # åˆå§‹åŒ–ç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨è¯æ±‡è¡¨ä¸­è¯è¯­åˆ°ç´¢å¼•çš„æ˜ å°„å…³ç³»
        word2idx, idx2word = {}, {}
        # è·å–æœ€å¤§è¯æ±‡é‡ N
        N = self.hyperparameters["max_tokens"]
        # æ ¹æ®è¯é¢‘å¯¹è¯æ±‡è¡¨ä¸­çš„è¯è¿›è¡Œæ’åº
        tokens = sorted(self._tokens, key=lambda x: x.count, reverse=True)

        # é‡æ–°ç´¢å¼•å‰ N ä¸ªè¯...
        unk_ix = None
        for idx, tt in enumerate(tokens[:N]):
            # å°†è¯è¯­å’Œå¯¹åº”çš„ç´¢å¼•å­˜å…¥å­—å…¸ä¸­
            word2idx[tt.word] = idx
            idx2word[idx] = tt.word

            # å¦‚æœè¯è¯­æ˜¯ "<unk>"ï¼Œè®°å½•å…¶ç´¢å¼•
            if tt.word == "<unk>":
                unk_ix = idx

        # ... å¦‚æœ "<unk>" ä¸åœ¨å‰ N ä¸ªè¯ä¸­ï¼Œå°†å…¶æ·»åŠ è¿›å»ï¼Œæ›¿æ¢ç¬¬ N ä¸ªæœ€å¸¸è§çš„è¯ï¼Œå¹¶ç›¸åº”è°ƒæ•´ "<unk>" çš„è®¡æ•° ...
        if unk_ix is None:
            unk_ix = self.token2idx["<unk>"]
            old_count = tokens[N - 1].count
            tokens[N - 1] = self._tokens[unk_ix]
            tokens[N - 1].count += old_count
            word2idx["<unk>"] = N - 1
            idx2word[N - 1] = "<unk>"

        # ... å°†æ‰€æœ‰è¢«åˆ é™¤çš„è¯é‡æ–°ç¼–ç ä¸º "<unk>"
        for tt in tokens[N:]:
            tokens[unk_ix].count += tt.count

        # æ›´æ–°è¯æ±‡è¡¨ä¸ºå‰ N ä¸ªè¯
        self._tokens = tokens[:N]
        self.token2idx = word2idx
        self.idx2token = idx2word

        # æ–­è¨€è¯æ±‡è¡¨é•¿åº¦ä¸è¶…è¿‡ N
        assert len(self._tokens) <= N
    def _drop_low_freq_tokens(self):
        """
        Replace all tokens that occur less than `min_count` with the `<unk>`
        token.
        """
        # è·å– `<unk>` token çš„ç´¢å¼•
        unk_idx = 0
        # è·å– `<unk>`ã€`<eol>`ã€`<bol>` token å¯¹åº”çš„ç´¢å¼•
        unk_token = self._tokens[self.token2idx["<unk>"]]
        eol_token = self._tokens[self.token2idx["<eol>"]]
        bol_token = self._tokens[self.token2idx["<bol>"]]

        # è·å–è¶…å‚æ•°
        H = self.hyperparameters
        # åˆå§‹åŒ–ç‰¹æ®Š token åˆ—è¡¨
        tokens = [unk_token, eol_token, bol_token]
        # åˆå§‹åŒ–ç‰¹æ®Š token åˆ°ç´¢å¼•çš„æ˜ å°„
        word2idx = {"<unk>": 0, "<eol>": 1, "<bol>": 2}
        # åˆå§‹åŒ–ç´¢å¼•åˆ°ç‰¹æ®Š token çš„æ˜ å°„
        idx2word = {0: "<unk>", 1: "<eol>", 2: "<bol>"}
        # ç‰¹æ®Š token é›†åˆ
        special = {"<eol>", "<bol>", "<unk>"}

        # éå†æ‰€æœ‰ token
        for tt in self._tokens:
            # å¦‚æœ token ä¸æ˜¯ç‰¹æ®Š token
            if tt.word not in special:
                # å¦‚æœ token å‡ºç°æ¬¡æ•°å°äº min_count
                if tt.count < H["min_count"]:
                    # å°†å‡ºç°æ¬¡æ•°å°äº min_count çš„ token æ›¿æ¢ä¸º `<unk>` token
                    tokens[unk_idx].count += tt.count
                else:
                    # æ›´æ–° token åˆ°ç´¢å¼•çš„æ˜ å°„
                    word2idx[tt.word] = len(tokens)
                    # æ›´æ–°ç´¢å¼•åˆ° token çš„æ˜ å°„
                    idx2word[len(tokens)] = tt.word
                    # æ·»åŠ å½“å‰ token åˆ° tokens åˆ—è¡¨ä¸­
                    tokens.append(tt)

        # æ›´æ–° tokens åˆ—è¡¨
        self._tokens = tokens
        # æ›´æ–° token åˆ°ç´¢å¼•çš„æ˜ å°„
        self.token2idx = word2idx
        # æ›´æ–°ç´¢å¼•åˆ° token çš„æ˜ å°„
        self.idx2token = idx2word
```