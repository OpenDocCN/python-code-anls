# CogVideo & CogVideoX å¾®è°ƒä»£ç æºç è§£æï¼ˆåä¸‰ï¼‰



# Video Caption

Typically, most video data does not come with corresponding descriptive text, so it is necessary to convert the video
data into textual descriptions to provide the essential training data for text-to-video models.

## Update and News
- ğŸ”¥ğŸ”¥ **News**: ```py/9/19```: The caption model used in the CogVideoX training process to convert video data into text
  descriptions, [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption), is now open-source. Feel
  free to download and use it.


## Video Caption via CogVLM2-Caption

ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-llama3-caption) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-llama3-caption/) 

CogVLM2-Caption is a video captioning model used to generate training data for the CogVideoX model.

### Install
```py
pip install -r requirements.txt
```

### Usage

```py
python video_caption.py
```

Example:
<div align="center">
    <img width="600px" height="auto" src="./assests/CogVLM2-Caption-example.png">
</div>

## Video Caption via CogVLM2-Video

[Code](https://github.com/THUDM/CogVLM2/tree/main/video_demo) | ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-video-llama3-chat) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-video-llama3-chat) | ğŸ“‘ [Blog](https://cogvlm2-video.github.io/) ï½œ [ğŸ’¬ Online Demo](http://cogvlm2-online.cogviewai.cn:7868/)

CogVLM2-Video is a versatile video understanding model equipped with timestamp-based question answering capabilities.
Users can input prompts such as `Please describe this video in detail.` to the model to obtain a detailed video caption:
<div align="center">
    <a href="https://cogvlm2-video.github.io/"><img width="600px" height="auto" src="./assests/cogvlm2-video-example.png"></a>
</div>

Users can use the provided [code](https://github.com/THUDM/CogVLM2/tree/main/video_demo) to load the model or configure a RESTful API to generate video captions.

## Citation

ğŸŒŸ If you find our work helpful, please leave us a star and cite our paper.

CogVLM2-Caption:
```py
@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}
```
CogVLM2-Video:
```py
@article{hong2024cogvlm2,
  title={CogVLM2: Visual Language Models for Image and Video Understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}
```

# ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³

é€šå¸¸ã€ã»ã¨ã‚“ã©ã®ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã«ã¯å¯¾å¿œã™ã‚‹èª¬æ˜æ–‡ãŒä»˜ã„ã¦ã„ãªã„ãŸã‚ã€ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã®èª¬æ˜ã«å¤‰æ›ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ“ãƒ‡ã‚ªã¸ã®ãƒ¢ãƒ‡ãƒ«ã«å¿…è¦ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## æ›´æ–°ã¨ãƒ‹ãƒ¥ãƒ¼ã‚¹
- ğŸ”¥ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```py/9/19```ï¼šCogVideoX
  ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã§ã€ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ« [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption)
  ãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¾ã—ãŸã€‚ãœã²ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚
## CogVLM2-Captionã«ã‚ˆã‚‹ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³

ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-llama3-caption) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-llama3-caption/) 

CogVLM2-Captionã¯ã€CogVideoXãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```py
pip install -r requirements.txt
```

### ä½¿ç”¨æ–¹æ³•
```py
python video_caption.py
```

ä¾‹:
<div align="center">
    <img width="600px" height="auto" src="./assests/CogVLM2-Caption-example.png">
</div>



## CogVLM2-Video ã‚’ä½¿ç”¨ã—ãŸãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³

[Code](https://github.com/THUDM/CogVLM2/tree/main/video_demo) | ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-video-llama3-chat) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-video-llama3-chat) | ğŸ“‘ [Blog](https://cogvlm2-video.github.io/) ï½œ [ğŸ’¬ Online Demo](http://cogvlm2-online.cogviewai.cn:7868/)


CogVLM2-Video ã¯ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®è³ªå•å¿œç­”æ©Ÿèƒ½ã‚’å‚™ãˆãŸå¤šæ©Ÿèƒ½ãªãƒ“ãƒ‡ã‚ªç†è§£ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ `ã“ã®ãƒ“ãƒ‡ã‚ªã‚’è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚` ãªã©ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã¦ã€è©³ç´°ãªãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã§ãã¾ã™ï¼š
<div align="center">
    <a href="https://cogvlm2-video.github.io/"><img width="600px" height="auto" src="./assests/cogvlm2-video-example.png"></a>
</div>

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æä¾›ã•ã‚ŒãŸ[ã‚³ãƒ¼ãƒ‰](https://github.com/THUDM/CogVLM2/tree/main/video_demo)ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€RESTful API ã‚’æ§‹æˆã—ã¦ãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚

## Citation

ğŸŒŸ If you find our work helpful, please leave us a star and cite our paper.

CogVLM2-Caption:
```py
@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}
```
CogVLM2-Video:
```py
@article{hong2024cogvlm2,
  title={CogVLM2: Visual Language Models for Image and Video Understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}
```


# è§†é¢‘Caption

é€šå¸¸ï¼Œå¤§å¤šæ•°è§†é¢‘æ•°æ®ä¸å¸¦æœ‰ç›¸åº”çš„æè¿°æ€§æ–‡æœ¬ï¼Œå› æ­¤éœ€è¦å°†è§†é¢‘æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œä»¥æä¾›å¿…è¦çš„è®­ç»ƒæ•°æ®ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ã€‚

## é¡¹ç›®æ›´æ–°
- ğŸ”¥ğŸ”¥ **News**: ```py/9/19```: CogVideoX è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºå°†è§†é¢‘æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°çš„ Caption
  æ¨¡å‹ [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption)
  å·²ç»å¼€æºã€‚æ¬¢è¿å‰å¾€ä¸‹è½½å¹¶ä½¿ç”¨ã€‚

## é€šè¿‡ CogVLM2-Caption æ¨¡å‹ç”Ÿæˆè§†é¢‘Caption

ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-llama3-caption) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-llama3-caption/) 

CogVLM2-Captionæ˜¯ç”¨äºç”ŸæˆCogVideoXæ¨¡å‹è®­ç»ƒæ•°æ®çš„è§†é¢‘captionæ¨¡å‹ã€‚

### å®‰è£…ä¾èµ–
```py
pip install -r requirements.txt
```

### è¿è¡Œcaptionæ¨¡å‹

```py
python video_caption.py
```

ç¤ºä¾‹ï¼š
<div align="center">
    <img width="600px" height="auto" src="./assests/CogVLM2-Caption-example.png">
</div>

## é€šè¿‡ CogVLM2-Video æ¨¡å‹ç”Ÿæˆè§†é¢‘Caption

[Code](https://github.com/THUDM/CogVLM2/tree/main/video_demo) | ğŸ¤— [Hugging Face](https://huggingface.co/THUDM/cogvlm2-video-llama3-chat) | ğŸ¤– [ModelScope](https://modelscope.cn/models/ZhipuAI/cogvlm2-video-llama3-chat) | ğŸ“‘ [Blog](https://cogvlm2-video.github.io/) ï½œ [ğŸ’¬ Online Demo](http://cogvlm2-online.cogviewai.cn:7868/)

CogVLM2-Video æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„è§†é¢‘ç†è§£æ¨¡å‹ï¼Œå…·å¤‡åŸºäºæ—¶é—´æˆ³çš„é—®é¢˜å›ç­”èƒ½åŠ›ã€‚ç”¨æˆ·å¯ä»¥è¾“å…¥è¯¸å¦‚ `Describe this video in detail.` çš„æç¤ºè¯­ç»™æ¨¡å‹ï¼Œä»¥è·å¾—è¯¦ç»†çš„è§†é¢‘Captionï¼š


<div align="center">
    <a href="https://cogvlm2-video.github.io/"><img width="600px" height="auto" src="./assests/cogvlm2-video-example.png"></a>
</div>

ç”¨æˆ·å¯ä»¥ä½¿ç”¨æä¾›çš„[ä»£ç ](https://github.com/THUDM/CogVLM2/tree/main/video_demo)åŠ è½½æ¨¡å‹æˆ–é…ç½® RESTful API æ¥ç”Ÿæˆè§†é¢‘Captionã€‚


## Citation

ğŸŒŸ If you find our work helpful, please leave us a star and cite our paper.

CogVLM2-Caption:
```py
@article{yang2024cogvideox,
  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}
```
CogVLM2-Video:
```py
@article{hong2024cogvlm2,
  title={CogVLM2: Visual Language Models for Image and Video Understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}
```

# `.\cogvideo-finetune\tools\caption\video_caption.py`

```py
# å¯¼å…¥å¿…è¦çš„åº“
import io

import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import numpy as np  # ç”¨äºæ•°å€¼è®¡ç®—
import torch  # PyTorchæ·±åº¦å­¦ä¹ åº“
from decord import cpu, VideoReader, bridge  # è§†é¢‘å¤„ç†åº“
from transformers import AutoModelForCausalLM, AutoTokenizer  # å˜æ¢å™¨æ¨¡å‹åº“

MODEL_PATH = "THUDM/cogvlm2-llama3-caption"  # æ¨¡å‹è·¯å¾„

# åˆ¤æ–­æ˜¯å¦ä½¿ç”¨GPUï¼Œè‹¥å¯ç”¨åˆ™ä½¿ç”¨CUDA
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
# æ ¹æ®è®¾å¤‡èƒ½åŠ›è®¾ç½®Torchæ•°æ®ç±»å‹
TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[
    0] >= 8 else torch.float16

# åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
parser = argparse.ArgumentParser(description="CogVLM2-Video CLI Demo")
# æ·»åŠ é‡åŒ–å‚æ•°çš„é€‰é¡¹ï¼Œé€‰æ‹©4ä½æˆ–8ä½ç²¾åº¦
parser.add_argument('--quant', type=int, choices=[4, 8], help='Enable 4-bit or 8-bit precision loading', default=0)
# è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œè¿”å›å‚æ•°å¯¹è±¡
args = parser.parse_args([])


def load_video(video_data, strategy='chat'):
    # è®¾ç½®Decordçš„æ¡¥æ¥ä¸ºPyTorch
    bridge.set_bridge('torch')
    mp4_stream = video_data  # å°†è¾“å…¥çš„è§†é¢‘æ•°æ®å­˜å‚¨åœ¨mp4_streamä¸­
    num_frames = 24  # è®¾å®šè¦æå–çš„å¸§æ•°
    # ä»å­—èŠ‚æµåˆ›å»ºè§†é¢‘è¯»å–å™¨
    decord_vr = VideoReader(io.BytesIO(mp4_stream), ctx=cpu(0))

    frame_id_list = None  # åˆå§‹åŒ–å¸§IDåˆ—è¡¨
    total_frames = len(decord_vr)  # è·å–è§†é¢‘æ€»å¸§æ•°
    # æ ¹æ®é€‰æ‹©çš„ç­–ç•¥å†³å®šå¸§æå–æ–¹å¼
    if strategy == 'base':
        clip_end_sec = 60  # è®¾ç½®è§†é¢‘ç‰‡æ®µç»“æŸæ—¶é—´
        clip_start_sec = 0  # è®¾ç½®è§†é¢‘ç‰‡æ®µå¼€å§‹æ—¶é—´
        # è®¡ç®—å¼€å§‹å¸§å’Œç»“æŸå¸§
        start_frame = int(clip_start_sec * decord_vr.get_avg_fps())
        end_frame = min(total_frames,
                        int(clip_end_sec * decord_vr.get_avg_fps())) if clip_end_sec is not None else total_frames
        # ç”Ÿæˆç­‰é—´éš”çš„å¸§IDåˆ—è¡¨
        frame_id_list = np.linspace(start_frame, end_frame - 1, num_frames, dtype=int)
    elif strategy == 'chat':
        # è·å–æ¯å¸§çš„æ—¶é—´æˆ³
        timestamps = decord_vr.get_frame_timestamp(np.arange(total_frames))
        timestamps = [i[0] for i in timestamps]  # æå–æ—¶é—´æˆ³çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
        max_second = round(max(timestamps)) + 1  # è®¡ç®—æœ€å¤§ç§’æ•°
        frame_id_list = []  # åˆå§‹åŒ–å¸§IDåˆ—è¡¨
        # éå†æ¯ç§’ï¼Œæ‰¾åˆ°æœ€æ¥è¿‘çš„å¸§
        for second in range(max_second):
            closest_num = min(timestamps, key=lambda x: abs(x - second))  # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ—¶é—´æˆ³
            index = timestamps.index(closest_num)  # è·å–å¯¹åº”å¸§çš„ç´¢å¼•
            frame_id_list.append(index)  # å°†ç´¢å¼•æ·»åŠ åˆ°å¸§IDåˆ—è¡¨
            if len(frame_id_list) >= num_frames:  # å¦‚æœè¾¾åˆ°æ‰€éœ€å¸§æ•°ï¼Œåˆ™åœæ­¢
                break

    # æ ¹æ®å¸§IDåˆ—è¡¨è·å–è§†é¢‘å¸§
    video_data = decord_vr.get_batch(frame_id_list)
    # è°ƒæ•´è§†é¢‘æ•°æ®çš„ç»´åº¦é¡ºåº
    video_data = video_data.permute(3, 0, 1, 2)
    return video_data  # è¿”å›æå–çš„è§†é¢‘æ•°æ®


# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
)

# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½è¯­è¨€æ¨¡å‹å¹¶è®¾ç½®è®¾å¤‡ç±»å‹
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=TORCH_TYPE,
    trust_remote_code=True
).eval().to(DEVICE)  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶è½¬ç§»åˆ°æŒ‡å®šè®¾å¤‡


def predict(prompt, video_data, temperature):
    strategy = 'chat'  # è®¾å®šç­–ç•¥ä¸ºèŠå¤©æ¨¡å¼

    # åŠ è½½è§†é¢‘æ•°æ®
    video = load_video(video_data, strategy=strategy)

    history = []  # åˆå§‹åŒ–å¯¹è¯å†å²
    query = prompt  # è®¾ç½®æŸ¥è¯¢å†…å®¹
    # æ„å»ºæ¨¡å‹è¾“å…¥
    inputs = model.build_conversation_input_ids(
        tokenizer=tokenizer,
        query=query,
        images=[video],
        history=history,
        template_version=strategy
    )
    # å‡†å¤‡è¾“å…¥å­—å…¸ï¼Œå°†æ•°æ®è½¬ç§»åˆ°CUDAè®¾å¤‡
    inputs = {
        'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),
        'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),
        'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),
        'images': [[inputs['images'][0].to('cuda').to(TORCH_TYPE)]],
    }
    # è®¾ç½®ç”Ÿæˆå‚æ•°
    gen_kwargs = {
        "max_new_tokens": 2048,  # è®¾ç½®æœ€å¤§ç”Ÿæˆçš„æ ‡è®°æ•°
        "pad_token_id": 128002,  # è®¾ç½®å¡«å……æ ‡è®°ID
        "top_k": 1,  # è®¾ç½®Top-ké‡‡æ ·
        "do_sample": False,  # æ˜¯å¦è¿›è¡Œé‡‡æ ·
        "top_p": 0.1,  # è®¾ç½®Top-pé‡‡æ ·
        "temperature": temperature,  # è®¾ç½®æ¸©åº¦
    }
    # åœ¨ä¸è®¡ç®—æ¢¯åº¦çš„ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œä»£ç ï¼Œä»¥èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿè®¡ç®—
        with torch.no_grad():
            # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡ºï¼Œè¾“å…¥å‚æ•°åŒ…å«è¾“å…¥æ•°æ®å’Œç”Ÿæˆæ—¶çš„é¢å¤–å‚æ•°
            outputs = model.generate(**inputs, **gen_kwargs)
            # æˆªå–ç”Ÿæˆè¾“å‡ºï¼Œä»ç¬¬äºŒä¸ªç»´åº¦å¼€å§‹ï¼Œå»é™¤è¾“å…¥éƒ¨åˆ†
            outputs = outputs[:, inputs['input_ids'].shape[1]:]
            # å°†è¾“å‡ºå¼ é‡è§£ç ä¸ºå¯è¯»å­—ç¬¦ä¸²ï¼Œè·³è¿‡ç‰¹æ®Šæ ‡è®°
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # è¿”å›è§£ç åçš„å“åº”
            return response
# å®šä¹‰ä¸€ä¸ªæµ‹è¯•å‡½æ•°
def test():
    # è®¾ç½®æç¤ºè¯­ï¼Œç”¨äºæè¿°è§†é¢‘çš„è¯·æ±‚
    prompt = "Please describe this video in detail."
    # è®¾ç½®æ¸©åº¦å€¼ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
    temperature = 0.1
    # ä»¥äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€è§†é¢‘æ–‡ä»¶ï¼Œå¹¶è¯»å–å…¶å†…å®¹
    video_data = open('test.mp4', 'rb').read()
    # è°ƒç”¨é¢„æµ‹å‡½æ•°ï¼Œä¼ å…¥æç¤ºè¯­ã€è§†é¢‘æ•°æ®å’Œæ¸©åº¦å€¼ï¼Œè·å–å“åº”
    response = predict(prompt, video_data, temperature)
    # æ‰“å°å“åº”ç»“æœ
    print(response)


# åˆ¤æ–­æ˜¯å¦ä¸ºä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == '__main__':
    # è°ƒç”¨æµ‹è¯•å‡½æ•°
    test()
```

# `.\cogvideo-finetune\tools\convert_weight_sat2hf.py`

```
"""
# æ­¤è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä»æ–‡æœ¬æç¤ºè½¬æ¢å’Œç”Ÿæˆè§†é¢‘
# ä½¿ç”¨ CogVideoX å’Œ ğŸ¤—Huggingface Diffusers Pipelineã€‚
# æ­¤è„šæœ¬éœ€è¦å®‰è£… `diffusers>=0.30.2` åº“ã€‚

# å‡½æ•°åˆ—è¡¨ï¼š
#     - reassign_query_key_value_inplace: å°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢ã€é”®å’Œå€¼çš„æƒé‡ã€‚
#     - reassign_query_key_layernorm_inplace: å°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢å’Œé”®çš„å±‚å½’ä¸€åŒ–ã€‚
#     - reassign_adaln_norm_inplace: å°±åœ°é‡æ–°åˆ†é…è‡ªé€‚åº”å±‚å½’ä¸€åŒ–ã€‚
#     - remove_keys_inplace: å°±åœ°ç§»é™¤çŠ¶æ€å­—å…¸ä¸­æŒ‡å®šçš„é”®ã€‚
#     - replace_up_keys_inplace: å°±åœ°æ›¿æ¢â€œupâ€å—ä¸­çš„é”®ã€‚
#     - get_state_dict: ä»ä¿å­˜çš„æ£€æŸ¥ç‚¹ä¸­æå–çŠ¶æ€å­—å…¸ã€‚
#     - update_state_dict_inplace: å°±åœ°æ›´æ–°çŠ¶æ€å­—å…¸ä»¥è¿›è¡Œæ–°çš„é”®åˆ†é…ã€‚
#     - convert_transformer: å°†å˜æ¢å™¨æ£€æŸ¥ç‚¹è½¬æ¢ä¸º CogVideoX æ ¼å¼ã€‚
#     - convert_vae: å°† VAE æ£€æŸ¥ç‚¹è½¬æ¢ä¸º CogVideoX æ ¼å¼ã€‚
#     - get_args: è§£æè„šæœ¬çš„å‘½ä»¤è¡Œå‚æ•°ã€‚
#     - generate_video: ä½¿ç”¨ CogVideoX ç®¡é“ä»æ–‡æœ¬æç¤ºç”Ÿæˆè§†é¢‘ã€‚
"""

# å¯¼å…¥ argparse æ¨¡å—ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
import argparse
# ä» typing å¯¼å…¥ Any å’Œ Dict ç±»å‹
from typing import Any, Dict

# å¯¼å…¥ PyTorch åº“
import torch
# ä» transformers åº“å¯¼å…¥ T5EncoderModel å’Œ T5Tokenizer
from transformers import T5EncoderModel, T5Tokenizer

# ä» diffusers åº“å¯¼å…¥å¤šä¸ªç±»
from diffusers import (
    AutoencoderKLCogVideoX,  # è‡ªåŠ¨ç¼–ç å™¨ç±»
    CogVideoXDDIMScheduler,   # è°ƒåº¦å™¨ç±»
    CogVideoXImageToVideoPipeline,  # å›¾åƒåˆ°è§†é¢‘çš„ç®¡é“ç±»
    CogVideoXPipeline,        # ä¸»ç®¡é“ç±»
    CogVideoXTransformer3DModel,  # 3D å˜æ¢å™¨æ¨¡å‹ç±»
)

# å‡½æ•°ï¼šå°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢ã€é”®å’Œå€¼çš„æƒé‡
def reassign_query_key_value_inplace(key: str, state_dict: Dict[str, Any]):
    # æ ¹æ®åŸå§‹é”®ç”Ÿæˆæ–°çš„é”®ï¼Œæ›¿æ¢æŸ¥è¯¢é”®å€¼
    to_q_key = key.replace("query_key_value", "to_q")
    to_k_key = key.replace("query_key_value", "to_k")
    to_v_key = key.replace("query_key_value", "to_v")
    # å°†çŠ¶æ€å­—å…¸ä¸­è¯¥é”®çš„å€¼åˆ†å‰²æˆä¸‰éƒ¨åˆ†ï¼ˆæŸ¥è¯¢ã€é”®å’Œå€¼ï¼‰
    to_q, to_k, to_v = torch.chunk(state_dict[key], chunks=3, dim=0)
    # å°†åˆ†å‰²åçš„æŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åˆ°çŠ¶æ€å­—å…¸ä¸­
    state_dict[to_q_key] = to_q
    state_dict[to_k_key] = to_k
    state_dict[to_v_key] = to_v
    # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤åŸå§‹é”®
    state_dict.pop(key)

# å‡½æ•°ï¼šå°±åœ°é‡æ–°åˆ†é…æŸ¥è¯¢å’Œé”®çš„å±‚å½’ä¸€åŒ–
def reassign_query_key_layernorm_inplace(key: str, state_dict: Dict[str, Any]):
    # ä»é”®ä¸­æå–å±‚ ID å’Œæƒé‡æˆ–åå·®ç±»å‹
    layer_id, weight_or_bias = key.split(".")[-2:]

    # æ ¹æ®é”®åç¡®å®šæ–°é”®å
    if "query" in key:
        new_key = f"transformer_blocks.{layer_id}.attn1.norm_q.{weight_or_bias}"
    elif "key" in key:
        new_key = f"transformer_blocks.{layer_id}.attn1.norm_k.{weight_or_bias}"

    # å°†çŠ¶æ€å­—å…¸ä¸­åŸé”®çš„å€¼ç§»åˆ°æ–°é”®ä¸­
    state_dict[new_key] = state_dict.pop(key)

# å‡½æ•°ï¼šå°±åœ°é‡æ–°åˆ†é…è‡ªé€‚åº”å±‚å½’ä¸€åŒ–
def reassign_adaln_norm_inplace(key: str, state_dict: Dict[str, Any]):
    # ä»é”®ä¸­æå–å±‚ ID å’Œæƒé‡æˆ–åå·®ç±»å‹
    layer_id, _, weight_or_bias = key.split(".")[-3:]

    # å°†çŠ¶æ€å­—å…¸ä¸­è¯¥é”®çš„å€¼åˆ†å‰²ä¸º 12 éƒ¨åˆ†
    weights_or_biases = state_dict[key].chunk(12, dim=0)
    # åˆå¹¶ç‰¹å®šéƒ¨åˆ†å½¢æˆæ–°çš„æƒé‡æˆ–åå·®
    norm1_weights_or_biases = torch.cat(weights_or_biases[0:3] + weights_or_biases[6:9])
    norm2_weights_or_biases = torch.cat(weights_or_biases[3:6] + weights_or_biases[9:12])

    # æ„å»ºæ–°é”®åå¹¶æ›´æ–°çŠ¶æ€å­—å…¸
    norm1_key = f"transformer_blocks.{layer_id}.norm1.linear.{weight_or_bias}"
    state_dict[norm1_key] = norm1_weights_or_biases

    norm2_key = f"transformer_blocks.{layer_id}.norm2.linear.{weight_or_bias}"
    state_dict[norm2_key] = norm2_weights_or_biases

    # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤åŸå§‹é”®
    state_dict.pop(key)

# å‡½æ•°ï¼šå°±åœ°ç§»é™¤çŠ¶æ€å­—å…¸ä¸­çš„æŒ‡å®šé”®
def remove_keys_inplace(key: str, state_dict: Dict[str, Any]):
    # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤æŒ‡å®šçš„é”®
    state_dict.pop(key)
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ›¿æ¢çŠ¶æ€å­—å…¸ä¸­çš„ç‰¹å®šé”®ï¼Œç›´æ¥åœ¨å­—å…¸ä¸­ä¿®æ”¹
def replace_up_keys_inplace(key: str, state_dict: Dict[str, Any]):
    # å°†é”®å­—ç¬¦ä¸²æŒ‰ç‚¹åˆ†å‰²æˆåˆ—è¡¨
    key_split = key.split(".")
    # è·å–æŒ‡å®šå±‚çš„ç´¢å¼•ï¼Œå‡è®¾ç´¢å¼•åœ¨ç¬¬ä¸‰ä¸ªä½ç½®
    layer_index = int(key_split[2])
    # è®¡ç®—æ›¿æ¢åçš„å±‚ç´¢å¼•
    replace_layer_index = 4 - 1 - layer_index

    # å°†åˆ†å‰²åçš„é”®æ›´æ–°ä¸º "up_blocks" ä½œä¸ºæ–°çš„ç¬¬äºŒå±‚
    key_split[1] = "up_blocks"
    # æ›´æ–°å±‚ç´¢å¼•ä¸ºè®¡ç®—åçš„æ–°ç´¢å¼•
    key_split[2] = str(replace_layer_index)
    # å°†åˆ†å‰²çš„é”®é‡æ–°æ‹¼æ¥ä¸ºå­—ç¬¦ä¸²
    new_key = ".".join(key_split)

    # åœ¨çŠ¶æ€å­—å…¸ä¸­ç”¨æ–°é”®æ›¿æ¢æ—§é”®å¯¹åº”çš„å€¼
    state_dict[new_key] = state_dict.pop(key)


# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºé‡å‘½å Transformer æ¨¡å‹çš„é”®
TRANSFORMER_KEYS_RENAME_DICT = {
    # é‡å‘½å final_layernorm é”®ä¸º norm_final
    "transformer.final_layernorm": "norm_final",
    # å°† transformer é”®é‡å‘½åä¸º transformer_blocks
    "transformer": "transformer_blocks",
    # é‡å‘½åæ³¨æ„åŠ›å±‚çš„é”®
    "attention": "attn1",
    # é‡å‘½å MLP å±‚çš„é”®
    "mlp": "ff.net",
    # é‡å‘½åå¯†é›†å±‚çš„é”®
    "dense_h_to_4h": "0.proj",
    "dense_4h_to_h": "2",
    # å¤„ç† layers é”®çš„é‡å‘½å
    ".layers": "",
    # å°† dense é”®é‡å‘½åä¸º to_out.0
    "dense": "to_out.0",
    # å¤„ç†è¾“å…¥å±‚å½’ä¸€åŒ–çš„é‡å‘½å
    "input_layernorm": "norm1.norm",
    # å¤„ç†åæ³¨æ„åŠ›å±‚å½’ä¸€åŒ–çš„é‡å‘½å
    "post_attn1_layernorm": "norm2.norm",
    # é‡å‘½åæ—¶é—´åµŒå…¥çš„å±‚
    "time_embed.0": "time_embedding.linear_1",
    "time_embed.2": "time_embedding.linear_2",
    # å¤„ç† Patch åµŒå…¥çš„é‡å‘½å
    "mixins.patch_embed": "patch_embed",
    # å¤„ç†æœ€ç»ˆå±‚çš„é‡å‘½å
    "mixins.final_layer.norm_final": "norm_out.norm",
    "mixins.final_layer.linear": "proj_out",
    # å¤„ç† ADA LN è°ƒåˆ¶å±‚çš„é‡å‘½å
    "mixins.final_layer.adaLN_modulation.1": "norm_out.linear",
    # å¤„ç†ç‰¹å®šäº CogVideoX-5b-I2V çš„é‡å‘½å
    "mixins.pos_embed.pos_embedding": "patch_embed.pos_embedding",  # Specific to CogVideoX-5b-I2V
}

# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºç‰¹æ®Šé”®çš„é‡æ˜ å°„
TRANSFORMER_SPECIAL_KEYS_REMAP = {
    # æ˜ å°„ç‰¹å®šçš„æŸ¥è¯¢é”®å€¼å¤„ç†å‡½æ•°
    "query_key_value": reassign_query_key_value_inplace,
    # æ˜ å°„æŸ¥è¯¢å±‚å½’ä¸€åŒ–åˆ—è¡¨çš„å¤„ç†å‡½æ•°
    "query_layernorm_list": reassign_query_key_layernorm_inplace,
    # æ˜ å°„é”®å±‚å½’ä¸€åŒ–åˆ—è¡¨çš„å¤„ç†å‡½æ•°
    "key_layernorm_list": reassign_query_key_layernorm_inplace,
    # æ˜ å°„ ADA LN è°ƒåˆ¶å±‚çš„å¤„ç†å‡½æ•°
    "adaln_layer.adaLN_modulations": reassign_adaln_norm_inplace,
    # æ˜ å°„åµŒå…¥ä»¤ç‰Œçš„å¤„ç†å‡½æ•°
    "embed_tokens": remove_keys_inplace,
    # æ˜ å°„é¢‘ç‡æ­£å¼¦çš„å¤„ç†å‡½æ•°
    "freqs_sin": remove_keys_inplace,
    # æ˜ å°„é¢‘ç‡ä½™å¼¦çš„å¤„ç†å‡½æ•°
    "freqs_cos": remove_keys_inplace,
    # æ˜ å°„ä½ç½®åµŒå…¥çš„å¤„ç†å‡½æ•°
    "position_embedding": remove_keys_inplace,
}

# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºé‡å‘½å VAE æ¨¡å‹çš„é”®
VAE_KEYS_RENAME_DICT = {
    # å°†å—çš„é”®é‡å‘½åä¸º resnets. 
    "block.": "resnets.",
    # å°† down çš„é”®é‡å‘½åä¸º down_blocks.
    "down.": "down_blocks.",
    # å°† downsample çš„é”®é‡å‘½åä¸º downsamplers.0
    "downsample": "downsamplers.0",
    # å°† upsample çš„é”®é‡å‘½åä¸º upsamplers.0
    "upsample": "upsamplers.0",
    # å°† nin_shortcut çš„é”®é‡å‘½åä¸º conv_shortcut
    "nin_shortcut": "conv_shortcut",
    # å°†ç¼–ç å™¨çš„å—é‡å‘½å
    "encoder.mid.block_1": "encoder.mid_block.resnets.0",
    "encoder.mid.block_2": "encoder.mid_block.resnets.1",
    # å°†è§£ç å™¨çš„å—é‡å‘½å
    "decoder.mid.block_1": "decoder.mid_block.resnets.0",
    "decoder.mid.block_2": "decoder.mid_block.resnets.1",
}

# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºç‰¹æ®Šé”®çš„é‡æ˜ å°„ï¼Œé€‚ç”¨äº VAE
VAE_SPECIAL_KEYS_REMAP = {
    # æ˜ å°„æŸå¤±çš„å¤„ç†å‡½æ•°
    "loss": remove_keys_inplace,
    # æ˜ å°„ up çš„å¤„ç†å‡½æ•°
    "up.": replace_up_keys_inplace,
}

# å®šä¹‰ä¸€ä¸ªå¸¸é‡ï¼Œè¡¨ç¤ºæ ‡è®°å™¨çš„æœ€å¤§é•¿åº¦
TOKENIZER_MAX_LENGTH = 226


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»ä¿å­˜çš„å­—å…¸ä¸­è·å–çŠ¶æ€å­—å…¸
def get_state_dict(saved_dict: Dict[str, Any]) -> Dict[str, Any]:
    # é»˜è®¤çŠ¶æ€å­—å…¸ä¸ºä¿å­˜çš„å­—å…¸
    state_dict = saved_dict
    # å¦‚æœä¿å­˜çš„å­—å…¸ä¸­åŒ…å« "model" é”®ï¼Œåˆ™æå–æ¨¡å‹éƒ¨åˆ†
    if "model" in saved_dict.keys():
        state_dict = state_dict["model"]
    # å¦‚æœä¿å­˜çš„å­—å…¸ä¸­åŒ…å« "module" é”®ï¼Œåˆ™æå–æ¨¡å—éƒ¨åˆ†
    if "module" in saved_dict.keys():
        state_dict = state_dict["module"]
    # å¦‚æœä¿å­˜çš„å­—å…¸ä¸­åŒ…å« "state_dict" é”®ï¼Œåˆ™æå–çŠ¶æ€å­—å…¸
    if "state_dict" in saved_dict.keys():
        state_dict = state_dict["state_dict"]
    # è¿”å›æœ€ç»ˆæå–çš„çŠ¶æ€å­—å…¸
    return state_dict


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç›´æ¥åœ¨çŠ¶æ€å­—å…¸ä¸­æ›´æ–°é”®
def update_state_dict_inplace(state_dict: Dict[str, Any], old_key: str, new_key: str) -> Dict[str, Any]:
    # ç”¨æ–°é”®æ›¿æ¢æ—§é”®åœ¨å­—å…¸ä¸­çš„å€¼
    state_dict[new_key] = state_dict.pop(old_key)


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè½¬æ¢ Transformer æ¨¡å‹
def convert_transformer(
    ckpt_path: str,
    num_layers: int,
    num_attention_heads: int,
    use_rotary_positional_embeddings: bool,
    i2v: bool,
    dtype: torch.dtype,
):
    # å®šä¹‰ä¸€ä¸ªå‰ç¼€é”®ï¼Œè¡¨ç¤ºæ¨¡å‹çš„å‰ç¼€éƒ¨åˆ†
    PREFIX_KEY = "model.diffusion_model."

    # ä»æŒ‡å®šè·¯å¾„åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸ï¼Œè®¾ç½® map_location ä¸º "cpu" å’Œ mmap ä¸º True
    original_state_dict = get_state_dict(torch.load(ckpt_path, map_location="cpu", mmap=True))
    # åˆ›å»ºä¸€ä¸ª CogVideoXTransformer3DModel å®ä¾‹ï¼Œè®¾ç½®è¾“å…¥é€šé“ã€å±‚æ•°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰å‚æ•°
    transformer = CogVideoXTransformer3DModel(
        # æ ¹æ® i2v çš„å€¼å†³å®šè¾“å…¥é€šé“æ•°
        in_channels=32 if i2v else 16,
        # è®¾ç½®æ¨¡å‹çš„å±‚æ•°
        num_layers=num_layers,
        # è®¾ç½®æ³¨æ„åŠ›å¤´çš„æ•°é‡
        num_attention_heads=num_attention_heads,
        # æ˜¯å¦ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥
        use_rotary_positional_embeddings=use_rotary_positional_embeddings,
        # æ˜¯å¦ä½¿ç”¨å­¦ä¹ åˆ°çš„ä½ç½®åµŒå…¥
        use_learned_positional_embeddings=i2v,
    ).to(dtype=dtype)  # å°†æ¨¡å‹è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹

    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®åˆ—è¡¨
    for key in list(original_state_dict.keys()):
        # ä»é”®ä¸­å»æ‰å‰ç¼€ï¼Œä»¥è·å¾—æ–°çš„é”®å
        new_key = key[len(PREFIX_KEY) :]
        # éå†é‡å‘½åå­—å…¸ï¼Œæ›¿æ¢é”®åä¸­çš„ç‰¹å®šéƒ¨åˆ†
        for replace_key, rename_key in TRANSFORMER_KEYS_RENAME_DICT.items():
            new_key = new_key.replace(replace_key, rename_key)
        # æ›´æ–°åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„é”®å€¼å¯¹
        update_state_dict_inplace(original_state_dict, key, new_key)

    # å†æ¬¡éå†åŸå§‹çŠ¶æ€å­—å…¸çš„é”®åˆ—è¡¨
    for key in list(original_state_dict.keys()):
        # éå†ç‰¹æ®Šé”®çš„æ˜ å°„å­—å…¸
        for special_key, handler_fn_inplace in TRANSFORMER_SPECIAL_KEYS_REMAP.items():
            # å¦‚æœç‰¹æ®Šé”®ä¸åœ¨å½“å‰é”®ä¸­ï¼Œåˆ™ç»§ç»­ä¸‹ä¸€ä¸ªé”®
            if special_key not in key:
                continue
            # è°ƒç”¨å¤„ç†å‡½æ•°ä»¥æ›´æ–°çŠ¶æ€å­—å…¸
            handler_fn_inplace(key, original_state_dict)
    
    # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸åˆ° transformer ä¸­ï¼Œä¸¥æ ¼åŒ¹é…é”®
    transformer.load_state_dict(original_state_dict, strict=True)
    # è¿”å› transformer å®ä¾‹
    return transformer
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°† VAE æ¨¡å‹ä»æ£€æŸ¥ç‚¹è·¯å¾„è½¬æ¢
def convert_vae(ckpt_path: str, scaling_factor: float, dtype: torch.dtype):
    # ä»æŒ‡å®šè·¯å¾„åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸ï¼Œä½¿ç”¨ CPU æ˜ å°„
    original_state_dict = get_state_dict(torch.load(ckpt_path, map_location="cpu", mmap=True))
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ VAE å¯¹è±¡ï¼Œå¹¶å°†å…¶æ•°æ®ç±»å‹è®¾ç½®ä¸ºæŒ‡å®šçš„ dtype
    vae = AutoencoderKLCogVideoX(scaling_factor=scaling_factor).to(dtype=dtype)

    # éå†åŸå§‹çŠ¶æ€å­—å…¸çš„æ‰€æœ‰é”®
    for key in list(original_state_dict.keys()):
        # å¤åˆ¶å½“å‰é”®ä»¥ä¾¿ä¿®æ”¹
        new_key = key[:]
        # éå†é‡å‘½åå­—å…¸ï¼Œå°†æ—§é”®æ›¿æ¢ä¸ºæ–°é”®
        for replace_key, rename_key in VAE_KEYS_RENAME_DICT.items():
            new_key = new_key.replace(replace_key, rename_key)
        # æ›´æ–°åŸå§‹çŠ¶æ€å­—å…¸ä¸­çš„é”®
        update_state_dict_inplace(original_state_dict, key, new_key)

    # å†æ¬¡éå†åŸå§‹çŠ¶æ€å­—å…¸çš„æ‰€æœ‰é”®
    for key in list(original_state_dict.keys()):
        # éå†ç‰¹æ®Šé”®æ˜ å°„å­—å…¸
        for special_key, handler_fn_inplace in VAE_SPECIAL_KEYS_REMAP.items():
            # å¦‚æœç‰¹æ®Šé”®ä¸åœ¨å½“å‰é”®ä¸­ï¼Œåˆ™è·³è¿‡
            if special_key not in key:
                continue
            # ä½¿ç”¨å¤„ç†å‡½æ•°å¤„ç†åŸå§‹çŠ¶æ€å­—å…¸
            handler_fn_inplace(key, original_state_dict)

    # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸åˆ° VAE æ¨¡å‹ä¸­ï¼Œä¸¥æ ¼åŒ¹é…
    vae.load_state_dict(original_state_dict, strict=True)
    # è¿”å›è½¬æ¢åçš„ VAE å¯¹è±¡
    return vae


# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    # æ·»åŠ åŸå§‹å˜æ¢å™¨æ£€æŸ¥ç‚¹è·¯å¾„å‚æ•°
    parser.add_argument(
        "--transformer_ckpt_path", type=str, default=None, help="Path to original transformer checkpoint")
    # æ·»åŠ åŸå§‹ VAE æ£€æŸ¥ç‚¹è·¯å¾„å‚æ•°
    parser.add_argument("--vae_ckpt_path", type=str, default=None, help="Path to original vae checkpoint")
    # æ·»åŠ è¾“å‡ºè·¯å¾„å‚æ•°ï¼Œä½œä¸ºå¿…éœ€å‚æ•°
    parser.add_argument("--output_path", type=str, required=True, help="Path where converted model should be saved")
    # æ·»åŠ æ˜¯å¦ä»¥ fp16 æ ¼å¼ä¿å­˜æ¨¡å‹æƒé‡çš„å¸ƒå°”å‚æ•°
    parser.add_argument("--fp16", action="store_true", default=False, help="Whether to save the model weights in fp16")
    # æ·»åŠ æ˜¯å¦ä»¥ bf16 æ ¼å¼ä¿å­˜æ¨¡å‹æƒé‡çš„å¸ƒå°”å‚æ•°
    parser.add_argument("--bf16", action="store_true", default=False, help="Whether to save the model weights in bf16")
    # æ·»åŠ æ˜¯å¦åœ¨ä¿å­˜åæ¨é€åˆ° HF Hub çš„å¸ƒå°”å‚æ•°
    parser.add_argument(
        "--push_to_hub", action="store_true", default=False, help="Whether to push to HF Hub after saving"
    )
    # æ·»åŠ æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜ç›®å½•è·¯å¾„å‚æ•°
    parser.add_argument(
        "--text_encoder_cache_dir", type=str, default=None, help="Path to text encoder cache directory"
    )
    # æ·»åŠ å˜æ¢å™¨å—æ•°é‡å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 30
    parser.add_argument("--num_layers", type=int, default=30, help="Number of transformer blocks")
    # æ·»åŠ æ³¨æ„åŠ›å¤´æ•°é‡å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 30
    parser.add_argument("--num_attention_heads", type=int, default=30, help="Number of attention heads")
    # æ·»åŠ æ˜¯å¦ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥çš„å¸ƒå°”å‚æ•°
    parser.add_argument(
        "--use_rotary_positional_embeddings", action="store_true", default=False, help="Whether to use RoPE or not"
    )
    # æ·»åŠ  VAE çš„ç¼©æ”¾å› å­å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 1.15258426
    parser.add_argument("--scaling_factor", type=float, default=1.15258426, help="Scaling factor in the VAE")
    # æ·»åŠ  SNR åç§»æ¯”ä¾‹å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 3.0
    parser.add_argument("--snr_shift_scale", type=float, default=3.0, help="Scaling factor in the VAE")
    # æ·»åŠ æ˜¯å¦ä»¥ fp16 æ ¼å¼ä¿å­˜æ¨¡å‹æƒé‡çš„å¸ƒå°”å‚æ•°
    parser.add_argument("--i2v", action="store_true", default=False, help="Whether to save the model weights in fp16")
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›
    return parser.parse_args()


# å¦‚æœè„šæœ¬ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()

    # åˆå§‹åŒ– transformer å’Œ vae ä¸º None
    transformer = None
    vae = None
    # æ£€æŸ¥æ˜¯å¦åŒæ—¶ä¼ é€’äº† --fp16 å’Œ --bf16 å‚æ•°
    if args.fp16 and args.bf16:
        # å¦‚æœåŒæ—¶å­˜åœ¨åˆ™æŠ›å‡ºå€¼é”™è¯¯
        raise ValueError("You cannot pass both --fp16 and --bf16 at the same time.")

    # æ ¹æ®è¾“å…¥å‚æ•°é€‰æ‹©æ•°æ®ç±»å‹
    dtype = torch.float16 if args.fp16 else torch.bfloat16 if args.bf16 else torch.float32

    # å¦‚æœæä¾›äº†å˜æ¢å™¨æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œåˆ™è½¬æ¢å˜æ¢å™¨
    if args.transformer_ckpt_path is not None:
        transformer = convert_transformer(
            # ä¼ é€’å˜æ¢å™¨æ£€æŸ¥ç‚¹è·¯å¾„åŠç›¸å…³å‚æ•°
            args.transformer_ckpt_path,
            args.num_layers,
            args.num_attention_heads,
            args.use_rotary_positional_embeddings,
            args.i2v,
            dtype,
        )
    # å¦‚æœæä¾›äº† VAE æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œåˆ™è½¬æ¢ VAE
    if args.vae_ckpt_path is not None:
        vae = convert_vae(args.vae_ckpt_path, args.scaling_factor, dtype)

    # è®¾ç½®æ–‡æœ¬ç¼–ç å™¨çš„æ¨¡å‹ ID
    text_encoder_id = "google/t5-v1_1-xxl"
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½åˆ†è¯å™¨
    tokenizer = T5Tokenizer.from_pretrained(text_encoder_id, model_max_length=TOKENIZER_MAX_LENGTH)
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½æ–‡æœ¬ç¼–ç å™¨
    text_encoder = T5EncoderModel.from_pretrained(text_encoder_id, cache_dir=args.text_encoder_cache_dir)
    # å¤„ç†å‚æ•°ä»¥ç¡®ä¿æ•°æ®è¿ç»­æ€§
    for param in text_encoder.parameters():
        # ä½¿å‚æ•°æ•°æ®è¿ç»­
        param.data = param.data.contiguous()

    # ä»é…ç½®ä¸­åˆ›å»ºè°ƒåº¦å™¨
    scheduler = CogVideoXDDIMScheduler.from_config(
        {
            # è®¾ç½®è°ƒåº¦å™¨çš„è¶…å‚æ•°
            "snr_shift_scale": args.snr_shift_scale,
            "beta_end": 0.012,
            "beta_schedule": "scaled_linear",
            "beta_start": 0.00085,
            "clip_sample": False,
            "num_train_timesteps": 1000,
            "prediction_type": "v_prediction",
            "rescale_betas_zero_snr": True,
            "set_alpha_to_one": True,
            "timestep_spacing": "trailing",
        }
    )
    # æ ¹æ® i2v å‚æ•°é€‰æ‹©ç®¡é“ç±»
    if args.i2v:
        pipeline_cls = CogVideoXImageToVideoPipeline
    else:
        pipeline_cls = CogVideoXPipeline

    # å®ä¾‹åŒ–ç®¡é“
    pipe = pipeline_cls(
        # ä¼ é€’æ‰€éœ€çš„ç»„ä»¶
        tokenizer=tokenizer,
        text_encoder=text_encoder,
        vae=vae,
        transformer=transformer,
        scheduler=scheduler,
    )

    # å¦‚æœé€‰æ‹© fp16 åˆ™å°†ç®¡é“è½¬ä¸º fp16
    if args.fp16:
        pipe = pipe.to(dtype=torch.float16)
    # å¦‚æœé€‰æ‹© bf16 åˆ™å°†ç®¡é“è½¬ä¸º bf16
    if args.bf16:
        pipe = pipe.to(dtype=torch.bfloat16)

    # ä¿å­˜é¢„è®­ç»ƒçš„ç®¡é“åˆ°æŒ‡å®šè·¯å¾„
    pipe.save_pretrained(args.output_path, safe_serialization=True, push_to_hub=args.push_to_hub)
```

# `.\cogvideo-finetune\tools\export_sat_lora_weight.py`

```py
# å¯¼å…¥æ‰€éœ€çš„ç±»å‹å’Œåº“
from typing import Any, Dict
import torch 
import argparse 
from diffusers.loaders.lora_base import LoraBaseMixin
from diffusers.models.modeling_utils import load_state_dict

# å®šä¹‰å‡½æ•°ï¼Œè·å–çŠ¶æ€å­—å…¸ï¼Œè¾“å…¥ä¸ºä¸€ä¸ªå­—å…¸ï¼Œè¾“å‡ºä¸ºä¸€ä¸ªå­—å…¸
def get_state_dict(saved_dict: Dict[str, Any]) -> Dict[str, Any]:
    # åˆå§‹åŒ–çŠ¶æ€å­—å…¸ä¸ºè¾“å…¥å­—å…¸
    state_dict = saved_dict
    # å¦‚æœå­—å…¸ä¸­åŒ…å«"model"é”®ï¼Œæ›´æ–°çŠ¶æ€å­—å…¸ä¸º"model"å¯¹åº”çš„å€¼
    if "model" in saved_dict.keys():
        state_dict = state_dict["model"]
    # å¦‚æœå­—å…¸ä¸­åŒ…å«"module"é”®ï¼Œæ›´æ–°çŠ¶æ€å­—å…¸ä¸º"module"å¯¹åº”çš„å€¼
    if "module" in saved_dict.keys():
        state_dict = state_dict["module"]
    # å¦‚æœå­—å…¸ä¸­åŒ…å«"state_dict"é”®ï¼Œæ›´æ–°çŠ¶æ€å­—å…¸ä¸º"state_dict"å¯¹åº”çš„å€¼
    if "state_dict" in saved_dict.keys():
        state_dict = state_dict["state_dict"]
    # è¿”å›æœ€ç»ˆçš„çŠ¶æ€å­—å…¸
    return state_dict

# å®šä¹‰LORAé”®é‡å‘½åçš„å­—å…¸
LORA_KEYS_RENAME = {
    'attention.query_key_value.matrix_A.0': 'attn1.to_q.lora_A.weight',
    'attention.query_key_value.matrix_A.1': 'attn1.to_k.lora_A.weight',
    'attention.query_key_value.matrix_A.2': 'attn1.to_v.lora_A.weight',
    'attention.query_key_value.matrix_B.0': 'attn1.to_q.lora_B.weight',
    'attention.query_key_value.matrix_B.1': 'attn1.to_k.lora_B.weight',
    'attention.query_key_value.matrix_B.2': 'attn1.to_v.lora_B.weight',
    'attention.dense.matrix_A.0': 'attn1.to_out.0.lora_A.weight',
    'attention.dense.matrix_B.0': 'attn1.to_out.0.lora_B.weight'
}

# å®šä¹‰å‰ç¼€é”®å’Œç›¸å…³å¸¸é‡
PREFIX_KEY = "model.diffusion_model."
SAT_UNIT_KEY = "layers"
LORA_PREFIX_KEY = "transformer_blocks"

# å¯¼å‡ºLORAæƒé‡çš„å‡½æ•°ï¼Œè¾“å…¥ä¸ºæ£€æŸ¥ç‚¹è·¯å¾„å’Œä¿å­˜ç›®å½•
def export_lora_weight(ckpt_path,lora_save_directory):
    # åŠ è½½æ£€æŸ¥ç‚¹å¹¶è·å–åˆå¹¶åçš„çŠ¶æ€å­—å…¸
    merge_original_state_dict = get_state_dict(torch.load(ckpt_path, map_location="cpu", mmap=True))

    # åˆå§‹åŒ–LORAçŠ¶æ€å­—å…¸
    lora_state_dict = {}
    # éå†åˆå¹¶åçš„çŠ¶æ€å­—å…¸çš„æ‰€æœ‰é”®
    for key in list(merge_original_state_dict.keys()):
        # è·å–æ–°é”®ï¼Œå»æ‰å‰ç¼€
        new_key = key[len(PREFIX_KEY) :]
        # éå†LORAé”®é‡å‘½åå­—å…¸
        for special_key, lora_keys in LORA_KEYS_RENAME.items():
            # å¦‚æœæ–°é”®ä»¥ç‰¹æ®Šé”®ç»“å°¾ï¼Œåˆ™è¿›è¡Œæ›¿æ¢
            if new_key.endswith(special_key):
                new_key = new_key.replace(special_key, lora_keys)
                new_key = new_key.replace(SAT_UNIT_KEY, LORA_PREFIX_KEY)
                # å°†æ›¿æ¢åçš„é”®åŠå…¶å¯¹åº”å€¼æ·»åŠ åˆ°LORAçŠ¶æ€å­—å…¸
                lora_state_dict[new_key] = merge_original_state_dict[key]

    # æ£€æŸ¥LORAçŠ¶æ€å­—å…¸çš„é•¿åº¦æ˜¯å¦ä¸º240
    if len(lora_state_dict) != 240:
        raise ValueError("lora_state_dict length is not 240")

    # è·å–LORAçŠ¶æ€å­—å…¸çš„æ‰€æœ‰é”®
    lora_state_dict.keys()

    # è°ƒç”¨LoraBaseMixinçš„å†™å…¥LORAå±‚å‡½æ•°ï¼Œä¿å­˜æƒé‡
    LoraBaseMixin.write_lora_layers(
        state_dict=lora_state_dict,
        save_directory=lora_save_directory,
        is_main_process=True,
        weight_name=None,
        save_function=None,
        safe_serialization=True
    )

# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser()
    # æ·»åŠ åŸå§‹æ£€æŸ¥ç‚¹è·¯å¾„å‚æ•°
    parser.add_argument(
        "--sat_pt_path", type=str, required=True, help="Path to original sat transformer checkpoint"
    )
    # æ·»åŠ LORAä¿å­˜ç›®å½•å‚æ•°
    parser.add_argument("--lora_save_directory", type=str, required=True, help="Path where converted lora should be saved") 
    # è¿”å›è§£æåçš„å‚æ•°
    return parser.parse_args()

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()
    # è°ƒç”¨å¯¼å‡ºLORAæƒé‡çš„å‡½æ•°
    export_lora_weight(args.sat_pt_path, args.lora_save_directory)
```

# `.\cogvideo-finetune\tools\llm_flux_cogvideox\gradio_page.py`

```
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¨¡å—
import os
# å¯¼å…¥ Gradio åº“ï¼Œç”¨äºæ„å»ºç”¨æˆ·ç•Œé¢
import gradio as gr
# å¯¼å…¥åƒåœ¾å›æ”¶æ¨¡å—
import gc
# å¯¼å…¥éšæœºæ•°ç”Ÿæˆæ¨¡å—
import random
# å¯¼å…¥ PyTorch åº“
import torch
# å¯¼å…¥ NumPy åº“
import numpy as np
# å¯¼å…¥å›¾åƒå¤„ç†åº“
from PIL import Image
# å¯¼å…¥ Transformers åº“
import transformers
# ä» Diffusers åº“å¯¼å…¥è§†é¢‘ç”Ÿæˆç›¸å…³çš„ç±»
from diffusers import CogVideoXImageToVideoPipeline, CogVideoXDPMScheduler, DiffusionPipeline
# ä» Diffusers åº“å¯¼å…¥å¯¼å‡ºè§†é¢‘çš„å·¥å…·
from diffusers.utils import export_to_video
# ä» Transformers åº“å¯¼å…¥è‡ªåŠ¨åˆ†è¯å™¨
from transformers import AutoTokenizer
# å¯¼å…¥æ—¥æœŸå’Œæ—¶é—´å¤„ç†æ¨¡å—
from datetime import datetime, timedelta
# å¯¼å…¥å¤šçº¿ç¨‹æ¨¡å—
import threading
# å¯¼å…¥æ—¶é—´æ¨¡å—
import time
# å¯¼å…¥ MoviePy åº“è¿›è¡Œè§†é¢‘ç¼–è¾‘
import moviepy.editor as mp

# è®¾ç½®æµ®ç‚¹æ•°çŸ©é˜µä¹˜æ³•çš„ç²¾åº¦ä¸ºé«˜
torch.set_float32_matmul_precision("high")

# è®¾ç½®é»˜è®¤å€¼
caption_generator_model_id = "/share/home/zyx/Models/Meta-Llama-3.1-8B-Instruct"  # ç”Ÿæˆè§†é¢‘æè¿°çš„æ¨¡å‹è·¯å¾„
image_generator_model_id = "/share/home/zyx/Models/FLUX.1-dev"  # ç”Ÿæˆå›¾åƒçš„æ¨¡å‹è·¯å¾„
video_generator_model_id = "/share/official_pretrains/hf_home/CogVideoX-5b-I2V"  # ç”Ÿæˆè§†é¢‘çš„æ¨¡å‹è·¯å¾„
seed = 1337  # éšæœºæ•°ç§å­

# åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œè‹¥å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
os.makedirs("./output", exist_ok=True)
# åˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œç”¨äº Gradio
os.makedirs("./gradio_tmp", exist_ok=True)

# ä»æŒ‡å®šæ¨¡å‹åŠ è½½è‡ªåŠ¨åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(caption_generator_model_id, trust_remote_code=True)
# åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“ï¼Œç”¨äºç”Ÿæˆè§†é¢‘æè¿°
caption_generator = transformers.pipeline(
    "text-generation",  # æŒ‡å®šä»»åŠ¡ä¸ºæ–‡æœ¬ç”Ÿæˆ
    model=caption_generator_model_id,  # æŒ‡å®šæ¨¡å‹
    device_map="balanced",  # è®¾ç½®è®¾å¤‡æ˜ å°„ä¸ºå¹³è¡¡æ¨¡å¼
    model_kwargs={  # æ¨¡å‹å‚æ•°
        "local_files_only": True,  # ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        "torch_dtype": torch.bfloat16,  # è®¾ç½®å¼ é‡æ•°æ®ç±»å‹
    },
    trust_remote_code=True,  # å…è®¸ä½¿ç”¨è¿œç¨‹ä»£ç 
    tokenizer=tokenizer  # ä½¿ç”¨åŠ è½½çš„åˆ†è¯å™¨
)

# ä»æŒ‡å®šæ¨¡å‹åŠ è½½å›¾åƒç”Ÿæˆç®¡é“
image_generator = DiffusionPipeline.from_pretrained(
    image_generator_model_id,  # æŒ‡å®šå›¾åƒç”Ÿæˆæ¨¡å‹
    torch_dtype=torch.bfloat16,  # è®¾ç½®å¼ é‡æ•°æ®ç±»å‹
    device_map="balanced"  # è®¾ç½®è®¾å¤‡æ˜ å°„ä¸ºå¹³è¡¡æ¨¡å¼
)
# image_generator.to("cuda")  # å¯é€‰æ‹©å°†ç”Ÿæˆå™¨ç§»åŠ¨åˆ° GPUï¼ˆè¢«æ³¨é‡Šæ‰ï¼‰

# ä»æŒ‡å®šæ¨¡å‹åŠ è½½è§†é¢‘ç”Ÿæˆç®¡é“
video_generator = CogVideoXImageToVideoPipeline.from_pretrained(
    video_generator_model_id,  # æŒ‡å®šè§†é¢‘ç”Ÿæˆæ¨¡å‹
    torch_dtype=torch.bfloat16,  # è®¾ç½®å¼ é‡æ•°æ®ç±»å‹
    device_map="balanced"  # è®¾ç½®è®¾å¤‡æ˜ å°„ä¸ºå¹³è¡¡æ¨¡å¼
)

# å¯ç”¨è§†é¢‘ç”Ÿæˆå™¨çš„ VAE åˆ‡ç‰‡åŠŸèƒ½
video_generator.vae.enable_slicing()
# å¯ç”¨è§†é¢‘ç”Ÿæˆå™¨çš„ VAE å¹³é“ºåŠŸèƒ½
video_generator.vae.enable_tiling()

# è®¾ç½®è§†é¢‘ç”Ÿæˆå™¨çš„è°ƒåº¦å™¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®
video_generator.scheduler = CogVideoXDPMScheduler.from_config(
    video_generator.scheduler.config, timestep_spacing="trailing"  # è®¾ç½®æ—¶é—´æ­¥é•¿ä¸ºåç»­æ¨¡å¼
)

# å®šä¹‰ç³»ç»Ÿæç¤º
SYSTEM_PROMPT = """
# ç³»ç»Ÿæç¤ºå†…å®¹ï¼Œè¯´æ˜è§†é¢‘ç”Ÿæˆä»»åŠ¡å’Œè§„åˆ™
You are part of a team of people that create videos using generative models. You use a video-generation model that can generate a video about anything you describe.

For example, if you respond with "A beautiful morning in the woods with the sun peaking through the trees", the video generation model will create a video of exactly as described. Your task is to summarize the descriptions of videos provided by users and create detailed prompts to feed into the generative model.

There are a few rules to follow:
- You will only ever output a single video description per request.
- If the user mentions to summarize the prompt in [X] words, make sure not to exceed the limit.

Your responses should just be the video generation prompt. Here are examples:
# å®šä¹‰åŒ…å«è¯¦ç»†æè¿°çš„å­—ç¬¦ä¸²ï¼Œæè¿°ç©å…·èˆ¹åœ¨è“è‰²åœ°æ¯¯ä¸Šçš„åœºæ™¯
- "A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting."
# å®šä¹‰åŒ…å«è¡—å¤´è‰ºæœ¯å®¶çš„å­—ç¬¦ä¸²ï¼Œæè¿°å…¶åœ¨åŸå¸‚å¢™å£ä¸Šå–·æ¶‚çš„æƒ…æ™¯
- "A street artist, clad in a worn-out denim jacket and a colorful bandana, stands before a vast concrete wall in the heart of the city, holding a can of spray paint, spray-painting a colorful bird on a mottled wall."
# å»é™¤å¤šä½™çš„ç©ºæ ¼å¹¶ä¿å­˜ä¸ºç”¨æˆ·æç¤º
""".strip()

# å®šä¹‰ç”¨æˆ·æç¤ºæ¨¡æ¿ï¼Œç”¨äºç”Ÿæˆè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æç¤º
USER_PROMPT = """
Could you generate a prompt for a video generation model? Please limit the prompt to [{0}] words.
""".strip()

# å®šä¹‰ç”Ÿæˆå­—å¹•çš„å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªæç¤ºå‚æ•°
def generate_caption(prompt):
    # éšæœºé€‰æ‹©å­—æ•°ï¼ˆ25ã€50ã€75æˆ–100ï¼‰ä»¥é™åˆ¶ç”Ÿæˆçš„å­—å¹•é•¿åº¦
    num_words = random.choice([25, 50, 75, 100])
    # æ ¼å¼åŒ–ç”¨æˆ·æç¤ºï¼Œå°†éšæœºå­—æ•°æ’å…¥æç¤ºæ¨¡æ¿ä¸­
    user_prompt = USER_PROMPT.format(num_words)

    # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»ç»Ÿè§’è‰²å’Œç”¨æˆ·è§’è‰²çš„å†…å®¹
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt + "\n" + user_prompt},
    ]

    # è°ƒç”¨å­—å¹•ç”Ÿæˆå™¨ç”Ÿæˆå­—å¹•ï¼ŒæŒ‡å®šæœ€å¤§æ–°ä»¤ç‰Œæ•°å’Œæ˜¯å¦è¿”å›å®Œæ•´æ–‡æœ¬
    response = caption_generator(
        messages,
        max_new_tokens=226,
        return_full_text=False
    )
    # è·å–ç”Ÿæˆçš„å­—å¹•æ–‡æœ¬
    caption = response[0]["generated_text"]
    # å¦‚æœå­—å¹•ä»¥åŒå¼•å·å¼€å¤´å’Œç»“å°¾ï¼Œå»æ‰è¿™ä¸¤ä¸ªå¼•å·
    if caption.startswith("\"") and caption.endswith("\""):
        caption = caption[1:-1]
    # è¿”å›ç”Ÿæˆçš„å­—å¹•
    return caption

# å®šä¹‰ç”Ÿæˆå›¾åƒçš„å‡½æ•°ï¼Œæ¥å—å­—å¹•å’Œè¿›åº¦å‚æ•°
def generate_image(caption, progress=gr.Progress(track_tqdm=True)):
    # è°ƒç”¨å›¾åƒç”Ÿæˆå™¨ç”Ÿæˆå›¾åƒï¼ŒæŒ‡å®šç›¸å…³å‚æ•°
    image = image_generator(
        prompt=caption,
        height=480,
        width=720,
        num_inference_steps=30,
        guidance_scale=3.5,
    ).images[0]
    # è¿”å›ç”Ÿæˆçš„å›¾åƒï¼Œé‡å¤ä¸€æ¬¡ä»¥ä¾¿äºåç»­å¤„ç†
    return image, image  # One for output One for State

# å®šä¹‰ç”Ÿæˆè§†é¢‘çš„å‡½æ•°ï¼Œæ¥å—å­—å¹•ã€å›¾åƒå’Œè¿›åº¦å‚æ•°
def generate_video(
        caption,
        image,
        progress=gr.Progress(track_tqdm=True)
):
    # åˆ›å»ºä¸€ä¸ªéšæœºç§å­ç”Ÿæˆå™¨
    generator = torch.Generator().manual_seed(seed)
    # è°ƒç”¨è§†é¢‘ç”Ÿæˆå™¨ç”Ÿæˆè§†é¢‘å¸§ï¼ŒæŒ‡å®šç›¸å…³å‚æ•°
    video_frames = video_generator(
        image=image,
        prompt=caption,
        height=480,
        width=720,
        num_frames=49,
        num_inference_steps=50,
        guidance_scale=6,
        use_dynamic_cfg=True,
        generator=generator,
    ).frames[0]
    # ä¿å­˜ç”Ÿæˆçš„è§†é¢‘å¹¶è·å–è§†é¢‘è·¯å¾„
    video_path = save_video(video_frames)
    # å°†è§†é¢‘è½¬æ¢ä¸º GIF å¹¶è·å– GIF è·¯å¾„
    gif_path = convert_to_gif(video_path)
    # è¿”å›è§†é¢‘è·¯å¾„å’Œ GIF è·¯å¾„
    return video_path, gif_path

# å®šä¹‰ä¿å­˜è§†é¢‘çš„å‡½æ•°ï¼Œæ¥å—å¼ é‡ä½œä¸ºå‚æ•°
def save_video(tensor):
    # è·å–å½“å‰æ—¶é—´æˆ³ä»¥å‘½åè§†é¢‘æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # åˆ›å»ºè§†é¢‘æ–‡ä»¶è·¯å¾„
    video_path = f"./output/{timestamp}.mp4"
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(os.path.dirname(video_path), exist_ok=True)
    # å¯¼å‡ºå¼ é‡ä¸ºè§†é¢‘æ–‡ä»¶ï¼ŒæŒ‡å®šå¸§ç‡
    export_to_video(tensor, video_path, fps=8)
    # è¿”å›è§†é¢‘æ–‡ä»¶è·¯å¾„
    return video_path

# å®šä¹‰å°†è§†é¢‘è½¬æ¢ä¸º GIF çš„å‡½æ•°ï¼Œæ¥å—è§†é¢‘è·¯å¾„ä½œä¸ºå‚æ•°
def convert_to_gif(video_path):
    # åŠ è½½è§†é¢‘æ–‡ä»¶
    clip = mp.VideoFileClip(video_path)
    # è®¾ç½®è§†é¢‘çš„å¸§ç‡
    clip = clip.set_fps(8)
    # è°ƒæ•´è§†é¢‘çš„é«˜åº¦ä»¥è¿›è¡Œ GIF è¾“å‡º
    clip = clip.resize(height=240)
    # åˆ›å»º GIF æ–‡ä»¶è·¯å¾„
    gif_path = video_path.replace(".mp4", ".gif")
    # å°†è§†é¢‘å†™å…¥ GIF æ–‡ä»¶ï¼ŒæŒ‡å®šå¸§ç‡
    clip.write_gif(gif_path, fps=8)
    # è¿”å› GIF æ–‡ä»¶è·¯å¾„
    return gif_path

# å®šä¹‰åˆ é™¤æ—§æ–‡ä»¶çš„å‡½æ•°ï¼ŒåŠŸèƒ½å°šæœªå®ç°
def delete_old_files():
    # æ— é™å¾ªç¯ï¼ŒæŒç»­æ‰§è¡Œæ–‡ä»¶æ¸…ç†æ“ä½œ
        while True:
            # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
            now = datetime.now()
            # è®¡ç®—æˆªæ­¢æ—¶é—´ï¼Œå½“å‰æ—¶é—´å‡å»10åˆ†é’Ÿ
            cutoff = now - timedelta(minutes=10)
            # å®šä¹‰è¦æ¸…ç†çš„ç›®å½•åˆ—è¡¨
            directories = ["./output", "./gradio_tmp"]
    
            # éå†ç›®å½•åˆ—è¡¨
            for directory in directories:
                # éå†å½“å‰ç›®å½•ä¸­çš„æ–‡ä»¶å
                for filename in os.listdir(directory):
                    # æ„é€ æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                    file_path = os.path.join(directory, filename)
                    # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºæ–‡ä»¶
                    if os.path.isfile(file_path):
                        # è·å–æ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´
                        file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                        # å¦‚æœæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æ—©äºæˆªæ­¢æ—¶é—´ï¼Œåˆ é™¤è¯¥æ–‡ä»¶
                        if file_mtime < cutoff:
                            os.remove(file_path)
            # æš‚åœ600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰ï¼Œç„¶åç»§ç»­å¾ªç¯
            time.sleep(600)
# å¯åŠ¨ä¸€ä¸ªæ–°çº¿ç¨‹æ¥åˆ é™¤æ—§æ–‡ä»¶ï¼Œè®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ä»¥ä¾¿ä¸»ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨ç»“æŸ
threading.Thread(target=delete_old_files, daemon=True).start()

# åˆ›å»ºä¸€ä¸ª Gradio åº”ç”¨ç¨‹åºçš„ç•Œé¢
with gr.Blocks() as demo:
    # æ·»åŠ ä¸€ä¸ª Markdown ç»„ä»¶ï¼Œæ˜¾ç¤ºæ ‡é¢˜
    gr.Markdown("""
           <div style="text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;">
               LLM + FLUX + CogVideoX-I2V Space ğŸ¤—
            </div>
    """)
    # åˆ›å»ºä¸€ä¸ªè¡Œå¸ƒå±€ä»¥æ’åˆ—ç»„ä»¶
    with gr.Row():
        # åˆ›å»ºç¬¬ä¸€åˆ—å¸ƒå±€
        with gr.Column():
            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç”¨äºè¾“å…¥æç¤º
            prompt = gr.Textbox(label="Prompt", placeholder="Enter your prompt here", lines=5)
            # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ç”¨äºç”Ÿæˆå­—å¹•
            generate_caption_button = gr.Button("Generate Caption")
            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç”¨äºæ˜¾ç¤ºç”Ÿæˆçš„å­—å¹•
            caption = gr.Textbox(label="Caption", placeholder="Caption will appear here", lines=5)
            # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ç”¨äºç”Ÿæˆå›¾åƒ
            generate_image_button = gr.Button("Generate Image")
            # åˆ›å»ºä¸€ä¸ªå›¾åƒç»„ä»¶ç”¨äºæ˜¾ç¤ºç”Ÿæˆçš„å›¾åƒ
            image_output = gr.Image(label="Generated Image")
            # åˆ›å»ºä¸€ä¸ªçŠ¶æ€ç»„ä»¶ï¼Œç”¨äºä¿å­˜å›¾åƒçŠ¶æ€
            state_image = gr.State()
            # è®¾ç½®ç”Ÿæˆå­—å¹•æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ï¼Œè°ƒç”¨ç”Ÿæˆå­—å¹•å‡½æ•°
            generate_caption_button.click(fn=generate_caption, inputs=prompt, outputs=caption)
            # è®¾ç½®ç”Ÿæˆå›¾åƒæŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ï¼Œè°ƒç”¨ç”Ÿæˆå›¾åƒå‡½æ•°
            generate_image_button.click(fn=generate_image, inputs=caption, outputs=[image_output, state_image])
        # åˆ›å»ºç¬¬äºŒåˆ—å¸ƒå±€
        with gr.Column():
            # åˆ›å»ºä¸€ä¸ªè§†é¢‘ç»„ä»¶ç”¨äºæ˜¾ç¤ºç”Ÿæˆçš„è§†é¢‘
            video_output = gr.Video(label="Generated Video", width=720, height=480)
            # åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ç»„ä»¶ç”¨äºä¸‹è½½è§†é¢‘ï¼Œåˆå§‹è®¾ç½®ä¸ºä¸å¯è§
            download_video_button = gr.File(label="ğŸ“¥ Download Video", visible=False)
            # åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ç»„ä»¶ç”¨äºä¸‹è½½ GIFï¼Œåˆå§‹è®¾ç½®ä¸ºä¸å¯è§
            download_gif_button = gr.File(label="ğŸ“¥ Download GIF", visible=False)
            # åˆ›å»ºä¸€ä¸ªæŒ‰é’®ç”¨äºä»å›¾åƒç”Ÿæˆè§†é¢‘
            generate_video_button = gr.Button("Generate Video from Image")
            # è®¾ç½®ç”Ÿæˆè§†é¢‘æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ï¼Œè°ƒç”¨ç”Ÿæˆè§†é¢‘å‡½æ•°
            generate_video_button.click(fn=generate_video, inputs=[caption, state_image],
                                        outputs=[video_output, download_gif_button])

# å¦‚æœå½“å‰æ¨¡å—æ˜¯ä¸»ç¨‹åºï¼Œåˆ™å¯åŠ¨ Gradio åº”ç”¨ç¨‹åº
if __name__ == "__main__":
    demo.launch()
```

# `.\cogvideo-finetune\tools\llm_flux_cogvideox\llm_flux_cogvideox.py`

```py
# æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæä¾›é¡¹ç›®åŸå§‹å®éªŒä»£ç çš„é“¾æ¥å’Œä½¿ç”¨è¯´æ˜
"""
The original experimental code for this project can be found at:

https://gist.github.com/a-r-r-o-w/d070cce059ab4ceab3a9f289ff83c69c

By using this code, description prompts will be generated through a local large language model, and images will be
generated using the black-forest-labs/FLUX.1-dev model, followed by video generation via CogVideoX.
The entire process utilizes open-source solutions, without the need for any API keys.

You can use the generate.sh file in the same folder to automate running this code
for batch generation of videos and images.

bash generate.sh

"""

# å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£æåº“
import argparse
# å¯¼å…¥åƒåœ¾å›æ”¶åº“
import gc
# å¯¼å…¥JSONå¤„ç†åº“
import json
# å¯¼å…¥æ“ä½œç³»ç»ŸåŠŸèƒ½åº“
import os
# å¯¼å…¥è·¯å¾„æ“ä½œåº“
import pathlib
# å¯¼å…¥éšæœºæ•°ç”Ÿæˆåº“
import random
# å¯¼å…¥ç±»å‹æç¤ºåŠŸèƒ½
from typing import Any, Dict

# ä»transformersåº“å¯¼å…¥è‡ªåŠ¨æ ‡è®°å™¨
from transformers import AutoTokenizer

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ŒæŒ‡å®šTORCH_LOGSçš„æ—¥å¿—å†…å®¹
os.environ["TORCH_LOGS"] = "+dynamo,recompiles,graph_breaks"
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼€å¯TORCHDYNAMOçš„è¯¦ç»†è¾“å‡º
os.environ["TORCHDYNAMO_VERBOSE"] = "1"

# å¯¼å…¥numpyåº“
import numpy as np
# å¯¼å…¥PyTorchåº“
import torch
# å¯¼å…¥transformersåº“
import transformers
# ä»diffusersåº“å¯¼å…¥è§†é¢‘ç”Ÿæˆç›¸å…³çš„ç®¡é“å’Œè°ƒåº¦å™¨
from diffusers import CogVideoXImageToVideoPipeline, CogVideoXDPMScheduler, DiffusionPipeline
# ä»diffusersåº“å¯¼å…¥æ—¥å¿—è®°å½•å·¥å…·
from diffusers.utils.logging import get_logger
# ä»diffusersåº“å¯¼å…¥è§†é¢‘å¯¼å‡ºå·¥å…·
from diffusers.utils import export_to_video

# è®¾ç½®PyTorchçš„æµ®ç‚¹æ•°ä¹˜æ³•ç²¾åº¦ä¸ºé«˜
torch.set_float32_matmul_precision("high")

# è·å–æ—¥å¿—è®°å½•å™¨å®ä¾‹
logger = get_logger(__name__)

# å®šä¹‰ç³»ç»Ÿæç¤ºå­—ç¬¦ä¸²ï¼ŒæŒ‡å¯¼ç”Ÿæˆè§†é¢‘æè¿°çš„ä»»åŠ¡
SYSTEM_PROMPT = """
You are part of a team of people that create videos using generative models. You use a video-generation model that can generate a video about anything you describe.

For example, if you respond with "A beautiful morning in the woods with the sun peaking through the trees", the video generation model will create a video of exactly as described. You task is to summarize the descriptions of videos provided to by users, and create details prompts to feed into the generative model.

There are a few rules to follow:
- You will only ever output a single video description per request.
- If the user mentions to summarize the prompt in [X] words, make sure to not exceed the limit.

You responses should just be the video generation prompt. Here are examples:
- â€œA lone figure stands on a city rooftop at night, gazing up at the full moon. The moon glows brightly, casting a gentle light over the quiet cityscape. Below, the windows of countless homes shine with warm lights, creating a contrast between the bustling life below and the peaceful solitude above. The scene captures the essence of the Mid-Autumn Festival, where despite the distance, the figure feels connected to loved ones through the shared beauty of the moonlit sky.â€
- "A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting."
# åŒ…å«ä¸€ä¸ªæè¿°åœºæ™¯çš„å­—ç¬¦ä¸²ï¼Œæè¿°äº†ä¸€ä½è¡—å¤´è‰ºæœ¯å®¶å’Œä»–çš„åˆ›ä½œ
- "A street artist, clad in a worn-out denim jacket and a colorful banana, stands before a vast concrete wall in the heart, holding a can of spray paint, spray-painting a colorful bird on a mottled wall"
""".strip()

# å®šä¹‰ç”¨æˆ·æç¤ºçš„æ¨¡æ¿ï¼Œè¦æ±‚ç”Ÿæˆè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æç¤ºï¼Œé™åˆ¶å­—æ•°
USER_PROMPT = """
Could you generate a prompt for a video generation model? 
Please limit the prompt to [{0}] words.
""".strip()


# å®šä¹‰ä¸€ä¸ªè·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨å®ä¾‹
    parser = argparse.ArgumentParser()
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè§†é¢‘æ•°é‡ï¼Œç±»å‹ä¸ºæ•´æ•°ï¼Œé»˜è®¤å€¼ä¸º5
    parser.add_argument(
        "--num_videos",
        type=int,
        default=5,
        help="Number of unique videos you would like to generate."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ¨¡å‹è·¯å¾„ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸ºæŒ‡å®šçš„æ¨¡å‹è·¯å¾„
    parser.add_argument(
        "--model_path",
        type=str,
        default="THUDM/CogVideoX-5B",
        help="The path of Image2Video CogVideoX-5B",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ ‡é¢˜ç”Ÿæˆæ¨¡å‹IDï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸ºæŒ‡å®šçš„æ¨¡å‹ID
    parser.add_argument(
        "--caption_generator_model_id",
        type=str,
        default="THUDM/glm-4-9b-chat",
        help="Caption generation model. default GLM-4-9B",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ ‡é¢˜ç”Ÿæˆæ¨¡å‹ç¼“å­˜ç›®å½•ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸ºNone
    parser.add_argument(
        "--caption_generator_cache_dir",
        type=str,
        default=None,
        help="Cache directory for caption generation model."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šå›¾åƒç”Ÿæˆæ¨¡å‹IDï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸ºæŒ‡å®šçš„æ¨¡å‹ID
    parser.add_argument(
        "--image_generator_model_id",
        type=str,
        default="black-forest-labs/FLUX.1-dev",
        help="Image generation model."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šå›¾åƒç”Ÿæˆæ¨¡å‹ç¼“å­˜ç›®å½•ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸ºNone
    parser.add_argument(
        "--image_generator_cache_dir",
        type=str,
        default=None,
        help="Cache directory for image generation model."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šå›¾åƒç”Ÿæˆæ¨ç†æ­¥éª¤æ•°é‡ï¼Œç±»å‹ä¸ºæ•´æ•°ï¼Œé»˜è®¤å€¼ä¸º50
    parser.add_argument(
        "--image_generator_num_inference_steps",
        type=int,
        default=50,
        help="Caption generation model."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šå¼•å¯¼æ¯”ä¾‹ï¼Œç±»å‹ä¸ºæµ®ç‚¹æ•°ï¼Œé»˜è®¤å€¼ä¸º7
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=7,
        help="Guidance scale to be use for generation."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ˜¯å¦ä½¿ç”¨åŠ¨æ€CFGï¼ŒåŠ¨ä½œç±»å‹ä¸ºå¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸ºFalse
    parser.add_argument(
        "--use_dynamic_cfg",
        action="store_true",
        help="Whether or not to use cosine dynamic guidance for generation [Recommended].",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šè¾“å‡ºç›®å½•ï¼Œç±»å‹ä¸ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤å€¼ä¸º"outputs/"
    parser.add_argument(
        "--output_dir",
        type=str,
        default="outputs/",
        help="Location where generated images and videos should be stored.",
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ˜¯å¦ç¼–è¯‘è½¬æ¢å™¨ï¼ŒåŠ¨ä½œç±»å‹ä¸ºå¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸ºFalse
    parser.add_argument(
        "--compile",
        action="store_true",
        help="Whether or not to compile the transformer of image and video generators."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šæ˜¯å¦å¯ç”¨VAEå¹³é“ºï¼ŒåŠ¨ä½œç±»å‹ä¸ºå¸ƒå°”å€¼ï¼Œé»˜è®¤å€¼ä¸ºFalse
    parser.add_argument(
        "--enable_vae_tiling",
        action="store_true",
        help="Whether or not to use VAE tiling when encoding/decoding."
    )
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼šéšæœºç§å­ï¼Œç±»å‹ä¸ºæ•´æ•°ï¼Œé»˜è®¤å€¼ä¸º42
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Seed for reproducibility."
    )
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ç»“æœ
    return parser.parse_args()


# å®šä¹‰ä¸€ä¸ªé‡ç½®å†…å­˜çš„å‡½æ•°
def reset_memory():
    # åƒåœ¾å›æ”¶å™¨æ”¶é›†æ‰€æœ‰æœªä½¿ç”¨çš„å¯¹è±¡
    gc.collect()
    # æ¸…ç©ºCUDAçš„ç¼“å­˜
    torch.cuda.empty_cache()
    # é‡ç½®CUDAçš„å³°å€¼å†…å­˜ç»Ÿè®¡ä¿¡æ¯
    torch.cuda.reset_peak_memory_stats()
    # é‡ç½®CUDAçš„ç´¯ç§¯å†…å­˜ç»Ÿè®¡ä¿¡æ¯
    torch.cuda.reset_accumulated_memory_stats()


# ä½¿ç”¨æ— æ¢¯åº¦è®¡ç®—çš„ä¸Šä¸‹æ–‡å®šä¹‰ä¸»å‡½æ•°
@torch.no_grad()
def main(args: Dict[str, Any]) -> None:
    # å°†è¾“å‡ºç›®å½•è½¬æ¢ä¸ºè·¯å¾„å¯¹è±¡
    output_dir = pathlib.Path(args.output_dir)
    # å¦‚æœè¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºè¯¥ç›®å½•
    os.makedirs(output_dir.as_posix(), exist_ok=True)

    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯é‡ç°
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    # ä¸ºæ‰€æœ‰ GPU è®¾å¤‡è®¾ç½®éšæœºç§å­ï¼Œä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    torch.cuda.manual_seed_all(args.seed)

    # é‡ç½®å†…å­˜ï¼Œä»¥æ¸…ç†ä¹‹å‰çš„è®¡ç®—å›¾å’Œå˜é‡
    reset_memory()
    # ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½åˆ†è¯å™¨ï¼Œå…è®¸ä¿¡ä»»è¿œç¨‹ä»£ç 
    tokenizer = AutoTokenizer.from_pretrained(args.caption_generator_model_id, trust_remote_code=True)
    # åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ï¼Œå¹¶é…ç½®ç›¸å…³å‚æ•°
    caption_generator = transformers.pipeline(
        "text-generation",  # è®¾ç½®ä»»åŠ¡ä¸ºæ–‡æœ¬ç”Ÿæˆ
        model=args.caption_generator_model_id,  # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ID
        device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆCPU/GPUï¼‰
        model_kwargs={  # æ¨¡å‹çš„å…¶ä»–å‚æ•°é…ç½®
            "local_files_only": True,  # ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            "cache_dir": args.caption_generator_cache_dir,  # è®¾ç½®ç¼“å­˜ç›®å½•
            "torch_dtype": torch.bfloat16,  # è®¾ç½®å¼ é‡çš„æ•°æ®ç±»å‹ä¸º bfloat16
        },
        trust_remote_code=True,  # å…è®¸ä¿¡ä»»è¿œç¨‹ä»£ç 
        tokenizer=tokenizer  # ä½¿ç”¨åŠ è½½çš„åˆ†è¯å™¨
    )

    # åˆå§‹åŒ–ç”¨äºå­˜å‚¨ç”Ÿæˆçš„æ ‡é¢˜çš„åˆ—è¡¨
    captions = []
    # éå†æŒ‡å®šæ•°é‡çš„è§†é¢‘
    for i in range(args.num_videos):
        # éšæœºé€‰æ‹©ç”Ÿæˆæ ‡é¢˜çš„å­—æ•°
        num_words = random.choice([50, 75, 100])
        # æ ¼å¼åŒ–ç”¨æˆ·æç¤ºï¼Œä»¥åŒ…å«å­—æ•°ä¿¡æ¯
        user_prompt = USER_PROMPT.format(num_words)

        # åˆ›å»ºåŒ…å«ç³»ç»Ÿå’Œç”¨æˆ·æ¶ˆæ¯çš„åˆ—è¡¨
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},  # ç³»ç»Ÿæ¶ˆæ¯
            {"role": "user", "content": user_prompt},  # ç”¨æˆ·æ¶ˆæ¯
        ]

        # ç”Ÿæˆæ ‡é¢˜ï¼Œé™åˆ¶æ–°ç”Ÿæˆçš„æ ‡è®°æ•°
        outputs = caption_generator(messages, max_new_tokens=226)
        # æå–ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        caption = outputs[0]["generated_text"][-1]["content"]
        # å¦‚æœæ ‡é¢˜ä»¥å¼•å·å¼€å§‹å’Œç»“æŸï¼Œå»é™¤å¼•å·
        if caption.startswith("\"") and caption.endswith("\""):
            caption = caption[1:-1]
        # å°†ç”Ÿæˆçš„æ ‡é¢˜æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        captions.append(caption)
        # è®°å½•ç”Ÿæˆçš„æ ‡é¢˜
        logger.info(f"Generated caption: {caption}")

    # å°†ç”Ÿæˆçš„æ ‡é¢˜ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­
    with open(output_dir / "captions.json", "w") as file:
        json.dump(captions, file)  # å°†æ ‡é¢˜åˆ—è¡¨å†™å…¥ JSON æ–‡ä»¶

    # åˆ é™¤æ ‡é¢˜ç”Ÿæˆå™¨ä»¥é‡Šæ”¾å†…å­˜
    del caption_generator
    # é‡ç½®å†…å­˜
    reset_memory()

    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å›¾åƒç”Ÿæˆå™¨
    image_generator = DiffusionPipeline.from_pretrained(
        args.image_generator_model_id,  # ä½¿ç”¨æŒ‡å®šçš„å›¾åƒç”Ÿæˆæ¨¡å‹ID
        cache_dir=args.image_generator_cache_dir,  # è®¾ç½®ç¼“å­˜ç›®å½•
        torch_dtype=torch.bfloat16  # è®¾ç½®å¼ é‡çš„æ•°æ®ç±»å‹ä¸º bfloat16
    )
    # å°†å›¾åƒç”Ÿæˆå™¨ç§»åŠ¨åˆ° GPU
    image_generator.to("cuda")

    # å¦‚æœç¼–è¯‘é€‰é¡¹è¢«å¯ç”¨ï¼Œåˆ™ç¼–è¯‘å›¾åƒç”Ÿæˆå™¨çš„è½¬æ¢å™¨
    if args.compile:
        image_generator.transformer = torch.compile(image_generator.transformer, mode="max-autotune", fullgraph=True)

    # å¦‚æœå¯ç”¨ VAE ç“¦ç‰‡åŠŸèƒ½ï¼Œåˆ™å…è®¸å›¾åƒç”Ÿæˆå™¨çš„ VAE ä½¿ç”¨ç“¦ç‰‡
    if args.enable_vae_tiling:
        image_generator.vae.enable_tiling()

    # åˆå§‹åŒ–ç”¨äºå­˜å‚¨ç”Ÿæˆçš„å›¾åƒçš„åˆ—è¡¨
    images = []
    # éå†ç”Ÿæˆçš„æ ‡é¢˜å¹¶ç”Ÿæˆå¯¹åº”çš„å›¾åƒ
    for index, caption in enumerate(captions):
        # ä½¿ç”¨å›¾åƒç”Ÿæˆå™¨ç”Ÿæˆå›¾åƒï¼ŒæŒ‡å®šç›¸å…³å‚æ•°
        image = image_generator(
            prompt=caption,  # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæç¤º
            height=480,  # è®¾ç½®ç”Ÿæˆå›¾åƒçš„é«˜åº¦
            width=720,  # è®¾ç½®ç”Ÿæˆå›¾åƒçš„å®½åº¦
            num_inference_steps=args.image_generator_num_inference_steps,  # è®¾ç½®æ¨ç†æ­¥éª¤æ•°é‡
            guidance_scale=3.5,  # è®¾ç½®æŒ‡å¯¼æ¯”ä¾‹
        ).images[0]  # è·å–ç”Ÿæˆçš„å›¾åƒ

        # å¤„ç†æ ‡é¢˜ä»¥åˆ›å»ºåˆæ³•çš„æ–‡ä»¶å
        filename = caption[:25].replace(".", "_").replace("'", "_").replace('"', "_").replace(",", "_")
        # ä¿å­˜ç”Ÿæˆçš„å›¾åƒåˆ°æŒ‡å®šç›®å½•
        image.save(output_dir / f"{index}_{filename}.png")
        # å°†ç”Ÿæˆçš„å›¾åƒæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        images.append(image)

    # åˆ é™¤å›¾åƒç”Ÿæˆå™¨ä»¥é‡Šæ”¾å†…å­˜
    del image_generator
    # é‡ç½®å†…å­˜
    reset_memory()

    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½è§†é¢‘ç”Ÿæˆå™¨
    video_generator = CogVideoXImageToVideoPipeline.from_pretrained(
        args.model_path, torch_dtype=torch.bfloat16).to("cuda")  # ç§»åŠ¨åˆ° GPU

    # è®¾ç½®è§†é¢‘ç”Ÿæˆå™¨çš„è°ƒåº¦å™¨
    video_generator.scheduler = CogVideoXDPMScheduler.from_config(
        video_generator.scheduler.config,  # ä½¿ç”¨å½“å‰è°ƒåº¦å™¨çš„é…ç½®
        timestep_spacing="trailing"  # è®¾ç½®æ—¶é—´æ­¥é—´éš”ä¸º trailing
    )

    # å¦‚æœç¼–è¯‘é€‰é¡¹è¢«å¯ç”¨ï¼Œåˆ™ç¼–è¯‘è§†é¢‘ç”Ÿæˆå™¨çš„è½¬æ¢å™¨
    if args.compile:
        video_generator.transformer = torch.compile(video_generator.transformer, mode="max-autotune", fullgraph=True)

    # å¦‚æœå¯ç”¨ VAE ç“¦ç‰‡åŠŸèƒ½ï¼Œåˆ™å…è®¸è§†é¢‘ç”Ÿæˆå™¨çš„ VAE ä½¿ç”¨ç“¦ç‰‡
    if args.enable_vae_tiling:
        video_generator.vae.enable_tiling()

    # åˆ›å»ºéšæœºæ•°ç”Ÿæˆå™¨å¹¶è®¾ç½®ç§å­
    generator = torch.Generator().manual_seed(args.seed)  # ç¡®ä¿éšæœºç»“æœå¯é‡ç°
    # éå† captions å’Œ images çš„ç»„åˆï¼Œè·å–ç´¢å¼•åŠå¯¹åº”çš„æè¿°å’Œå›¾åƒ
        for index, (caption, image) in enumerate(zip(captions, images)):
            # è°ƒç”¨è§†é¢‘ç”Ÿæˆå™¨ï¼Œç”Ÿæˆè§†é¢‘å¸§
            video = video_generator(
                # è®¾ç½®ç”Ÿæˆè§†é¢‘çš„å›¾åƒå’Œæè¿°
                image=image,
                prompt=caption,
                # æŒ‡å®šè§†é¢‘çš„é«˜åº¦å’Œå®½åº¦
                height=480,
                width=720,
                # è®¾ç½®ç”Ÿæˆçš„å¸§æ•°å’Œæ¨ç†æ­¥éª¤
                num_frames=49,
                num_inference_steps=50,
                # è®¾ç½®å¼•å¯¼æ¯”ä¾‹å’ŒåŠ¨æ€é…ç½®é€‰é¡¹
                guidance_scale=args.guidance_scale,
                use_dynamic_cfg=args.use_dynamic_cfg,
                # æä¾›éšæœºæ•°ç”Ÿæˆå™¨
                generator=generator,
            ).frames[0]  # è·å–ç”Ÿæˆçš„è§†é¢‘çš„ç¬¬ä¸€å¸§
            # æ ¼å¼åŒ–æ–‡ä»¶åï¼Œé™åˆ¶ä¸ºå‰25ä¸ªå­—ç¬¦å¹¶æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
            filename = caption[:25].replace(".", "_").replace("'", "_").replace('"', "_").replace(",", "_")
            # å¯¼å‡ºç”Ÿæˆçš„è§†é¢‘åˆ°æŒ‡å®šç›®å½•ï¼Œå‘½åä¸ºç´¢å¼•åŠ æ–‡ä»¶å
            export_to_video(video, output_dir / f"{index}_{filename}.mp4", fps=8)  # è®¾ç½®æ¯ç§’å¸§æ•°ä¸º8
# åˆ¤æ–­å½“å‰æ¨¡å—æ˜¯å¦æ˜¯ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()
    # è°ƒç”¨ä¸»å‡½æ•°ï¼Œå¹¶ä¼ å…¥è·å–çš„å‚æ•°
    main(args)
```

# `.\cogvideo-finetune\tools\load_cogvideox_lora.py`

```py
# ç‰ˆæƒå£°æ˜ï¼Œè¡¨æ˜ç‰ˆæƒæ‰€æœ‰è€…åŠå…¶ä¿ç•™æƒåˆ©
# Copyright 2024 The HuggingFace Team.
# All rights reserved.
#
# æ ¹æ® Apache License, Version 2.0 è¿›è¡Œæˆæƒ
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# ä½ å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬
#     http://www.apache.org/licenses/LICENSE-2.0
#
# é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œè½¯ä»¶ä»¥â€œåŸæ ·â€åŸºç¡€åˆ†å‘ï¼Œä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯æˆ–æ¡ä»¶
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# æŸ¥çœ‹è®¸å¯è¯ä»¥è·å–æœ‰å…³æƒé™å’Œé™åˆ¶çš„å…·ä½“ä¿¡æ¯
# See the License for the specific language governing permissions and
# limitations under the License.

# å¯¼å…¥æ•°å­¦æ¨¡å—ä»¥è¿›è¡Œæ•°å­¦è®¡ç®—
import math 
# å¯¼å…¥éšæœºæ¨¡å—ä»¥ç”Ÿæˆéšæœºæ•°
import random 
# å¯¼å…¥æ—¶é—´æ¨¡å—ä»¥è¿›è¡Œæ—¶é—´ç›¸å…³æ“ä½œ
import time
# ä» diffusers.utils å¯¼å…¥å¯¼å‡ºè§†é¢‘çš„åŠŸèƒ½
from diffusers.utils import export_to_video
# ä» diffusers.image_processor å¯¼å…¥ VAE å›¾åƒå¤„ç†å™¨
from diffusers.image_processor import VaeImageProcessor
# å¯¼å…¥æ—¥æœŸå’Œæ—¶é—´å¤„ç†çš„æ¨¡å—
from datetime import datetime, timedelta
# ä» diffusers å¯¼å…¥å¤šä¸ªç±»ä»¥ä¾›åç»­ä½¿ç”¨
from diffusers import CogVideoXPipeline, CogVideoXDDIMScheduler, CogVideoXDPMScheduler
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¨¡å—ä»¥è¿›è¡Œç³»ç»Ÿçº§æ“ä½œ
import os
# å¯¼å…¥ PyTorch åº“ä»¥è¿›è¡Œæ·±åº¦å­¦ä¹ 
import torch
# å¯¼å…¥å‚æ•°è§£ææ¨¡å—ä»¥å¤„ç†å‘½ä»¤è¡Œå‚æ•°
import argparse

# æ ¹æ®æ˜¯å¦æœ‰å¯ç”¨çš„ GPU è®¾å®šè®¾å¤‡ç±»å‹
device = "cuda" if torch.cuda.is_available() else "cpu"

# å®šä¹‰è·å–å‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°
def get_args():
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡
    parser = argparse.ArgumentParser()
    # æ·»åŠ é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„å‚æ•°
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        default=None,
        required=True,
        help="Path to pretrained model or model identifier from huggingface.co/models.",
    )
    # æ·»åŠ  LoRA æƒé‡è·¯å¾„å‚æ•°
    parser.add_argument(
        "--lora_weights_path",
        type=str,
        default=None,
        required=True,
        help="Path to lora weights.",
    )
    # æ·»åŠ  LoRA æƒé‡çš„ç§©å‚æ•°
    parser.add_argument(
        "--lora_r",
        type=int,
        default=128,
        help="""LoRA weights have a rank parameter, with the default for 2B trans set at 128 and 5B trans set at 256. 
        This part is used to calculate the value for lora_scale, which is by default divided by the alpha value, 
        used for stable learning and to prevent underflow. In the SAT training framework,
        alpha is set to 1 by default. The higher the rank, the better the expressive capability,
        but it requires more memory and training time. Increasing this number blindly isn't always better.
        The formula for lora_scale is: lora_r / alpha.
        """,
    )
    # æ·»åŠ  LoRA æƒé‡çš„ alpha å‚æ•°
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=1,
        help="""LoRA weights have a rank parameter, with the default for 2B trans set at 128 and 5B trans set at 256. 
        This part is used to calculate the value for lora_scale, which is by default divided by the alpha value, 
        used for stable learning and to prevent underflow. In the SAT training framework,
        alpha is set to 1 by default. The higher the rank, the better the expressive capability,
        but it requires more memory and training time. Increasing this number blindly isn't always better.
        The formula for lora_scale is: lora_r / alpha.
        """,
    )
    # æ·»åŠ ç”¨äºç”Ÿæˆå†…å®¹çš„æç¤ºå‚æ•°
    parser.add_argument(
        "--prompt",
        type=str,
        help="prompt",
    )
    # å‘è§£æå™¨æ·»åŠ ä¸€ä¸ªåä¸º output_dir çš„å‚æ•°
        parser.add_argument(
            "--output_dir",  # å‚æ•°çš„åç§°
            type=str,  # å‚æ•°ç±»å‹ä¸ºå­—ç¬¦ä¸²
            default="output",  # é»˜è®¤å€¼ä¸º "output"
            help="The output directory where the model predictions and checkpoints will be written.",  # å‚æ•°çš„å¸®åŠ©è¯´æ˜
        )
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›ç»“æœ
        return parser.parse_args()
# å¦‚æœè¯¥è„šæœ¬æ˜¯ä¸»ç¨‹åºï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹ä»£ç 
if __name__ == "__main__":
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    args = get_args()
    # ä»é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºè§†é¢‘å¤„ç†ç®¡é“ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    pipe = CogVideoXPipeline.from_pretrained(args.pretrained_model_name_or_path, torch_dtype=torch.bfloat16).to(device)
    # åŠ è½½ LoRA æƒé‡ï¼ŒæŒ‡å®šæƒé‡æ–‡ä»¶åå’Œé€‚é…å™¨åç§°
    pipe.load_lora_weights(args.lora_weights_path,  weight_name="pytorch_lora_weights.safetensors", adapter_name="cogvideox-lora")
    # pipe.fuse_lora(lora_scale=args.lora_alpha/args.lora_r, ['transformer'])  # æ³¨é‡Šæ‰çš„ä»£ç ï¼Œç”¨äºèåˆ LoRA æƒé‡
    # è®¡ç®— LoRA ç¼©æ”¾å› å­
    lora_scaling=args.lora_alpha/args.lora_r
    # è®¾ç½®é€‚é…å™¨åŠå…¶å¯¹åº”çš„ç¼©æ”¾å› å­
    pipe.set_adapters(["cogvideox-lora"], [lora_scaling])

    # æ ¹æ®è°ƒåº¦å™¨é…ç½®åˆ›å»ºè§†é¢‘è°ƒåº¦å™¨
    pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing="trailing")

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
    os.makedirs(args.output_dir, exist_ok=True)

    # ç”Ÿæˆè§†é¢‘å¸§ï¼Œè®¾ç½®ç›¸å…³å‚æ•°
    latents = pipe(
        prompt=args.prompt,
        num_videos_per_prompt=1,
        num_inference_steps=50,
        num_frames=49,
        use_dynamic_cfg=True,
        output_type="pt",
        guidance_scale=3.0,
        generator=torch.Generator(device="cpu").manual_seed(42),
    ).frames
    # è·å–ç”Ÿæˆçš„å¸§çš„æ‰¹é‡å¤§å°
    batch_size = latents.shape[0]
    # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨è§†é¢‘å¸§
    batch_video_frames = []
    # éå†æ¯ä¸€å¸§ï¼Œå¤„ç†å¹¶è½¬æ¢ä¸º PIL å›¾åƒ
    for batch_idx in range(batch_size):
        pt_image = latents[batch_idx]
        # å°†å½“å‰å¸§çš„å„ä¸ªé€šé“å †å æˆä¸€ä¸ªå¼ é‡
        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])

        # å°† PyTorch å›¾åƒè½¬æ¢ä¸º NumPy æ ¼å¼
        image_np = VaeImageProcessor.pt_to_numpy(pt_image)
        # å°† NumPy å›¾åƒè½¬æ¢ä¸º PIL å›¾åƒ
        image_pil = VaeImageProcessor.numpy_to_pil(image_np)
        # å°†å¤„ç†åçš„ PIL å›¾åƒæ·»åŠ åˆ°å¸§åˆ—è¡¨ä¸­
        batch_video_frames.append(image_pil)

    # è·å–å½“å‰æ—¶é—´æˆ³ï¼Œç”¨äºè§†é¢‘æ–‡ä»¶å‘½å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # æ„é€ è§†é¢‘æ–‡ä»¶è·¯å¾„
    video_path = f"{args.output_dir}/{timestamp}.mp4"
    # åˆ›å»ºè§†é¢‘æ–‡ä»¶ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(os.path.dirname(video_path), exist_ok=True)
    # é€‰æ‹©ç¬¬ä¸€ç»„å¸§ä½œä¸ºè§†é¢‘å†…å®¹
    tensor = batch_video_frames[0]
    # è®¡ç®—å¸§ç‡ï¼Œå‡è®¾æ¯ 6 å¸§ä¸º 1 ç§’
    fps=math.ceil((len(batch_video_frames[0]) - 1) / 6)

    # å°†å¤„ç†åçš„å¸§å¯¼å‡ºä¸ºè§†é¢‘æ–‡ä»¶
    export_to_video(tensor, video_path, fps=fps)
```

# `.\cogvideo-finetune\tools\parallel_inference\parallel_inference_xdit.py`

```py
# è¿™æ˜¯ä¸€ä¸ªç”¨äº CogVideo çš„å¹¶è¡Œæ¨ç†è„šæœ¬ï¼ŒåŸå§‹è„šæœ¬æ¥æºäº xDiT é¡¹ç›®
"""
This is a parallel inference script for CogVideo. The original script
can be found from the xDiT project at

https://github.com/xdit-project/xDiT/blob/main/examples/cogvideox_example.py

By using this code, the inference process is parallelized on multiple GPUs,
and thus speeded up.

Usage:
1. pip install xfuser
2. mkdir results
3. run the following command to generate video
torchrun --nproc_per_node=4 parallel_inference_xdit.py \
    --model <cogvideox-model-path> --ulysses_degree 1 --ring_degree 2 \
    --use_cfg_parallel --height 480 --width 720 --num_frames 9 \
    --prompt 'A small dog.'

You can also use the run.sh file in the same folder to automate running this
code for batch generation of videos, by running:

sh ./run.sh

"""

# å¯¼å…¥å¿…è¦çš„åº“
import time
import torch
import torch.distributed
from diffusers import AutoencoderKLTemporalDecoder
from xfuser import xFuserCogVideoXPipeline, xFuserArgs
from xfuser.config import FlexibleArgumentParser
from xfuser.core.distributed import (
    get_world_group,
    get_data_parallel_rank,
    get_data_parallel_world_size,
    get_runtime_state,
    is_dp_last_group,
)
from diffusers.utils import export_to_video

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºå‚æ•°è§£æå™¨å¹¶æè¿°ç”¨é€”
    parser = FlexibleArgumentParser(description="xFuser Arguments")
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°å¹¶è§£æ
    args = xFuserArgs.add_cli_args(parser).parse_args()
    # ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºå¼•æ“é…ç½®
    engine_args = xFuserArgs.from_cli_args(args)

    # æ£€æŸ¥ ulysses_degree æ˜¯å¦æœ‰æ•ˆ
    num_heads = 30
    # å¦‚æœ ulysses_degree å¤§äº 0 ä¸”ä¸æ˜¯ num_heads çš„å› å­ï¼Œåˆ™å¼•å‘é”™è¯¯
    if engine_args.ulysses_degree > 0 and num_heads % engine_args.ulysses_degree != 0:
        raise ValueError(
            f"ulysses_degree ({engine_args.ulysses_degree}) must be a divisor of the number of heads ({num_heads})"
        )

    # åˆ›å»ºå¼•æ“å’Œè¾“å…¥é…ç½®
    engine_config, input_config = engine_args.create_config()
    # è·å–æœ¬åœ°è¿›ç¨‹çš„æ’å
    local_rank = get_world_group().local_rank

    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç®¡é“
    pipe = xFuserCogVideoXPipeline.from_pretrained(
        pretrained_model_name_or_path=engine_config.model_config.model,
        engine_config=engine_config,
        torch_dtype=torch.bfloat16,
    )
    # å¦‚æœå¯ç”¨ CPU ç¦»çº¿ï¼Œè¿›è¡Œç›¸åº”è®¾ç½®
    if args.enable_sequential_cpu_offload:
        pipe.enable_model_cpu_offload(gpu_id=local_rank)
        pipe.vae.enable_tiling()
    else:
        # å°†ç®¡é“ç§»åŠ¨åˆ°æŒ‡å®šçš„ GPU è®¾å¤‡
        device = torch.device(f"cuda:{local_rank}")
        pipe = pipe.to(device)

    # é‡ç½® GPU çš„å³°å€¼å†…å­˜ç»Ÿè®¡
    torch.cuda.reset_peak_memory_stats()
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # æ‰§è¡Œæ¨ç†ï¼Œç”Ÿæˆè§†é¢‘å¸§
    output = pipe(
        height=input_config.height,
        width=input_config.width,
        num_frames=input_config.num_frames,
        prompt=input_config.prompt,
        num_inference_steps=input_config.num_inference_steps,
        generator=torch.Generator(device="cuda").manual_seed(input_config.seed),
        guidance_scale=6,
    ).frames[0]

    # è®°å½•ç»“æŸæ—¶é—´
    end_time = time.time()
    # è®¡ç®—æ¨ç†è€—æ—¶
    elapsed_time = end_time - start_time
    # è·å–å½“å‰è®¾å¤‡çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡
    peak_memory = torch.cuda.max_memory_allocated(device=f"cuda:{local_rank}")
    # æ„å»ºåŒ…å«å„ç§å¹¶è¡Œé…ç½®å‚æ•°çš„å­—ç¬¦ä¸²ï¼Œç”¨äºè¾“å‡ºæ–‡ä»¶å
        parallel_info = (
            f"dp{engine_args.data_parallel_degree}_cfg{engine_config.parallel_config.cfg_degree}_"
            f"ulysses{engine_args.ulysses_degree}_ring{engine_args.ring_degree}_"
            f"tp{engine_args.tensor_parallel_degree}_"
            f"pp{engine_args.pipefusion_parallel_degree}_patch{engine_args.num_pipeline_patch}"
        )
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°æ®å¹¶è¡Œçš„æœ€åä¸€ç»„
        if is_dp_last_group():
            # è·å–æ•°æ®å¹¶è¡Œçš„å…¨å±€å¤§å°
            world_size = get_data_parallel_world_size()
            # æ ¹æ®è¾“å…¥é…ç½®æ„å»ºåˆ†è¾¨ç‡å­—ç¬¦ä¸²
            resolution = f"{input_config.width}x{input_config.height}"
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ŒåŒ…å«å¹¶è¡Œä¿¡æ¯å’Œåˆ†è¾¨ç‡
            output_filename = f"results/cogvideox_{parallel_info}_{resolution}.mp4"
            # å°†è¾“å‡ºå†…å®¹å¯¼å‡ºä¸ºè§†é¢‘æ–‡ä»¶
            export_to_video(output, output_filename, fps=8)
            # æ‰“å°ä¿å­˜çš„è¾“å‡ºæ–‡ä»¶å
            print(f"output saved to {output_filename}")
    
        # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªè¿›ç¨‹
        if get_world_group().rank == get_world_group().world_size - 1:
            # æ‰“å°å½“å‰å‘¨æœŸçš„è€—æ—¶å’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
            print(f"epoch time: {elapsed_time:.2f} sec, memory: {peak_memory/1e9} GB")
        # é”€æ¯åˆ†å¸ƒå¼ç¯å¢ƒçš„è¿è¡Œæ—¶çŠ¶æ€
        get_runtime_state().destory_distributed_env()
# åˆ¤æ–­å½“å‰è„šæœ¬æ˜¯å¦ä¸ºä¸»ç¨‹åº
if __name__ == "__main__":
    # è°ƒç”¨ä¸»å‡½æ•°
    main()
```

# `.\cogvideo-finetune\tools\replicate\predict_i2v.py`

```py
# Cog çš„é¢„æµ‹æ¥å£ âš™ï¸
# https://cog.run/python

# å¯¼å…¥å¿…è¦çš„åº“
import os  # ç”¨äºæ“ä½œç³»ç»ŸåŠŸèƒ½
import subprocess  # ç”¨äºæ‰§è¡Œå­è¿›ç¨‹å‘½ä»¤
import time  # ç”¨äºæ—¶é—´ç›¸å…³æ“ä½œ
import torch  # ç”¨äºæ·±åº¦å­¦ä¹ åº“
from diffusers import CogVideoXImageToVideoPipeline  # å¯¼å…¥è§†é¢‘ç”Ÿæˆç®¡é“
from diffusers.utils import export_to_video, load_image  # å¯¼å…¥å·¥å…·å‡½æ•°
from cog import BasePredictor, Input, Path  # å¯¼å…¥ Cog çš„åŸºç¡€é¢„æµ‹å™¨å’Œè¾“å…¥å¤„ç†

# å®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•
MODEL_CACHE = "model_cache_i2v"
# å®šä¹‰æ¨¡å‹ä¸‹è½½çš„ URL
MODEL_URL = (
    f"https://weights.replicate.delivery/default/THUDM/CogVideo/{MODEL_CACHE}.tar"
)
# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¦»çº¿æ¨¡å¼è¿è¡Œ
os.environ["HF_DATASETS_OFFLINE"] = "1"  # ç¦ç”¨æ•°æ®é›†åœ¨çº¿ä¸‹è½½
os.environ["TRANSFORMERS_OFFLINE"] = "1"  # ç¦ç”¨å˜æ¢å™¨åœ¨çº¿ä¸‹è½½
os.environ["HF_HOME"] = MODEL_CACHE  # è®¾ç½® Hugging Face çš„ç¼“å­˜ç›®å½•
os.environ["TORCH_HOME"] = MODEL_CACHE  # è®¾ç½® PyTorch çš„ç¼“å­˜ç›®å½•
os.environ["HF_DATASETS_CACHE"] = MODEL_CACHE  # è®¾ç½®æ•°æ®é›†ç¼“å­˜ç›®å½•
os.environ["TRANSFORMERS_CACHE"] = MODEL_CACHE  # è®¾ç½®å˜æ¢å™¨ç¼“å­˜ç›®å½•
os.environ["HUGGINGFACE_HUB_CACHE"] = MODEL_CACHE  # è®¾ç½® Hugging Face Hub ç¼“å­˜ç›®å½•

# å®šä¹‰ä¸‹è½½æ¨¡å‹æƒé‡çš„å‡½æ•°
def download_weights(url, dest):
    start = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    print("downloading url: ", url)  # è¾“å‡ºä¸‹è½½ URL
    print("downloading to: ", dest)  # è¾“å‡ºä¸‹è½½ç›®æ ‡è·¯å¾„
    subprocess.check_call(["pget", "-x", url, dest], close_fds=False)  # è°ƒç”¨ pget ä¸‹è½½æ¨¡å‹æƒé‡
    print("downloading took: ", time.time() - start)  # è¾“å‡ºä¸‹è½½æ‰€ç”¨æ—¶é—´

# å®šä¹‰é¢„æµ‹ç±»
class Predictor(BasePredictor):
    def setup(self) -> None:
        """å°†æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ä»¥æé«˜å¤šä¸ªé¢„æµ‹çš„æ•ˆç‡"""

        # å¦‚æœæ¨¡å‹ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™ä¸‹è½½æ¨¡å‹æƒé‡
        if not os.path.exists(MODEL_CACHE):
            download_weights(MODEL_URL, MODEL_CACHE)

        # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–ç®¡é“
        # model_id: THUDM/CogVideoX-5b-I2V
        self.pipe = CogVideoXImageToVideoPipeline.from_pretrained(
            MODEL_CACHE, torch_dtype=torch.bfloat16  # ä½¿ç”¨ bfloat16 æ•°æ®ç±»å‹
        ).to("cuda")  # å°†æ¨¡å‹è½¬ç§»åˆ° GPU

        self.pipe.enable_model_cpu_offload()  # å¯ç”¨æ¨¡å‹ CPU ç¦»çº¿å¤„ç†
        self.pipe.vae.enable_tiling()  # å¯ç”¨ VAE çš„å¹³é“ºå¤„ç†

    def predict(
        self,
        prompt: str = Input(
            description="Input prompt", default="Starry sky slowly rotating."
        ),  # è¾“å…¥æç¤ºçš„é»˜è®¤å€¼
        image: Path = Input(description="Input image"),  # è¾“å…¥å›¾åƒè·¯å¾„
        num_inference_steps: int = Input(
            description="Number of denoising steps", ge=1, le=500, default=50
        ),  # å»å™ªæ­¥éª¤æ•°é‡çš„è¾“å…¥
        guidance_scale: float = Input(
            description="Scale for classifier-free guidance", ge=1, le=20, default=6
        ),  # åˆ†ç±»æ— å…³å¼•å¯¼çš„æ¯”ä¾‹è¾“å…¥
        num_frames: int = Input(
            description="Number of frames for the output video", default=49
        ),  # è¾“å‡ºè§†é¢‘çš„å¸§æ•°è¾“å…¥
        seed: int = Input(
            description="Random seed. Leave blank to randomize the seed", default=None
        ),  # éšæœºç§å­çš„è¾“å…¥
    ) -> Path:
        """å¯¹æ¨¡å‹è¿›è¡Œå•æ¬¡é¢„æµ‹"""

        # å¦‚æœæ²¡æœ‰æä¾›ç§å­ï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªéšæœºç§å­
        if seed is None:
            seed = int.from_bytes(os.urandom(2), "big")  # ç”Ÿæˆéšæœºç§å­
        print(f"Using seed: {seed}")  # è¾“å‡ºä½¿ç”¨çš„ç§å­

        img = load_image(image=str(image))  # åŠ è½½è¾“å…¥å›¾åƒ

        # è°ƒç”¨ç®¡é“è¿›è¡Œè§†é¢‘ç”Ÿæˆ
        video = self.pipe(
            prompt=prompt,  # è¾“å…¥æç¤º
            image=img,  # è¾“å…¥å›¾åƒ
            num_videos_per_prompt=1,  # æ¯ä¸ªæç¤ºç”Ÿæˆä¸€ä¸ªè§†é¢‘
            num_inference_steps=num_inference_steps,  # å»å™ªæ­¥éª¤æ•°é‡
            num_frames=num_frames,  # è¾“å‡ºè§†é¢‘å¸§æ•°
            guidance_scale=guidance_scale,  # åˆ†ç±»æ— å…³å¼•å¯¼æ¯”ä¾‹
            generator=torch.Generator(device="cuda").manual_seed(seed),  # éšæœºæ•°ç”Ÿæˆå™¨
        ).frames[0]  # è·å–ç”Ÿæˆçš„è§†é¢‘å¸§

        out_path = "/tmp/out.mp4"  # è®¾ç½®è¾“å‡ºè§†é¢‘çš„è·¯å¾„

        export_to_video(video, out_path, fps=8)  # å¯¼å‡ºè§†é¢‘åˆ°æŒ‡å®šè·¯å¾„
        return Path(out_path)  # è¿”å›è¾“å‡ºè§†é¢‘è·¯å¾„
```

# `.\cogvideo-finetune\tools\replicate\predict_t2v.py`

```py
# Cogçš„é¢„æµ‹æ¥å£ âš™ï¸
# https://cog.run/python

# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import os  # ç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’
import subprocess  # ç”¨äºæ‰§è¡Œå­è¿›ç¨‹
import time  # ç”¨äºæ—¶é—´ç®¡ç†
import torch  # æ·±åº¦å­¦ä¹ æ¡†æ¶
from diffusers import CogVideoXPipeline  # å¯¼å…¥CogVideoXPipelineç±»
from diffusers.utils import export_to_video  # å¯¼å…¥è§†é¢‘å¯¼å‡ºå·¥å…·
from cog import BasePredictor, Input, Path  # å¯¼å…¥Cognitionæ¡†æ¶çš„åŸºç¡€ç±»å’Œè¾“å…¥ç±»

MODEL_CACHE = "model_cache"  # å®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•
MODEL_URL = (  # å®šä¹‰æ¨¡å‹æƒé‡ä¸‹è½½URL
    f"https://weights.replicate.delivery/default/THUDM/CogVideo/{MODEL_CACHE}.tar"
)
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼ä»¥é¿å…ä¸‹è½½æ¨¡å‹
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HOME"] = MODEL_CACHE  # è®¾ç½®Hugging Faceçš„æ ¹ç›®å½•
os.environ["TORCH_HOME"] = MODEL_CACHE  # è®¾ç½®PyTorchçš„æ ¹ç›®å½•
os.environ["HF_DATASETS_CACHE"] = MODEL_CACHE  # è®¾ç½®æ•°æ®é›†ç¼“å­˜ç›®å½•
os.environ["TRANSFORMERS_CACHE"] = MODEL_CACHE  # è®¾ç½®å˜æ¢å™¨ç¼“å­˜ç›®å½•
os.environ["HUGGINGFACE_HUB_CACHE"] = MODEL_CACHE  # è®¾ç½®Hugging Face Hubç¼“å­˜ç›®å½•

# å®šä¹‰ä¸‹è½½æƒé‡çš„å‡½æ•°
def download_weights(url, dest):
    start = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    print("downloading url: ", url)  # è¾“å‡ºä¸‹è½½URL
    print("downloading to: ", dest)  # è¾“å‡ºä¸‹è½½ç›®æ ‡è·¯å¾„
    # ä½¿ç”¨å­è¿›ç¨‹å‘½ä»¤ä¸‹è½½æ–‡ä»¶
    subprocess.check_call(["pget", "-x", url, dest], close_fds=False)
    print("downloading took: ", time.time() - start)  # è¾“å‡ºä¸‹è½½æ‰€éœ€æ—¶é—´

# å®šä¹‰é¢„æµ‹å™¨ç±»ï¼Œç»§æ‰¿è‡ªBasePredictor
class Predictor(BasePredictor):
    def setup(self) -> None:
        """å°†æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œä»¥æé«˜å¤šæ¬¡é¢„æµ‹çš„æ•ˆç‡"""

        # æ£€æŸ¥æ¨¡å‹ç¼“å­˜ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(MODEL_CACHE):
            download_weights(MODEL_URL, MODEL_CACHE)  # å¦‚æœä¸å­˜åœ¨ï¼Œä¸‹è½½æ¨¡å‹æƒé‡

        # åŠ è½½æŒ‡å®šçš„æ¨¡å‹
        # model_id: THUDM/CogVideoX-5b
        self.pipe = CogVideoXPipeline.from_pretrained(
            MODEL_CACHE,  # ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹
            torch_dtype=torch.bfloat16,  # è®¾ç½®æ¨¡å‹çš„æ•°æ®ç±»å‹
        ).to("cuda")  # å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU

        self.pipe.enable_model_cpu_offload()  # å¯ç”¨CPUå¸è½½ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        self.pipe.vae.enable_tiling()  # å¯ç”¨VAEçš„åˆ†å—å¤„ç†

    # å®šä¹‰é¢„æµ‹æ–¹æ³•
    def predict(
        self,
        prompt: str = Input(  # è¾“å…¥æç¤ºï¼Œæè¿°ç”Ÿæˆå†…å®¹
            description="Input prompt",
            default="A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.",
        ),
        num_inference_steps: int = Input(  # è¾“å…¥å»å™ªæ­¥éª¤æ•°é‡
            description="Number of denoising steps", ge=1, le=500, default=50
        ),
        guidance_scale: float = Input(  # è¾“å…¥æ— åˆ†ç±»æŒ‡å¯¼çš„æ¯”ä¾‹
            description="Scale for classifier-free guidance", ge=1, le=20, default=6
        ),
        num_frames: int = Input(  # è¾“å…¥è¾“å‡ºè§†é¢‘çš„å¸§æ•°
            description="Number of frames for the output video", default=49
        ),
        seed: int = Input(  # è¾“å…¥éšæœºç§å­ï¼Œç•™ç©ºä»¥éšæœºåŒ–
            description="Random seed. Leave blank to randomize the seed", default=None
        ),
    # é¢„æµ‹æ¨¡å‹çš„å•æ¬¡è¿è¡Œï¼Œè¿”å›ç”Ÿæˆçš„è§†é¢‘è·¯å¾„
    ) -> Path:
        # æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜å‡½æ•°çš„åŠŸèƒ½
        """Run a single prediction on the model"""
    
        # å¦‚æœæ²¡æœ‰æä¾›ç§å­ï¼Œåˆ™éšæœºç”Ÿæˆä¸€ä¸ªç§å­
        if seed is None:
            seed = int.from_bytes(os.urandom(2), "big")
        # è¾“å‡ºå½“å‰ä½¿ç”¨çš„ç§å­
        print(f"Using seed: {seed}")
    
        # è°ƒç”¨æ¨¡å‹ç®¡é“ç”Ÿæˆè§†é¢‘ï¼Œä½¿ç”¨æä¾›çš„å‚æ•°
        video = self.pipe(
            # ä¼ å…¥çš„æç¤ºæ–‡æœ¬
            prompt=prompt,
            # æ¯ä¸ªæç¤ºç”Ÿæˆä¸€ä¸ªè§†é¢‘
            num_videos_per_prompt=1,
            # æ¨ç†æ­¥éª¤çš„æ•°é‡
            num_inference_steps=num_inference_steps,
            # è§†é¢‘å¸§æ•°
            num_frames=num_frames,
            # æŒ‡å¯¼æ¯”ä¾‹
            guidance_scale=guidance_scale,
            # è®¾å®šéšæœºæ•°ç”Ÿæˆå™¨ï¼Œä½¿ç”¨æŒ‡å®šçš„ç§å­
            generator=torch.Generator(device="cuda").manual_seed(seed),
        ).frames[0]  # å–å¾—ç”Ÿæˆçš„è§†é¢‘çš„ç¬¬ä¸€å¸§
    
        # è®¾ç½®è§†é¢‘è¾“å‡ºè·¯å¾„
        out_path = "/tmp/out.mp4"
    
        # å°†ç”Ÿæˆçš„è§†é¢‘å¯¼å‡ºä¸º MP4 æ–‡ä»¶ï¼Œå¸§ç‡ä¸º8
        export_to_video(video, out_path, fps=8)
        # è¿”å›è¾“å‡ºè·¯å¾„çš„ Path å¯¹è±¡
        return Path(out_path)
```

# Enhance CogVideoX Generated Videos with VEnhancer

This tutorial will guide you through using the VEnhancer tool to enhance videos generated by CogVideoX, including
achieving higher frame rates and higher resolutions.

## Model Introduction

VEnhancer implements spatial super-resolution, temporal super-resolution (frame interpolation), and video refinement in
a unified framework. It can flexibly adapt to different upsampling factors (e.g., 1x~8x) for spatial or temporal
super-resolution. Additionally, it provides flexible control to modify the refinement strength, enabling it to handle
diverse video artifacts.

VEnhancer follows the design of ControlNet, copying the architecture and weights of the multi-frame encoder and middle
block from a pre-trained video diffusion model to build a trainable conditional network. This video ControlNet accepts
low-resolution keyframes and noisy full-frame latents as inputs. In addition to the time step t and prompt, our proposed
video-aware conditioning also includes noise augmentation level Ïƒ and downscaling factor s as additional network
conditioning inputs.

## Hardware Requirements

+ Operating System: Linux (requires xformers dependency)
+ Hardware: NVIDIA GPU with at least 60GB of VRAM per card. Machines such as H100, A100 are recommended.

## Quick Start

1. Clone the repository and install dependencies as per the official instructions:

```py
git clone https://github.com/Vchitect/VEnhancer.git
cd VEnhancer
## Torch and other dependencies can use those from CogVideoX. If you need to create a new environment, use the following commands:
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2

## Install required dependencies
pip install -r requirements.txt
```

Where:

- `input_path` is the path to the input video
- `prompt` is the description of the video content. The prompt used by this tool should be shorter, not exceeding 77
  words. You may need to simplify the prompt used for generating the CogVideoX video.
- `target_fps` is the target frame rate for the video. Typically, 16 fps is already smooth, with 24 fps as the default
  value.
- `up_scale` is recommend to be set to 2,3,4. The target resolution is limited to be around 2k and below.
- `noise_aug` value depends on the input video quality. Lower quality needs higher noise levels, which corresponds to
  stronger refinement. 250~300 is for very low-quality videos. good videos: <= 200.
- `steps`  if you want fewer steps, please change solver_mode to "normal" first, then decline the number of steps. "
  fast" solver_mode has fixed steps (15).
  The code will automatically download the required models from Hugging Face during execution.

Typical runtime logs are as follows:

```py
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
2024-08-20 13:25:17,553 - video_to_video - INFO - checkpoint_path: ./ckpts/venhancer_paper.pt
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/open_clip/factory.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=map_location)
2024-08-20 13:25:37,486 - video_to_video - INFO - Build encoder with FrozenOpenCLIPEmbedder
/share/home/zyx/Code/VEnhancer/video_to_video/video_to_video_model.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  load_dict = torch.load(cfg.model_path, map_location='cpu')
2024-08-20 13:25:55,391 - video_to_video - INFO - Load model path ./ckpts/venhancer_paper.pt, with local status <All keys matched successfully>
2024-08-20 13:25:55,392 - video_to_video - INFO - Build diffusion with GaussianDiffusion
2024-08-20 13:26:16,092 - video_to_video - INFO - input video path: inputs/000000.mp4
2024-08-20 13:26:16,093 - video_to_video - INFO - text: Wide-angle aerial shot at dawn,soft morning light casting long shadows,an elderly man walking his dog through a quiet,foggy park,trees and benches in the background,peaceful and serene atmosphere
2024-08-20 13:26:16,156 - video_to_video - INFO - input frames length: 49
2024-08-20 13:26:16,156 - video_to_video - INFO - input fps: 8.0
2024-08-20 13:26:16,156 - video_to_video - INFO - target_fps: 24.0
2024-08-20 13:26:16,311 - video_to_video - INFO - input resolution: (480, 720)
2024-08-20 13:26:16,312 - video_to_video - INFO - target resolution: (1320, 1982)
2024-08-20 13:26:16,312 - video_to_video - INFO - noise augmentation: 250
2024-08-20 13:26:16,312 - video_to_video - INFO - scale s is set to: 8
2024-08-20 13:26:16,399 - video_to_video - INFO - video_data shape: torch.Size([145, 3, 1320, 1982])
/share/home/zyx/Code/VEnhancer/video_to_video/video_to_video_model.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(enabled=True):
2024-08-20 13:27:19,605 - video_to_video - INFO - step: 0
2024-08-20 13:30:12,020 - video_to_video - INFO - step: 1
2024-08-20 13:33:04,956 - video_to_video - INFO - step: 2
2024-08-20 13:35:58,691 - video_to_video - INFO - step: 3
2024-08-20 13:38:51,254 - video_to_video - INFO - step: 4
2024-08-20 13:41:44,150 - video_to_video - INFO - step: 5
2024-08-20 13:44:37,017 - video_to_video - INFO - step: 6
2024-08-20 13:47:30,037 - video_to_video - INFO - step: 7
2024-08-20 13:50:22,838 - video_to_video - INFO - step: 8
2024-08-20 13:53:15,844 - video_to_video - INFO - step: 9
2024-08-20 13:56:08,657 - video_to_video - INFO - step: 10
2024-08-20 13:59:01,648 - video_to_video - INFO - step: 11
2024-08-20 14:01:54,541 - video_to_video - INFO - step: 12
2024-08-20 14:04:47,488 - video_to_video - INFO - step: 13
2024-08-20 14:10:13,637 - video_to_video - INFO - sampling, finished.

```

Running on a single A100 GPU, enhancing each 6-second CogVideoX generated video with default settings will consume 60GB
of VRAM and take 40-50 minutes.


# VEnhancer ã§ CogVideoX ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªã‚’å¼·åŒ–ã™ã‚‹

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€VEnhancer ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€CogVideoX ã§ç”Ÿæˆã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªã‚’å¼·åŒ–ã—ã€ã‚ˆã‚Šé«˜ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã¨é«˜ã„è§£åƒåº¦ã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ãƒ¢ãƒ‡ãƒ«ã®ç´¹ä»‹

VEnhancer ã¯ã€ç©ºé–“è¶…è§£åƒã€æ™‚é–“è¶…è§£åƒï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ è£œé–“ï¼‰ã€ãŠã‚ˆã³ãƒ“ãƒ‡ã‚ªã®ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‚’çµ±ä¸€ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§å®Ÿç¾ã—ã¾ã™ã€‚ç©ºé–“ã¾ãŸã¯æ™‚é–“ã®è¶…è§£åƒã®ãŸã‚ã«ã€ã•ã¾ã–ã¾ãªã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆä¾‹ï¼š1xã€œ8xï¼‰ã«æŸ”è»Ÿã«å¯¾å¿œã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€å¤šæ§˜ãªãƒ“ãƒ‡ã‚ªã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«ã€ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆå¼·åº¦ã‚’å¤‰æ›´ã™ã‚‹æŸ”è»Ÿãªåˆ¶å¾¡ã‚’æä¾›ã—ã¾ã™ã€‚

VEnhancer ã¯ ControlNet ã®è¨­è¨ˆã«å¾“ã„ã€äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ«ãƒãƒ•ãƒ¬ãƒ¼ãƒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ãƒŸãƒ‰ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã‚¦ã‚§ã‚¤ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªæ¡ä»¶ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã®ãƒ“ãƒ‡ã‚ª ControlNet ã¯ã€ä½è§£åƒåº¦ã®ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ãƒã‚¤ã‚ºã‚’å«ã‚€å®Œå…¨ãªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— t ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŠ ãˆã¦ã€ææ¡ˆã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªå¯¾å¿œæ¡ä»¶ã«ã‚ˆã‚Šã€ãƒã‚¤ã‚ºå¢—å¹…ãƒ¬ãƒ™ãƒ« Ïƒ ãŠã‚ˆã³ãƒ€ã‚¦ãƒ³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ s ãŒè¿½åŠ ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¡ä»¶ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

## ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

+ ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ : Linux (xformers ä¾å­˜é–¢ä¿‚ãŒå¿…è¦)
+ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢: å˜ä¸€ã‚«ãƒ¼ãƒ‰ã‚ãŸã‚Šå°‘ãªãã¨ã‚‚ 60GB ã® VRAM ã‚’æŒã¤ NVIDIA GPUã€‚H100ã€A100 ãªã©ã®ãƒã‚·ãƒ³ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

1. å…¬å¼ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã€ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```py
git clone https://github.com/Vchitect/VEnhancer.git
cd VEnhancer
## Torch ãªã©ã®ä¾å­˜é–¢ä¿‚ã¯ CogVideoX ã®ä¾å­˜é–¢ä¿‚ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚æ–°ã—ã„ç’°å¢ƒã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2

## å¿…é ˆã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚
pip install -r requirements.txt
```

2. ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```py
python enhance_a_video.py --up_scale 4 --target_fps 24 --noise_aug 250 --solver_mode 'fast' --steps 15 --input_path inputs/000000.mp4 --prompt 'Wide-angle aerial shot at dawn, soft morning light casting long shadows, an elderly man walking his dog through a quiet, foggy park, trees and benches in the background, peaceful and serene atmosphere' --save_dir 'results/'
```

æ¬¡ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š

- `input_path` æ˜¯è¾“å…¥è§†é¢‘çš„è·¯å¾„
- `prompt` æ˜¯è§†é¢‘å†…å®¹çš„æè¿°ã€‚æ­¤å·¥å…·ä½¿ç”¨çš„æç¤ºè¯åº”æ›´çŸ­ï¼Œä¸è¶…è¿‡77ä¸ªå­—ã€‚æ‚¨å¯èƒ½éœ€è¦ç®€åŒ–ç”¨äºç”ŸæˆCogVideoXè§†é¢‘çš„æç¤ºè¯ã€‚
- `target_fps` æ˜¯è§†é¢‘çš„ç›®æ ‡å¸§ç‡ã€‚é€šå¸¸ï¼Œ16 fpså·²ç»å¾ˆæµç•…ï¼Œé»˜è®¤å€¼ä¸º24 fpsã€‚
- `up_scale` æ¨èè®¾ç½®ä¸º2ã€3æˆ–4ã€‚ç›®æ ‡åˆ†è¾¨ç‡é™åˆ¶åœ¨2kå·¦å³åŠä»¥ä¸‹ã€‚
- `noise_aug` çš„å€¼å–å†³äºè¾“å…¥è§†é¢‘çš„è´¨é‡ã€‚è´¨é‡è¾ƒä½çš„è§†é¢‘éœ€è¦æ›´é«˜çš„å™ªå£°çº§åˆ«ï¼Œè¿™å¯¹åº”äºæ›´å¼ºçš„ä¼˜åŒ–ã€‚250~300é€‚ç”¨äºéå¸¸ä½è´¨é‡çš„è§†é¢‘ã€‚å¯¹äºé«˜è´¨é‡è§†é¢‘ï¼Œè®¾ç½®ä¸ºâ‰¤200ã€‚
- `steps` å¦‚æœæƒ³å‡å°‘æ­¥æ•°ï¼Œè¯·å…ˆå°†solver_modeæ”¹ä¸ºâ€œnormalâ€ï¼Œç„¶åå‡å°‘æ­¥æ•°ã€‚â€œfastâ€æ¨¡å¼çš„æ­¥æ•°æ˜¯å›ºå®šçš„ï¼ˆ15æ­¥ï¼‰ã€‚
  ä»£ç åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä»Hugging Faceä¸‹è½½æ‰€éœ€çš„æ¨¡å‹ã€‚

ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œä¸­ã«ã€å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã¯ Hugging Face ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚

```py
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
2024-08-20 13:25:17,553 - video_to_video - INFO - checkpoint_path: ./ckpts/venhancer_paper.pt
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/open_clip/factory.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=map_location)
2024-08-20 13:25:37,486 - video_to_video - INFO - Build encoder with FrozenOpenCLIPEmbedder
/share/home/zyx/Code/VEnhancer/video_to_video/video_to_video_model.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  load_dict = torch.load(cfg.model_path, map_location='cpu')
2024-08-20 13:25:55,391 - video_to_video - INFO - Load model path ./ckpts/venhancer_paper.pt, with local status <All keys matched successfully>
2024-08-20 13:25:55,392 - video_to_video - INFO - Build diffusion with GaussianDiffusion
2024-08-20 13:26:16,092 - video_to_video - INFO - input video path: inputs/000000.mp4
2024-08-20 13:26:16,093 - video_to_video - INFO - text: Wide-angle aerial shot at dawn,soft morning light casting long shadows,an elderly man walking his dog through a quiet,foggy park,trees and benches in the background,peaceful and serene atmosphere
2024-08-20 13:26:16,156 - video_to_video - INFO - input frames length: 49
2024-08-20 13:26:16,156 - video_to_video - INFO - input fps: 8.0
2024-08-20 13:26:16,156 - video_to_video - INFO - target_fps: 24.0
2024-08-20 13:26:16,311 - video_to_video - INFO - input resolution: (480, 720)
2024-08-20 13:26:16,312 - video_to_video - INFO - target resolution: (1320, 1982)
2024-08-20 13:26:16,312 - video_to_video - INFO - noise augmentation: 250
2024-08-20 13:26:16,312 - video_to_video - INFO - scale s is set to: 8
2024-08-20 13:26:16,399 - video_to_video - INFO - video_data shape: torch.Size([145, 3, 1320, 1982])
/share/home/zyx/Code/VEnhancer/video_to_video/video_to_video_model.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(enabled=True):
2024-08-20 13:27:19,605 - video_to_video - INFO - step: 0
2024-08-20 13:30:12,020 - video_to_video - INFO - step: 1
2024-08-20 13:33:04,956 - video_to_video - INFO - step: 2
2024-08-20 13:35:58,691 - video_to_video - INFO - step: 3
2024-08-20 13:38:51,254 - video_to_video - INFO - step: 4
2024-08-20 13:41:44,150 - video_to_video - INFO - step: 5
2024-08-20 13:44:37,017 - video_to_video - INFO - step: 6
2024-08-20 13:47:30,037 - video_to_video - INFO - step: 7
2024-08-20 13:50:22,838 - video_to_video - INFO - step: 8
2024-08-20 13:53:15,844 - video_to_video - INFO - step: 9
2024-08-20 13:56:08,657 - video_to_video - INFO - step: 10
2024-08-20 13:59:01,648 - video_to_video - INFO - step: 11
2024-08-20 14:01:54,541 - video_to_video - INFO - step: 12
2024-08-20 14:04:47,488 - video_to_video - INFO - step: 13
2024-08-20 14:10:13,637 - video_to_video - INFO - sampling, finished.

```

A100 GPU ã‚’å˜ä¸€ã§ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€CogVideoX ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸ 6 ç§’é–“ã®ãƒ“ãƒ‡ã‚ªã‚’å¼·åŒ–ã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ 60GB ã® VRAM ã‚’æ¶ˆè²»ã—ã€40ã€œ50 åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚


# ä½¿ç”¨ VEnhancer å¯¹ CogVdieoX ç”Ÿæˆè§†é¢‘è¿›è¡Œå¢å¼º

æœ¬æ•™ç¨‹å°†è¦ä½¿ç”¨ VEnhancer å·¥å…· å¯¹ CogVdieoX ç”Ÿæˆè§†é¢‘è¿›è¡Œå¢å¼º, åŒ…æ‹¬æ›´é«˜çš„å¸§ç‡å’Œæ›´é«˜çš„åˆ†è¾¨ç‡

## æ¨¡å‹ä»‹ç»

VEnhancer åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­å®ç°äº†ç©ºé—´è¶…åˆ†è¾¨ç‡ã€æ—¶é—´è¶…åˆ†è¾¨ç‡ï¼ˆå¸§æ’å€¼ï¼‰å’Œè§†é¢‘ä¼˜åŒ–ã€‚å®ƒå¯ä»¥çµæ´»åœ°é€‚åº”ä¸åŒçš„ä¸Šé‡‡æ ·å› å­ï¼ˆä¾‹å¦‚ï¼Œ1x~
8xï¼‰ç”¨äºç©ºé—´æˆ–æ—¶é—´è¶…åˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼Œå®ƒæä¾›äº†çµæ´»çš„æ§åˆ¶ï¼Œä»¥ä¿®æ”¹ä¼˜åŒ–å¼ºåº¦ï¼Œä»è€Œå¤„ç†å¤šæ ·åŒ–çš„è§†é¢‘ä¼ªå½±ã€‚

VEnhancer éµå¾ª ControlNet çš„è®¾è®¡ï¼Œå¤åˆ¶äº†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¤šå¸§ç¼–ç å™¨å’Œä¸­é—´å—çš„æ¶æ„å’Œæƒé‡ï¼Œæ„å»ºäº†ä¸€ä¸ªå¯è®­ç»ƒçš„æ¡ä»¶ç½‘ç»œã€‚è¿™ä¸ªè§†é¢‘
ControlNet æ¥å—ä½åˆ†è¾¨ç‡å…³é”®å¸§å’ŒåŒ…å«å™ªå£°çš„å®Œæ•´å¸§ä½œä¸ºè¾“å…¥ã€‚æ­¤å¤–ï¼Œé™¤äº†æ—¶é—´æ­¥ t å’Œæç¤ºè¯å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„è§†é¢‘æ„ŸçŸ¥æ¡ä»¶è¿˜å°†å™ªå£°å¢å¼ºçš„å™ªå£°çº§åˆ«
Ïƒ å’Œé™å°ºåº¦å› å­ s ä½œä¸ºé™„åŠ çš„ç½‘ç»œæ¡ä»¶è¾“å…¥ã€‚

## ç¡¬ä»¶éœ€æ±‚

+ æ“ä½œç³»ç»Ÿ: Linux (éœ€è¦ä¾èµ–xformers)
+ ç¡¬ä»¶: NVIDIA GPU å¹¶è‡³å°‘ä¿è¯å•å¡æ˜¾å­˜è¶…è¿‡60Gï¼Œæ¨èä½¿ç”¨ H100ï¼ŒA100ç­‰æœºå™¨ã€‚

## å¿«é€Ÿä¸Šæ‰‹

1. æŒ‰ç…§å®˜æ–¹æŒ‡å¼•å…‹éš†ä»“åº“å¹¶å®‰è£…ä¾èµ–

```py
git clone https://github.com/Vchitect/VEnhancer.git
cd VEnhancer
## torchç­‰ä¾èµ–å¯ä»¥ä½¿ç”¨CogVideoXçš„ä¾èµ–ï¼Œå¦‚æœä½ éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„ç¯å¢ƒï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2

## å®‰è£…å¿…é¡»çš„ä¾èµ–
pip install -r requirements.txt
```

2. è¿è¡Œä»£ç 

```py
python enhance_a_video.py \
--up_scale 4 --target_fps 24 --noise_aug 250 \
--solver_mode 'fast' --steps 15 \
--input_path inputs/000000.mp4 \
--prompt 'Wide-angle aerial shot at dawn,soft morning light casting long shadows,an elderly man walking his dog through a quiet,foggy park,trees and benches in the background,peaceful and serene atmosphere' \
--save_dir 'results/' 
```

å…¶ä¸­:

- `input_path` æ˜¯è¾“å…¥è§†é¢‘çš„è·¯å¾„
- `prompt` æ˜¯è§†é¢‘å†…å®¹çš„æè¿°ã€‚æ­¤å·¥å…·ä½¿ç”¨çš„æç¤ºè¯åº”æ›´çŸ­ï¼Œä¸è¶…è¿‡77ä¸ªå­—ã€‚æ‚¨å¯èƒ½éœ€è¦ç®€åŒ–ç”¨äºç”ŸæˆCogVideoXè§†é¢‘çš„æç¤ºè¯ã€‚
- `target_fps` æ˜¯è§†é¢‘çš„ç›®æ ‡å¸§ç‡ã€‚é€šå¸¸ï¼Œ16 fpså·²ç»å¾ˆæµç•…ï¼Œé»˜è®¤å€¼ä¸º24 fpsã€‚
- `up_scale` æ¨èè®¾ç½®ä¸º2ã€3æˆ–4ã€‚ç›®æ ‡åˆ†è¾¨ç‡é™åˆ¶åœ¨2kå·¦å³åŠä»¥ä¸‹ã€‚
- `noise_aug` çš„å€¼å–å†³äºè¾“å…¥è§†é¢‘çš„è´¨é‡ã€‚è´¨é‡è¾ƒä½çš„è§†é¢‘éœ€è¦æ›´é«˜çš„å™ªå£°çº§åˆ«ï¼Œè¿™å¯¹åº”äºæ›´å¼ºçš„ä¼˜åŒ–ã€‚250~300é€‚ç”¨äºéå¸¸ä½è´¨é‡çš„è§†é¢‘ã€‚å¯¹äºé«˜è´¨é‡è§†é¢‘ï¼Œè®¾ç½®ä¸ºâ‰¤200ã€‚
- `steps` å¦‚æœæƒ³å‡å°‘æ­¥æ•°ï¼Œè¯·å…ˆå°†solver_modeæ”¹ä¸ºâ€œnormalâ€ï¼Œç„¶åå‡å°‘æ­¥æ•°ã€‚â€œfastâ€æ¨¡å¼çš„æ­¥æ•°æ˜¯å›ºå®šçš„ï¼ˆ15æ­¥ï¼‰ã€‚
  ä»£ç åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä»Hugging Faceä¸‹è½½æ‰€éœ€çš„æ¨¡å‹ã€‚

ä»£ç è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œä¼šè‡ªåŠ¨ä»Huggingfaceæ‹‰å–éœ€è¦çš„æ¨¡å‹

è¿è¡Œæ—¥å¿—é€šå¸¸å¦‚ä¸‹:

```py
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
2024-08-20 13:25:17,553 - video_to_video - INFO - checkpoint_path: ./ckpts/venhancer_paper.pt
/share/home/zyx/.conda/envs/cogvideox/lib/python3.10/site-packages/open_clip/factory.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=map_location)
2024-08-20 13:25:37,486 - video_to_video - INFO - Build encoder with FrozenOpenCLIPEmbedder
/share/home/zyx/Code/VEnhancer/video_to_video/video_to_video_model.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  load_dict = torch.load(cfg.model_path, map_location='cpu')
2024-08-20 13:25:55,391 - video_to_video - INFO - Load model path ./ckpts/venhancer_paper.pt, with local status <All keys matched successfully>
2024-08-20 13:25:55,392 - video_to_video - INFO - Build diffusion with GaussianDiffusion
2024-08-20 13:26:16,092 - video_to_video - INFO - input video path: inputs/000000.mp4
2024-08-20 13:26:16,093 - video_to_video - INFO - text: Wide-angle aerial shot at dawn,soft morning light casting long shadows,an elderly man walking his dog through a quiet,foggy park,trees and benches in the background,peaceful and serene atmosphere
2024-08-20 13:26:16,156 - video_to_video - INFO - input frames length: 49
2024-08-20 13:26:16,156 - video_to_video - INFO - input fps: 8.0
2024-08-20 13:26:16,156 - video_to_video - INFO - target_fps: 24.0
2024-08-20 13:26:16,311 - video_to_video - INFO - input resolution: (480, 720)
2024-08-20 13:26:16,312 - video_to_video - INFO - target resolution: (1320, 1982)
2024-08-20 13:26:16,312 - video_to_video - INFO - noise augmentation: 250
2024-08-20 13:26:16,312 - video_to_video - INFO - scale s is set to: 8
2024-08-20 13:26:16,399 - video_to_video - INFO - video_data shape: torch.Size([145, 3, 1320, 1982])
/share/home/zyx/Code/VEnhancer/video_to_video/video_to_video_model.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(enabled=True):
2024-08-20 13:27:19,605 - video_to_video - INFO - step: 0
2024-08-20 13:30:12,020 - video_to_video - INFO - step: 1
2024-08-20 13:33:04,956 - video_to_video - INFO - step: 2
2024-08-20 13:35:58,691 - video_to_video - INFO - step: 3
2024-08-20 13:38:51,254 - video_to_video - INFO - step: 4
2024-08-20 13:41:44,150 - video_to_video - INFO - step: 5
2024-08-20 13:44:37,017 - video_to_video - INFO - step: 6
2024-08-20 13:47:30,037 - video_to_video - INFO - step: 7
2024-08-20 13:50:22,838 - video_to_video - INFO - step: 8
2024-08-20 13:53:15,844 - video_to_video - INFO - step: 9
2024-08-20 13:56:08,657 - video_to_video - INFO - step: 10
2024-08-20 13:59:01,648 - video_to_video - INFO - step: 11
2024-08-20 14:01:54,541 - video_to_video - INFO - step: 12
2024-08-20 14:04:47,488 - video_to_video - INFO - step: 13
2024-08-20 14:10:13,637 - video_to_video - INFO - sampling, finished.

```

ä½¿ç”¨A100å•å¡è¿è¡Œï¼Œå¯¹äºæ¯ä¸ªCogVideoXç”Ÿäº§çš„6ç§’è§†é¢‘ï¼ŒæŒ‰ç…§é»˜è®¤é…ç½®ï¼Œä¼šæ¶ˆè€—60Gæ˜¾å­˜ï¼Œå¹¶ç”¨æ—¶40-50åˆ†é’Ÿã€‚